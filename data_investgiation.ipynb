{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"password_trainv3.csv\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    char_ststistics = {}\n",
    "    for i, row in enumerate(reader):\n",
    "        if(i == 0): continue\n",
    "        for c in row[1]:\n",
    "            if c in char_ststistics: char_ststistics[c] += 1\n",
    "            else: char_ststistics[c] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 66\n",
      "\" 72\n",
      "# 114\n",
      "$ 99\n",
      "% 66\n",
      "& 63\n",
      "' 84\n",
      "( 54\n",
      ") 66\n",
      "* 96\n",
      "+ 54\n",
      ", 72\n",
      "- 84\n",
      ". 81\n",
      "/ 57\n",
      "0 114\n",
      "1 120\n",
      "2 114\n",
      "3 105\n",
      "4 138\n",
      "5 111\n",
      "6 114\n",
      "7 99\n",
      "8 90\n",
      "9 105\n",
      ": 57\n",
      "; 69\n",
      "< 75\n",
      "= 84\n",
      "> 72\n",
      "? 66\n",
      "@ 60\n",
      "A 54\n",
      "B 78\n",
      "C 84\n",
      "D 75\n",
      "E 84\n",
      "F 69\n",
      "G 84\n",
      "H 54\n",
      "I 87\n",
      "J 78\n",
      "K 39\n",
      "L 72\n",
      "M 69\n",
      "N 78\n",
      "O 75\n",
      "P 75\n",
      "Q 66\n",
      "R 81\n",
      "S 81\n",
      "T 99\n",
      "U 42\n",
      "V 69\n",
      "W 96\n",
      "X 75\n",
      "Y 69\n",
      "Z 72\n",
      "[ 60\n",
      "\\ 63\n",
      "] 87\n",
      "^ 84\n",
      "_ 54\n",
      "` 81\n",
      "a 93\n",
      "b 69\n",
      "c 63\n",
      "d 96\n",
      "e 60\n",
      "f 72\n",
      "g 87\n",
      "h 87\n",
      "i 81\n",
      "j 66\n",
      "k 66\n",
      "l 45\n",
      "m 111\n",
      "n 114\n",
      "o 63\n",
      "p 72\n",
      "q 69\n",
      "r 69\n",
      "s 48\n",
      "t 69\n",
      "u 60\n",
      "v 63\n",
      "w 81\n",
      "x 60\n",
      "y 66\n",
      "z 63\n",
      "{ 57\n",
      "| 63\n",
      "} 81\n",
      "~ 60\n",
      "min value 39\n",
      "max value 138\n",
      "94\n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "alphapet = ''\n",
    "for c in sorted(char_ststistics):\n",
    "    print(c, char_ststistics[c])\n",
    "    values.append(char_ststistics[c])\n",
    "    alphapet += c\n",
    "\n",
    "print(f'min value {min(values)}\\nmax value {max(values)}')\n",
    "print(len(alphapet))\n",
    "print (alphapet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, sampler, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Passwords_data(Dataset):\n",
    "    def __init__ (self, csv_Path, imgs_path, transformers = None):\n",
    "        with open(csv_Path, 'r') as csv_file:\n",
    "            # images names and labels in matching indices\n",
    "            reader = csv.reader(csv_file)\n",
    "            self.imgs = []\n",
    "            self.lables = []\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0: continue\n",
    "                self.imgs.append(row[0])\n",
    "                self.lables.append(row[1])\n",
    "                \n",
    "            self.imgs_file = imgs_path\n",
    "            self.transformers = transformers\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # image\n",
    "        img_path = self.imgs_file + '/' + self.imgs[index]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        #print(img.size)\n",
    "        \n",
    "        # image augmentation\n",
    "        if self.transformers != None:\n",
    "            img = self.transformers(img)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # lable\n",
    "        lable = self.lables[index]\n",
    "        \n",
    "        return (img_path, img, lable)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeNormalize (object):\n",
    "    def __init__(self, img_size):\n",
    "        self.img_size = img_size # imgH, imgW\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img = transforms.Resize(size=self.img_size)(transforms.ToPILImage()(img))\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img.sub_(0.5).div_(0.5) # normalize gray scale\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignBatch(object):\n",
    "    def __init__(self, imgH = 32, imgW = 100, keep_ratio = True, min_ratio = 1, padding = False):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio = keep_ratio\n",
    "        self.min_ratio = min_ratio\n",
    "        self.padding = padding\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        img_paths, imgs, lables = zip(*batch)\n",
    "        imgH = self.imgH\n",
    "        imgW = self.imgW\n",
    "        \n",
    "        if(self.keep_ratio):\n",
    "            max_ratio = 0\n",
    "            for img in imgs:\n",
    "                #print(img.shape)\n",
    "                _, h, w = img.shape\n",
    "                if max_ratio < (w/h): max_ratio = (w/h)\n",
    "            if self.padding:\n",
    "                imgs_padded = []\n",
    "                for i, img in enumerate(imgs):\n",
    "                    _, h, w = img.shape\n",
    "                    w_new = h * max_ratio\n",
    "                    pad = int((w_new - w) / 2)\n",
    "                    #print(img.shape)\n",
    "                    imgs_padded.append(F.pad(img, (pad, pad), \"constant\"))\n",
    "                imgs = imgs_padded\n",
    "            \n",
    "            imgW = int(max_ratio * imgH)\n",
    "            \n",
    "        resizer = ResizeNormalize((imgH, imgW))\n",
    "        imgs = [resizer(img).unsqueeze(0) for img in imgs]\n",
    "        imgs = torch.cat(imgs, 0)\n",
    "        return img_paths, imgs, lables\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingSampler(sampler.Sampler):\n",
    "    \n",
    "    def __init__ (self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.number_sampels = len(data_source)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__ (self):\n",
    "        n_batches = len(self)//self.batch_size\n",
    "        tail = len(self) % self.batch_size\n",
    "        index = torch.LongTensor(len(self)).fill_(0)\n",
    "        for i in range(n_batches):\n",
    "            random_start = random.randint(0, len(self)-self.batch_size)\n",
    "            b_indx = random_start + torch.arange(0, self.batch_size)\n",
    "            index[i*self.batch_size : (i+1) * self.batch_size] = b_indx\n",
    "        if tail:\n",
    "            random_start = random.randint(0, len(self)-tail)\n",
    "            b_indx = random_start + torch.arange(0, tail-1)\n",
    "            index[n_batches * self.batch_size:] = b_indx\n",
    "            \n",
    "        return iter(index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.number_sampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<6S>z9X=j|\n",
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABRCAYAAADLnv0YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO19W2xd13nmt87hOYc8vIg3UaJI0aIo0ZIsS7It25Icx7FSx5HTRpO0SdMEmQxQIH1ogRbow2SmL/PYl+kAA3QKZNCinUExbYrEiZ3GiWLF8UWNfJFkXUmLEknxfifF27mfPQ+H/69vL65NKbFLmcn+AEHnHO6911r/Wutf/30bz/MQIkSIECE2HiL3uwMhQoQIEeJXQ8jAQ4QIEWKDImTgIUKECLFBETLwECFChNigCBl4iBAhQmxQhAw8RIgQITYoPhQDN8Z81hjzgTHmhjHmWx9Vp0KECBEixN1hftU4cGNMFMB1AM8BGALwLoA/8Dzv2kfXvRAhQoQIEYQPI4E/AeCG53m9nudlAfwTgJMfTbdChAgRIsTdUPYh7m0BMEjfhwA8udYNiUTCq6ys/KUaMcY4f4tEIvpZrikWiygWi2CtwnU/I+jau913t76uR4brr9LHtfr1YcfP90o7nuf52iwWi/p70Ge5Xn4L6r9rLGv9/V7mZy36uP7GY13r+g9DT7u9tX63/3f1iefml8W93nMv47WvCdrrru+e593T9fz7vazvta4Lom0QTdZaG0H3utav53lYWlqa8jxvs33/h2HgLgqs6q0x5psAvgkAFRUVePrpp+V37aAwY7vjkUgE0WgUcm9raysAoLy8XDd3JpNBoVAAAMzNzWFxcVG/O/qiz3UxGP6d+xWNRn1MJhKJ+D7b1wGrmY89RgC+fkrfZLy8SOQ3+3tZ2Z3p4/bsz9KnfD5/TwdWNBr1LVbXYo9EIr72gTvzxuMqFovI5/MAgGw2i1wuB6A0b/J7Op1GKpXSa+R3vtfzPBQKBd/fpB2eQ7tt+V4sFnUOeC7kXqFZoVDwHSZ8rev38vJy39+z2aw+x6Z10DqTz/Z8sJDC4D7Zn2OxGIDS2pDP0WhU6cZrIJfLKX349yDmaI+L+8j38OeysjLfvgt6Lq8n+zqeN94LQdfIZ57zWCwGY4y2EY1G9VncXjQa9e1noSHfG4/Hfe0Wi0V9Ft9fLBaRyWSUvgKbafN+5XmS3/P5PE6fPn3LSTfXj/eIIQDb6XsrgBH7Is/zvu153mHP8w7zwEOECBEixIfDh5HA3wWw2xjTDmAYwFcAfHWtGzzPUwnFVq0FfKqXl5cjkUgAAB544AHs3bsXAFBVVaX3Dg4OYnx8XJ9ZKBR8UpdL4mQJWu7jv8tv9gnPUrcLfPry81yStqsNY4yewLaEIIdfVVUV6uvrAQCNjY0qIRQKBczNzQEoaSLz8/MAgOXlZW2fJVgXWOJj6Vr6GIvFfNK/XFNVVYV8Po/FxUV9DkvdLqmiUCiodJLL5ZwSsI21tAeWmlhyYanQpXXZEji3w9ImS3YsgfOc53I5X7+E7kIzaYMlQ5bebMmcx+cye7gkVdacZH5isZh+5vng9cZtiClS2mP6Mu25PZuGTJ+g6+3Pck9ZWdkqmtnP4s9MF5bkWXuU/eRqw6abrQ3LNfydJfna2lo8/vjjAIBkMql9GRgYwAcffACgtA9Ze+R+89pnLUR4ZRC/AT4EA/c8L2+M+RMAPwEQBfB3nuddvcs9SKfTvu/yP0+yMCtmNtXV1WhsbAQAdHZ26jUdHR04e/YsAGB+fh4LCwtOQgHBDJW/82fbnOFi9LyQCoWCfk4mk9rHbDbrVKXsZ/HYWX3bvHkztmzZAgBoampCU1MTAGD79u2qwheLRczOzgIAuru7lc5TU1MYHR0FUGLsbJZwjc0eUzQaRV1dHQCgsrISFRUVAIDm5mbt06ZNm5DNZtHb2wsAmJ2d1UM1nU5jYWEBQInBiQmFD1eeJ6YH/y60ZcbHa8Z1DyPIrCYHBh9YQYye+8hz7jp8mIHKc3i88rmsrMyn/jMjcR2oiUTCx9zYPAbAZ0IR4YfHVlZW5jtI2ITCh67tk5B2mAna5hBmai7w7zxPQgOXX6usrMxpsuHPfC+vBfvwYObOAogxxtlnZtousxtQovdTTz2F5557Tr8Lent7dU8ODAw4Dxz74GP68DwF4cNI4PA870cAfvRhnhEiRIgQIX41fCgG/suiWCyqBLaWl1qkArkWKJ2mIm02NjaqdFFbW6uSw+3btzEzM4Pl5WUAqx0zLhU8yKPsOhltKVUgJ3lFRQUaGhoAlCTlmpoaACWJ9Pr16wCApaUl38nP0kU8Htd76uvrsX17ycWwc+dO1NbWarsy9oqKCv2cyWSUPgcPHlSpOZfLoaurCwDwzjvvoLe3V7UBz/N8WgZLFeIw3rdvH/bs2QOgJIXMzMwAKDlytm3bBqAkFcozAWBxcREPPvgggJImMjg4qO0PDQ0B8JsnWHJlyQiAz9HJmkM+n3dK3qxh2PPvQiQS8Znd1nK6uRyPmUzGKSHZ97HGEI1GfaoyO9pEgmMNrr6+Hrt27QIAbN26FdXV1QBKdB4ZKbmdBgYGMD4+7qOj3B+Px7WNRCKhbTzwwANIJpMASnMgWlNPT4+aw0R7tE0q0ndevxJhVlVVpes1Ho9jenpaaSV7Op1Or9JCgkwFLI0LbFOJy+Rna5IsUbNZyTYRBa0V1q4EZWVlaG9vR1VV1arrGxoafNoSm3ZZI3TtA9u0EoR1ZeCe5/lsbqwOBW006fzCwoISLpFI6L3xeBw7duwAADz88MO4fv262n8BONUQXoQ8ybaayzZPRiQSUWYZj8fR2dkJoMRohQGLmQMomTEmJiYAlMwYHFnBtrvKykq0tbUBKDHOnTt3AihtCFn4k5OTygTLysrUpMET3tnZiU2bNulzZcPX1dXhe9/7nppX+IDkDb9161Z85jOfAQDs2bNHNzmr2ZOTkzqmSCSCdDqtm76qqkoZeCKRUPrU1tbizTffBADcvHkTt2/fBuBfoBUVFTqfNTU1+szJyUmMjIzo90Qi4Yw8YTowfVkttw9ze7O4TDNCI3mWgO3sdtiYyx4t7bEvgQ/khx9+GABw5MgRvT4ajaK5uRkAsHfvXmXA8/Pzejh3dXUpPQFgeHhYBRnbX7Fv3z4AwBe/+EVdr8ViET09PQCAf/mXf8Hly5cBlPYdrxPeO8YY7UssFlMT59NPP43Pfe5zes/bb78NAPj5z3+uhwRHwMjhJt9terkYu21uknvXimCJxWIq2Ozbt08PnEKhoGv51q1bvv0psO370l4sFkMymXQKpDwONkuxwMCCDAsobEJZ028V+JcQIUKECPGxxrpK4AB8J6UrdtlWoUWKmJiYwNLSEgC/NGWMUfVl165d2LRpEyYnJwGUVDaX2suf14pftaUpduCJhN3a2opPfepTAEoSo5gSlpeXNSpkeXnZJ8W4JPB4PI6WlhacOHFCnyuYnZ1VM0RPTw9u3bqlNJP+svklFoupVNfY2KgS9NatW1FTU6Ptc/RHPB7H1q1bAQDPP/88Dh8+vIoOMzMz6OvrA1CSoJk+2WxW6dPZ2enTlkRbOXLkiEqew8PDSh+Wmjo7O/H5z39e6SkO0OvXr6O/v1+9+rOzsyqNs2Oc1wbTh38X2gOl9Wirz6ydcSQJS80u5xbHgdvaHF/L6n9FRYU6g5999ll84QtfAFBaY7L2u7q6fKYudjZKRNLhw4exdetWpcmlS5dw/vx5AH5zU01NDR566CEAQEtLi0qhZWVlqrXdvHkTY2Nj2kaQ85klzO3bt+NrX/saAOD48eO6P3K5nGoG3d3dqh1LP5nubFYS2I5IhkvqteecNfWqqir81m/9FgDgc5/7nK7LbDaLS5cuAQC+853vKP/wPM9n8uVoI54Pux/8XdaDrXEIDbPZrC+fQVAoFHw0D8K6M3BWbxkuNYLDaiRJR+BiumLncqkqdqQD26bkfrYpM8Q+zeqtLPx4PI7Nm0sJUpFIRA+Z3t5eDA8PAwBu3LihiyKXyzlt6TU1NThw4AB2794NwG9XvnjxIs6dOwegpOLJxmbTQSwWU1vj0tKS0urAgQO68AYGBjAxMaGLKpvNqtmktrYWzzzzDADgkUce8SUwyJjeffddvPvuuwBKDFieIxCaZLNZ/XzgwAFVW2tqavRgSiaTvgUqzF8OGaAUfSOfW1paUCwW0d/fDwD48Y9/rPZfNhek02kfo3StN3vt2ZuQQ82EPpwkw36IeDzuEziE7q5kKmZ88qzGxkb8/u//PgDgy1/+sjJRz/P08BoeHtZIImb+hUJBr9+xYwe2bdumc8L+pqGhIU2UKhaLypz7+vqwf/9+HavYrU+cOKHPeemll3yHHO/PRCKhZpPjx4/jd37ndwCUbL9Cz1wup5EYy8vLPl8J09uOJHH5qIJs5LZJzJVcFIlEUFVVpaappqYm30Evh2gsFtP7mX/ws+LxuDOCytUvES44hNcOp3WZSNYymzBCE0qIECFCbFCsexSKLbUJWOqR0ycSiagUsbS05Iwhd8HltTXGBHrS2Wwi0iKrtlu3bkVdXR0eeOABANBIE6AkcYl5I5PJaLTJ+fPnVdKZnp72Sc0uB9a2bduwa9cu/c7S49DQkErXZWVlGv2xbds2leQikYhKOiMjI/jFL34BoCSxi3R769YtjI6O+qQgaaOpqQkHDx4EgFVOGZHAe3p61Am1tLS0KlpIJEZ2aNbW1qoUzclG8l3mQ/oxOjqK999/X8cnZp3du3fDGKPmIE5THhgYwFtvvaVj54QiNmm4PPyiGrPEJ6p1MplUCbWtrU2lcdYM2bHreZ5PAubY6Xw+r46y4eFhXe8PPfQQPvnJTwIoOZnl9+npaXX+vfXWW+q4ZmdYeXm5mirS6TSSyaQ6vh999FG0tLQAAH7yk5+oiSCdTuO9994DAKRSKTXBtLa26lpqaWnRuOahoSFcuHABU1NTOl4Ze21tLR577DEAwAsvvKB7JxKJ6BycO3dOtbbR0VHdwyxx20k5HJfP+5zbZtOe7Vjl4Ah2BMZiMZ1D21wh0VX5fN63rnluZd+yxL9WeQrP85zOVV4nHNjB/crn89rexyYKBQiu2cHqkSv6g9UWlwos/7NdjlVbO8OPDwxR99va2tSEsXnzZt0cFRUVqKysVOa6uLioC/qDDz7QyZ+ZmdGNNjk5qYwvSB1idVjs6i6VccuWLTrhW7du1ZCy5uZmn11O+nTmzBk9MKamptSUMzc3h3Q67VT7ksmkMkd7sbGdXmiVy+V0fAJ57tzcHG7evAmgtIG5Psg777yj/eL5lGd1d3frIbFlyxa110ajUdTU1Ohi3r9/v96/a9cupePVq1d1vFNTU/rcVCqlzMOlbgt4PRw9ehR/9Ed/BKC0NoTB2RFKPA7ebJx0lM1m1X7/z//8z2pSO3jwoB7I3Jfx8XG88cYbAPwRJsxIIpGI2pTFRi8RJg0NDTqOlpYWZeCZTEYP2nw+ryaQT33qU2hvb1caiKnr+PHjmJmZUYacz+d1H+zbtw9f/vKXAZSiY6Rfy8vLan//4Q9/iO7ubgClqBlXJqTsWde6tOfHlRHKTNuuzyPI5/NIpVK4cOGC0kHGMTExoetyfn7eF2bLYLNJULIgg9cJRyjZSWxB0VQclROE0IQSIkSIEBsU91UCd/3GpxAnVQB3HERLS0uqltvPMcb4kltYkmSVTRyP+/fvV9MB117ZtGmTnuapVArT09NqohgfH8eNGzcAlNR3kY5SqZRKm9w2Oy041ZadYR0dHT7nD6vyjz76qKq6xhifmYcdWoJnnnlGxzo7O4uf/vSnAErOUDbh8Cm/tLSkKn55eblPkpG2P//5z+PKlSsASlKhSJRSX0MkF/agnzt3Th2P2WxWI2gmJyd9zlShTzqdVql5cnJSJdX+/n60traq+aqjo0MdeBUVFRo1097erin9t27dUml8cHBQJc9UKuVTje3Udhl7e3u7xl/X1dX55o6xloTEdJY2a2trVZU/dOiQRlHxc27fvu0rR8D5DNxXofng4CAefPBBXU/RaFRNZ/X19drG7du3fTkFr7/+uo5PpPGqqiqV3g8fPoypqSldJ0NDQxqn//Wvf10d+4lEQq/p6urCT37yEwCl+Zd9E4vFtE/JZFLXYXl5uS8pyI4ec2mDtkbNzuYg6Zbj5rds2aK04jolvCeYZ8h36Q+b49aqVSIISsW3E3w4YsmupePCujNwF9ZSk9gmJJ74Gzdu4MCBAwBWm0l4A8ZiMZ+tUwjR1taGo0ePAgCee+45ZVCe52kb6XRa1dOxsTFf+N7k5KQeJnNzc7oheFEFZZZxoojnebqxOGFG+svRMfJ5cHBQTTYcscGTz4uzqalJowtaW1vxox/9SDcUL/D+/n6cOXMGQGkDywFnh4oJrfbt26fmIklOYpqI/Z/DBZeXl5WmzLS57wA0YiKdTuvhODw8jLq6OjVr3bx5U1X+5uZmHW9lZSWOHz+uzx0YGAAAvPrqq9qn+fl5nedMJuML5YpE7hTiGh4e1jo7HR0dynzsDcXRKTKftbW1vvlPp9N66E9NTamtetu2bc4NmsvlfKYnNsPx9TJ/6XQay8vLznoidXV1Op8LCws6H5lMRg+Jt99+W8dx9OhRXTN1dXX47d/+bfUH/exnP1O79/79+5UmhUJBfUGvv/46rl27pnMgc5NMJnVujh496jPZ8d7homB2qKqYwbLZrDPiBrjDzPv6+nDx4kV9ZjKZxCc+8QkApYNJxpvL5fTzjRs3lD6cZGPb4jmKbS0zCke0uExta9UBcplWVj0/8C8hQoQIEeJjjXWXwFkNCfq7y+kXiURUFX/33Xc1IqS+vt53UjU1Nen9AwMDejKz02T//v146qmnAJQcPOztlRNzfn5eJbahoSF0d3dr+8vLyz4Viiu6CdiTbldeE4kkkUjoOJqamgJrb0xNTWl0y7Vr11SKTSaTqupWVlaqM+zIkSMavcEvwjhy5Ah6enrUocWmhOnpaXXwdHR0+J7LUp1I/FVVVVqrhTUQoY8k/Fy/fl0dmn19fXqtHY3kivtl09P8/DzS6bRKjH19fWoqaWtr03IB27ZtUxU/FospDU+ePKljGhoawksvvaT9W1xc9I1BpLxz586pY7ihocGXNs6RS/LcpqYmX1KXoFAooLu7G6dOnQJQMsHJvC8vLztTtnO5nEqSXDKBaWRLcqypAXck9dbWVnR0dAAoOdpdL50Q7QAo7SmJvqmqqsKmTZvwxBNPAChpYbKeuP5Hf3+/ju+9995TzUmiP4DSXpMkrQcffFDHZ0vbazkPbTrZ10ibQGn+WEusrq5WOrC2G4vFdL/YzlWO3XbxLNaoXXCVxbXB4+bPa90juG+1UAD4vOk8SWw7FqTTaU3cuHz5sqpxXBcln89jz549qv7FYjHdjEtLS2pT5reo2LY2sbHevn17VfKH62BZK5xRrueSoWzXZzul/bILz/M0kuTtt9/W0K+hoSH9ndXTaDSqG6tQKGhSTktLiy/sq62tTRkfL8pUKqXmhtOnT6u9+NChQ2o2YRuwXYSJUVlZqXb6/fv3q239nXfe0XC/7u5uX73joMXKZghWs+fm5tTUMjQ0pAx89+7dOr91dXW6xg4ePOjL2JVx//CHP0RXV5eOlzEzM6OHJZvmuBZOIpHQYl+PP/64hu7x5u/t7cUrr7yi4XTZbFZNOF1dXXpPZWWlji+ZTKq5SK616c40a2ho8IUhMlNpamrS6JSBgQGlWyaT0bU0NzenTPzNN9/UOX300UcRiUR0T23atMn3bPExnDp1Sud2dHTUt0Y5QYdr1rsKQAmCMmj5c9AbqcSUNzU1pfs/n88jmUz6EvVch50tQNk1lOT/tezSDNfbpfjgDIpIWeuwYoQmlBAhQoTYoLivEjiX8BTY9SfYVCGSw8jIiMZtZrNZddAUi0W0tLToKcvec47DnJiY0GiKTCajDhqWzCsrK1WibWhoQDKZVElyYWFB+8XOOFuSdDkxuS4F10ew3+YC3HHm3bp1S805nNAksa0yPo5zfuSRRwCU1GcZd3l5OXbt2qXRI5yMw/Tt7u7WxKH+/n51uNXV1anGUFFRoVEgHLEj4+ZSpkLXyspKNVWw2YJLJLCkIv2S8XG8L3Bn/fCzuru79Z6Ojg5Nna6trfX1UaTe9vZ23LhxwycpueaH542lr46ODnzjG98AUKqGKWPN5/Na3e/ll1/GmTNnfNEYou1cvHgRjz76qNJU2ti/fz++9KUvaTviIMxms751Imv9kUcewYkTJ3ymAUFZWZmvPIH0Y2Jiwrd+ZKzXrl1Tk8KOHTtQX1/vLDucSqVUMzx79qyuUS79y/QcGxvDd7/7XaU7V9KUZwt9Xe+YZO2VS+/yu2gBqNnr8uXLvlwFfq4Nl8Rrp+i7zHycRGSDJXh+E5L93tag8iKCtaJc1p2Bc1SIdJiZl51EIuDNywOqra3VhWCMwfLysqpvsjilDSFgV1eXmmMuXryo5U7b2tp8dbSFsReLRdTW1mq/xsfHlZlfu3bNWf8iSNW1Q4UkTG50dBTV1dW+DDRhyPPz8z6PO0dvMLOTexcXF32ZXlxYqrm5WcfIoV+27U9sh6Ojo5qUUV5ervbempoafU5nZye2b9+uTIKv49rXjY2NmnW4sLCg5omgDW/TzlZpZQPX1tYqw2lra1Mz2IEDB7SsbTQaVXqOjY2pP2NkZMTXvq1Wc3iaIB6P63O/+tWvqnmivLxc12xPTw9efvllAMBrr72GsbExX5ilzPvVq1c10uWJJ57QgyUejytj37x5s88UKDSIRqO61hsbG9Ha2upkJiw4lZWVqemioqLCF5HCIaWc/CUmNAG3wcWe2L8hn3kPz87Oqp+lu7t71WHMzJXfTMVrlP0C/Jkh+35xcXGVMBDEbF3mQM6yDIoqk9cMBj2X962AQwelX0KDe62BIrhvceD2+yNdn+1JkoW3c+dOtYG3trb6ijX19/druF9/f79PWhUiT05Oql2xv79fHWOdnZ26gRoaGnzhTw0NDWpXnpiYUObT3t6uEu3IyIj+zhvNPmU5o0+khf7+fmzfvl1tucCdk5qlfKYhg+3sXJDHXlhlZWU+qVsOr127dqlTMpFI6JjeeOMN3cy29CLS3uXLl9HQ0KCMs6GhQe3CBw4c8KX7S1z1M888o1Io27M5/V2+S185pLGqqkrjlrdv345nn30WQImBy7xVV1frWG/fvq0ZgRcuXFAN7vr165ibm3M6EuPxuG/9yXh3796tVQOfeuopny1XBINXXnkFr732GoCSjT6dTvucvLIue3p68OKLLwIorUvRGFpaWvRAfOihh5zMw/PuFLyanZ1FKpVy2ngljwEorSkReKqrq1Uaz+Vy2kZ5eblqXc3NzYFSazwe13Denp4ePRS5vADPmR1yx/kInJXMTJvtxVwIjpmgnVvBB6V9IAftCwaH/nFZD17H/DsLo0GVCW1bvsvnx23fiwMTCG3gIUKECLFhcd8SeVgdtlVmLtvI0rhIb8eOHdMaGclk0heZcOHCBZXsxsfHfeYNljxFyuPMNC6exaaYhoYG1NTUqOSybds2lTAPHTqkUR2vv/66SvP9/f0q9XAmHUsBhUJBw636+/tx+PBhnzQndCgUCk5VTJ4HlKQQkdjq6up8phgumzk9Pa12wXg8rlmoTzzxhIYhAlBa9/X1qS03nU6rdFMsFlX6m5iYQDweV+mvoaFBJbvdu3f7ImykX5xgxNFCfI09Tg673Lt3r4aEtbS0qL+irKxMpfn+/n5fTRhJ6rhy5YpqaZI963q9Ftvvy8vLdc6/8pWvqHmjsrJS6Ts2NqZS9y9+8QuVxpeWlgLrQc/Pz+Pq1dK7wOfm5jT88tixY2qm2bJli8+kIMhkMqpJnj9/Hs3NzRrGyMWexsfHfckpbJKQOUun02oq+d3f/V1NuJG3rAsdOdOZI58OHjyo4aLd3d2+gmJCz6qqKq13//jjj/vqcbOJiyVS26wgY0okEtonhud5ukavXLmifcpms2vakjlhkKXpoExM10uwbdiatyvbk0NlGUERNquuC/zLvwPsSmGumEeOn87n87pwY7GYLujHHnvMl8UlkPhjYaKsqtox2qyiidljZGTEV5BHnDLJZNKXnbhz5061j3MFuKamJn3ujRs3NIVd1HR7rLlcTn8fGhpaVSuZ42dlYxcKBf3dzvyU3+vr632mGKZPb2+v0oeLU9XX1/tijWWsbW1tqhqzT8G24XEqfT6fV9qx2stqZDab1bGzeYEXK5tJmpqacOjQITUxlJeX+17wIc+anp5W+313d7cyuPHxcT1U7NfaSZ+B1S9rECZz9OhRdSo+9NBDPlOF2LNPnTqlsdC9vb2rKtzxRhdBIR6Pa9sTExPKlPr7+/Wwam5uVocxO+K4NMHAwACefPJJzTRkU9nc3JwetpzrEIvFdHyFQkHNUMeOHdP1Y4zB5OSkVkbM5/N6eLG56vDhwzq+73znO2qC46zFlpYWfPaznwVQMlfaceAC2/cVxCCD+Ies0+rqap3/bDa7Zsivi4Gz09S+l0Odg+zW3Hd5t6h8dr231T4w7ilF/24XGGO2G2NeM8Z0GWOuGmP+dOX3emPMT40xPSv/1921tRAhQoQI8ZHhXiTwPIA/9zzvvDGmGsA5Y8xPAfwnAKc9z/tLY8y3AHwLwH++28OCvOQchcKnoZzSNTU16rjkMqr8THnrtZgb0um0L7SOT00+vTnagyVHRjQaVSlobm7O5/2XPm7btk3baGxsVIk4Ho9rnRFWCT3PUxPK2NgYpqamNJqC23vmmWdUmvrggw+0jxyFUlNTo7VBjhw54qtlItLJxMQErl27ppJoRUWFSiipVEqjSgCoBPbII4+oJNnV1eXTJDhEMxqN+sYr6ritgnKookjKLN2Ul5er+ebEiRNaOjeRSGhddqA0t3L/yMiI743q4nSdnZ31Sfk852yeshNFZD6TySSOHTsGoFS4SUoNs9NsenoaP/7xjwGU3hIkxZLm5+d97XGIKY+XTYMdHR26rvr6+nyvzuMkIm5f1ilHMAmkvVQqpXTn0DgumLZ37151zIP+TdAAABQwSURBVDY2Nuq8LS0t4cyZM/jXf/1XpaPMe3l5uc9xLbRKp9NKk6tXr/q0RNF2Z2ZmdI3Jnnc582wwn7DHKRAasnnUlm6FFvYzWEtkUxfTjU2aa4E1A/tzUPlb1iRcUXc27srAPc8bBTC68nnBGNMFoAXASQCfWrnsHwD8HHdh4EwEO21cwATkyIr6+np9Y3uQV1yezweAq9JhUFjf0tKSPpsL3AijZIYhEQ2cun3gwAG160YiEWVi9fX1yhTsdwHKQpBUdjENtbe3a/u7du3SDVhXV6eFgowxegC0t7er+vzYY4/5YtplQV+5cgU9PT2+qnzCcJ588kk1CzEzPnjwoI7j/Pnz+Ld/+zcA/rrJomqKOWbv3r144YUXAPhTyvP5vNp733jjDedLAnbs2IHnn38eAPDpT39afQKZTAa3b99W5jw6OopXX30VQIlpswlFTCv2K/WCCgfZmbIyjv379+PkyZMASmYzZpxC0wsXLihNRkdHfeo0P5M3red5anqoq6vTmtpPPvmkzs2rr76Kn//85wBK641NVhwvLfP88MMP48knn/T5PoQO4+Pj2t9MJuOL+BGb+dNPP637KxK582rAc+fO4dSpU/qW+kgkonXK4/G4vmOS8wKOHTum7Q0PD6sAMDw8jB/84AcASvtG1oYdoRGJRHwRKmzGELqxqcsOL+Va/dIPeTdrUPQJx2jzIcGv/eN7meGvVaWSs8SDaoPbz5Tx8XOC8EvZwI0xOwA8AuBtAFtWmDs8zxs1xjQF3PNNAN8E1i6LGCJEiBAhfjncMwM3xlQB+C6AP/M8b36tU4Hhed63AXwbAOLxuMdqtysukgP6+UQqLy931k3me9Pp9CoHAaskApZm+DPXBV5eXvYlcRSLd14HNzY2pt7turo6jZ/u7e3VSIXq6mqNQhkbG/OpXC4pbWJiQiU5oKSmizRfKBRUCu7s7NTIkVgsppKS53kaERCPx33xz1KD48yZM5iamlItgB1rP/vZz7Ttzs5O1QTYpLF//36VmmKxmKrAmUwGs7OzKq3s2bPHJ83J7319fVp/+sqVK6o9ZLNZHeuePXvUbBKPx1Xl7u/vx+XLl1Wa6+/vV01kYWHBNyZXZqwtgdvSkMx1XV2dFm76vd/7PRw6dEjpICgWixp5dPHiRZ1bydgFSpoWx/Ryud2FhQVdw/X19WoaFJrJs4TuL774opq6crmcjqO2tlbNFl/84hexc+dO/dvMzIxKzT09Pb7YaFkbnZ2d+iLiLVu2+Iq6iWb2/e9/HxcvXlTNwBij0TzyIm7AX3Nn8+bNukavX7+uL+QeGRnBm2++CaAkEYvTXCRujj6TcXCkCQuAyWRS56xYLKoUzK9kTKVSPm3ezlJlyV4+874Nko7ZuX03PijXVVRUOJ2Ya0WYuDI/bdwTAzfGxFBi3v/oed73Vn4eN8Y0r0jfzQAm7vacoNTUoI7btb2DXpckCzKVSq168zUfDK7P3J5dY5gZgf3uQ35XJ799XphKfX29Lj6peAfcOWSkXXlmNpvFpUuX1B6fTqd9iTVyHdsdE4mEHmrj4+O+sDWhwfXr15Vp3rx5E7Ozs9ovDm06ffq02saPHz+uCU0c7VEoFDTxZ/PmzarOep6HxcVFtc3HYjFltJFIRJnd2bNnNeuQ35TOL4BYWlrSw3FiYkJt29euXUN3d7cvUUo+F4tFn6rLjNNlw/Q8z/favUQioZt7165d+NrXvgbAnxov8yX/y9ycPHlS3x8pYwH8mY3yggJ5T+nLL7+san4kEtGw18bGRg3lbGtrU8bOr+fjjMdNmzapqWrHjh0oFAoarfTWW29p1iPXCa+urtaD4vjx41owi8tWDA4O4pVXXtHnTE9P+0wG0vfz589rVccTJ05oWGcsFsPevXsBlDJVBYuLi7r2FhcXda2LjV9MVxxJMj8/r/ewacXOkuUXRfC7S2V9x2KxVfuewVm9dqVCwO8T46QjMZPw2mAzMScDcigx28NdfJDX8ZpJR4F/WYEp3f23ALo8z/sr+tNLAL6x8vkbAH5wt2eFCBEiRIiPDvcigT8F4OsALhtj3l/57b8C+EsA3zHG/CGAAQBfCrg/EK6UVU6jLRaLPmePnIa26YXfnMORJ5w8waqjHcPMTiFWy/gaO15TwFJ7KpXSmOCKigodk9SyBla/xVrayWQyyOfzKjWNjY2pNFZZWanmiq1bt/oSccR5Nzg4qFI+v7mkr69PJXOR/l3mnHQ6rVLP4OCgr22R8rdu3arSWyKR0D5JSQCRJKempnRct2/fVgm8t7dXtZVUKuWbBxnH+++/74t5l7mdnZ31OU45vpznVuYLWP0CXJckI45Akf46OzvVKc0JSHa6tDjsqqur1yxmJFhaWlL6vvbaa7pOhoaG8P3vf19pIuYbY4yO/eDBg6p11dXV+aRToUd/fz9mZma01vtrr72m9XoqKip0ruLxuL4GrbOz0+f0lLl58cUXNYfB5UCVcY2OjqpDU17KDZRMK7JXd+7cqUlBc3Nzas5jTdRVTli+p9Npn+Nf5oS1UrtOjUvTXlhY8JlUbA1c6JBIJHzJTZzkx9K/XON6y72Ak5g4Goz7zFoiw3Z6BuFeolDeAhD0hE/f7X4bNsNaaeNOh6yqdqIabd++3entXV5eVrPFjRs3sLS05HvXIkek2FXAXOD6CvIcVpfs+/m5uVxON9fMzIwvVC3Iy82v8srlcr6QRgkjSyaTqspXV1drqGE8Hve9xkwYey6X8zE6+Szt8WEp9EmlUr6oBX59mITutba2ql22ublZGXs+n8fw8LCveL6ovWyf5iJbnODBWXi3bt3yRWtw5iczbQ5j5AgPY+5U1eP6NxxpwvMn5hsZ+/z8vL48o6Ojw5kwxghSge21Mjo6qgfZwsKCjmtpaUnpMz09rbZnTqpqb29XH0g2m/VV5ZSDYHZ2FkNDQ1rTe2xsTJmPXcddDgM2PQ0ODuL06dMASjXShZmLT4HXqWBxcVGvO3v2rK7RZ5991lfMjNcuMy72hxWLRZ2DTCaj6yGVSvlMD4Igc4gUl+LxAlhzHtkkOj8/r/RdXl72mU7Y7MaVEIPA4YbMwNmEZ/vpmC/di508rIUSIkSIEBsU614LhdUeVkntymRA6TQVdbalpcV5el++fFnjgW/duuWT8jhyhKMTWJ22zRmsuvGJy9eyFMtSPsejcnu2pOAyoYhEyc+Svi8tLamqNj4+rpK5q06DtO2SVG0J3E5WYlMQV+ETr/zo6KhKVsYYlRLz+bxPJWaJgVOIWVuxTVesxbCmxJqLrZK6krnsyAHWgvha1gQ5Iertt99W7aG1tVXHy44qW8JzVZDjvmUyGUxOTmqU0cjIiLbBUtqlS5dUSpfyDUBJExCTVkVFhTqbBwYGtE64SI7sGBaUlZWpFDw9Pa1mFnZov//+++pg7uvr80X18LP42awhvfXWWxqpUlFRoWUAFhYWtBzxzZs39blcrTOXy61KBOO1xGULBLyugTvrJJVK+aRj+SzXu3IBWALPZDJqupLEQBmzzHsymVxlWgmKqHPFe/N3l/nIxsemFkoQI7OD9QVsa+I3aEejUQ2rOnXqlNrVhoeHtXAQsNpuyaq5q21m7HzA2LWNue4H27C4PfmbCy5mI31yMWW7PKeol7Y66Cq6b2ca8iHFC4ztgNw3vmZ+ft5XIN9VMEvACQwuFZHngF9SYKvGdg0b/uzaNNyebfdmGrL/RfoAlBiA2I7tsDOOBOKQO9ehyzSRF28I7fiw474vLCwoE+QD4+bNm9oXrnEidl0XPRjFYlEZ561btzSZpqKiQk0wY2NjejDYL/qwE2UEPI5bt27pulxYWFB7eDqdVp/G0NCQho7azDcajeqzFhcXfYel0JrXJ99vZ29LX+PxuI+BB0WxcdITh2lmMhmfnV7AZknZsy7/GAsKdugy/24fkIK7MXYgNKGECBEixIbFfTOhcKUvwF/gXE69+fl5lbqvXr2q0RTLy8ta4rS7u1sdKSK9uOoJsFRqawJsshGJn4u229XGbI3BFQHher783aVWSf9cziLug51m7EpKsk1E/NnWPrhfrNa5NAlW95g+LnAylqseBOB/mwu35UpgkDGxNhQUPxuk2bnoLmPiyCcZl0iLgF9SYhMKx/LzfNhSFbdhR0CwSctlVuQXMtiOWI64sfcU7wMxlUxOTmrEz+Lioi/Gf620cZ4Tl8SYz+c1CmliYsK3Rm0zmDxHYK8320HtSsbj6+04bDvGHyhpU1wvhvnB8vKyal1sNrFfl+jqnx1FYu9/XuOsZbrMq7ZDNMj8wrhv9cDZpmQzV07EkZojc3NzPtVRkglYNZUN62Ki9mbniZFJLS8v14niYHuBa3MFZXi6+iC/uybcZULhNoLCn4Lgqp0h9/NiD1qg/JkLKTGCbPzM3IMOziBzU7FY9G1+7odtw3TR2Kav/Xz5bKu8zAhdDIqfy9EtfKixOcUOF7X7ECQACJipBbUt18nzbLMC913+xmF5mUzGZwpw+XLW2lN8He8J+41KvKfsMTJtpA2+jqOV7EObo1M4skpQVlamPrQHH3wQjz/+uC/5TEw+165d0/K3CwsLvoNM2uB64sbcqX9imygZxtwpFmbva9cr/Oz98JGUkw0RIkSIEB9PrLsEzu/HE/BJbkd4iJNmampKT1d+CS3gl2ZsNZ3/xo4rWyKT9jg+045T5xOSHXgczeGSSoOciCwt2G2s5ZRyxbPbzwrqB0uYLNHYElCQGYLBUg9LgPx+Rbtf96qlSJ/Wkpxd6ctraSiuv7kcdC4nM2sr9stuBUFRB3Iv3+Oiu62BBWk4Li3EjsqxHdSyRvP5vHO98rNcznWXWYthS+NMK37XZVBFPh6jy/xkw95THG8tEVSbNm3SlP59+/aho6ND1+zS0pLG3J89e1ZNssvLyz5tkLUJTvJjnhHkhGSa2KWt+Rr5HjTuD1VO9qMEq1n2QuXkCxe4ngMvTrssbZBNKiiJp1gs+uyqHKnCG8i2s3GEgctGZ3+37abyd54kmynZkQD2/TbDCNpkro1lX2d78qXtRCLho49LrRc1kvvrOpxsut8tWoTvtRnmWuYjl92bmZC9Ie7FR2Hbyfl3Vxv2dQxuY63IEdd6s8fEY7ZNO672uF9B68R+ru2LcLVvM3C+3nXNWkzJ9lEIbHOFq5ys3XdOmOrt7dXrBgYGtNjXwMCA+gX4LV42L2EhUwRLqY/u4jn5fF5DEhcWFnyCaVCSTpD/JwihCSVEiBAhNijW3YTCSQ9BDkDX6cySsu04sqU9lm44MsOV+MFq/VrSFOCPs3ZFedjOO/Z4B8Wwcr/Xkm5cdR/uRQrl9uzrmSZBal0mk/GlYrscXfbf+IW6gFsStcfh0mJc0h/T2mUSczng5F6XlCSah0uzkGcDq18O4qK9fS/DXjMsrbr6Za+FtSIR7HuB1TS309YFQQ5j7rf9t7Ucb3KPzA0HCfA65vhuzhmQ59xLzgaPgWkrZo/JyUmtLTQ8PIyqqiqNxpE3YAH+midstrUrELr2ppikeN6EDqOjoxo9xNUpmZexVcI2K9m5Ci6YoAX37wFjzCSAJQBT69boxkQjQhrdDSGN7g0hne6OjUCjBzzP22z/uK4MHACMMe95nnd4XRvdYAhpdHeENLo3hHS6OzYyjUIbeIgQIUJsUIQMPESIECE2KO4HA//2fWhzoyGk0d0R0ujeENLp7tiwNFp3G3iIECFChPhoEJpQQoQIEWKDYt0YuDHms8aYD4wxN4wx31qvdj/uMMb0G2MuG2PeN8a8t/JbvTHmp8aYnpX/6+53P9cbxpi/M8ZMGGOu0G9OupgS/ufK2rpkjHn0/vV8/RBAo/9mjBleWU/vG2NeoL/9lxUafWCMef7+9Hr9YYzZbox5zRjTZYy5aoz505XfN/x6WhcGboyJAvhrACcA7APwB8aYfevR9gbBs57nHaJQpm8BOO153m4Ap1e+/6bh7wF81votiC4nAOxe+fdNAH+zTn283/h7rKYRAPyPlfV0yPO8HwHAyn77CoCHVu75Xyv78jcBeQB/7nneXgBHAPzxCj02/HpaLwn8CQA3PM/r9TwvC+CfAJxcp7Y3Ik4C+IeVz/8A4D/cx77cF3ie9waAGevnILqcBPB/vBLOAqg1xjSvT0/vHwJoFISTAP7J87yM53l9AG6gtC9/7eF53qjneedXPi8A6ALQgl+D9bReDLwFwCB9H1r5LQTgAThljDlnjPnmym9bPM8bBUqLD0DTfevdxwtBdAnXlx9/sqL6/x2Z30IaATDG7ADwCIC38WuwntaLgbuKdoThLyU85XneoyipbX9sjPnk/e7QBkS4vu7gbwB0ADgEYBTAf1/5/TeeRsaYKgDfBfBnnufNr3Wp47ePJa3Wi4EPAdhO31sBjKxT2x9reJ43svL/BIAXUVJrx0VlW/l/4v718GOFILqE62sFnueNe55X8DyvCOB/446Z5DeaRsaYGErM+x89z/veys8bfj2tFwN/F8BuY0y7MSaOkjPlpXVq+2MLY0ylMaZaPgP4DIArKNHmGyuXfQPAD+5PDz92CKLLSwD+40r0wBEAt0U1/k2DZav9AkrrCSjR6CvGmIQxph0lB907692/+wFTKlX4twC6PM/7K/rTxl9PUq7x3/sfgBcAXAdwE8BfrFe7H+d/AHYCuLjy76rQBUADSl7xnpX/6+93X+8Dbf4fSiaAHEoS0R8G0QUllfevV9bWZQCH73f/7yON/u8KDS6hxIia6fq/WKHRBwBO3O/+ryOdPoGSCeQSgPdX/r3w67CewkzMECFChNigCDMxQ4QIEWKDImTgIUKECLFBETLwECFChNigCBl4iBAhQmxQhAw8RIgQITYoQgYeIkSIEBsUIQMPESJEiA2KkIGHCBEixAbF/wdICzL1gUuKcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<6S>z9X=j|\n",
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<6S>z9X=j|\n",
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n}HG0oh+^|<%\n"
     ]
    }
   ],
   "source": [
    "dataset = Passwords_data('password_trainv3.csv', 'train_passwordv3')\n",
    "\n",
    "sampler = MatchingSampler(dataset, 4)\n",
    "align = AlignBatch()\n",
    "dataloder = DataLoader(dataset, batch_size=4, collate_fn=align, shuffle= False)\n",
    "\n",
    "img_pathes, imgs, lables = next(iter(dataloder))\n",
    "\n",
    "for sample in zip(img_pathes, imgs, lables):\n",
    "    #print(sample)\n",
    "    img_path, img, lable = sample\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.squeeze(0).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDrictionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BiDrictionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        t_steps, b_size, h_num = recurrent.shape\n",
    "        recurrent = recurrent.view(t_steps*b_size, h_num) # prepare the linear layer input\n",
    "        \n",
    "        output = self.embedding(recurrent)\n",
    "        output = output.view(t_steps, b_size, -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, inFilt, outFilt, kerSiz, padSiz, strideSiz, b_n = False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(inFilt, outFilt, kerSiz, padSiz, strideSiz)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, inChannal, nClasses, nHidden, nLSTMs = 2):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH == 32 , 'the image input hight must be 32'\n",
    "        \n",
    "        ks = [3, 3, 3, 3, 3, 3, 2] # kernal Size\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0] # padding\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1] # stride\n",
    "        fn = [64, 128, 256, 256, 512, 512, 512] # filters number\n",
    "        \n",
    "        cnn = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        def conv_layer(layNum, b_n = False):\n",
    "            nIn = inChannal if layNum == 0 else fn[layNum-1]\n",
    "            nOut = fn[layNum]\n",
    "            # Conv Layer\n",
    "            cnn.add_module(f'conv{layNum}', nn.Conv2d(nIn, nOut, ks[layNum], ss[layNum], ps[layNum]))\n",
    "            # btach normalization\n",
    "            if b_n:\n",
    "                cnn.add_module(f'batchnorm{layNum}', nn.BatchNorm2d(nOut))\n",
    "            # non Linearity (ReLU)\n",
    "            cnn.add_module(f'relu{layNum}', nn.ReLU(inplace=True))\n",
    "            \n",
    "        # Cnn Arch\n",
    "        conv_layer(0)\n",
    "        cnn.add_module(f'pooling{0}', nn.MaxPool2d(2, 2))  # 64 x 16\n",
    "        conv_layer(1)\n",
    "        cnn.add_module(f'pooling{1}', nn.MaxPool2d(2, 2))  # 128 x 8\n",
    "        conv_layer(2, b_n=True)\n",
    "        conv_layer(3)\n",
    "        # the irregular shape of stride and padding beacause of the shape of some char like (i, ..)\n",
    "        cnn.add_module(f'pooling{2}', nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 256 x 4\n",
    "        conv_layer(4, b_n=True)\n",
    "        conv_layer(5)\n",
    "        cnn.add_module(f'pooling{3}', nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 512 x 2\n",
    "        conv_layer(6, b_n=True) #  512 x 1\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        # Rnn Arch\n",
    "        rnn = nn.Sequential(\n",
    "            BiDrictionalLSTM(512, nHidden, nHidden),\n",
    "            BiDrictionalLSTM(nHidden, nHidden, nClasses)\n",
    "        )\n",
    "\n",
    "        self.rnn = rnn\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # cnn pass\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.shape\n",
    "        \n",
    "        assert h == 1, 'the hight after cnn must equal 1'\n",
    "        \n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1) # sequance, batch, features\n",
    "        \n",
    "        # rnn pass \n",
    "        rnn = self.rnn(conv)\n",
    "        \n",
    "        output = self.softmax(rnn)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN(32, 1, 37, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.load_state_dict(torch.load('crnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model state dict\n",
    "st = crnn.state_dict()\n",
    "for name in st:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint state dic\n",
    "st = torch.load('crnn.pth')\n",
    "for name,k in st.items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "        \"\"\"Log scalar variables.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value) for tag, value in tag_value_pairs])\n",
    "        self.writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label String Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class strLabelConverter(object):\n",
    "    \"\"\"Convert between str and label.\n",
    "\n",
    "    NOTE:\n",
    "        Insert `blank` to the alphabet for CTC.\n",
    "\n",
    "    Args:\n",
    "        alphabet (str): set of the possible characters.\n",
    "        ignore_case (bool, default=True): whether or not to ignore all of the case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, ignore_case=True):\n",
    "        self._ignore_case = ignore_case\n",
    "        if self._ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        self.alphabet = alphabet + '-'  # for `-1` index\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(alphabet):\n",
    "            # NOTE: 0 is reserved for 'blank' required by wrap_ctc\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Support batch or single str.\n",
    "\n",
    "        Args:\n",
    "            text (str or list of str): texts to convert.\n",
    "\n",
    "        Returns:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [\n",
    "                self.dict[char.lower() if self._ignore_case else char]\n",
    "                for char in text\n",
    "            ]\n",
    "            length = [len(text)]\n",
    "        elif isinstance(text, collections.abc.Iterable):\n",
    "            length = [len(s) for s in text]\n",
    "            text = ''.join(text)\n",
    "            text, _ = self.encode(text)\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode(self, t, length, raw=False):\n",
    "        \"\"\"Decode encoded texts back into strs.\n",
    "\n",
    "        Args:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: when the texts and its length does not match.\n",
    "\n",
    "        Returns:\n",
    "            text (str or list of str): texts to convert.\n",
    "        \"\"\"\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(), length)\n",
    "            if raw:\n",
    "                return ''.join([self.alphabet[i - 1] for i in t]), [i - 1 for i in t]\n",
    "            else:\n",
    "                char_list = []\n",
    "                lables_list = []\n",
    "                for i in range(length):\n",
    "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
    "                        char_list.append(self.alphabet[t[i] - 1])\n",
    "                        lables_list.append(t[i] - 1)\n",
    "                return ''.join(char_list), lables_list\n",
    "        else:\n",
    "            # batch mode\n",
    "            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(t.numel(), length.sum())\n",
    "            texts = []\n",
    "            lables = []\n",
    "            index = 0\n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                text, lable = self.decode(\n",
    "                        t[index:index + l], torch.IntTensor([l]), raw=raw)\n",
    "                texts.append(text)\n",
    "                lables.append(lable)\n",
    "                index += l\n",
    "            return texts, lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, translate=(.03,.03))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, scale=(.95,1.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, shear=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(degrees=3, expand=True), \n",
    "                                       transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomPerspective()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Passwords_data('password_trainv3.csv', 'train_passwordv3', transformers=train_transforms)\n",
    "\n",
    "sampler = MatchingSampler(dataset, 4)\n",
    "align = AlignBatch()\n",
    "dataloder = DataLoader(dataset, batch_size=4, collate_fn=align, shuffle= True)\n",
    "\n",
    "img_pathes, imgs, lables = next(iter(dataloder))\n",
    "#print(converter.encode(lables))\n",
    "\n",
    "for sample in zip(img_pathes, imgs, lables):\n",
    "    #print(sample)\n",
    "    img_path, img, lable = sample\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.squeeze(0).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect cuda device? True\n",
      "number of classes id 94 + blank\n"
     ]
    }
   ],
   "source": [
    "# training variables \n",
    "epochs_num = 100\n",
    "batch_size = 16\n",
    "cuda = torch.cuda.is_available()\n",
    "n_workers = 4\n",
    "nClasses = len(char_ststistics) + 1\n",
    "inChannels = 1\n",
    "imgH = 32\n",
    "nHidden = 256\n",
    "lr = .001\n",
    "test_display = 4\n",
    "val_each = 1\n",
    "use_pretrained = True\n",
    "pre_trained = 'crnn.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'detect cuda device? {cuda}')\n",
    "print(f'number of classes id {nClasses-1} + blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=0, translate=(.03,.03)),\n",
    "            transforms.RandomAffine(degrees=0, scale=(.95,1.05)),\n",
    "            transforms.RandomAffine(degrees=0, shear=20),\n",
    "            transforms.RandomRotation(degrees=3, expand=True)]),\n",
    "        transforms.ColorJitter(brightness=.3, contrast=.3, saturation=.3)],  p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Passwords_data('password_train.csv', 'train_passwordv3', transformers=train_transforms)\n",
    "\n",
    "#sampler = MatchingSampler(dataset, batch_size)\n",
    "align = AlignBatch()\n",
    "train_dataloder = DataLoader(train_dataset, batch_size=batch_size, collate_fn=align, \n",
    "                       shuffle= True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Passwords_data('password_val.csv', 'train_passwordv3')\n",
    "\n",
    "#sampler = MatchingSampler(dataset, batch_size)\n",
    "align = AlignBatch()\n",
    "val_dataloder = DataLoader(val_dataset, batch_size=batch_size, collate_fn=align, \n",
    "                       shuffle= True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = strLabelConverter(alphapet, ignore_case=False)\n",
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu0): ReLU(inplace)\n",
      "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): ReLU(inplace)\n",
      "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace)\n",
      "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu3): ReLU(inplace)\n",
      "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace)\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu5): ReLU(inplace)\n",
      "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu6): ReLU(inplace)\n",
      "  )\n",
      "  (rnn): Sequential(\n",
      "    (0): BiDrictionalLSTM(\n",
      "      (rnn): LSTM(512, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): BiDrictionalLSTM(\n",
      "      (rnn): LSTM(256, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=95, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "crnn = CRNN(imgH, inChannels, nClasses, nHidden)\n",
    "crnn.apply(weights_init)\n",
    "if use_pretrained :\n",
    "    model_dict = crnn.state_dict() # state of the current model\n",
    "    pretrained_dict = torch.load(pre_trained) # state of the pretrained model\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k != 'rnn.1.embedding.weight' and k != 'rnn.1.embedding.bias'} # remove the classifier from the state\n",
    "    classifier_dict = {k: v for k, v in model_dict.items() if k == 'rnn.1.embedding.weight' or k == 'rnn.1.embedding.bias'} # get the classifier weight from new model\n",
    "    pretrained_dict.update(classifier_dict) # update without classifier\n",
    "    crnn.load_state_dict(pretrained_dict)\n",
    "    \n",
    "print(crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda :\n",
    "    crnn = crnn.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(crnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logger file\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logger = Logger('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, logger, train_dataloder, batch_size, epoch_num):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    samples_num = 0\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(train_dataloder):\n",
    "        batches_done = len(train_dataloder) * epoch_num + batch_i\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device))\n",
    "        targets, lenghts = converter.encode(targets)\n",
    "        targets = Variable(targets.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        # pass to the network\n",
    "        preds = model(imgs)\n",
    "        preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "        \n",
    "        # loss\n",
    "        #print(preds_size.shape)\n",
    "        loss = criterion(preds, targets.cpu(), preds_size, t_lens)\n",
    "        epoch_loss += loss * imgs.shape[0]\n",
    "        logger.scalar_summary('loss_batches', loss, batches_done)\n",
    "        print(f'Epoch {epoch_num}, Batch {batch_i}/{len(train_dataloder)} : Loss = {loss}')\n",
    "        \n",
    "        # optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    epoch_loss /= samples_num\n",
    "    logger.scalar_summary('loss_epochs', epoch_loss, epoch_num)\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, criterion, logger, val_dataloder, epoch_num, batch_size=16):\n",
    "    model.eval()\n",
    "    \n",
    "    nCorrect_words = 0\n",
    "    #nCorrect_chars = 0\n",
    "    val_loss = 0\n",
    "    samples_num = 0\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(val_dataloder):\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device), requires_grad=False)\n",
    "        targets_encoded, lenghts = converter.encode(targets)\n",
    "        targets_encoded = Variable(targets_encoded.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # pass to the network\n",
    "            preds = model(imgs)\n",
    "            preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(preds, targets_encoded.cpu(), preds_size, t_lens)\n",
    "            val_loss += loss * imgs.shape[0]\n",
    "            \n",
    "            # get the nework prediction\n",
    "            _, preds = preds.max(2)\n",
    "            preds = preds.transpose(1,0).contiguous().view(-1)\n",
    "            words_preds, lables_preds = converter.decode(preds, preds_size)\n",
    "            \n",
    "            for word_pred, target in zip(words_preds, targets):\n",
    "                if word_pred == target:\n",
    "                    nCorrect_words += 1\n",
    "    \n",
    "    # display some of the network prediction\n",
    "    row_preds, _ = converter.decode(preds, preds_size, raw=True)[:test_display]\n",
    "\n",
    "    for row_pred, word_pred, gt in zip(row_preds, words_preds, targets):\n",
    "        print(f'{row_pred} => {word_pred}, Ground Truth is {gt}')\n",
    "    \n",
    "    #compute loss and accurcy\n",
    "    word_accurcy = nCorrect_words / samples_num\n",
    "    val_loss /= samples_num\n",
    "    logger.scalar_summary('val_loss', val_loss, epoch_num)\n",
    "    logger.scalar_summary('val_WordAccurcy', word_accurcy, epoch_num)\n",
    "    \n",
    "    return val_loss, word_accurcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0/32 : Loss = 20.01679229736328\n",
      "Epoch 0, Batch 1/32 : Loss = 20.371015548706055\n",
      "Epoch 0, Batch 2/32 : Loss = 21.97369956970215\n",
      "Epoch 0, Batch 3/32 : Loss = 17.863319396972656\n",
      "Epoch 0, Batch 4/32 : Loss = 17.32358169555664\n",
      "Epoch 0, Batch 5/32 : Loss = 14.57248306274414\n",
      "Epoch 0, Batch 6/32 : Loss = 13.094563484191895\n",
      "Epoch 0, Batch 7/32 : Loss = 10.318499565124512\n",
      "Epoch 0, Batch 8/32 : Loss = 8.31141471862793\n",
      "Epoch 0, Batch 9/32 : Loss = 6.312054634094238\n",
      "Epoch 0, Batch 10/32 : Loss = 5.199183464050293\n",
      "Epoch 0, Batch 11/32 : Loss = 4.747130393981934\n",
      "Epoch 0, Batch 12/32 : Loss = 4.83128547668457\n",
      "Epoch 0, Batch 13/32 : Loss = 5.124048233032227\n",
      "Epoch 0, Batch 14/32 : Loss = 5.444733142852783\n",
      "Epoch 0, Batch 15/32 : Loss = 5.670377731323242\n",
      "Epoch 0, Batch 16/32 : Loss = 5.671440601348877\n",
      "Epoch 0, Batch 17/32 : Loss = 5.710597038269043\n",
      "Epoch 0, Batch 18/32 : Loss = 5.50178337097168\n",
      "Epoch 0, Batch 19/32 : Loss = 5.337872505187988\n",
      "Epoch 0, Batch 20/32 : Loss = 5.098989486694336\n",
      "Epoch 0, Batch 21/32 : Loss = 4.8880767822265625\n",
      "Epoch 0, Batch 22/32 : Loss = 4.738654136657715\n",
      "Epoch 0, Batch 23/32 : Loss = 4.710811138153076\n",
      "Epoch 0, Batch 24/32 : Loss = 4.780316352844238\n",
      "Epoch 0, Batch 25/32 : Loss = 4.917211055755615\n",
      "Epoch 0, Batch 26/32 : Loss = 4.911026477813721\n",
      "Epoch 0, Batch 27/32 : Loss = 4.986318588256836\n",
      "Epoch 0, Batch 28/32 : Loss = 4.78897762298584\n",
      "Epoch 0, Batch 29/32 : Loss = 4.704338073730469\n",
      "Epoch 0, Batch 30/32 : Loss = 4.697702407836914\n",
      "Epoch 0, Batch 31/32 : Loss = 4.666870594024658\n",
      "Epoch 0 finished in 0.04253646930058797 minutes\n",
      "Epoch 0 training_loss = 8.263507843017578\n",
      "-------------------------------------------------------- => , Ground Truth is o\"}!Br&9;`Ow}\n",
      "-------------------------------------------------------- => , Ground Truth is cR;9y?2diO{!\n",
      "-------------------------------------------------------- => , Ground Truth is cR;9y?2diO{!\n",
      "-------------------------------------------------------- => , Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 0 val_loss = 4.754650592803955, word_accuracy = 0.0\n",
      "Epoch 1, Batch 0/32 : Loss = 4.686738014221191\n",
      "Epoch 1, Batch 1/32 : Loss = 4.684096336364746\n",
      "Epoch 1, Batch 2/32 : Loss = 4.665193557739258\n",
      "Epoch 1, Batch 3/32 : Loss = 4.695435523986816\n",
      "Epoch 1, Batch 4/32 : Loss = 4.687691688537598\n",
      "Epoch 1, Batch 5/32 : Loss = 4.688615322113037\n",
      "Epoch 1, Batch 6/32 : Loss = 4.679523944854736\n",
      "Epoch 1, Batch 7/32 : Loss = 4.658557891845703\n",
      "Epoch 1, Batch 8/32 : Loss = 4.6522321701049805\n",
      "Epoch 1, Batch 9/32 : Loss = 4.635345458984375\n",
      "Epoch 1, Batch 10/32 : Loss = 4.67860221862793\n",
      "Epoch 1, Batch 11/32 : Loss = 4.666306018829346\n",
      "Epoch 1, Batch 12/32 : Loss = 4.751795768737793\n",
      "Epoch 1, Batch 13/32 : Loss = 4.758100986480713\n",
      "Epoch 1, Batch 14/32 : Loss = 4.668072700500488\n",
      "Epoch 1, Batch 15/32 : Loss = 4.661859512329102\n",
      "Epoch 1, Batch 16/32 : Loss = 4.698851108551025\n",
      "Epoch 1, Batch 17/32 : Loss = 4.641212463378906\n",
      "Epoch 1, Batch 18/32 : Loss = 4.656928539276123\n",
      "Epoch 1, Batch 19/32 : Loss = 4.625952243804932\n",
      "Epoch 1, Batch 20/32 : Loss = 4.629806995391846\n",
      "Epoch 1, Batch 21/32 : Loss = 4.68820333480835\n",
      "Epoch 1, Batch 22/32 : Loss = 4.664840221405029\n",
      "Epoch 1, Batch 23/32 : Loss = 4.682497501373291\n",
      "Epoch 1, Batch 24/32 : Loss = 4.627284526824951\n",
      "Epoch 1, Batch 25/32 : Loss = 4.622142791748047\n",
      "Epoch 1, Batch 26/32 : Loss = 4.627469062805176\n",
      "Epoch 1, Batch 27/32 : Loss = 4.650739669799805\n",
      "Epoch 1, Batch 28/32 : Loss = 4.639287948608398\n",
      "Epoch 1, Batch 29/32 : Loss = 4.61512565612793\n",
      "Epoch 1, Batch 30/32 : Loss = 4.642398357391357\n",
      "Epoch 1, Batch 31/32 : Loss = 4.638740539550781\n",
      "Epoch 1 finished in 0.08844966491063436 minutes\n",
      "Epoch 1 training_loss = 4.665405750274658\n",
      "--------------------------------------------------------------- => , Ground Truth is o\"}!Br&9;`Ow}\n",
      "--------------------------------------------------------------- => , Ground Truth is J;q+/zyU%U1x_\n",
      "--------------------------------------------------------------- => , Ground Truth is \"]t4e^WQ4>g\n",
      "--------------------------------------------------------------- => , Ground Truth is Err:509\n",
      "Epoch 1 val_loss = 4.684543609619141, word_accuracy = 0.0\n",
      "Epoch 2, Batch 0/32 : Loss = 4.551715850830078\n",
      "Epoch 2, Batch 1/32 : Loss = 4.573644638061523\n",
      "Epoch 2, Batch 2/32 : Loss = 4.547080993652344\n",
      "Epoch 2, Batch 3/32 : Loss = 4.616365432739258\n",
      "Epoch 2, Batch 4/32 : Loss = 4.51311731338501\n",
      "Epoch 2, Batch 5/32 : Loss = 4.609067916870117\n",
      "Epoch 2, Batch 6/32 : Loss = 4.62619686126709\n",
      "Epoch 2, Batch 7/32 : Loss = 4.666206359863281\n",
      "Epoch 2, Batch 8/32 : Loss = 4.5438690185546875\n",
      "Epoch 2, Batch 9/32 : Loss = 4.5565314292907715\n",
      "Epoch 2, Batch 10/32 : Loss = 4.574138641357422\n",
      "Epoch 2, Batch 11/32 : Loss = 4.545331001281738\n",
      "Epoch 2, Batch 12/32 : Loss = 4.519207000732422\n",
      "Epoch 2, Batch 13/32 : Loss = 4.507613182067871\n",
      "Epoch 2, Batch 14/32 : Loss = 4.52608585357666\n",
      "Epoch 2, Batch 15/32 : Loss = 4.49410343170166\n",
      "Epoch 2, Batch 16/32 : Loss = 4.603633880615234\n",
      "Epoch 2, Batch 17/32 : Loss = 4.5232133865356445\n",
      "Epoch 2, Batch 18/32 : Loss = 4.465733051300049\n",
      "Epoch 2, Batch 19/32 : Loss = 4.444229602813721\n",
      "Epoch 2, Batch 20/32 : Loss = 4.417682647705078\n",
      "Epoch 2, Batch 21/32 : Loss = 4.440615177154541\n",
      "Epoch 2, Batch 22/32 : Loss = 4.421237945556641\n",
      "Epoch 2, Batch 23/32 : Loss = 4.491980075836182\n",
      "Epoch 2, Batch 24/32 : Loss = 4.466191291809082\n",
      "Epoch 2, Batch 25/32 : Loss = 4.469975471496582\n",
      "Epoch 2, Batch 26/32 : Loss = 4.417701721191406\n",
      "Epoch 2, Batch 27/32 : Loss = 4.44050407409668\n",
      "Epoch 2, Batch 28/32 : Loss = 4.387066841125488\n",
      "Epoch 2, Batch 29/32 : Loss = 4.355012893676758\n",
      "Epoch 2, Batch 30/32 : Loss = 4.369178771972656\n",
      "Epoch 2, Batch 31/32 : Loss = 4.297311782836914\n",
      "Epoch 2 finished in 0.04268452326456706 minutes\n",
      "Epoch 2 training_loss = 4.505105018615723\n",
      "-------------------------------------------------------------- => , Ground Truth is {BYRayh#2>E4\n",
      "-------------------------------------------------------------- => , Ground Truth is B.YIW6FhX'Y2\n",
      "-------------------------------------------------------------- => , Ground Truth is d:X9eaF,8-VR\n",
      "-------------------------------------------------------------- => , Ground Truth is /MoE^3x/&6X\n",
      "Epoch 2 val_loss = 4.488137245178223, word_accuracy = 0.0\n",
      "Epoch 3, Batch 0/32 : Loss = 4.337015151977539\n",
      "Epoch 3, Batch 1/32 : Loss = 4.333339214324951\n",
      "Epoch 3, Batch 2/32 : Loss = 4.266153335571289\n",
      "Epoch 3, Batch 3/32 : Loss = 4.282520771026611\n",
      "Epoch 3, Batch 4/32 : Loss = 4.225973129272461\n",
      "Epoch 3, Batch 5/32 : Loss = 4.210526943206787\n",
      "Epoch 3, Batch 6/32 : Loss = 4.214511394500732\n",
      "Epoch 3, Batch 7/32 : Loss = 4.2463788986206055\n",
      "Epoch 3, Batch 8/32 : Loss = 4.162744045257568\n",
      "Epoch 3, Batch 9/32 : Loss = 4.177208423614502\n",
      "Epoch 3, Batch 10/32 : Loss = 4.167694568634033\n",
      "Epoch 3, Batch 11/32 : Loss = 4.264216423034668\n",
      "Epoch 3, Batch 12/32 : Loss = 4.110978603363037\n",
      "Epoch 3, Batch 13/32 : Loss = 4.0803070068359375\n",
      "Epoch 3, Batch 14/32 : Loss = 4.156135082244873\n",
      "Epoch 3, Batch 15/32 : Loss = 4.286567211151123\n",
      "Epoch 3, Batch 16/32 : Loss = 4.011577606201172\n",
      "Epoch 3, Batch 17/32 : Loss = 4.0490617752075195\n",
      "Epoch 3, Batch 18/32 : Loss = 4.043462753295898\n",
      "Epoch 3, Batch 19/32 : Loss = 4.050400733947754\n",
      "Epoch 3, Batch 20/32 : Loss = 3.948575496673584\n",
      "Epoch 3, Batch 21/32 : Loss = 3.9415135383605957\n",
      "Epoch 3, Batch 22/32 : Loss = 3.9402596950531006\n",
      "Epoch 3, Batch 23/32 : Loss = 3.9960827827453613\n",
      "Epoch 3, Batch 24/32 : Loss = 4.005488395690918\n",
      "Epoch 3, Batch 25/32 : Loss = 3.883880853652954\n",
      "Epoch 3, Batch 26/32 : Loss = 3.773576259613037\n",
      "Epoch 3, Batch 27/32 : Loss = 3.78586483001709\n",
      "Epoch 3, Batch 28/32 : Loss = 3.778071880340576\n",
      "Epoch 3, Batch 29/32 : Loss = 3.833252429962158\n",
      "Epoch 3, Batch 30/32 : Loss = 3.7078938484191895\n",
      "Epoch 3, Batch 31/32 : Loss = 3.8991775512695312\n",
      "Epoch 3 finished in 0.042349815368652344 minutes\n",
      "Epoch 3 training_loss = 4.072566032409668\n",
      "-------------------------------------------------------------- => , Ground Truth is d:X9eaF,8-VR\n",
      "-------------------------------------------------------------- => , Ground Truth is 8KIZ5p$a}w,\n",
      "-------------------------------------------------------------- => , Ground Truth is 4r{%/')w&N+P\n",
      "-------------------------------------------------------------- => , Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 3 val_loss = 3.864292860031128, word_accuracy = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 0/32 : Loss = 3.659669876098633\n",
      "Epoch 4, Batch 1/32 : Loss = 3.6335577964782715\n",
      "Epoch 4, Batch 2/32 : Loss = 3.5150325298309326\n",
      "Epoch 4, Batch 3/32 : Loss = 3.4759788513183594\n",
      "Epoch 4, Batch 4/32 : Loss = 3.392533779144287\n",
      "Epoch 4, Batch 5/32 : Loss = 3.4705770015716553\n",
      "Epoch 4, Batch 6/32 : Loss = 3.4189515113830566\n",
      "Epoch 4, Batch 7/32 : Loss = 3.5661818981170654\n",
      "Epoch 4, Batch 8/32 : Loss = 3.424992561340332\n",
      "Epoch 4, Batch 9/32 : Loss = 3.2429518699645996\n",
      "Epoch 4, Batch 10/32 : Loss = 3.1485354900360107\n",
      "Epoch 4, Batch 11/32 : Loss = 3.083436965942383\n",
      "Epoch 4, Batch 12/32 : Loss = 3.0631425380706787\n",
      "Epoch 4, Batch 13/32 : Loss = 3.1195287704467773\n",
      "Epoch 4, Batch 14/32 : Loss = 3.23420786857605\n",
      "Epoch 4, Batch 15/32 : Loss = 2.9789061546325684\n",
      "Epoch 4, Batch 16/32 : Loss = 3.008449077606201\n",
      "Epoch 4, Batch 17/32 : Loss = 3.091019630432129\n",
      "Epoch 4, Batch 18/32 : Loss = 2.956968307495117\n",
      "Epoch 4, Batch 19/32 : Loss = 2.7396697998046875\n",
      "Epoch 4, Batch 20/32 : Loss = 2.936345338821411\n",
      "Epoch 4, Batch 21/32 : Loss = 2.6926722526550293\n",
      "Epoch 4, Batch 22/32 : Loss = 2.594053268432617\n",
      "Epoch 4, Batch 23/32 : Loss = 2.6634435653686523\n",
      "Epoch 4, Batch 24/32 : Loss = 2.588308811187744\n",
      "Epoch 4, Batch 25/32 : Loss = 2.6586039066314697\n",
      "Epoch 4, Batch 26/32 : Loss = 2.569699287414551\n",
      "Epoch 4, Batch 27/32 : Loss = 2.5415501594543457\n",
      "Epoch 4, Batch 28/32 : Loss = 2.437950611114502\n",
      "Epoch 4, Batch 29/32 : Loss = 2.2677993774414062\n",
      "Epoch 4, Batch 30/32 : Loss = 2.477417469024658\n",
      "Epoch 4, Batch 31/32 : Loss = 2.5272912979125977\n",
      "Epoch 4 finished in 0.0426674485206604 minutes\n",
      "Epoch 4 training_loss = 3.0190539360046387\n",
      "------------------------------3-------------&----66----------- => 3&6, Ground Truth is /MoE^3x/&6X\n",
      "----7--------D-------------------------------------a----R----- => 7DaR, Ground Truth is 7nDPntwd\\QaR\n",
      "---------99--------------------------x-------X---------------- => 9xX, Ground Truth is 59gmJuxcx.d\\\n",
      "-------------X----9----------aa---------88--------V----R------ => X9a8VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 4 val_loss = 2.4945292472839355, word_accuracy = 0.0\n",
      "Epoch 5, Batch 0/32 : Loss = 2.2484073638916016\n",
      "Epoch 5, Batch 1/32 : Loss = 2.2808964252471924\n",
      "Epoch 5, Batch 2/32 : Loss = 2.0885708332061768\n",
      "Epoch 5, Batch 3/32 : Loss = 2.320265769958496\n",
      "Epoch 5, Batch 4/32 : Loss = 2.48868465423584\n",
      "Epoch 5, Batch 5/32 : Loss = 2.1140902042388916\n",
      "Epoch 5, Batch 6/32 : Loss = 1.8322789669036865\n",
      "Epoch 5, Batch 7/32 : Loss = 1.883265733718872\n",
      "Epoch 5, Batch 8/32 : Loss = 1.7274938821792603\n",
      "Epoch 5, Batch 9/32 : Loss = 1.797296404838562\n",
      "Epoch 5, Batch 10/32 : Loss = 1.7681113481521606\n",
      "Epoch 5, Batch 11/32 : Loss = 1.7695481777191162\n",
      "Epoch 5, Batch 12/32 : Loss = 1.5260958671569824\n",
      "Epoch 5, Batch 13/32 : Loss = 1.8174593448638916\n",
      "Epoch 5, Batch 14/32 : Loss = 1.5208165645599365\n",
      "Epoch 5, Batch 15/32 : Loss = 1.771751880645752\n",
      "Epoch 5, Batch 16/32 : Loss = 1.5458420515060425\n",
      "Epoch 5, Batch 17/32 : Loss = 1.737004280090332\n",
      "Epoch 5, Batch 18/32 : Loss = 1.471501111984253\n",
      "Epoch 5, Batch 19/32 : Loss = 1.7605299949645996\n",
      "Epoch 5, Batch 20/32 : Loss = 1.6922650337219238\n",
      "Epoch 5, Batch 21/32 : Loss = 1.6423004865646362\n",
      "Epoch 5, Batch 22/32 : Loss = 1.2883888483047485\n",
      "Epoch 5, Batch 23/32 : Loss = 1.3002030849456787\n",
      "Epoch 5, Batch 24/32 : Loss = 1.3035340309143066\n",
      "Epoch 5, Batch 25/32 : Loss = 1.2536942958831787\n",
      "Epoch 5, Batch 26/32 : Loss = 1.1570848226547241\n",
      "Epoch 5, Batch 27/32 : Loss = 1.0447652339935303\n",
      "Epoch 5, Batch 28/32 : Loss = 0.9991112947463989\n",
      "Epoch 5, Batch 29/32 : Loss = 0.9836500883102417\n",
      "Epoch 5, Batch 30/32 : Loss = 1.0998365879058838\n",
      "Epoch 5, Batch 31/32 : Loss = 1.7079356908798218\n",
      "Epoch 5 finished in 0.042539560794830324 minutes\n",
      "Epoch 5 training_loss = 1.6529552936553955\n",
      "----J------q-------------z---y---UU----%%-----UU----11---x----- => JqzyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "----00----JJ--------------A-----33----------------------77----- => 0JA37, Ground Truth is 0J!(;A3,')rr7\n",
      "------------33----gg------z----#-----Y------]---qq----+----*--- => 3gz#Y]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---55--->>------*---Y---AA------0----DD---**--#----00----g----- => 5>*YA0D*#0g, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 5 val_loss = 1.3267985582351685, word_accuracy = 0.03\n",
      "Epoch 6, Batch 0/32 : Loss = 0.9282094836235046\n",
      "Epoch 6, Batch 1/32 : Loss = 1.3887121677398682\n",
      "Epoch 6, Batch 2/32 : Loss = 1.2474231719970703\n",
      "Epoch 6, Batch 3/32 : Loss = 0.7937371730804443\n",
      "Epoch 6, Batch 4/32 : Loss = 1.072767734527588\n",
      "Epoch 6, Batch 5/32 : Loss = 0.7888325452804565\n",
      "Epoch 6, Batch 6/32 : Loss = 0.8714904189109802\n",
      "Epoch 6, Batch 7/32 : Loss = 0.7370182275772095\n",
      "Epoch 6, Batch 8/32 : Loss = 0.8048689365386963\n",
      "Epoch 6, Batch 9/32 : Loss = 0.8381707668304443\n",
      "Epoch 6, Batch 10/32 : Loss = 0.5926569104194641\n",
      "Epoch 6, Batch 11/32 : Loss = 0.6241206526756287\n",
      "Epoch 6, Batch 12/32 : Loss = 0.567609965801239\n",
      "Epoch 6, Batch 13/32 : Loss = 1.5789039134979248\n",
      "Epoch 6, Batch 14/32 : Loss = 0.689001202583313\n",
      "Epoch 6, Batch 15/32 : Loss = 1.1125030517578125\n",
      "Epoch 6, Batch 16/32 : Loss = 0.8754743933677673\n",
      "Epoch 6, Batch 17/32 : Loss = 0.514067530632019\n",
      "Epoch 6, Batch 18/32 : Loss = 0.6548360586166382\n",
      "Epoch 6, Batch 19/32 : Loss = 0.5252910852432251\n",
      "Epoch 6, Batch 20/32 : Loss = 0.46176499128341675\n",
      "Epoch 6, Batch 21/32 : Loss = 0.555083692073822\n",
      "Epoch 6, Batch 22/32 : Loss = 0.5663124322891235\n",
      "Epoch 6, Batch 23/32 : Loss = 0.821837842464447\n",
      "Epoch 6, Batch 24/32 : Loss = 0.48311513662338257\n",
      "Epoch 6, Batch 25/32 : Loss = 0.3916172981262207\n",
      "Epoch 6, Batch 26/32 : Loss = 0.5103427767753601\n",
      "Epoch 6, Batch 27/32 : Loss = 1.182321548461914\n",
      "Epoch 6, Batch 28/32 : Loss = 0.8320217728614807\n",
      "Epoch 6, Batch 29/32 : Loss = 0.8383991122245789\n",
      "Epoch 6, Batch 30/32 : Loss = 0.46522724628448486\n",
      "Epoch 6, Batch 31/32 : Loss = 0.5140038132667542\n",
      "Epoch 6 finished in 0.04370567798614502 minutes\n",
      "Epoch 6 training_loss = 0.7832286357879639\n",
      "--------Q---66---<---<<------T---N---5-------P------------ => Q6<<TN5P, Ground Truth is 0Q6<<(TN5=P(m\n",
      "---88----K-------ZZ----5----p-----$----a----}--ww--------- => 8KZ5p$a}w, Ground Truth is 8KIZ5p$a}w,\n",
      "-----Z----v----O-----TT---`--44----v-------0--------Q----- => ZvOT`4v0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---J---;--q----+---/---z---y---U----%%-----U-----1--xx---- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 6 val_loss = 0.7778212428092957, word_accuracy = 0.18\n",
      "Epoch 7, Batch 0/32 : Loss = 0.4491509199142456\n",
      "Epoch 7, Batch 1/32 : Loss = 0.6945134997367859\n",
      "Epoch 7, Batch 2/32 : Loss = 0.4298967719078064\n",
      "Epoch 7, Batch 3/32 : Loss = 0.3952710032463074\n",
      "Epoch 7, Batch 4/32 : Loss = 0.6504597663879395\n",
      "Epoch 7, Batch 5/32 : Loss = 0.42927777767181396\n",
      "Epoch 7, Batch 6/32 : Loss = 0.386300265789032\n",
      "Epoch 7, Batch 7/32 : Loss = 0.32451438903808594\n",
      "Epoch 7, Batch 8/32 : Loss = 0.33872753381729126\n",
      "Epoch 7, Batch 9/32 : Loss = 0.615688145160675\n",
      "Epoch 7, Batch 10/32 : Loss = 0.5544621348381042\n",
      "Epoch 7, Batch 11/32 : Loss = 0.27380138635635376\n",
      "Epoch 7, Batch 12/32 : Loss = 0.27613329887390137\n",
      "Epoch 7, Batch 13/32 : Loss = 0.3589158058166504\n",
      "Epoch 7, Batch 14/32 : Loss = 0.3182337284088135\n",
      "Epoch 7, Batch 15/32 : Loss = 0.36395978927612305\n",
      "Epoch 7, Batch 16/32 : Loss = 0.6596114039421082\n",
      "Epoch 7, Batch 17/32 : Loss = 0.8112942576408386\n",
      "Epoch 7, Batch 18/32 : Loss = 0.5621263980865479\n",
      "Epoch 7, Batch 19/32 : Loss = 0.5474896430969238\n",
      "Epoch 7, Batch 20/32 : Loss = 0.29754284024238586\n",
      "Epoch 7, Batch 21/32 : Loss = 0.2644057273864746\n",
      "Epoch 7, Batch 22/32 : Loss = 0.3216838240623474\n",
      "Epoch 7, Batch 23/32 : Loss = 0.2634318768978119\n",
      "Epoch 7, Batch 24/32 : Loss = 0.2301623672246933\n",
      "Epoch 7, Batch 25/32 : Loss = 0.8402882814407349\n",
      "Epoch 7, Batch 26/32 : Loss = 0.2318589687347412\n",
      "Epoch 7, Batch 27/32 : Loss = 0.4368511438369751\n",
      "Epoch 7, Batch 28/32 : Loss = 0.22202186286449432\n",
      "Epoch 7, Batch 29/32 : Loss = 0.5281007289886475\n",
      "Epoch 7, Batch 30/32 : Loss = 0.30483323335647583\n",
      "Epoch 7, Batch 31/32 : Loss = 0.156002014875412\n",
      "Epoch 7 finished in 0.04252740542093913 minutes\n",
      "Epoch 7 training_loss = 0.43053847551345825\n",
      "----C-----DD----E----g----m------'---m------F---<<----------8---2----- => CDEgm'mF<82, Ground Truth is CDEgm\"mF<Q82\n",
      "---55---->>----:--*---YY----A----'--O------D----**--#-----OO----gg---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---#-----GG-----9-----E----I--=----h----55---#-----2----J----)---k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "------k-----$----I--F-----DD-----e-----h----]---k----0----\\---XX------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 7 val_loss = 0.6001969575881958, word_accuracy = 0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 0/32 : Loss = 0.2218661904335022\n",
      "Epoch 8, Batch 1/32 : Loss = 0.4220491051673889\n",
      "Epoch 8, Batch 2/32 : Loss = 0.4126681685447693\n",
      "Epoch 8, Batch 3/32 : Loss = 0.2537296712398529\n",
      "Epoch 8, Batch 4/32 : Loss = 0.14774686098098755\n",
      "Epoch 8, Batch 5/32 : Loss = 0.14554472267627716\n",
      "Epoch 8, Batch 6/32 : Loss = 0.15946334600448608\n",
      "Epoch 8, Batch 7/32 : Loss = 0.18833039700984955\n",
      "Epoch 8, Batch 8/32 : Loss = 0.1531006246805191\n",
      "Epoch 8, Batch 9/32 : Loss = 0.4207683205604553\n",
      "Epoch 8, Batch 10/32 : Loss = 0.17777591943740845\n",
      "Epoch 8, Batch 11/32 : Loss = 0.1980592906475067\n",
      "Epoch 8, Batch 12/32 : Loss = 0.21464583277702332\n",
      "Epoch 8, Batch 13/32 : Loss = 0.16967032849788666\n",
      "Epoch 8, Batch 14/32 : Loss = 0.19698956608772278\n",
      "Epoch 8, Batch 15/32 : Loss = 0.47624337673187256\n",
      "Epoch 8, Batch 16/32 : Loss = 0.16868478059768677\n",
      "Epoch 8, Batch 17/32 : Loss = 0.2140936255455017\n",
      "Epoch 8, Batch 18/32 : Loss = 0.13635307550430298\n",
      "Epoch 8, Batch 19/32 : Loss = 0.45144665241241455\n",
      "Epoch 8, Batch 20/32 : Loss = 0.44363686442375183\n",
      "Epoch 8, Batch 21/32 : Loss = 0.16443735361099243\n",
      "Epoch 8, Batch 22/32 : Loss = 0.14926134049892426\n",
      "Epoch 8, Batch 23/32 : Loss = 0.44048500061035156\n",
      "Epoch 8, Batch 24/32 : Loss = 0.7820485830307007\n",
      "Epoch 8, Batch 25/32 : Loss = 0.18668845295906067\n",
      "Epoch 8, Batch 26/32 : Loss = 0.11459126323461533\n",
      "Epoch 8, Batch 27/32 : Loss = 0.607333779335022\n",
      "Epoch 8, Batch 28/32 : Loss = 0.2991217076778412\n",
      "Epoch 8, Batch 29/32 : Loss = 0.41030654311180115\n",
      "Epoch 8, Batch 30/32 : Loss = 0.15205532312393188\n",
      "Epoch 8, Batch 31/32 : Loss = 0.23775199055671692\n",
      "Epoch 8 finished in 0.04120318492253621 minutes\n",
      "Epoch 8 training_loss = 0.279804527759552\n",
      "-----d----!---N-----r---AA---j---*---$----33----h----5-----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "--77---m----DD----P----n---t---w-----d---\\\\---Q----a----RR------ => 7mDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----dd----!--NN-----r--AA----j--*---$$---33----hh---55----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---k----$----I--F-----D-----e----hh---]---k----0----\\---X------- => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 8 val_loss = 0.5592711567878723, word_accuracy = 0.4\n",
      "Epoch 9, Batch 0/32 : Loss = 0.29045629501342773\n",
      "Epoch 9, Batch 1/32 : Loss = 0.15600991249084473\n",
      "Epoch 9, Batch 2/32 : Loss = 0.12457820773124695\n",
      "Epoch 9, Batch 3/32 : Loss = 0.2725815176963806\n",
      "Epoch 9, Batch 4/32 : Loss = 0.25746774673461914\n",
      "Epoch 9, Batch 5/32 : Loss = 0.11878426373004913\n",
      "Epoch 9, Batch 6/32 : Loss = 0.15971127152442932\n",
      "Epoch 9, Batch 7/32 : Loss = 0.16730958223342896\n",
      "Epoch 9, Batch 8/32 : Loss = 0.10741865634918213\n",
      "Epoch 9, Batch 9/32 : Loss = 0.2674994170665741\n",
      "Epoch 9, Batch 10/32 : Loss = 0.24199926853179932\n",
      "Epoch 9, Batch 11/32 : Loss = 0.11753721535205841\n",
      "Epoch 9, Batch 12/32 : Loss = 0.33996155858039856\n",
      "Epoch 9, Batch 13/32 : Loss = 0.12096843868494034\n",
      "Epoch 9, Batch 14/32 : Loss = 0.12948903441429138\n",
      "Epoch 9, Batch 15/32 : Loss = 0.1225636899471283\n",
      "Epoch 9, Batch 16/32 : Loss = 0.10713262856006622\n",
      "Epoch 9, Batch 17/32 : Loss = 0.1459432691335678\n",
      "Epoch 9, Batch 18/32 : Loss = 0.09589862823486328\n",
      "Epoch 9, Batch 19/32 : Loss = 0.15552477538585663\n",
      "Epoch 9, Batch 20/32 : Loss = 0.15799427032470703\n",
      "Epoch 9, Batch 21/32 : Loss = 0.3208232820034027\n",
      "Epoch 9, Batch 22/32 : Loss = 0.4896247386932373\n",
      "Epoch 9, Batch 23/32 : Loss = 0.09682223200798035\n",
      "Epoch 9, Batch 24/32 : Loss = 0.16364774107933044\n",
      "Epoch 9, Batch 25/32 : Loss = 0.28710606694221497\n",
      "Epoch 9, Batch 26/32 : Loss = 0.08741798251867294\n",
      "Epoch 9, Batch 27/32 : Loss = 0.5299829244613647\n",
      "Epoch 9, Batch 28/32 : Loss = 0.129709392786026\n",
      "Epoch 9, Batch 29/32 : Loss = 0.09774383157491684\n",
      "Epoch 9, Batch 30/32 : Loss = 0.12570399045944214\n",
      "Epoch 9, Batch 31/32 : Loss = 0.27820947766304016\n",
      "Epoch 9 finished in 0.042725229263305665 minutes\n",
      "Epoch 9 training_loss = 0.1934196799993515\n",
      "---k---$----|--'-,--9----Y----N-----W------m------T---88----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----Y----W-------]--i--\\---|----MM-----<-----MM----88---88--- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "--.---c----0-----w-----u----`-u----.--R----{--3----\"---#----- => .c0wu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----k--$$---|---'---9----Y----N-----W------m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 9 val_loss = 0.5084251165390015, word_accuracy = 0.55\n",
      "Epoch 10, Batch 0/32 : Loss = 0.13322365283966064\n",
      "Epoch 10, Batch 1/32 : Loss = 0.08102035522460938\n",
      "Epoch 10, Batch 2/32 : Loss = 0.1986132711172104\n",
      "Epoch 10, Batch 3/32 : Loss = 0.08442546427249908\n",
      "Epoch 10, Batch 4/32 : Loss = 0.13850831985473633\n",
      "Epoch 10, Batch 5/32 : Loss = 0.10703223943710327\n",
      "Epoch 10, Batch 6/32 : Loss = 0.14005549252033234\n",
      "Epoch 10, Batch 7/32 : Loss = 0.11414764076471329\n",
      "Epoch 10, Batch 8/32 : Loss = 0.17572739720344543\n",
      "Epoch 10, Batch 9/32 : Loss = 0.08125045150518417\n",
      "Epoch 10, Batch 10/32 : Loss = 0.08666916936635971\n",
      "Epoch 10, Batch 11/32 : Loss = 0.23488439619541168\n",
      "Epoch 10, Batch 12/32 : Loss = 0.0844147801399231\n",
      "Epoch 10, Batch 13/32 : Loss = 0.08383189141750336\n",
      "Epoch 10, Batch 14/32 : Loss = 0.11344687640666962\n",
      "Epoch 10, Batch 15/32 : Loss = 0.22826401889324188\n",
      "Epoch 10, Batch 16/32 : Loss = 0.24621111154556274\n",
      "Epoch 10, Batch 17/32 : Loss = 0.15748104453086853\n",
      "Epoch 10, Batch 18/32 : Loss = 0.2781410217285156\n",
      "Epoch 10, Batch 19/32 : Loss = 0.0820385068655014\n",
      "Epoch 10, Batch 20/32 : Loss = 0.16121549904346466\n",
      "Epoch 10, Batch 21/32 : Loss = 0.1606156975030899\n",
      "Epoch 10, Batch 22/32 : Loss = 0.13630206882953644\n",
      "Epoch 10, Batch 23/32 : Loss = 0.1407230794429779\n",
      "Epoch 10, Batch 24/32 : Loss = 0.09600624442100525\n",
      "Epoch 10, Batch 25/32 : Loss = 0.10287593305110931\n",
      "Epoch 10, Batch 26/32 : Loss = 0.21992641687393188\n",
      "Epoch 10, Batch 27/32 : Loss = 0.16916169226169586\n",
      "Epoch 10, Batch 28/32 : Loss = 0.1422731876373291\n",
      "Epoch 10, Batch 29/32 : Loss = 0.23700137436389923\n",
      "Epoch 10, Batch 30/32 : Loss = 0.2217063158750534\n",
      "Epoch 10, Batch 31/32 : Loss = 0.12167372554540634\n",
      "Epoch 10 finished in 0.041371595859527585 minutes\n",
      "Epoch 10 training_loss = 0.14947481453418732\n",
      "----X----77---0---jj--@@-----S----Z----LL---4-----C----mm------MM-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----55---->>----:--*----Y----A----'--OO-----DD----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---{---BB-----YY----RR-----a----yy---hh---#------2---->>-----E----44----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "------z------O-----\"----GG------//----~~-----$------c-----11---jj--tt---- => zO\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 10 val_loss = 0.5173012018203735, word_accuracy = 0.57\n",
      "Epoch 11, Batch 0/32 : Loss = 0.05907754600048065\n",
      "Epoch 11, Batch 1/32 : Loss = 0.13209329545497894\n",
      "Epoch 11, Batch 2/32 : Loss = 0.1057610809803009\n",
      "Epoch 11, Batch 3/32 : Loss = 0.11057774722576141\n",
      "Epoch 11, Batch 4/32 : Loss = 0.06613947451114655\n",
      "Epoch 11, Batch 5/32 : Loss = 0.12274253368377686\n",
      "Epoch 11, Batch 6/32 : Loss = 0.10573883354663849\n",
      "Epoch 11, Batch 7/32 : Loss = 0.0585622638463974\n",
      "Epoch 11, Batch 8/32 : Loss = 0.0681094080209732\n",
      "Epoch 11, Batch 9/32 : Loss = 0.09278123080730438\n",
      "Epoch 11, Batch 10/32 : Loss = 0.32688087224960327\n",
      "Epoch 11, Batch 11/32 : Loss = 0.09357266128063202\n",
      "Epoch 11, Batch 12/32 : Loss = 0.0565602146089077\n",
      "Epoch 11, Batch 13/32 : Loss = 0.042222049087285995\n",
      "Epoch 11, Batch 14/32 : Loss = 0.11299879103899002\n",
      "Epoch 11, Batch 15/32 : Loss = 0.046711765229701996\n",
      "Epoch 11, Batch 16/32 : Loss = 0.18791210651397705\n",
      "Epoch 11, Batch 17/32 : Loss = 0.23182082176208496\n",
      "Epoch 11, Batch 18/32 : Loss = 0.17323003709316254\n",
      "Epoch 11, Batch 19/32 : Loss = 0.053219474852085114\n",
      "Epoch 11, Batch 20/32 : Loss = 0.283502995967865\n",
      "Epoch 11, Batch 21/32 : Loss = 0.0584600456058979\n",
      "Epoch 11, Batch 22/32 : Loss = 0.05145613104104996\n",
      "Epoch 11, Batch 23/32 : Loss = 0.1850070059299469\n",
      "Epoch 11, Batch 24/32 : Loss = 0.05559927597641945\n",
      "Epoch 11, Batch 25/32 : Loss = 0.11685436218976974\n",
      "Epoch 11, Batch 26/32 : Loss = 0.1867791712284088\n",
      "Epoch 11, Batch 27/32 : Loss = 0.15541303157806396\n",
      "Epoch 11, Batch 28/32 : Loss = 0.10165476053953171\n",
      "Epoch 11, Batch 29/32 : Loss = 0.08489082753658295\n",
      "Epoch 11, Batch 30/32 : Loss = 0.2693791091442108\n",
      "Epoch 11, Batch 31/32 : Loss = 0.07694950699806213\n",
      "Epoch 11 finished in 0.04168653090794881 minutes\n",
      "Epoch 11 training_loss = 0.12225953489542007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----dd-----!--NN-------r---AA----j---*----$-----33----hh-----5-----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----C-----DD----E----gg----m------\"---mm------F----<-----Q----88----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "------C-----RR-----;---9------y----??----22-----d-----ii--OO------{---!--- => CR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "------zz-----O-----\"----GG-------/-----~------$------c-----1----jj---t---- => zO\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 11 val_loss = 0.5236402153968811, word_accuracy = 0.58\n",
      "Epoch 12, Batch 0/32 : Loss = 0.04485951364040375\n",
      "Epoch 12, Batch 1/32 : Loss = 0.06273914873600006\n",
      "Epoch 12, Batch 2/32 : Loss = 0.10512404143810272\n",
      "Epoch 12, Batch 3/32 : Loss = 0.07843194156885147\n",
      "Epoch 12, Batch 4/32 : Loss = 0.06385329365730286\n",
      "Epoch 12, Batch 5/32 : Loss = 0.058190539479255676\n",
      "Epoch 12, Batch 6/32 : Loss = 0.10689099878072739\n",
      "Epoch 12, Batch 7/32 : Loss = 0.05898717790842056\n",
      "Epoch 12, Batch 8/32 : Loss = 0.03824838995933533\n",
      "Epoch 12, Batch 9/32 : Loss = 0.1731073409318924\n",
      "Epoch 12, Batch 10/32 : Loss = 0.22488561272621155\n",
      "Epoch 12, Batch 11/32 : Loss = 0.21684405207633972\n",
      "Epoch 12, Batch 12/32 : Loss = 0.10295368731021881\n",
      "Epoch 12, Batch 13/32 : Loss = 0.028424818068742752\n",
      "Epoch 12, Batch 14/32 : Loss = 0.07871104031801224\n",
      "Epoch 12, Batch 15/32 : Loss = 0.255786657333374\n",
      "Epoch 12, Batch 16/32 : Loss = 0.14830382168293\n",
      "Epoch 12, Batch 17/32 : Loss = 0.09107432514429092\n",
      "Epoch 12, Batch 18/32 : Loss = 0.1041809692978859\n",
      "Epoch 12, Batch 19/32 : Loss = 0.04869920015335083\n",
      "Epoch 12, Batch 20/32 : Loss = 0.10548311471939087\n",
      "Epoch 12, Batch 21/32 : Loss = 0.05639734864234924\n",
      "Epoch 12, Batch 22/32 : Loss = 0.10717403143644333\n",
      "Epoch 12, Batch 23/32 : Loss = 0.03876938298344612\n",
      "Epoch 12, Batch 24/32 : Loss = 0.04799436032772064\n",
      "Epoch 12, Batch 25/32 : Loss = 0.06516951322555542\n",
      "Epoch 12, Batch 26/32 : Loss = 0.08816559612751007\n",
      "Epoch 12, Batch 27/32 : Loss = 0.18500864505767822\n",
      "Epoch 12, Batch 28/32 : Loss = 0.03021913208067417\n",
      "Epoch 12, Batch 29/32 : Loss = 0.05724974721670151\n",
      "Epoch 12, Batch 30/32 : Loss = 0.21208667755126953\n",
      "Epoch 12, Batch 31/32 : Loss = 0.1914980411529541\n",
      "Epoch 12 finished in 0.044705180327097575 minutes\n",
      "Epoch 12 training_loss = 0.09985385835170746\n",
      "---XX---7---0--j--@@----S---Z----L--4----C----m-----MM------ => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----kk---OO-----/--,--y----c---*----1---PP---}}--#----BB---- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "--77---n----D-----P----n---t--w-----d----\\---Q----aa---RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----Y----W------]--i--\\--|----MM-----<----MM----88---8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 12 val_loss = 0.495710164308548, word_accuracy = 0.68\n",
      "Epoch 13, Batch 0/32 : Loss = 0.07224977761507034\n",
      "Epoch 13, Batch 1/32 : Loss = 0.14319059252738953\n",
      "Epoch 13, Batch 2/32 : Loss = 0.11542315781116486\n",
      "Epoch 13, Batch 3/32 : Loss = 0.03208606317639351\n",
      "Epoch 13, Batch 4/32 : Loss = 0.1285676658153534\n",
      "Epoch 13, Batch 5/32 : Loss = 0.13302187621593475\n",
      "Epoch 13, Batch 6/32 : Loss = 0.04639308154582977\n",
      "Epoch 13, Batch 7/32 : Loss = 0.06137394160032272\n",
      "Epoch 13, Batch 8/32 : Loss = 0.03554283827543259\n",
      "Epoch 13, Batch 9/32 : Loss = 0.07686755061149597\n",
      "Epoch 13, Batch 10/32 : Loss = 0.09217710793018341\n",
      "Epoch 13, Batch 11/32 : Loss = 0.06506790220737457\n",
      "Epoch 13, Batch 12/32 : Loss = 0.04084477946162224\n",
      "Epoch 13, Batch 13/32 : Loss = 0.14078572392463684\n",
      "Epoch 13, Batch 14/32 : Loss = 0.11513541638851166\n",
      "Epoch 13, Batch 15/32 : Loss = 0.04759614169597626\n",
      "Epoch 13, Batch 16/32 : Loss = 0.09695148468017578\n",
      "Epoch 13, Batch 17/32 : Loss = 0.026279117912054062\n",
      "Epoch 13, Batch 18/32 : Loss = 0.03412409499287605\n",
      "Epoch 13, Batch 19/32 : Loss = 0.16681671142578125\n",
      "Epoch 13, Batch 20/32 : Loss = 0.05409925431013107\n",
      "Epoch 13, Batch 21/32 : Loss = 0.0906471312046051\n",
      "Epoch 13, Batch 22/32 : Loss = 0.029587240889668465\n",
      "Epoch 13, Batch 23/32 : Loss = 0.17344099283218384\n",
      "Epoch 13, Batch 24/32 : Loss = 0.19696909189224243\n",
      "Epoch 13, Batch 25/32 : Loss = 0.05409938469529152\n",
      "Epoch 13, Batch 26/32 : Loss = 0.03460083529353142\n",
      "Epoch 13, Batch 27/32 : Loss = 0.06131092458963394\n",
      "Epoch 13, Batch 28/32 : Loss = 0.05381417274475098\n",
      "Epoch 13, Batch 29/32 : Loss = 0.03241880610585213\n",
      "Epoch 13, Batch 30/32 : Loss = 0.07353591918945312\n",
      "Epoch 13, Batch 31/32 : Loss = 0.06394737213850021\n",
      "Epoch 13 finished in 0.0428107738494873 minutes\n",
      "Epoch 13 training_loss = 0.0813819095492363\n",
      "----d----R---;;-99---y---?---22---dd--ii--0----{--!--- => dR;9y?2di0{!, Ground Truth is cR;9y?2diO{!\n",
      "--8-----K----l--ZZ----5----pp----$----a----}---w------ => 8KlZ5p$a}w, Ground Truth is 8KIZ5p$a}w,\n",
      "---0----JJ--!--(--;;-AA---33---,,-'--)--r--rr--77----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---8----KK---l--Z----5----p----$$---aa---}--ww----,--- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "Epoch 13 val_loss = 0.49148792028427124, word_accuracy = 0.67\n",
      "Epoch 14, Batch 0/32 : Loss = 0.13968907296657562\n",
      "Epoch 14, Batch 1/32 : Loss = 0.03598099946975708\n",
      "Epoch 14, Batch 2/32 : Loss = 0.03844623267650604\n",
      "Epoch 14, Batch 3/32 : Loss = 0.053052112460136414\n",
      "Epoch 14, Batch 4/32 : Loss = 0.023894039914011955\n",
      "Epoch 14, Batch 5/32 : Loss = 0.02361704409122467\n",
      "Epoch 14, Batch 6/32 : Loss = 0.0795951634645462\n",
      "Epoch 14, Batch 7/32 : Loss = 0.23222020268440247\n",
      "Epoch 14, Batch 8/32 : Loss = 0.07367060333490372\n",
      "Epoch 14, Batch 9/32 : Loss = 0.030492983758449554\n",
      "Epoch 14, Batch 10/32 : Loss = 0.12145093083381653\n",
      "Epoch 14, Batch 11/32 : Loss = 0.029177844524383545\n",
      "Epoch 14, Batch 12/32 : Loss = 0.1657223105430603\n",
      "Epoch 14, Batch 13/32 : Loss = 0.06477341055870056\n",
      "Epoch 14, Batch 14/32 : Loss = 0.0238640233874321\n",
      "Epoch 14, Batch 15/32 : Loss = 0.03656161576509476\n",
      "Epoch 14, Batch 16/32 : Loss = 0.047700025141239166\n",
      "Epoch 14, Batch 17/32 : Loss = 0.03278037905693054\n",
      "Epoch 14, Batch 18/32 : Loss = 0.034558482468128204\n",
      "Epoch 14, Batch 19/32 : Loss = 0.05135089159011841\n",
      "Epoch 14, Batch 20/32 : Loss = 0.22096839547157288\n",
      "Epoch 14, Batch 21/32 : Loss = 0.2105725109577179\n",
      "Epoch 14, Batch 22/32 : Loss = 0.07693932950496674\n",
      "Epoch 14, Batch 23/32 : Loss = 0.02212986722588539\n",
      "Epoch 14, Batch 24/32 : Loss = 0.04380829632282257\n",
      "Epoch 14, Batch 25/32 : Loss = 0.021792035549879074\n",
      "Epoch 14, Batch 26/32 : Loss = 0.035932756960392\n",
      "Epoch 14, Batch 27/32 : Loss = 0.025628067553043365\n",
      "Epoch 14, Batch 28/32 : Loss = 0.08114692568778992\n",
      "Epoch 14, Batch 29/32 : Loss = 0.02997036650776863\n",
      "Epoch 14, Batch 30/32 : Loss = 0.25008395314216614\n",
      "Epoch 14, Batch 31/32 : Loss = 0.3041893541812897\n",
      "Epoch 14 finished in 0.04363418817520141 minutes\n",
      "Epoch 14 training_loss = 0.07696689665317535\n",
      "---k----$---I--F----DD----e----hh---]---k---0----\\---X------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----dd---:--XX---99---ee---aa----F--,--8-------VV---RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----#----G----99---E---I-=---hh---5---#---2----J---)-kk----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----kk---OO-----/--,--y----c---*---11---P----}--#----BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 14 val_loss = 0.5122401118278503, word_accuracy = 0.65\n",
      "Epoch 15, Batch 0/32 : Loss = 0.047981224954128265\n",
      "Epoch 15, Batch 1/32 : Loss = 0.10347001254558563\n",
      "Epoch 15, Batch 2/32 : Loss = 0.028888307511806488\n",
      "Epoch 15, Batch 3/32 : Loss = 0.14964695274829865\n",
      "Epoch 15, Batch 4/32 : Loss = 0.06991839408874512\n",
      "Epoch 15, Batch 5/32 : Loss = 0.052915509790182114\n",
      "Epoch 15, Batch 6/32 : Loss = 0.1383471041917801\n",
      "Epoch 15, Batch 7/32 : Loss = 0.06014666333794594\n",
      "Epoch 15, Batch 8/32 : Loss = 0.10561294853687286\n",
      "Epoch 15, Batch 9/32 : Loss = 0.03784248232841492\n",
      "Epoch 15, Batch 10/32 : Loss = 0.11730621010065079\n",
      "Epoch 15, Batch 11/32 : Loss = 0.0252644345164299\n",
      "Epoch 15, Batch 12/32 : Loss = 0.07571201771497726\n",
      "Epoch 15, Batch 13/32 : Loss = 0.03224273398518562\n",
      "Epoch 15, Batch 14/32 : Loss = 0.12385787069797516\n",
      "Epoch 15, Batch 15/32 : Loss = 0.04757431522011757\n",
      "Epoch 15, Batch 16/32 : Loss = 0.06467951834201813\n",
      "Epoch 15, Batch 17/32 : Loss = 0.025709638372063637\n",
      "Epoch 15, Batch 18/32 : Loss = 0.04457906633615494\n",
      "Epoch 15, Batch 19/32 : Loss = 0.0843757838010788\n",
      "Epoch 15, Batch 20/32 : Loss = 0.02755133807659149\n",
      "Epoch 15, Batch 21/32 : Loss = 0.0735255554318428\n",
      "Epoch 15, Batch 22/32 : Loss = 0.025121748447418213\n",
      "Epoch 15, Batch 23/32 : Loss = 0.059292636811733246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 24/32 : Loss = 0.07630486786365509\n",
      "Epoch 15, Batch 25/32 : Loss = 0.08049128204584122\n",
      "Epoch 15, Batch 26/32 : Loss = 0.089272640645504\n",
      "Epoch 15, Batch 27/32 : Loss = 0.033135488629341125\n",
      "Epoch 15, Batch 28/32 : Loss = 0.15749669075012207\n",
      "Epoch 15, Batch 29/32 : Loss = 0.15937215089797974\n",
      "Epoch 15, Batch 30/32 : Loss = 0.08390744030475616\n",
      "Epoch 15, Batch 31/32 : Loss = 0.07516008615493774\n",
      "Epoch 15 finished in 0.042635480562845864 minutes\n",
      "Epoch 15 training_loss = 0.07424699515104294\n",
      "----kk---$---|---'---99---Y----N-----W------m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---\"----]--t---4---------^----W--------Q----44---->-----g---- => \"]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---BB----.---Y---l--W-------6----F----hh----X---'--Y----2---- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----cc---RR----;--99----y----?---22----d----i---O-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "Epoch 15 val_loss = 0.5027710795402527, word_accuracy = 0.64\n",
      "Epoch 16, Batch 0/32 : Loss = 0.027409367263317108\n",
      "Epoch 16, Batch 1/32 : Loss = 0.04771976172924042\n",
      "Epoch 16, Batch 2/32 : Loss = 0.04112842306494713\n",
      "Epoch 16, Batch 3/32 : Loss = 0.023549983277916908\n",
      "Epoch 16, Batch 4/32 : Loss = 0.06723347306251526\n",
      "Epoch 16, Batch 5/32 : Loss = 0.2325265258550644\n",
      "Epoch 16, Batch 6/32 : Loss = 0.020095903426408768\n",
      "Epoch 16, Batch 7/32 : Loss = 0.06436586380004883\n",
      "Epoch 16, Batch 8/32 : Loss = 0.03175356239080429\n",
      "Epoch 16, Batch 9/32 : Loss = 0.04027793928980827\n",
      "Epoch 16, Batch 10/32 : Loss = 0.08251956850290298\n",
      "Epoch 16, Batch 11/32 : Loss = 0.05018243193626404\n",
      "Epoch 16, Batch 12/32 : Loss = 0.08248354494571686\n",
      "Epoch 16, Batch 13/32 : Loss = 0.04680195450782776\n",
      "Epoch 16, Batch 14/32 : Loss = 0.07347685843706131\n",
      "Epoch 16, Batch 15/32 : Loss = 0.06529752165079117\n",
      "Epoch 16, Batch 16/32 : Loss = 0.06905321776866913\n",
      "Epoch 16, Batch 17/32 : Loss = 0.03572304546833038\n",
      "Epoch 16, Batch 18/32 : Loss = 0.04314970225095749\n",
      "Epoch 16, Batch 19/32 : Loss = 0.02907617762684822\n",
      "Epoch 16, Batch 20/32 : Loss = 0.026315923780202866\n",
      "Epoch 16, Batch 21/32 : Loss = 0.02180631086230278\n",
      "Epoch 16, Batch 22/32 : Loss = 0.05166083574295044\n",
      "Epoch 16, Batch 23/32 : Loss = 0.016753386706113815\n",
      "Epoch 16, Batch 24/32 : Loss = 0.1219969317317009\n",
      "Epoch 16, Batch 25/32 : Loss = 0.10069672763347626\n",
      "Epoch 16, Batch 26/32 : Loss = 0.19891345500946045\n",
      "Epoch 16, Batch 27/32 : Loss = 0.04708004742860794\n",
      "Epoch 16, Batch 28/32 : Loss = 0.017006592825055122\n",
      "Epoch 16, Batch 29/32 : Loss = 0.025431480258703232\n",
      "Epoch 16, Batch 30/32 : Loss = 0.04409921169281006\n",
      "Epoch 16, Batch 31/32 : Loss = 0.04643932729959488\n",
      "Epoch 16 finished in 0.04241248369216919 minutes\n",
      "Epoch 16 training_loss = 0.059482429176568985\n",
      "---CC----D----E----g---mm-----\"--m------F---<<----Q----8---22--- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "--77---n----DD----P----n----t--w-----d---\\\\---Q----aa---RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----X----7---0---j---@----SS---Z----L---4----C----m-------M----- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "--77---n----DD----PP----n----t--w-----d----\\---QQ----a----RR---- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 16 val_loss = 0.49382567405700684, word_accuracy = 0.64\n",
      "Epoch 17, Batch 0/32 : Loss = 0.018430614843964577\n",
      "Epoch 17, Batch 1/32 : Loss = 0.04442554712295532\n",
      "Epoch 17, Batch 2/32 : Loss = 0.046345654875040054\n",
      "Epoch 17, Batch 3/32 : Loss = 0.030070433393120766\n",
      "Epoch 17, Batch 4/32 : Loss = 0.042052727192640305\n",
      "Epoch 17, Batch 5/32 : Loss = 0.08375178277492523\n",
      "Epoch 17, Batch 6/32 : Loss = 0.018885575234889984\n",
      "Epoch 17, Batch 7/32 : Loss = 0.05443980544805527\n",
      "Epoch 17, Batch 8/32 : Loss = 0.025364380329847336\n",
      "Epoch 17, Batch 9/32 : Loss = 0.02280591055750847\n",
      "Epoch 17, Batch 10/32 : Loss = 0.10276344418525696\n",
      "Epoch 17, Batch 11/32 : Loss = 0.03885553032159805\n",
      "Epoch 17, Batch 12/32 : Loss = 0.07940711826086044\n",
      "Epoch 17, Batch 13/32 : Loss = 0.01883750408887863\n",
      "Epoch 17, Batch 14/32 : Loss = 0.014514004811644554\n",
      "Epoch 17, Batch 15/32 : Loss = 0.01871541514992714\n",
      "Epoch 17, Batch 16/32 : Loss = 0.044513948261737823\n",
      "Epoch 17, Batch 17/32 : Loss = 0.028079191222786903\n",
      "Epoch 17, Batch 18/32 : Loss = 0.08081786334514618\n",
      "Epoch 17, Batch 19/32 : Loss = 0.0237762238830328\n",
      "Epoch 17, Batch 20/32 : Loss = 0.042245738208293915\n",
      "Epoch 17, Batch 21/32 : Loss = 0.022849177941679955\n",
      "Epoch 17, Batch 22/32 : Loss = 0.04642552137374878\n",
      "Epoch 17, Batch 23/32 : Loss = 0.10802074521780014\n",
      "Epoch 17, Batch 24/32 : Loss = 0.018012434244155884\n",
      "Epoch 17, Batch 25/32 : Loss = 0.20068728923797607\n",
      "Epoch 17, Batch 26/32 : Loss = 0.012465910986065865\n",
      "Epoch 17, Batch 27/32 : Loss = 0.012946134433150291\n",
      "Epoch 17, Batch 28/32 : Loss = 0.029626600444316864\n",
      "Epoch 17, Batch 29/32 : Loss = 0.027275729924440384\n",
      "Epoch 17, Batch 30/32 : Loss = 0.10891922563314438\n",
      "Epoch 17, Batch 31/32 : Loss = 0.03544877469539642\n",
      "Epoch 17 finished in 0.04231792688369751 minutes\n",
      "Epoch 17 training_loss = 0.04725328087806702\n",
      "------z-----0-----\"-----G------/-----~-----$$-----cc----11---jj--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---------3------\\----$$----->>-------S------\\----MM--------ii--B------ => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "------Q------33-----g-----I--zz---#------YY---:---]---q------+----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----C-----D-----E----g----m------\"--mm------F---<<-----Q----8---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 17 val_loss = 0.510118305683136, word_accuracy = 0.64\n",
      "Epoch 18, Batch 0/32 : Loss = 0.06951219588518143\n",
      "Epoch 18, Batch 1/32 : Loss = 0.05751107260584831\n",
      "Epoch 18, Batch 2/32 : Loss = 0.04095102846622467\n",
      "Epoch 18, Batch 3/32 : Loss = 0.07838509976863861\n",
      "Epoch 18, Batch 4/32 : Loss = 0.01801912859082222\n",
      "Epoch 18, Batch 5/32 : Loss = 0.06424704939126968\n",
      "Epoch 18, Batch 6/32 : Loss = 0.017159968614578247\n",
      "Epoch 18, Batch 7/32 : Loss = 0.02082216553390026\n",
      "Epoch 18, Batch 8/32 : Loss = 0.012423175387084484\n",
      "Epoch 18, Batch 9/32 : Loss = 0.016757596284151077\n",
      "Epoch 18, Batch 10/32 : Loss = 0.02808573842048645\n",
      "Epoch 18, Batch 11/32 : Loss = 0.016320373862981796\n",
      "Epoch 18, Batch 12/32 : Loss = 0.016916194930672646\n",
      "Epoch 18, Batch 13/32 : Loss = 0.2034638673067093\n",
      "Epoch 18, Batch 14/32 : Loss = 0.018327906727790833\n",
      "Epoch 18, Batch 15/32 : Loss = 0.024144353345036507\n",
      "Epoch 18, Batch 16/32 : Loss = 0.08779770135879517\n",
      "Epoch 18, Batch 17/32 : Loss = 0.02746741846203804\n",
      "Epoch 18, Batch 18/32 : Loss = 0.12850748002529144\n",
      "Epoch 18, Batch 19/32 : Loss = 0.041824813932180405\n",
      "Epoch 18, Batch 20/32 : Loss = 0.02990865521132946\n",
      "Epoch 18, Batch 21/32 : Loss = 0.07824951410293579\n",
      "Epoch 18, Batch 22/32 : Loss = 0.02285599149763584\n",
      "Epoch 18, Batch 23/32 : Loss = 0.2802621126174927\n",
      "Epoch 18, Batch 24/32 : Loss = 0.04088205099105835\n",
      "Epoch 18, Batch 25/32 : Loss = 0.02973543293774128\n",
      "Epoch 18, Batch 26/32 : Loss = 0.26737046241760254\n",
      "Epoch 18, Batch 27/32 : Loss = 0.015247800387442112\n",
      "Epoch 18, Batch 28/32 : Loss = 0.019218524917960167\n",
      "Epoch 18, Batch 29/32 : Loss = 0.02355601079761982\n",
      "Epoch 18, Batch 30/32 : Loss = 0.01569833606481552\n",
      "Epoch 18, Batch 31/32 : Loss = 0.013603093102574348\n",
      "Epoch 18 finished in 0.043352301915486655 minutes\n",
      "Epoch 18 training_loss = 0.05825958400964737\n",
      "--{--BB-----Y----R----a----y---h---#----22--->>----E---44----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----J---;--q----++---/---z---y---U-----%%-----U-----1----x---- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "----8-----K----l--ZZ----55----p-----$$----a----}---w-----,---- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----0----Q---6----<----<---(---T---N----5---==---P---(--m----- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 18 val_loss = 0.5293563008308411, word_accuracy = 0.6\n",
      "Epoch 19, Batch 0/32 : Loss = 0.030897323042154312\n",
      "Epoch 19, Batch 1/32 : Loss = 0.014986809343099594\n",
      "Epoch 19, Batch 2/32 : Loss = 0.01527328323572874\n",
      "Epoch 19, Batch 3/32 : Loss = 0.024457059800624847\n",
      "Epoch 19, Batch 4/32 : Loss = 0.02593560703098774\n",
      "Epoch 19, Batch 5/32 : Loss = 0.034034449607133865\n",
      "Epoch 19, Batch 6/32 : Loss = 0.06126843020319939\n",
      "Epoch 19, Batch 7/32 : Loss = 0.012619128450751305\n",
      "Epoch 19, Batch 8/32 : Loss = 0.04747852310538292\n",
      "Epoch 19, Batch 9/32 : Loss = 0.01826091855764389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 10/32 : Loss = 0.01851266622543335\n",
      "Epoch 19, Batch 11/32 : Loss = 0.1398165076971054\n",
      "Epoch 19, Batch 12/32 : Loss = 0.037405237555503845\n",
      "Epoch 19, Batch 13/32 : Loss = 0.013355277478694916\n",
      "Epoch 19, Batch 14/32 : Loss = 0.11632584035396576\n",
      "Epoch 19, Batch 15/32 : Loss = 0.15068380534648895\n",
      "Epoch 19, Batch 16/32 : Loss = 0.07509665936231613\n",
      "Epoch 19, Batch 17/32 : Loss = 0.019024036824703217\n",
      "Epoch 19, Batch 18/32 : Loss = 0.10234421491622925\n",
      "Epoch 19, Batch 19/32 : Loss = 0.015141695737838745\n",
      "Epoch 19, Batch 20/32 : Loss = 0.030203353613615036\n",
      "Epoch 19, Batch 21/32 : Loss = 0.078713059425354\n",
      "Epoch 19, Batch 22/32 : Loss = 0.12654556334018707\n",
      "Epoch 19, Batch 23/32 : Loss = 0.13168081641197205\n",
      "Epoch 19, Batch 24/32 : Loss = 0.01684965193271637\n",
      "Epoch 19, Batch 25/32 : Loss = 0.013646194711327553\n",
      "Epoch 19, Batch 26/32 : Loss = 0.019188838079571724\n",
      "Epoch 19, Batch 27/32 : Loss = 0.016220280900597572\n",
      "Epoch 19, Batch 28/32 : Loss = 0.07310077548027039\n",
      "Epoch 19, Batch 29/32 : Loss = 0.02027355507016182\n",
      "Epoch 19, Batch 30/32 : Loss = 0.024310190230607986\n",
      "Epoch 19, Batch 31/32 : Loss = 0.0826088935136795\n",
      "Epoch 19 finished in 0.041664524873097734 minutes\n",
      "Epoch 19 training_loss = 0.04928436875343323\n",
      "-----X----7----0---j---@@-----S----ZZ----L----4----C-----m-------MM------ => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "------dd-----:--XX----99----ee-----a-----F---,--88---------V-----R------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "---55----->----:--*----Y-----A----'--O------D-----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----55---->>----:--*----Y----A----'--O------D-----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 19 val_loss = 0.5534600615501404, word_accuracy = 0.62\n",
      "Epoch 20, Batch 0/32 : Loss = 0.012205623090267181\n",
      "Epoch 20, Batch 1/32 : Loss = 0.030906900763511658\n",
      "Epoch 20, Batch 2/32 : Loss = 0.032670751214027405\n",
      "Epoch 20, Batch 3/32 : Loss = 0.06639251112937927\n",
      "Epoch 20, Batch 4/32 : Loss = 0.025305641815066338\n",
      "Epoch 20, Batch 5/32 : Loss = 0.017145391553640366\n",
      "Epoch 20, Batch 6/32 : Loss = 0.01592176780104637\n",
      "Epoch 20, Batch 7/32 : Loss = 0.04439719021320343\n",
      "Epoch 20, Batch 8/32 : Loss = 0.14772891998291016\n",
      "Epoch 20, Batch 9/32 : Loss = 0.04273906350135803\n",
      "Epoch 20, Batch 10/32 : Loss = 0.05033339932560921\n",
      "Epoch 20, Batch 11/32 : Loss = 0.0159396193921566\n",
      "Epoch 20, Batch 12/32 : Loss = 0.04400625824928284\n",
      "Epoch 20, Batch 13/32 : Loss = 0.07403744012117386\n",
      "Epoch 20, Batch 14/32 : Loss = 0.02781195566058159\n",
      "Epoch 20, Batch 15/32 : Loss = 0.030093811452388763\n",
      "Epoch 20, Batch 16/32 : Loss = 0.014295533299446106\n",
      "Epoch 20, Batch 17/32 : Loss = 0.025058433413505554\n",
      "Epoch 20, Batch 18/32 : Loss = 0.020320920273661613\n",
      "Epoch 20, Batch 19/32 : Loss = 0.01318071037530899\n",
      "Epoch 20, Batch 20/32 : Loss = 0.03294335678219795\n",
      "Epoch 20, Batch 21/32 : Loss = 0.08592260628938675\n",
      "Epoch 20, Batch 22/32 : Loss = 0.029435355216264725\n",
      "Epoch 20, Batch 23/32 : Loss = 0.04301796853542328\n",
      "Epoch 20, Batch 24/32 : Loss = 0.08567731082439423\n",
      "Epoch 20, Batch 25/32 : Loss = 0.08256885409355164\n",
      "Epoch 20, Batch 26/32 : Loss = 0.0881119966506958\n",
      "Epoch 20, Batch 27/32 : Loss = 0.043112095445394516\n",
      "Epoch 20, Batch 28/32 : Loss = 0.017934471368789673\n",
      "Epoch 20, Batch 29/32 : Loss = 0.09077613800764084\n",
      "Epoch 20, Batch 30/32 : Loss = 0.01587638631463051\n",
      "Epoch 20, Batch 31/32 : Loss = 0.02049833908677101\n",
      "Epoch 20 finished in 0.041119794050852455 minutes\n",
      "Epoch 20 training_loss = 0.04396563768386841\n",
      "----0----JJ---!--(--;;--A----33---,,-'--)---r--r---77---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "-----0----JJ---!--(--;--A-----3----,--'-)---r---r---7---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "-----<----vv---O-----T----`--44---V----[--0-------QQ----- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----X---7---0--j--@@----S---Z---L---4---C----m------M---- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 20 val_loss = 0.5455056428909302, word_accuracy = 0.65\n",
      "Epoch 21, Batch 0/32 : Loss = 0.01071447879076004\n",
      "Epoch 21, Batch 1/32 : Loss = 0.030977142974734306\n",
      "Epoch 21, Batch 2/32 : Loss = 0.02089933305978775\n",
      "Epoch 21, Batch 3/32 : Loss = 0.013416006229817867\n",
      "Epoch 21, Batch 4/32 : Loss = 0.03231530636548996\n",
      "Epoch 21, Batch 5/32 : Loss = 0.08660205453634262\n",
      "Epoch 21, Batch 6/32 : Loss = 0.01257174089550972\n",
      "Epoch 21, Batch 7/32 : Loss = 0.060101017355918884\n",
      "Epoch 21, Batch 8/32 : Loss = 0.02925606071949005\n",
      "Epoch 21, Batch 9/32 : Loss = 0.015184512361884117\n",
      "Epoch 21, Batch 10/32 : Loss = 0.011160556226968765\n",
      "Epoch 21, Batch 11/32 : Loss = 0.09878602623939514\n",
      "Epoch 21, Batch 12/32 : Loss = 0.061839550733566284\n",
      "Epoch 21, Batch 13/32 : Loss = 0.013353710994124413\n",
      "Epoch 21, Batch 14/32 : Loss = 0.012973434291779995\n",
      "Epoch 21, Batch 15/32 : Loss = 0.04454989731311798\n",
      "Epoch 21, Batch 16/32 : Loss = 0.011522579938173294\n",
      "Epoch 21, Batch 17/32 : Loss = 0.017477981746196747\n",
      "Epoch 21, Batch 18/32 : Loss = 0.02902708761394024\n",
      "Epoch 21, Batch 19/32 : Loss = 0.013720355927944183\n",
      "Epoch 21, Batch 20/32 : Loss = 0.04890625551342964\n",
      "Epoch 21, Batch 21/32 : Loss = 0.05002133548259735\n",
      "Epoch 21, Batch 22/32 : Loss = 0.07039868831634521\n",
      "Epoch 21, Batch 23/32 : Loss = 0.06588059663772583\n",
      "Epoch 21, Batch 24/32 : Loss = 0.014007695019245148\n",
      "Epoch 21, Batch 25/32 : Loss = 0.04573676362633705\n",
      "Epoch 21, Batch 26/32 : Loss = 0.019723352044820786\n",
      "Epoch 21, Batch 27/32 : Loss = 0.013069864362478256\n",
      "Epoch 21, Batch 28/32 : Loss = 0.05596637725830078\n",
      "Epoch 21, Batch 29/32 : Loss = 0.018467262387275696\n",
      "Epoch 21, Batch 30/32 : Loss = 0.0872495248913765\n",
      "Epoch 21, Batch 31/32 : Loss = 0.06337125599384308\n",
      "Epoch 21 finished in 0.041191967328389485 minutes\n",
      "Epoch 21 training_loss = 0.036105960607528687\n",
      "------Q------3-----g----I--z----#-----Y----:--]--qq----++---***- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---BB----.---Y---l---W-------6----FF---hh----XX---'--Y----22---- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----kk---$----l--F----DD----ee----hh---]--kk---0----\\\\---X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---55--->>---:--*----Y---AA---'--O-----D----*---#----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 21 val_loss = 0.5391204953193665, word_accuracy = 0.6\n",
      "Epoch 22, Batch 0/32 : Loss = 0.008140059188008308\n",
      "Epoch 22, Batch 1/32 : Loss = 0.0119332205504179\n",
      "Epoch 22, Batch 2/32 : Loss = 0.012235925532877445\n",
      "Epoch 22, Batch 3/32 : Loss = 0.043168459087610245\n",
      "Epoch 22, Batch 4/32 : Loss = 0.009825891815125942\n",
      "Epoch 22, Batch 5/32 : Loss = 0.016853177919983864\n",
      "Epoch 22, Batch 6/32 : Loss = 0.02146758884191513\n",
      "Epoch 22, Batch 7/32 : Loss = 0.16844245791435242\n",
      "Epoch 22, Batch 8/32 : Loss = 0.010292728431522846\n",
      "Epoch 22, Batch 9/32 : Loss = 0.013745280914008617\n",
      "Epoch 22, Batch 10/32 : Loss = 0.016252364963293076\n",
      "Epoch 22, Batch 11/32 : Loss = 0.048445120453834534\n",
      "Epoch 22, Batch 12/32 : Loss = 0.011256708763539791\n",
      "Epoch 22, Batch 13/32 : Loss = 0.05001724511384964\n",
      "Epoch 22, Batch 14/32 : Loss = 0.054260414093732834\n",
      "Epoch 22, Batch 15/32 : Loss = 0.05232847109436989\n",
      "Epoch 22, Batch 16/32 : Loss = 0.012328031472861767\n",
      "Epoch 22, Batch 17/32 : Loss = 0.022046785801649094\n",
      "Epoch 22, Batch 18/32 : Loss = 0.013349752873182297\n",
      "Epoch 22, Batch 19/32 : Loss = 0.04919978603720665\n",
      "Epoch 22, Batch 20/32 : Loss = 0.012452075257897377\n",
      "Epoch 22, Batch 21/32 : Loss = 0.03976896032691002\n",
      "Epoch 22, Batch 22/32 : Loss = 0.11322051286697388\n",
      "Epoch 22, Batch 23/32 : Loss = 0.01343641895800829\n",
      "Epoch 22, Batch 24/32 : Loss = 0.017256032675504684\n",
      "Epoch 22, Batch 25/32 : Loss = 0.010506905615329742\n",
      "Epoch 22, Batch 26/32 : Loss = 0.02360629104077816\n",
      "Epoch 22, Batch 27/32 : Loss = 0.014653056859970093\n",
      "Epoch 22, Batch 28/32 : Loss = 0.1169532835483551\n",
      "Epoch 22, Batch 29/32 : Loss = 0.046536948531866074\n",
      "Epoch 22, Batch 30/32 : Loss = 0.03629695251584053\n",
      "Epoch 22, Batch 31/32 : Loss = 0.01351325772702694\n",
      "Epoch 22 finished in 0.042070305347442626 minutes\n",
      "Epoch 22 training_loss = 0.03508324548602104\n",
      "----55---->>----:--*----Y----A----'--O------D-----*---#-----O-----g------ => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "-----k----$-----|---'-,--99----Y-----N------W-------m-------TT----8------ => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "------0-----JJ-----!--((---;---A------33----,---'--)----r----r---77------ => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---OO------c-----++-----bb-----|--\"-----bb-----66-----.---Q-------------- => Oc+b|\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 22 val_loss = 0.5352720618247986, word_accuracy = 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 0/32 : Loss = 0.01592218317091465\n",
      "Epoch 23, Batch 1/32 : Loss = 0.02434641495347023\n",
      "Epoch 23, Batch 2/32 : Loss = 0.02632610872387886\n",
      "Epoch 23, Batch 3/32 : Loss = 0.009746566414833069\n",
      "Epoch 23, Batch 4/32 : Loss = 0.007985658943653107\n",
      "Epoch 23, Batch 5/32 : Loss = 0.013876061886548996\n",
      "Epoch 23, Batch 6/32 : Loss = 0.018599864095449448\n",
      "Epoch 23, Batch 7/32 : Loss = 0.030043262988328934\n",
      "Epoch 23, Batch 8/32 : Loss = 0.02110685035586357\n",
      "Epoch 23, Batch 9/32 : Loss = 0.070782870054245\n",
      "Epoch 23, Batch 10/32 : Loss = 0.015270286239683628\n",
      "Epoch 23, Batch 11/32 : Loss = 0.02290547452867031\n",
      "Epoch 23, Batch 12/32 : Loss = 0.016522802412509918\n",
      "Epoch 23, Batch 13/32 : Loss = 0.007376984693109989\n",
      "Epoch 23, Batch 14/32 : Loss = 0.04929715767502785\n",
      "Epoch 23, Batch 15/32 : Loss = 0.037638887763023376\n",
      "Epoch 23, Batch 16/32 : Loss = 0.04477442055940628\n",
      "Epoch 23, Batch 17/32 : Loss = 0.018798381090164185\n",
      "Epoch 23, Batch 18/32 : Loss = 0.12630046904087067\n",
      "Epoch 23, Batch 19/32 : Loss = 0.09787224978208542\n",
      "Epoch 23, Batch 20/32 : Loss = 0.013210469856858253\n",
      "Epoch 23, Batch 21/32 : Loss = 0.016414768993854523\n",
      "Epoch 23, Batch 22/32 : Loss = 0.0763562023639679\n",
      "Epoch 23, Batch 23/32 : Loss = 0.010749146342277527\n",
      "Epoch 23, Batch 24/32 : Loss = 0.014305458404123783\n",
      "Epoch 23, Batch 25/32 : Loss = 0.0163319930434227\n",
      "Epoch 23, Batch 26/32 : Loss = 0.052545685321092606\n",
      "Epoch 23, Batch 27/32 : Loss = 0.12159489840269089\n",
      "Epoch 23, Batch 28/32 : Loss = 0.053933918476104736\n",
      "Epoch 23, Batch 29/32 : Loss = 0.020628061145544052\n",
      "Epoch 23, Batch 30/32 : Loss = 0.011890213936567307\n",
      "Epoch 23, Batch 31/32 : Loss = 0.19521483778953552\n",
      "Epoch 23 finished in 0.042375179131825765 minutes\n",
      "Epoch 23 training_loss = 0.035593755543231964\n",
      "----4---r---{--%%-----/--'--)--w-----&-----N-----+---P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----X---7---0--j--@@----S---Z---LL--4----C---mm-----M----- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---0----JJ---!--(--;;--A----33---,--'--)--r---r---77------ => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----k----O-----/--,--y---c----*---1----P---}--##---BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 23 val_loss = 0.54152911901474, word_accuracy = 0.62\n",
      "Epoch 24, Batch 0/32 : Loss = 0.012643642723560333\n",
      "Epoch 24, Batch 1/32 : Loss = 0.09016714990139008\n",
      "Epoch 24, Batch 2/32 : Loss = 0.00725330226123333\n",
      "Epoch 24, Batch 3/32 : Loss = 0.07920265942811966\n",
      "Epoch 24, Batch 4/32 : Loss = 0.046321433037519455\n",
      "Epoch 24, Batch 5/32 : Loss = 0.00886319950222969\n",
      "Epoch 24, Batch 6/32 : Loss = 0.02155875787138939\n",
      "Epoch 24, Batch 7/32 : Loss = 0.09466083347797394\n",
      "Epoch 24, Batch 8/32 : Loss = 0.014236992225050926\n",
      "Epoch 24, Batch 9/32 : Loss = 0.02970234490931034\n",
      "Epoch 24, Batch 10/32 : Loss = 0.007538983598351479\n",
      "Epoch 24, Batch 11/32 : Loss = 0.011596273630857468\n",
      "Epoch 24, Batch 12/32 : Loss = 0.07745718955993652\n",
      "Epoch 24, Batch 13/32 : Loss = 0.08332396298646927\n",
      "Epoch 24, Batch 14/32 : Loss = 0.01230627577751875\n",
      "Epoch 24, Batch 15/32 : Loss = 0.023636233061552048\n",
      "Epoch 24, Batch 16/32 : Loss = 0.009086193516850471\n",
      "Epoch 24, Batch 17/32 : Loss = 0.007837597280740738\n",
      "Epoch 24, Batch 18/32 : Loss = 0.06696582585573196\n",
      "Epoch 24, Batch 19/32 : Loss = 0.037776172161102295\n",
      "Epoch 24, Batch 20/32 : Loss = 0.09141657501459122\n",
      "Epoch 24, Batch 21/32 : Loss = 0.012317830696702003\n",
      "Epoch 24, Batch 22/32 : Loss = 0.00949927605688572\n",
      "Epoch 24, Batch 23/32 : Loss = 0.04803438112139702\n",
      "Epoch 24, Batch 24/32 : Loss = 0.02440178394317627\n",
      "Epoch 24, Batch 25/32 : Loss = 0.01396264135837555\n",
      "Epoch 24, Batch 26/32 : Loss = 0.012653425335884094\n",
      "Epoch 24, Batch 27/32 : Loss = 0.011638663709163666\n",
      "Epoch 24, Batch 28/32 : Loss = 0.16776446998119354\n",
      "Epoch 24, Batch 29/32 : Loss = 0.016955723986029625\n",
      "Epoch 24, Batch 30/32 : Loss = 0.015215648338198662\n",
      "Epoch 24, Batch 31/32 : Loss = 0.04760931804776192\n",
      "Epoch 24 finished in 0.04185274839401245 minutes\n",
      "Epoch 24 training_loss = 0.03765289485454559\n",
      "----00---QQ---66---<----<<---(--T----N----5---=----P-------------- => 0Q6<<(TN5=P, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----55----99---gg----mm------JJ---uu----x---C----x--..-dd----\\---- => 59gmJuxCx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "----dd----!--NN-----rr---A----j--**---$----3-----hh---55----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----8-----KK-----l--Z-----55-----p-----$-----a-----}---w------,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "Epoch 24 val_loss = 0.5220263004302979, word_accuracy = 0.7\n",
      "Epoch 25, Batch 0/32 : Loss = 0.050364166498184204\n",
      "Epoch 25, Batch 1/32 : Loss = 0.013606810942292213\n",
      "Epoch 25, Batch 2/32 : Loss = 0.007241194136440754\n",
      "Epoch 25, Batch 3/32 : Loss = 0.0824015662074089\n",
      "Epoch 25, Batch 4/32 : Loss = 0.05276835709810257\n",
      "Epoch 25, Batch 5/32 : Loss = 0.0933765321969986\n",
      "Epoch 25, Batch 6/32 : Loss = 0.008596401661634445\n",
      "Epoch 25, Batch 7/32 : Loss = 0.008950944058597088\n",
      "Epoch 25, Batch 8/32 : Loss = 0.020293699577450752\n",
      "Epoch 25, Batch 9/32 : Loss = 0.009763067588210106\n",
      "Epoch 25, Batch 10/32 : Loss = 0.00696011446416378\n",
      "Epoch 25, Batch 11/32 : Loss = 0.04117502272129059\n",
      "Epoch 25, Batch 12/32 : Loss = 0.04087798297405243\n",
      "Epoch 25, Batch 13/32 : Loss = 0.0859265998005867\n",
      "Epoch 25, Batch 14/32 : Loss = 0.05752229318022728\n",
      "Epoch 25, Batch 15/32 : Loss = 0.06664539873600006\n",
      "Epoch 25, Batch 16/32 : Loss = 0.08369936794042587\n",
      "Epoch 25, Batch 17/32 : Loss = 0.0075745172798633575\n",
      "Epoch 25, Batch 18/32 : Loss = 0.10233404487371445\n",
      "Epoch 25, Batch 19/32 : Loss = 0.007509334944188595\n",
      "Epoch 25, Batch 20/32 : Loss = 0.04095171391963959\n",
      "Epoch 25, Batch 21/32 : Loss = 0.017630422487854958\n",
      "Epoch 25, Batch 22/32 : Loss = 0.0675584226846695\n",
      "Epoch 25, Batch 23/32 : Loss = 0.009961407631635666\n",
      "Epoch 25, Batch 24/32 : Loss = 0.011535716243088245\n",
      "Epoch 25, Batch 25/32 : Loss = 0.027023913338780403\n",
      "Epoch 25, Batch 26/32 : Loss = 0.01947501301765442\n",
      "Epoch 25, Batch 27/32 : Loss = 0.09242871403694153\n",
      "Epoch 25, Batch 28/32 : Loss = 0.009942807257175446\n",
      "Epoch 25, Batch 29/32 : Loss = 0.04322049394249916\n",
      "Epoch 25, Batch 30/32 : Loss = 0.009550289250910282\n",
      "Epoch 25, Batch 31/32 : Loss = 0.0677567645907402\n",
      "Epoch 25 finished in 0.04260917901992798 minutes\n",
      "Epoch 25 training_loss = 0.03872564807534218\n",
      "----Y----W------]-ii-\\---|---MM-----<----MM-----8---8--- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----Q-----3----gg---I--z---#----Y---:--]--qq---+----**- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "--00----c----++----b----I--\"---b----66---..--Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----dd---!--N-----r--AA---j--*---$---3----hh---5----n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 25 val_loss = 0.5415704846382141, word_accuracy = 0.62\n",
      "Epoch 26, Batch 0/32 : Loss = 0.009691701270639896\n",
      "Epoch 26, Batch 1/32 : Loss = 0.024055656045675278\n",
      "Epoch 26, Batch 2/32 : Loss = 0.007938733324408531\n",
      "Epoch 26, Batch 3/32 : Loss = 0.013702129945158958\n",
      "Epoch 26, Batch 4/32 : Loss = 0.01785128563642502\n",
      "Epoch 26, Batch 5/32 : Loss = 0.0571756549179554\n",
      "Epoch 26, Batch 6/32 : Loss = 0.00964297167956829\n",
      "Epoch 26, Batch 7/32 : Loss = 0.0532890260219574\n",
      "Epoch 26, Batch 8/32 : Loss = 0.029997587203979492\n",
      "Epoch 26, Batch 9/32 : Loss = 0.011542847380042076\n",
      "Epoch 26, Batch 10/32 : Loss = 0.012478598393499851\n",
      "Epoch 26, Batch 11/32 : Loss = 0.0157360527664423\n",
      "Epoch 26, Batch 12/32 : Loss = 0.007849871180951595\n",
      "Epoch 26, Batch 13/32 : Loss = 0.011942298151552677\n",
      "Epoch 26, Batch 14/32 : Loss = 0.013080969452857971\n",
      "Epoch 26, Batch 15/32 : Loss = 0.009750084951519966\n",
      "Epoch 26, Batch 16/32 : Loss = 0.04008170962333679\n",
      "Epoch 26, Batch 17/32 : Loss = 0.052186742424964905\n",
      "Epoch 26, Batch 18/32 : Loss = 0.009412127546966076\n",
      "Epoch 26, Batch 19/32 : Loss = 0.008675307966768742\n",
      "Epoch 26, Batch 20/32 : Loss = 0.02399599365890026\n",
      "Epoch 26, Batch 21/32 : Loss = 0.22117498517036438\n",
      "Epoch 26, Batch 22/32 : Loss = 0.008877668529748917\n",
      "Epoch 26, Batch 23/32 : Loss = 0.012458225712180138\n",
      "Epoch 26, Batch 24/32 : Loss = 0.009418047964572906\n",
      "Epoch 26, Batch 25/32 : Loss = 0.016615064814686775\n",
      "Epoch 26, Batch 26/32 : Loss = 0.010925939306616783\n",
      "Epoch 26, Batch 27/32 : Loss = 0.00974925234913826\n",
      "Epoch 26, Batch 28/32 : Loss = 0.0621647983789444\n",
      "Epoch 26, Batch 29/32 : Loss = 0.016409780830144882\n",
      "Epoch 26, Batch 30/32 : Loss = 0.11907665431499481\n",
      "Epoch 26, Batch 31/32 : Loss = 0.2918037474155426\n",
      "Epoch 26 finished in 0.04495055278142293 minutes\n",
      "Epoch 26 training_loss = 0.030953358858823776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---/----MMM------o----EE----^^---3-----X----/---&&----66-----X------ => /MoE^3X/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----{---B------Y----R----aa----y---hh---#-----2---->----EE----44---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "---J---;--qq----++---/---z----y---UU-----%%------U-----11---x---__-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "---\"----]---t---44----e-----^----W--------Q-------44--->>-----gg---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 26 val_loss = 0.5310602188110352, word_accuracy = 0.69\n",
      "Epoch 27, Batch 0/32 : Loss = 0.016999103128910065\n",
      "Epoch 27, Batch 1/32 : Loss = 0.01571406051516533\n",
      "Epoch 27, Batch 2/32 : Loss = 0.04085012525320053\n",
      "Epoch 27, Batch 3/32 : Loss = 0.010768542066216469\n",
      "Epoch 27, Batch 4/32 : Loss = 0.012272440828382969\n",
      "Epoch 27, Batch 5/32 : Loss = 0.04899726063013077\n",
      "Epoch 27, Batch 6/32 : Loss = 0.1122015044093132\n",
      "Epoch 27, Batch 7/32 : Loss = 0.02930767834186554\n",
      "Epoch 27, Batch 8/32 : Loss = 0.12850235402584076\n",
      "Epoch 27, Batch 9/32 : Loss = 0.01047188974916935\n",
      "Epoch 27, Batch 10/32 : Loss = 0.01612924411892891\n",
      "Epoch 27, Batch 11/32 : Loss = 0.019576378166675568\n",
      "Epoch 27, Batch 12/32 : Loss = 0.06866148114204407\n",
      "Epoch 27, Batch 13/32 : Loss = 0.010967005044221878\n",
      "Epoch 27, Batch 14/32 : Loss = 0.03475525230169296\n",
      "Epoch 27, Batch 15/32 : Loss = 0.0112066101282835\n",
      "Epoch 27, Batch 16/32 : Loss = 0.0092119500041008\n",
      "Epoch 27, Batch 17/32 : Loss = 0.04008694738149643\n",
      "Epoch 27, Batch 18/32 : Loss = 0.25741615891456604\n",
      "Epoch 27, Batch 19/32 : Loss = 0.0284816175699234\n",
      "Epoch 27, Batch 20/32 : Loss = 0.008084772154688835\n",
      "Epoch 27, Batch 21/32 : Loss = 0.017244882881641388\n",
      "Epoch 27, Batch 22/32 : Loss = 0.007389476988464594\n",
      "Epoch 27, Batch 23/32 : Loss = 0.10733707249164581\n",
      "Epoch 27, Batch 24/32 : Loss = 0.019776249304413795\n",
      "Epoch 27, Batch 25/32 : Loss = 0.0070473672822117805\n",
      "Epoch 27, Batch 26/32 : Loss = 0.07644842565059662\n",
      "Epoch 27, Batch 27/32 : Loss = 0.013219019398093224\n",
      "Epoch 27, Batch 28/32 : Loss = 0.01826336234807968\n",
      "Epoch 27, Batch 29/32 : Loss = 0.01807529106736183\n",
      "Epoch 27, Batch 30/32 : Loss = 0.023988589644432068\n",
      "Epoch 27, Batch 31/32 : Loss = 0.03286299854516983\n",
      "Epoch 27 finished in 0.043464255332946775 minutes\n",
      "Epoch 27 training_loss = 0.03995373472571373\n",
      "------zz-----0-----\"----GG-------//---~~-----$$------c-----11---jj--tt---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "-----WW------=----22----+---EE----1----n----T----X----r--CC----a----I----- => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "-----o-----\"---}---!---BB-----r----&------9----;--`---OO------w------}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----k----$----||---'-,--99----Y-----N------WW-------m-------TT----8----.-- => k$|',9YNWmT8., Ground Truth is k$|',9YNWmT8\n",
      "Epoch 27 val_loss = 0.545465886592865, word_accuracy = 0.68\n",
      "Epoch 28, Batch 0/32 : Loss = 0.13092321157455444\n",
      "Epoch 28, Batch 1/32 : Loss = 0.011445536278188229\n",
      "Epoch 28, Batch 2/32 : Loss = 0.017215002328157425\n",
      "Epoch 28, Batch 3/32 : Loss = 0.021709753200411797\n",
      "Epoch 28, Batch 4/32 : Loss = 0.028579946607351303\n",
      "Epoch 28, Batch 5/32 : Loss = 0.015477826818823814\n",
      "Epoch 28, Batch 6/32 : Loss = 0.02747455984354019\n",
      "Epoch 28, Batch 7/32 : Loss = 0.05759594589471817\n",
      "Epoch 28, Batch 8/32 : Loss = 0.03773703798651695\n",
      "Epoch 28, Batch 9/32 : Loss = 0.006159056909382343\n",
      "Epoch 28, Batch 10/32 : Loss = 0.007512369193136692\n",
      "Epoch 28, Batch 11/32 : Loss = 0.006393905729055405\n",
      "Epoch 28, Batch 12/32 : Loss = 0.016456909477710724\n",
      "Epoch 28, Batch 13/32 : Loss = 0.02559264563024044\n",
      "Epoch 28, Batch 14/32 : Loss = 0.05527839437127113\n",
      "Epoch 28, Batch 15/32 : Loss = 0.022161047905683517\n",
      "Epoch 28, Batch 16/32 : Loss = 0.010710562579333782\n",
      "Epoch 28, Batch 17/32 : Loss = 0.00781992170959711\n",
      "Epoch 28, Batch 18/32 : Loss = 0.07111258804798126\n",
      "Epoch 28, Batch 19/32 : Loss = 0.09498231112957001\n",
      "Epoch 28, Batch 20/32 : Loss = 0.014398057013750076\n",
      "Epoch 28, Batch 21/32 : Loss = 0.023729166015982628\n",
      "Epoch 28, Batch 22/32 : Loss = 0.008909245952963829\n",
      "Epoch 28, Batch 23/32 : Loss = 0.057020075619220734\n",
      "Epoch 28, Batch 24/32 : Loss = 0.009802114218473434\n",
      "Epoch 28, Batch 25/32 : Loss = 0.03312632441520691\n",
      "Epoch 28, Batch 26/32 : Loss = 0.011026866734027863\n",
      "Epoch 28, Batch 27/32 : Loss = 0.056704770773649216\n",
      "Epoch 28, Batch 28/32 : Loss = 0.006680413149297237\n",
      "Epoch 28, Batch 29/32 : Loss = 0.007369425613433123\n",
      "Epoch 28, Batch 30/32 : Loss = 0.026920000091195107\n",
      "Epoch 28, Batch 31/32 : Loss = 0.01397546473890543\n",
      "Epoch 28 finished in 0.042588114738464355 minutes\n",
      "Epoch 28 training_loss = 0.029872186481952667\n",
      "----X----7----0---j---@-----S----Z----L----4----C----mm-----MM-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "-----d-----!--NN------r---A-----j--*----$-----3----hhh---55----nn----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----++--::--z----7----8-----dd----S-----v---5-----S----JJ----B------- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----\"----]---t---44----------^----W---------Q------44---->>-----g----- => \"]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 28 val_loss = 0.5609142780303955, word_accuracy = 0.62\n",
      "Epoch 29, Batch 0/32 : Loss = 0.013111989945173264\n",
      "Epoch 29, Batch 1/32 : Loss = 0.0102715864777565\n",
      "Epoch 29, Batch 2/32 : Loss = 0.05353035405278206\n",
      "Epoch 29, Batch 3/32 : Loss = 0.029902523383498192\n",
      "Epoch 29, Batch 4/32 : Loss = 0.007031350862234831\n",
      "Epoch 29, Batch 5/32 : Loss = 0.03679681569337845\n",
      "Epoch 29, Batch 6/32 : Loss = 0.011705953627824783\n",
      "Epoch 29, Batch 7/32 : Loss = 0.009361360222101212\n",
      "Epoch 29, Batch 8/32 : Loss = 0.010601525194942951\n",
      "Epoch 29, Batch 9/32 : Loss = 0.04763869196176529\n",
      "Epoch 29, Batch 10/32 : Loss = 0.024739131331443787\n",
      "Epoch 29, Batch 11/32 : Loss = 0.006490166299045086\n",
      "Epoch 29, Batch 12/32 : Loss = 0.016909215599298477\n",
      "Epoch 29, Batch 13/32 : Loss = 0.013750135898590088\n",
      "Epoch 29, Batch 14/32 : Loss = 0.024608328938484192\n",
      "Epoch 29, Batch 15/32 : Loss = 0.02225467748939991\n",
      "Epoch 29, Batch 16/32 : Loss = 0.010805321857333183\n",
      "Epoch 29, Batch 17/32 : Loss = 0.030605847015976906\n",
      "Epoch 29, Batch 18/32 : Loss = 0.08509333431720734\n",
      "Epoch 29, Batch 19/32 : Loss = 0.009937846101820469\n",
      "Epoch 29, Batch 20/32 : Loss = 0.02177797444164753\n",
      "Epoch 29, Batch 21/32 : Loss = 0.008082255721092224\n",
      "Epoch 29, Batch 22/32 : Loss = 0.04858173429965973\n",
      "Epoch 29, Batch 23/32 : Loss = 0.004977546166628599\n",
      "Epoch 29, Batch 24/32 : Loss = 0.03766631335020065\n",
      "Epoch 29, Batch 25/32 : Loss = 0.01849253475666046\n",
      "Epoch 29, Batch 26/32 : Loss = 0.015211135149002075\n",
      "Epoch 29, Batch 27/32 : Loss = 0.008936211466789246\n",
      "Epoch 29, Batch 28/32 : Loss = 0.0331081822514534\n",
      "Epoch 29, Batch 29/32 : Loss = 0.08438132703304291\n",
      "Epoch 29, Batch 30/32 : Loss = 0.036281440407037735\n",
      "Epoch 29, Batch 31/32 : Loss = 0.008276831358671188\n",
      "Epoch 29 finished in 0.04174129565556844 minutes\n",
      "Epoch 29 training_loss = 0.025499675422906876\n",
      "----JJ---;---qq----++----/---zz---y----U------%%------UU------1----x---__- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "-----k----$$----l--FF----DD-----ee-----hh----]---k----0-----\\----X-------- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "------C-----RR----;---9-----yy-----?----22----dd----ii---O------{{--!----- => CR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "-----C-----DD----E----gg----m------\"---m-------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 29 val_loss = 0.564976155757904, word_accuracy = 0.65\n",
      "Epoch 30, Batch 0/32 : Loss = 0.038425035774707794\n",
      "Epoch 30, Batch 1/32 : Loss = 0.013643213547766209\n",
      "Epoch 30, Batch 2/32 : Loss = 0.013656810857355595\n",
      "Epoch 30, Batch 3/32 : Loss = 0.008049351163208485\n",
      "Epoch 30, Batch 4/32 : Loss = 0.009265044704079628\n",
      "Epoch 30, Batch 5/32 : Loss = 0.014094268903136253\n",
      "Epoch 30, Batch 6/32 : Loss = 0.044901732355356216\n",
      "Epoch 30, Batch 7/32 : Loss = 0.02153458446264267\n",
      "Epoch 30, Batch 8/32 : Loss = 0.02255965769290924\n",
      "Epoch 30, Batch 9/32 : Loss = 0.01516186073422432\n",
      "Epoch 30, Batch 10/32 : Loss = 0.013638836331665516\n",
      "Epoch 30, Batch 11/32 : Loss = 0.0220932699739933\n",
      "Epoch 30, Batch 12/32 : Loss = 0.06816184520721436\n",
      "Epoch 30, Batch 13/32 : Loss = 0.19912616908550262\n",
      "Epoch 30, Batch 14/32 : Loss = 0.08193302899599075\n",
      "Epoch 30, Batch 15/32 : Loss = 0.011942397803068161\n",
      "Epoch 30, Batch 16/32 : Loss = 0.0055215065367519855\n",
      "Epoch 30, Batch 17/32 : Loss = 0.021550357341766357\n",
      "Epoch 30, Batch 18/32 : Loss = 0.006240839138627052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 19/32 : Loss = 0.0083196135237813\n",
      "Epoch 30, Batch 20/32 : Loss = 0.09378185868263245\n",
      "Epoch 30, Batch 21/32 : Loss = 0.007010996341705322\n",
      "Epoch 30, Batch 22/32 : Loss = 0.012264009565114975\n",
      "Epoch 30, Batch 23/32 : Loss = 0.010769091546535492\n",
      "Epoch 30, Batch 24/32 : Loss = 0.024650195613503456\n",
      "Epoch 30, Batch 25/32 : Loss = 0.006684851832687855\n",
      "Epoch 30, Batch 26/32 : Loss = 0.010008025914430618\n",
      "Epoch 30, Batch 27/32 : Loss = 0.034486547112464905\n",
      "Epoch 30, Batch 28/32 : Loss = 0.02658650651574135\n",
      "Epoch 30, Batch 29/32 : Loss = 0.01133674755692482\n",
      "Epoch 30, Batch 30/32 : Loss = 0.010387195274233818\n",
      "Epoch 30, Batch 31/32 : Loss = 0.05730655789375305\n",
      "Epoch 30 finished in 0.043453677495320635 minutes\n",
      "Epoch 30 training_loss = 0.02875337563455105\n",
      "--44---r--{---%------/--'-)---w-----&----N------+---P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----dd---!--N-----r---A----j--*---$---33---hh---55---n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "----Y----W------]--i-\\\\--|----M------<-----M-----8----8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "----44---r--{---%------/--'-)---w-----&----N------+---PP--- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 30 val_loss = 0.5873122811317444, word_accuracy = 0.6\n",
      "Epoch 31, Batch 0/32 : Loss = 0.06933934986591339\n",
      "Epoch 31, Batch 1/32 : Loss = 0.029362576082348824\n",
      "Epoch 31, Batch 2/32 : Loss = 0.008899671025574207\n",
      "Epoch 31, Batch 3/32 : Loss = 0.011118266731500626\n",
      "Epoch 31, Batch 4/32 : Loss = 0.016836920753121376\n",
      "Epoch 31, Batch 5/32 : Loss = 0.007455071434378624\n",
      "Epoch 31, Batch 6/32 : Loss = 0.21355727314949036\n",
      "Epoch 31, Batch 7/32 : Loss = 0.033981259912252426\n",
      "Epoch 31, Batch 8/32 : Loss = 0.005172690376639366\n",
      "Epoch 31, Batch 9/32 : Loss = 0.02344300039112568\n",
      "Epoch 31, Batch 10/32 : Loss = 0.04683677852153778\n",
      "Epoch 31, Batch 11/32 : Loss = 0.009608656167984009\n",
      "Epoch 31, Batch 12/32 : Loss = 0.006867504213005304\n",
      "Epoch 31, Batch 13/32 : Loss = 0.030296070501208305\n",
      "Epoch 31, Batch 14/32 : Loss = 0.005543988198041916\n",
      "Epoch 31, Batch 15/32 : Loss = 0.06121201440691948\n",
      "Epoch 31, Batch 16/32 : Loss = 0.013837127946317196\n",
      "Epoch 31, Batch 17/32 : Loss = 0.00487165292724967\n",
      "Epoch 31, Batch 18/32 : Loss = 0.015749163925647736\n",
      "Epoch 31, Batch 19/32 : Loss = 0.06767947971820831\n",
      "Epoch 31, Batch 20/32 : Loss = 0.08581440895795822\n",
      "Epoch 31, Batch 21/32 : Loss = 0.08956777304410934\n",
      "Epoch 31, Batch 22/32 : Loss = 0.006655591540038586\n",
      "Epoch 31, Batch 23/32 : Loss = 0.008564010262489319\n",
      "Epoch 31, Batch 24/32 : Loss = 0.059979990124702454\n",
      "Epoch 31, Batch 25/32 : Loss = 0.00720981415361166\n",
      "Epoch 31, Batch 26/32 : Loss = 0.009845152497291565\n",
      "Epoch 31, Batch 27/32 : Loss = 0.010226095095276833\n",
      "Epoch 31, Batch 28/32 : Loss = 0.03000630810856819\n",
      "Epoch 31, Batch 29/32 : Loss = 0.011018355377018452\n",
      "Epoch 31, Batch 30/32 : Loss = 0.02713439054787159\n",
      "Epoch 31, Batch 31/32 : Loss = 0.13425949215888977\n",
      "Epoch 31 finished in 0.11516666412353516 minutes\n",
      "Epoch 31 training_loss = 0.033557359129190445\n",
      "---o----\"--}--!--BB----r--&&----9---;-`--O-----ww----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----<----v----O-----TT---`--44---VV---[--0--------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---J---;--q----+---/---z---y---U----%%-----U-----1---x---- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "--BB---.---Y---l--W------66----F---hh----X---'--Y----2---- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 31 val_loss = 0.5819883942604065, word_accuracy = 0.6\n",
      "Epoch 32, Batch 0/32 : Loss = 0.020625039935112\n",
      "Epoch 32, Batch 1/32 : Loss = 0.007031999062746763\n",
      "Epoch 32, Batch 2/32 : Loss = 0.13326393067836761\n",
      "Epoch 32, Batch 3/32 : Loss = 0.016893120482563972\n",
      "Epoch 32, Batch 4/32 : Loss = 0.01282768975943327\n",
      "Epoch 32, Batch 5/32 : Loss = 0.007478685118257999\n",
      "Epoch 32, Batch 6/32 : Loss = 0.0429181233048439\n",
      "Epoch 32, Batch 7/32 : Loss = 0.17394019663333893\n",
      "Epoch 32, Batch 8/32 : Loss = 0.009918848983943462\n",
      "Epoch 32, Batch 9/32 : Loss = 0.027327604591846466\n",
      "Epoch 32, Batch 10/32 : Loss = 0.074724942445755\n",
      "Epoch 32, Batch 11/32 : Loss = 0.037013474851846695\n",
      "Epoch 32, Batch 12/32 : Loss = 0.013985507190227509\n",
      "Epoch 32, Batch 13/32 : Loss = 0.010510299354791641\n",
      "Epoch 32, Batch 14/32 : Loss = 0.030637552961707115\n",
      "Epoch 32, Batch 15/32 : Loss = 0.022974606603384018\n",
      "Epoch 32, Batch 16/32 : Loss = 0.007089460268616676\n",
      "Epoch 32, Batch 17/32 : Loss = 0.006687772460281849\n",
      "Epoch 32, Batch 18/32 : Loss = 0.04820806533098221\n",
      "Epoch 32, Batch 19/32 : Loss = 0.04542339965701103\n",
      "Epoch 32, Batch 20/32 : Loss = 0.08618949353694916\n",
      "Epoch 32, Batch 21/32 : Loss = 0.023605305701494217\n",
      "Epoch 32, Batch 22/32 : Loss = 0.005766252521425486\n",
      "Epoch 32, Batch 23/32 : Loss = 0.01934981346130371\n",
      "Epoch 32, Batch 24/32 : Loss = 0.025151731446385384\n",
      "Epoch 32, Batch 25/32 : Loss = 0.026247326284646988\n",
      "Epoch 32, Batch 26/32 : Loss = 0.008335096761584282\n",
      "Epoch 32, Batch 27/32 : Loss = 0.03164517879486084\n",
      "Epoch 32, Batch 28/32 : Loss = 0.01727283000946045\n",
      "Epoch 32, Batch 29/32 : Loss = 0.05454734340310097\n",
      "Epoch 32, Batch 30/32 : Loss = 0.008333238773047924\n",
      "Epoch 32, Batch 31/32 : Loss = 0.02596004121005535\n",
      "Epoch 32 finished in 0.04330358107884725 minutes\n",
      "Epoch 32 training_loss = 0.034029521048069\n",
      "-;-------33------\\----$----->>------SS-----\\\\----MM-------ii--B------- => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----X----7----0---j--@@-----S----Z----L----4---CC----mm------M-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---7----n----DD-----PP----n----tt--w------d----\\\\---Q-----aa----RR---- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "--{{--BB-----Y-----R-----a----y----h----#----22---->-----E-----4------ => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 32 val_loss = 0.5951544046401978, word_accuracy = 0.65\n",
      "Epoch 33, Batch 0/32 : Loss = 0.027965595945715904\n",
      "Epoch 33, Batch 1/32 : Loss = 0.007504462264478207\n",
      "Epoch 33, Batch 2/32 : Loss = 0.006973777897655964\n",
      "Epoch 33, Batch 3/32 : Loss = 0.013536680489778519\n",
      "Epoch 33, Batch 4/32 : Loss = 0.006518840324133635\n",
      "Epoch 33, Batch 5/32 : Loss = 0.008455028757452965\n",
      "Epoch 33, Batch 6/32 : Loss = 0.06190810725092888\n",
      "Epoch 33, Batch 7/32 : Loss = 0.005290336906909943\n",
      "Epoch 33, Batch 8/32 : Loss = 0.014427216723561287\n",
      "Epoch 33, Batch 9/32 : Loss = 0.009142382070422173\n",
      "Epoch 33, Batch 10/32 : Loss = 0.019896214827895164\n",
      "Epoch 33, Batch 11/32 : Loss = 0.008359271101653576\n",
      "Epoch 33, Batch 12/32 : Loss = 0.005947367288172245\n",
      "Epoch 33, Batch 13/32 : Loss = 0.00898242648690939\n",
      "Epoch 33, Batch 14/32 : Loss = 0.007934031076729298\n",
      "Epoch 33, Batch 15/32 : Loss = 0.0050429850816726685\n",
      "Epoch 33, Batch 16/32 : Loss = 0.014636212028563023\n",
      "Epoch 33, Batch 17/32 : Loss = 0.024665016680955887\n",
      "Epoch 33, Batch 18/32 : Loss = 0.011103016324341297\n",
      "Epoch 33, Batch 19/32 : Loss = 0.049973003566265106\n",
      "Epoch 33, Batch 20/32 : Loss = 0.04392252117395401\n",
      "Epoch 33, Batch 21/32 : Loss = 0.015774937346577644\n",
      "Epoch 33, Batch 22/32 : Loss = 0.06798084825277328\n",
      "Epoch 33, Batch 23/32 : Loss = 0.01720496267080307\n",
      "Epoch 33, Batch 24/32 : Loss = 0.04605527222156525\n",
      "Epoch 33, Batch 25/32 : Loss = 0.015754595398902893\n",
      "Epoch 33, Batch 26/32 : Loss = 0.09972529858350754\n",
      "Epoch 33, Batch 27/32 : Loss = 0.003738091327250004\n",
      "Epoch 33, Batch 28/32 : Loss = 0.0664321631193161\n",
      "Epoch 33, Batch 29/32 : Loss = 0.006172520108520985\n",
      "Epoch 33, Batch 30/32 : Loss = 0.17001859843730927\n",
      "Epoch 33, Batch 31/32 : Loss = 0.03983063995838165\n",
      "Epoch 33 finished in 0.0429243008295695 minutes\n",
      "Epoch 33 training_loss = 0.028145240619778633\n",
      "----+---:--z----77---8----dd----S-----v---5----S----JJ---B----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "-----k---$----|--'-,--9----Y----N-----W------m------TT---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---55--->>---:--*---Y----A---'-OO-----D----*--#----00----g----- => 5>:*YA'OD*#0g, Ground Truth is 5>:*YA'OD*#Og\n",
      "------Z----0----\"---GG------//---~-----$-----c----11---j--t---- => Z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 33 val_loss = 0.5954585075378418, word_accuracy = 0.65\n",
      "Epoch 34, Batch 0/32 : Loss = 0.006198388058692217\n",
      "Epoch 34, Batch 1/32 : Loss = 0.006396828219294548\n",
      "Epoch 34, Batch 2/32 : Loss = 0.024231309071183205\n",
      "Epoch 34, Batch 3/32 : Loss = 0.019618596881628036\n",
      "Epoch 34, Batch 4/32 : Loss = 0.006109858863055706\n",
      "Epoch 34, Batch 5/32 : Loss = 0.03518514335155487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 6/32 : Loss = 0.011891201138496399\n",
      "Epoch 34, Batch 7/32 : Loss = 0.10655810683965683\n",
      "Epoch 34, Batch 8/32 : Loss = 0.011752600781619549\n",
      "Epoch 34, Batch 9/32 : Loss = 0.015797467902302742\n",
      "Epoch 34, Batch 10/32 : Loss = 0.013171333819627762\n",
      "Epoch 34, Batch 11/32 : Loss = 0.08788221329450607\n",
      "Epoch 34, Batch 12/32 : Loss = 0.02597935125231743\n",
      "Epoch 34, Batch 13/32 : Loss = 0.005392485298216343\n",
      "Epoch 34, Batch 14/32 : Loss = 0.009206283837556839\n",
      "Epoch 34, Batch 15/32 : Loss = 0.00615257304161787\n",
      "Epoch 34, Batch 16/32 : Loss = 0.015401997603476048\n",
      "Epoch 34, Batch 17/32 : Loss = 0.01728394627571106\n",
      "Epoch 34, Batch 18/32 : Loss = 0.007388126105070114\n",
      "Epoch 34, Batch 19/32 : Loss = 0.005812538787722588\n",
      "Epoch 34, Batch 20/32 : Loss = 0.06558314710855484\n",
      "Epoch 34, Batch 21/32 : Loss = 0.00897958967834711\n",
      "Epoch 34, Batch 22/32 : Loss = 0.07430063188076019\n",
      "Epoch 34, Batch 23/32 : Loss = 0.013472095131874084\n",
      "Epoch 34, Batch 24/32 : Loss = 0.11595921963453293\n",
      "Epoch 34, Batch 25/32 : Loss = 0.007356541231274605\n",
      "Epoch 34, Batch 26/32 : Loss = 0.07278385013341904\n",
      "Epoch 34, Batch 27/32 : Loss = 0.044427573680877686\n",
      "Epoch 34, Batch 28/32 : Loss = 0.03841608762741089\n",
      "Epoch 34, Batch 29/32 : Loss = 0.019790232181549072\n",
      "Epoch 34, Batch 30/32 : Loss = 0.04270869866013527\n",
      "Epoch 34, Batch 31/32 : Loss = 0.008469733409583569\n",
      "Epoch 34 finished in 0.04227197964986165 minutes\n",
      "Epoch 34 training_loss = 0.030272984877228737\n",
      "----\"----]---t----4-----e-----^----W--------Q--------4---->>-----gg----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---WW------=----22----+---EE----1----n----T----XX---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "---O------cc-----++-----bb-----I--\"----bb-----66-----.---Q-------------- => Oc+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----X----77---0---j---@@-----S----Z----L----4----C----mm------M--------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 34 val_loss = 0.6162911057472229, word_accuracy = 0.63\n",
      "Epoch 35, Batch 0/32 : Loss = 0.009665776044130325\n",
      "Epoch 35, Batch 1/32 : Loss = 0.007509971037507057\n",
      "Epoch 35, Batch 2/32 : Loss = 0.005834993906319141\n",
      "Epoch 35, Batch 3/32 : Loss = 0.012679414823651314\n",
      "Epoch 35, Batch 4/32 : Loss = 0.01678900048136711\n",
      "Epoch 35, Batch 5/32 : Loss = 0.014975355938076973\n",
      "Epoch 35, Batch 6/32 : Loss = 0.006245918571949005\n",
      "Epoch 35, Batch 7/32 : Loss = 0.02962002158164978\n",
      "Epoch 35, Batch 8/32 : Loss = 0.030159395188093185\n",
      "Epoch 35, Batch 9/32 : Loss = 0.009437975473701954\n",
      "Epoch 35, Batch 10/32 : Loss = 0.030152080580592155\n",
      "Epoch 35, Batch 11/32 : Loss = 0.024411441758275032\n",
      "Epoch 35, Batch 12/32 : Loss = 0.02523890882730484\n",
      "Epoch 35, Batch 13/32 : Loss = 0.009211335331201553\n",
      "Epoch 35, Batch 14/32 : Loss = 0.013866282999515533\n",
      "Epoch 35, Batch 15/32 : Loss = 0.006271202117204666\n",
      "Epoch 35, Batch 16/32 : Loss = 0.1028326153755188\n",
      "Epoch 35, Batch 17/32 : Loss = 0.06961199641227722\n",
      "Epoch 35, Batch 18/32 : Loss = 0.007546232547610998\n",
      "Epoch 35, Batch 19/32 : Loss = 0.11309762299060822\n",
      "Epoch 35, Batch 20/32 : Loss = 0.015619898214936256\n",
      "Epoch 35, Batch 21/32 : Loss = 0.008668607100844383\n",
      "Epoch 35, Batch 22/32 : Loss = 0.006737199611961842\n",
      "Epoch 35, Batch 23/32 : Loss = 0.006131649482995272\n",
      "Epoch 35, Batch 24/32 : Loss = 0.03273903951048851\n",
      "Epoch 35, Batch 25/32 : Loss = 0.006856418680399656\n",
      "Epoch 35, Batch 26/32 : Loss = 0.014264901168644428\n",
      "Epoch 35, Batch 27/32 : Loss = 0.010625377297401428\n",
      "Epoch 35, Batch 28/32 : Loss = 0.08858679980039597\n",
      "Epoch 35, Batch 29/32 : Loss = 0.008179851807653904\n",
      "Epoch 35, Batch 30/32 : Loss = 0.006587223149836063\n",
      "Epoch 35, Batch 31/32 : Loss = 0.0533156618475914\n",
      "Epoch 35 finished in 0.043888370196024575 minutes\n",
      "Epoch 35 training_loss = 0.02431546524167061\n",
      "----\"----]---t----44----e-----^----W---------Q-------44---->>-----gg----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "----55---->>----:--*----Y----A----'--O------D-----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----JJ---;--qq-----+----/----z---yy---UU-----%%------UU------1----x---_-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "-----k----$-----I--F-----DD-----ee----hh----]---k----00----\\----X-------- => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 35 val_loss = 0.5680457949638367, word_accuracy = 0.7\n",
      "Epoch 36, Batch 0/32 : Loss = 0.031010063365101814\n",
      "Epoch 36, Batch 1/32 : Loss = 0.024519283324480057\n",
      "Epoch 36, Batch 2/32 : Loss = 0.013113787397742271\n",
      "Epoch 36, Batch 3/32 : Loss = 0.008802257478237152\n",
      "Epoch 36, Batch 4/32 : Loss = 0.008177759125828743\n",
      "Epoch 36, Batch 5/32 : Loss = 0.024655643850564957\n",
      "Epoch 36, Batch 6/32 : Loss = 0.03741180896759033\n",
      "Epoch 36, Batch 7/32 : Loss = 0.006533411331474781\n",
      "Epoch 36, Batch 8/32 : Loss = 0.007666774094104767\n",
      "Epoch 36, Batch 9/32 : Loss = 0.018717993050813675\n",
      "Epoch 36, Batch 10/32 : Loss = 0.014302334748208523\n",
      "Epoch 36, Batch 11/32 : Loss = 0.034018732607364655\n",
      "Epoch 36, Batch 12/32 : Loss = 0.005131128244102001\n",
      "Epoch 36, Batch 13/32 : Loss = 0.01032294612377882\n",
      "Epoch 36, Batch 14/32 : Loss = 0.15329496562480927\n",
      "Epoch 36, Batch 15/32 : Loss = 0.02795027382671833\n",
      "Epoch 36, Batch 16/32 : Loss = 0.009505435824394226\n",
      "Epoch 36, Batch 17/32 : Loss = 0.004420699551701546\n",
      "Epoch 36, Batch 18/32 : Loss = 0.02681349590420723\n",
      "Epoch 36, Batch 19/32 : Loss = 0.006910268217325211\n",
      "Epoch 36, Batch 20/32 : Loss = 0.0069245644845068455\n",
      "Epoch 36, Batch 21/32 : Loss = 0.057597316801548004\n",
      "Epoch 36, Batch 22/32 : Loss = 0.007634152192622423\n",
      "Epoch 36, Batch 23/32 : Loss = 0.03192117065191269\n",
      "Epoch 36, Batch 24/32 : Loss = 0.00612973514944315\n",
      "Epoch 36, Batch 25/32 : Loss = 0.010377071797847748\n",
      "Epoch 36, Batch 26/32 : Loss = 0.0036740328650921583\n",
      "Epoch 36, Batch 27/32 : Loss = 0.010619042441248894\n",
      "Epoch 36, Batch 28/32 : Loss = 0.023244155570864677\n",
      "Epoch 36, Batch 29/32 : Loss = 0.017737165093421936\n",
      "Epoch 36, Batch 30/32 : Loss = 0.005680727772414684\n",
      "Epoch 36, Batch 31/32 : Loss = 0.013880742713809013\n",
      "Epoch 36 finished in 0.042865256468454994 minutes\n",
      "Epoch 36 training_loss = 0.021094080060720444\n",
      "----k---$----|--!---9----Y---N----WW-----m-----TT---8---- => k$|!9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---k---$----I-FF---DD----e----h---]--kk---0---\\---X------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---kk---O-----/--,--y----c---*---1----P---}--##---BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----d----!--N-----r--A----j-**--$$---3----h----5---n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 36 val_loss = 0.625813364982605, word_accuracy = 0.64\n",
      "Epoch 37, Batch 0/32 : Loss = 0.008968940004706383\n",
      "Epoch 37, Batch 1/32 : Loss = 0.0053297001868486404\n",
      "Epoch 37, Batch 2/32 : Loss = 0.009516777470707893\n",
      "Epoch 37, Batch 3/32 : Loss = 0.009574975818395615\n",
      "Epoch 37, Batch 4/32 : Loss = 0.009256096556782722\n",
      "Epoch 37, Batch 5/32 : Loss = 0.03627359867095947\n",
      "Epoch 37, Batch 6/32 : Loss = 0.00515096727758646\n",
      "Epoch 37, Batch 7/32 : Loss = 0.020100915804505348\n",
      "Epoch 37, Batch 8/32 : Loss = 0.02797161228954792\n",
      "Epoch 37, Batch 9/32 : Loss = 0.03571059927344322\n",
      "Epoch 37, Batch 10/32 : Loss = 0.004311596043407917\n",
      "Epoch 37, Batch 11/32 : Loss = 0.006844657938927412\n",
      "Epoch 37, Batch 12/32 : Loss = 0.015555365942418575\n",
      "Epoch 37, Batch 13/32 : Loss = 0.004871785175055265\n",
      "Epoch 37, Batch 14/32 : Loss = 0.004754592664539814\n",
      "Epoch 37, Batch 15/32 : Loss = 0.005863351747393608\n",
      "Epoch 37, Batch 16/32 : Loss = 0.016881253570318222\n",
      "Epoch 37, Batch 17/32 : Loss = 0.005647853948175907\n",
      "Epoch 37, Batch 18/32 : Loss = 0.012691304087638855\n",
      "Epoch 37, Batch 19/32 : Loss = 0.008114861324429512\n",
      "Epoch 37, Batch 20/32 : Loss = 0.12849566340446472\n",
      "Epoch 37, Batch 21/32 : Loss = 0.08134794235229492\n",
      "Epoch 37, Batch 22/32 : Loss = 0.004016966558992863\n",
      "Epoch 37, Batch 23/32 : Loss = 0.004801808390766382\n",
      "Epoch 37, Batch 24/32 : Loss = 0.024972401559352875\n",
      "Epoch 37, Batch 25/32 : Loss = 0.009795000776648521\n",
      "Epoch 37, Batch 26/32 : Loss = 0.005013817921280861\n",
      "Epoch 37, Batch 27/32 : Loss = 0.004938237834721804\n",
      "Epoch 37, Batch 28/32 : Loss = 0.01787601038813591\n",
      "Epoch 37, Batch 29/32 : Loss = 0.0047208815813064575\n",
      "Epoch 37, Batch 30/32 : Loss = 0.011827357113361359\n",
      "Epoch 37, Batch 31/32 : Loss = 0.026394950225949287\n",
      "Epoch 37 finished in 0.04324342409769694 minutes\n",
      "Epoch 37 training_loss = 0.01781514100730419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----0----JJ---!--(--;--AA----33---,--'--)--r---r---77---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----dd----R---;--99---y----?---22---dd---i--O-----{--!---- => dR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "-:------3----\\----$---->>-----S-----\\----MM-----i--BB----- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----4---r---{--%%-----/--`--)--w-----&&---N------+---P---- => 4r{%/`)w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 37 val_loss = 0.5706621408462524, word_accuracy = 0.66\n",
      "Epoch 38, Batch 0/32 : Loss = 0.19900956749916077\n",
      "Epoch 38, Batch 1/32 : Loss = 0.009420474991202354\n",
      "Epoch 38, Batch 2/32 : Loss = 0.013406418263912201\n",
      "Epoch 38, Batch 3/32 : Loss = 0.08417503535747528\n",
      "Epoch 38, Batch 4/32 : Loss = 0.012626741081476212\n",
      "Epoch 38, Batch 5/32 : Loss = 0.0067283995449543\n",
      "Epoch 38, Batch 6/32 : Loss = 0.021632933989167213\n",
      "Epoch 38, Batch 7/32 : Loss = 0.00565203744918108\n",
      "Epoch 38, Batch 8/32 : Loss = 0.007319361437112093\n",
      "Epoch 38, Batch 9/32 : Loss = 0.022269174456596375\n",
      "Epoch 38, Batch 10/32 : Loss = 0.012403883039951324\n",
      "Epoch 38, Batch 11/32 : Loss = 0.00797109492123127\n",
      "Epoch 38, Batch 12/32 : Loss = 0.0073215714655816555\n",
      "Epoch 38, Batch 13/32 : Loss = 0.03971342369914055\n",
      "Epoch 38, Batch 14/32 : Loss = 0.006677681114524603\n",
      "Epoch 38, Batch 15/32 : Loss = 0.006688082590699196\n",
      "Epoch 38, Batch 16/32 : Loss = 0.00428510457277298\n",
      "Epoch 38, Batch 17/32 : Loss = 0.06438826024532318\n",
      "Epoch 38, Batch 18/32 : Loss = 0.005598250776529312\n",
      "Epoch 38, Batch 19/32 : Loss = 0.00872186478227377\n",
      "Epoch 38, Batch 20/32 : Loss = 0.022919626906514168\n",
      "Epoch 38, Batch 21/32 : Loss = 0.01925739273428917\n",
      "Epoch 38, Batch 22/32 : Loss = 0.00617909524589777\n",
      "Epoch 38, Batch 23/32 : Loss = 0.00680245878174901\n",
      "Epoch 38, Batch 24/32 : Loss = 0.011269843205809593\n",
      "Epoch 38, Batch 25/32 : Loss = 0.014855476096272469\n",
      "Epoch 38, Batch 26/32 : Loss = 0.08467333018779755\n",
      "Epoch 38, Batch 27/32 : Loss = 0.004665247164666653\n",
      "Epoch 38, Batch 28/32 : Loss = 0.03540434688329697\n",
      "Epoch 38, Batch 29/32 : Loss = 0.006562418304383755\n",
      "Epoch 38, Batch 30/32 : Loss = 0.023657772690057755\n",
      "Epoch 38, Batch 31/32 : Loss = 0.659557580947876\n",
      "Epoch 38 finished in 0.043770992755889894 minutes\n",
      "Epoch 38 training_loss = 0.027781562879681587\n",
      "-----C-----DD----E----gg----m------\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----#-----GG-----9----E----I--=----hh----5----#-----2---JJ---)---k------- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-----0----Q-----6----<<----<----(---T----N-----5----=----P---(------------ => 0Q6<<(TN5=P(, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----0------J----!!--(----;---A------33-----,--'---)---r----rr----7-------- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 38 val_loss = 0.5820910334587097, word_accuracy = 0.7\n",
      "Epoch 39, Batch 0/32 : Loss = 0.09024461358785629\n",
      "Epoch 39, Batch 1/32 : Loss = 0.008743984624743462\n",
      "Epoch 39, Batch 2/32 : Loss = 0.006108155008405447\n",
      "Epoch 39, Batch 3/32 : Loss = 0.011864151805639267\n",
      "Epoch 39, Batch 4/32 : Loss = 0.01079988107085228\n",
      "Epoch 39, Batch 5/32 : Loss = 0.00612242054194212\n",
      "Epoch 39, Batch 6/32 : Loss = 0.1291857659816742\n",
      "Epoch 39, Batch 7/32 : Loss = 0.01232251524925232\n",
      "Epoch 39, Batch 8/32 : Loss = 0.02095831371843815\n",
      "Epoch 39, Batch 9/32 : Loss = 0.030477110296487808\n",
      "Epoch 39, Batch 10/32 : Loss = 0.023166686296463013\n",
      "Epoch 39, Batch 11/32 : Loss = 0.018517978489398956\n",
      "Epoch 39, Batch 12/32 : Loss = 0.022421594709157944\n",
      "Epoch 39, Batch 13/32 : Loss = 0.034954894334077835\n",
      "Epoch 39, Batch 14/32 : Loss = 0.07582737505435944\n",
      "Epoch 39, Batch 15/32 : Loss = 0.010779744945466518\n",
      "Epoch 39, Batch 16/32 : Loss = 0.03139844909310341\n",
      "Epoch 39, Batch 17/32 : Loss = 0.0076434677466750145\n",
      "Epoch 39, Batch 18/32 : Loss = 0.026024427264928818\n",
      "Epoch 39, Batch 19/32 : Loss = 0.008840473368763924\n",
      "Epoch 39, Batch 20/32 : Loss = 0.009658077731728554\n",
      "Epoch 39, Batch 21/32 : Loss = 0.008734701201319695\n",
      "Epoch 39, Batch 22/32 : Loss = 0.11388055235147476\n",
      "Epoch 39, Batch 23/32 : Loss = 0.09859286993741989\n",
      "Epoch 39, Batch 24/32 : Loss = 0.06634920090436935\n",
      "Epoch 39, Batch 25/32 : Loss = 0.0067351702600717545\n",
      "Epoch 39, Batch 26/32 : Loss = 0.046089813113212585\n",
      "Epoch 39, Batch 27/32 : Loss = 0.01471675094217062\n",
      "Epoch 39, Batch 28/32 : Loss = 0.03943108394742012\n",
      "Epoch 39, Batch 29/32 : Loss = 0.006458096671849489\n",
      "Epoch 39, Batch 30/32 : Loss = 0.011035509407520294\n",
      "Epoch 39, Batch 31/32 : Loss = 0.013912089169025421\n",
      "Epoch 39 finished in 0.042271812756856285 minutes\n",
      "Epoch 39 training_loss = 0.03244410827755928\n",
      "----kk---$----|--'',--9----Y----N-----W------mm------T---88----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---7---n----DD----P----n---tt--w-----d---\\\\--Q-----a----R------- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "---7---n----DD----PP----n---t--ww-----d----\\---Q-----a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----k---$----|---'----9----Y----N-----W------m------T----8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 39 val_loss = 0.6026666760444641, word_accuracy = 0.66\n",
      "Epoch 40, Batch 0/32 : Loss = 0.017245374619960785\n",
      "Epoch 40, Batch 1/32 : Loss = 0.04217218980193138\n",
      "Epoch 40, Batch 2/32 : Loss = 0.02094918116927147\n",
      "Epoch 40, Batch 3/32 : Loss = 0.0075728632509708405\n",
      "Epoch 40, Batch 4/32 : Loss = 0.005519762169569731\n",
      "Epoch 40, Batch 5/32 : Loss = 0.02129005268216133\n",
      "Epoch 40, Batch 6/32 : Loss = 0.0144880972802639\n",
      "Epoch 40, Batch 7/32 : Loss = 0.014722789637744427\n",
      "Epoch 40, Batch 8/32 : Loss = 0.02306457795202732\n",
      "Epoch 40, Batch 9/32 : Loss = 0.0434667244553566\n",
      "Epoch 40, Batch 10/32 : Loss = 0.08878263086080551\n",
      "Epoch 40, Batch 11/32 : Loss = 0.006655351258814335\n",
      "Epoch 40, Batch 12/32 : Loss = 0.1496913880109787\n",
      "Epoch 40, Batch 13/32 : Loss = 0.008223351091146469\n",
      "Epoch 40, Batch 14/32 : Loss = 0.19851090013980865\n",
      "Epoch 40, Batch 15/32 : Loss = 0.009264172054827213\n",
      "Epoch 40, Batch 16/32 : Loss = 0.006408832035958767\n",
      "Epoch 40, Batch 17/32 : Loss = 0.11966290324926376\n",
      "Epoch 40, Batch 18/32 : Loss = 0.01443125493824482\n",
      "Epoch 40, Batch 19/32 : Loss = 0.12396141141653061\n",
      "Epoch 40, Batch 20/32 : Loss = 0.03288426995277405\n",
      "Epoch 40, Batch 21/32 : Loss = 0.024960417300462723\n",
      "Epoch 40, Batch 22/32 : Loss = 0.008572334423661232\n",
      "Epoch 40, Batch 23/32 : Loss = 0.013013822957873344\n",
      "Epoch 40, Batch 24/32 : Loss = 0.03687547892332077\n",
      "Epoch 40, Batch 25/32 : Loss = 0.022777676582336426\n",
      "Epoch 40, Batch 26/32 : Loss = 0.012552710250020027\n",
      "Epoch 40, Batch 27/32 : Loss = 0.014154018834233284\n",
      "Epoch 40, Batch 28/32 : Loss = 0.01738903485238552\n",
      "Epoch 40, Batch 29/32 : Loss = 0.014112305827438831\n",
      "Epoch 40, Batch 30/32 : Loss = 0.05147172510623932\n",
      "Epoch 40, Batch 31/32 : Loss = 0.08249661326408386\n",
      "Epoch 40 finished in 0.04531716505686442 minutes\n",
      "Epoch 40 training_loss = 0.038398705422878265\n",
      "----o----\"---}--!--BB-----r---&-----9---;--`--O------w-----}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---55--->>---:--*---Y---AA---'--O----DD----*--#----OO----g----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "--BB----.--YY---l--WW------66----F----hh----X----'--Y----22---- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----dd----!--N------r--AA----j--*---$$----3----hh---55----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 40 val_loss = 0.6273747682571411, word_accuracy = 0.53\n",
      "Epoch 41, Batch 0/32 : Loss = 0.04887402430176735\n",
      "Epoch 41, Batch 1/32 : Loss = 0.029983308166265488\n",
      "Epoch 41, Batch 2/32 : Loss = 0.012502288445830345\n",
      "Epoch 41, Batch 3/32 : Loss = 0.010007037781178951\n",
      "Epoch 41, Batch 4/32 : Loss = 0.011235331185162067\n",
      "Epoch 41, Batch 5/32 : Loss = 0.01742567867040634\n",
      "Epoch 41, Batch 6/32 : Loss = 0.01250167191028595\n",
      "Epoch 41, Batch 7/32 : Loss = 0.2359478771686554\n",
      "Epoch 41, Batch 8/32 : Loss = 0.011864963918924332\n",
      "Epoch 41, Batch 9/32 : Loss = 0.026511378586292267\n",
      "Epoch 41, Batch 10/32 : Loss = 0.005503085907548666\n",
      "Epoch 41, Batch 11/32 : Loss = 0.020321544259786606\n",
      "Epoch 41, Batch 12/32 : Loss = 0.034708455204963684\n",
      "Epoch 41, Batch 13/32 : Loss = 0.019666491076350212\n",
      "Epoch 41, Batch 14/32 : Loss = 0.08130229264497757\n",
      "Epoch 41, Batch 15/32 : Loss = 0.007850077003240585\n",
      "Epoch 41, Batch 16/32 : Loss = 0.016946446150541306\n",
      "Epoch 41, Batch 17/32 : Loss = 0.10806285589933395\n",
      "Epoch 41, Batch 18/32 : Loss = 0.00963506381958723\n",
      "Epoch 41, Batch 19/32 : Loss = 0.008733957074582577\n",
      "Epoch 41, Batch 20/32 : Loss = 0.04888951778411865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Batch 21/32 : Loss = 0.03038225695490837\n",
      "Epoch 41, Batch 22/32 : Loss = 0.016071531921625137\n",
      "Epoch 41, Batch 23/32 : Loss = 0.04434879869222641\n",
      "Epoch 41, Batch 24/32 : Loss = 0.005566623527556658\n",
      "Epoch 41, Batch 25/32 : Loss = 0.28599122166633606\n",
      "Epoch 41, Batch 26/32 : Loss = 0.01444239355623722\n",
      "Epoch 41, Batch 27/32 : Loss = 0.008862178772687912\n",
      "Epoch 41, Batch 28/32 : Loss = 0.1058935672044754\n",
      "Epoch 41, Batch 29/32 : Loss = 0.008719351142644882\n",
      "Epoch 41, Batch 30/32 : Loss = 0.008503518998622894\n",
      "Epoch 41, Batch 31/32 : Loss = 0.033613819628953934\n",
      "Epoch 41 finished in 0.04294631481170654 minutes\n",
      "Epoch 41 training_loss = 0.042135149240493774\n",
      "---##----G-----9----E---I--=----h---55---#----2----J---)--k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---{--BB-----Y----R----aa---y----h---#----2---->>----E----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "---z----0-----\"---GGG-----/----~~----$$-----c-----1---j---t----- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---C----DD----E---gg---mm-----\"--mm-----F---<----Q-----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 41 val_loss = 0.5814875364303589, word_accuracy = 0.66\n",
      "Epoch 42, Batch 0/32 : Loss = 0.006292672827839851\n",
      "Epoch 42, Batch 1/32 : Loss = 0.10043274611234665\n",
      "Epoch 42, Batch 2/32 : Loss = 0.011205006390810013\n",
      "Epoch 42, Batch 3/32 : Loss = 0.020821433514356613\n",
      "Epoch 42, Batch 4/32 : Loss = 0.012657497078180313\n",
      "Epoch 42, Batch 5/32 : Loss = 0.030447836965322495\n",
      "Epoch 42, Batch 6/32 : Loss = 0.017091473564505577\n",
      "Epoch 42, Batch 7/32 : Loss = 0.038782402873039246\n",
      "Epoch 42, Batch 8/32 : Loss = 0.10143516212701797\n",
      "Epoch 42, Batch 9/32 : Loss = 0.013953212648630142\n",
      "Epoch 42, Batch 10/32 : Loss = 0.02817203477025032\n",
      "Epoch 42, Batch 11/32 : Loss = 0.00704631581902504\n",
      "Epoch 42, Batch 12/32 : Loss = 0.010180167853832245\n",
      "Epoch 42, Batch 13/32 : Loss = 0.00695502944290638\n",
      "Epoch 42, Batch 14/32 : Loss = 0.0116301653906703\n",
      "Epoch 42, Batch 15/32 : Loss = 0.06273063272237778\n",
      "Epoch 42, Batch 16/32 : Loss = 0.027366556227207184\n",
      "Epoch 42, Batch 17/32 : Loss = 0.0059798713773489\n",
      "Epoch 42, Batch 18/32 : Loss = 0.005225703120231628\n",
      "Epoch 42, Batch 19/32 : Loss = 0.01004702877253294\n",
      "Epoch 42, Batch 20/32 : Loss = 0.026806555688381195\n",
      "Epoch 42, Batch 21/32 : Loss = 0.05576423555612564\n",
      "Epoch 42, Batch 22/32 : Loss = 0.006387217901647091\n",
      "Epoch 42, Batch 23/32 : Loss = 0.011217732913792133\n",
      "Epoch 42, Batch 24/32 : Loss = 0.03157440945506096\n",
      "Epoch 42, Batch 25/32 : Loss = 0.007991400547325611\n",
      "Epoch 42, Batch 26/32 : Loss = 0.02673347294330597\n",
      "Epoch 42, Batch 27/32 : Loss = 0.004150083754211664\n",
      "Epoch 42, Batch 28/32 : Loss = 0.006998306605964899\n",
      "Epoch 42, Batch 29/32 : Loss = 0.008252305909991264\n",
      "Epoch 42, Batch 30/32 : Loss = 0.004433881491422653\n",
      "Epoch 42, Batch 31/32 : Loss = 0.0025114677846431732\n",
      "Epoch 42 finished in 0.04422959089279175 minutes\n",
      "Epoch 42 training_loss = 0.023102859035134315\n",
      "---55---99---g----m------J---u----x---c---x--.--d---\\\\-- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "--00----c----++----b----I--\"---b----66---.---Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "--zz---0----\"---GG-----/----~----$-----c----1---j--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---\"---]--t--4----e----^---W-------Q----44---->----g---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 42 val_loss = 0.5903627872467041, word_accuracy = 0.73\n",
      "Epoch 43, Batch 0/32 : Loss = 0.004865079186856747\n",
      "Epoch 43, Batch 1/32 : Loss = 0.005635709967464209\n",
      "Epoch 43, Batch 2/32 : Loss = 0.004953136667609215\n",
      "Epoch 43, Batch 3/32 : Loss = 0.010692223906517029\n",
      "Epoch 43, Batch 4/32 : Loss = 0.004934374243021011\n",
      "Epoch 43, Batch 5/32 : Loss = 0.007455620914697647\n",
      "Epoch 43, Batch 6/32 : Loss = 0.1083264872431755\n",
      "Epoch 43, Batch 7/32 : Loss = 0.03473486378788948\n",
      "Epoch 43, Batch 8/32 : Loss = 0.0100520309060812\n",
      "Epoch 43, Batch 9/32 : Loss = 0.011286653578281403\n",
      "Epoch 43, Batch 10/32 : Loss = 0.0038213818334043026\n",
      "Epoch 43, Batch 11/32 : Loss = 0.03405541181564331\n",
      "Epoch 43, Batch 12/32 : Loss = 0.022762347012758255\n",
      "Epoch 43, Batch 13/32 : Loss = 0.003477394115179777\n",
      "Epoch 43, Batch 14/32 : Loss = 0.017299335449934006\n",
      "Epoch 43, Batch 15/32 : Loss = 0.024043072015047073\n",
      "Epoch 43, Batch 16/32 : Loss = 0.00544767128303647\n",
      "Epoch 43, Batch 17/32 : Loss = 0.006401280872523785\n",
      "Epoch 43, Batch 18/32 : Loss = 0.027253303676843643\n",
      "Epoch 43, Batch 19/32 : Loss = 0.007827776484191418\n",
      "Epoch 43, Batch 20/32 : Loss = 0.024992700666189194\n",
      "Epoch 43, Batch 21/32 : Loss = 0.13705025613307953\n",
      "Epoch 43, Batch 22/32 : Loss = 0.04278510808944702\n",
      "Epoch 43, Batch 23/32 : Loss = 0.0040260679088532925\n",
      "Epoch 43, Batch 24/32 : Loss = 0.01561388373374939\n",
      "Epoch 43, Batch 25/32 : Loss = 0.060219187289476395\n",
      "Epoch 43, Batch 26/32 : Loss = 0.017636923119425774\n",
      "Epoch 43, Batch 27/32 : Loss = 0.025086447596549988\n",
      "Epoch 43, Batch 28/32 : Loss = 0.016570962965488434\n",
      "Epoch 43, Batch 29/32 : Loss = 0.017283987253904343\n",
      "Epoch 43, Batch 30/32 : Loss = 0.028411542996764183\n",
      "Epoch 43, Batch 31/32 : Loss = 0.02846943773329258\n",
      "Epoch 43 finished in 0.042274351914723715 minutes\n",
      "Epoch 43 training_loss = 0.02405015006661415\n",
      "---7---n----DD----PP---nn---t--w-----dd---\\---Q-----a----RR---- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "---<<----vv----O------T----`--44----V-----[---0--------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "--JJ--;;-qq----+---/---z---y----U----%%-----UU-----1---x---__-) => J;q+/zyU%U1x_), Ground Truth is J;q+/zyU%U1x_\n",
      "---dd---:---X----99---ee----a-----F--,--8--------VV----R------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 43 val_loss = 0.6413703560829163, word_accuracy = 0.66\n",
      "Epoch 44, Batch 0/32 : Loss = 0.010902144014835358\n",
      "Epoch 44, Batch 1/32 : Loss = 0.009837709367275238\n",
      "Epoch 44, Batch 2/32 : Loss = 0.00562564842402935\n",
      "Epoch 44, Batch 3/32 : Loss = 0.012144677340984344\n",
      "Epoch 44, Batch 4/32 : Loss = 0.005588460713624954\n",
      "Epoch 44, Batch 5/32 : Loss = 0.011645350605249405\n",
      "Epoch 44, Batch 6/32 : Loss = 0.0049237776547670364\n",
      "Epoch 44, Batch 7/32 : Loss = 0.01565893553197384\n",
      "Epoch 44, Batch 8/32 : Loss = 0.050881192088127136\n",
      "Epoch 44, Batch 9/32 : Loss = 0.017878955230116844\n",
      "Epoch 44, Batch 10/32 : Loss = 0.006107292138040066\n",
      "Epoch 44, Batch 11/32 : Loss = 0.013080768287181854\n",
      "Epoch 44, Batch 12/32 : Loss = 0.12470298260450363\n",
      "Epoch 44, Batch 13/32 : Loss = 0.017104441300034523\n",
      "Epoch 44, Batch 14/32 : Loss = 0.007757391780614853\n",
      "Epoch 44, Batch 15/32 : Loss = 0.013032844290137291\n",
      "Epoch 44, Batch 16/32 : Loss = 0.029549721628427505\n",
      "Epoch 44, Batch 17/32 : Loss = 0.014327448792755604\n",
      "Epoch 44, Batch 18/32 : Loss = 0.005609236657619476\n",
      "Epoch 44, Batch 19/32 : Loss = 0.003437167964875698\n",
      "Epoch 44, Batch 20/32 : Loss = 0.004072150215506554\n",
      "Epoch 44, Batch 21/32 : Loss = 0.01717059686779976\n",
      "Epoch 44, Batch 22/32 : Loss = 0.013059502467513084\n",
      "Epoch 44, Batch 23/32 : Loss = 0.024054203182458878\n",
      "Epoch 44, Batch 24/32 : Loss = 0.004622556269168854\n",
      "Epoch 44, Batch 25/32 : Loss = 0.007804051041603088\n",
      "Epoch 44, Batch 26/32 : Loss = 0.011375700123608112\n",
      "Epoch 44, Batch 27/32 : Loss = 0.07159554958343506\n",
      "Epoch 44, Batch 28/32 : Loss = 0.015777824446558952\n",
      "Epoch 44, Batch 29/32 : Loss = 0.01471532229334116\n",
      "Epoch 44, Batch 30/32 : Loss = 0.003361808368936181\n",
      "Epoch 44, Batch 31/32 : Loss = 0.005574775859713554\n",
      "Epoch 44 finished in 0.04132941166559855 minutes\n",
      "Epoch 44 training_loss = 0.018252281472086906\n",
      "-----=-----0-----[---X---))---R-----88---ii--P-----w------)---- => =0[X)R8iPw), Ground Truth is Err:509\n",
      "----<-----v----O------T----`--44----V----[---0--------Q-------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---55--->>---:--*---Y---AA---'--0----DD----*--#-----O----g----- => 5>:*YA'0D*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---z----0-----\"---GG-----//----~~----$-----c-----1---jj--t----- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 44 val_loss = 0.6633270978927612, word_accuracy = 0.63\n",
      "Epoch 45, Batch 0/32 : Loss = 0.009625132195651531\n",
      "Epoch 45, Batch 1/32 : Loss = 0.01205766387283802\n",
      "Epoch 45, Batch 2/32 : Loss = 0.006855125539004803\n",
      "Epoch 45, Batch 3/32 : Loss = 0.007898539304733276\n",
      "Epoch 45, Batch 4/32 : Loss = 0.031213801354169846\n",
      "Epoch 45, Batch 5/32 : Loss = 0.023004818707704544\n",
      "Epoch 45, Batch 6/32 : Loss = 0.0037919459864497185\n",
      "Epoch 45, Batch 7/32 : Loss = 0.00681532546877861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Batch 8/32 : Loss = 0.007273559458553791\n",
      "Epoch 45, Batch 9/32 : Loss = 0.033350326120853424\n",
      "Epoch 45, Batch 10/32 : Loss = 0.0038203734438866377\n",
      "Epoch 45, Batch 11/32 : Loss = 0.30839043855667114\n",
      "Epoch 45, Batch 12/32 : Loss = 0.0890103429555893\n",
      "Epoch 45, Batch 13/32 : Loss = 0.00496981805190444\n",
      "Epoch 45, Batch 14/32 : Loss = 0.02266310527920723\n",
      "Epoch 45, Batch 15/32 : Loss = 0.0027631621342152357\n",
      "Epoch 45, Batch 16/32 : Loss = 0.01484375074505806\n",
      "Epoch 45, Batch 17/32 : Loss = 0.014357134699821472\n",
      "Epoch 45, Batch 18/32 : Loss = 0.07487907260656357\n",
      "Epoch 45, Batch 19/32 : Loss = 0.004704214166849852\n",
      "Epoch 45, Batch 20/32 : Loss = 0.02970922738313675\n",
      "Epoch 45, Batch 21/32 : Loss = 0.004832460545003414\n",
      "Epoch 45, Batch 22/32 : Loss = 0.01929674856364727\n",
      "Epoch 45, Batch 23/32 : Loss = 0.006230478174984455\n",
      "Epoch 45, Batch 24/32 : Loss = 0.004870887380093336\n",
      "Epoch 45, Batch 25/32 : Loss = 0.005678072571754456\n",
      "Epoch 45, Batch 26/32 : Loss = 0.023642154410481453\n",
      "Epoch 45, Batch 27/32 : Loss = 0.21636581420898438\n",
      "Epoch 45, Batch 28/32 : Loss = 0.0037036696448922157\n",
      "Epoch 45, Batch 29/32 : Loss = 0.005078174639493227\n",
      "Epoch 45, Batch 30/32 : Loss = 0.008958007209002972\n",
      "Epoch 45, Batch 31/32 : Loss = 0.07739043980836868\n",
      "Epoch 45 finished in 0.04545991023381551 minutes\n",
      "Epoch 45 training_loss = 0.03278159722685814\n",
      "-----W-------=----2----++---EE----1----n----T----X----r--CC----a---------- => W=2+E1nTXrCa, Ground Truth is W=2+E1nTXrCan\n",
      "----+-----:--z-----7----88-----d------S-----v----5-----S-----JJ----B------ => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----\"----]---t---44----e------^---WW---------Q------4------>------g------- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---k-----$----||--''-,---9----Y-----N------W--------m-------T-----8------- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 45 val_loss = 0.5785037279129028, word_accuracy = 0.64\n",
      "Epoch 46, Batch 0/32 : Loss = 0.005959733389317989\n",
      "Epoch 46, Batch 1/32 : Loss = 0.012208489701151848\n",
      "Epoch 46, Batch 2/32 : Loss = 0.017474893480539322\n",
      "Epoch 46, Batch 3/32 : Loss = 0.00343099283054471\n",
      "Epoch 46, Batch 4/32 : Loss = 0.05819207429885864\n",
      "Epoch 46, Batch 5/32 : Loss = 0.003252076217904687\n",
      "Epoch 46, Batch 6/32 : Loss = 0.0057120416313409805\n",
      "Epoch 46, Batch 7/32 : Loss = 0.005500718019902706\n",
      "Epoch 46, Batch 8/32 : Loss = 0.11343782395124435\n",
      "Epoch 46, Batch 9/32 : Loss = 0.008194224908947945\n",
      "Epoch 46, Batch 10/32 : Loss = 0.004640650004148483\n",
      "Epoch 46, Batch 11/32 : Loss = 0.013872907496988773\n",
      "Epoch 46, Batch 12/32 : Loss = 0.010255416855216026\n",
      "Epoch 46, Batch 13/32 : Loss = 0.02575887180864811\n",
      "Epoch 46, Batch 14/32 : Loss = 0.02431143820285797\n",
      "Epoch 46, Batch 15/32 : Loss = 0.02027704566717148\n",
      "Epoch 46, Batch 16/32 : Loss = 0.02189183048903942\n",
      "Epoch 46, Batch 17/32 : Loss = 0.01742047443985939\n",
      "Epoch 46, Batch 18/32 : Loss = 0.04949139058589935\n",
      "Epoch 46, Batch 19/32 : Loss = 0.004651516675949097\n",
      "Epoch 46, Batch 20/32 : Loss = 0.09911181777715683\n",
      "Epoch 46, Batch 21/32 : Loss = 0.014654600992798805\n",
      "Epoch 46, Batch 22/32 : Loss = 0.13594114780426025\n",
      "Epoch 46, Batch 23/32 : Loss = 0.016007089987397194\n",
      "Epoch 46, Batch 24/32 : Loss = 0.003893018700182438\n",
      "Epoch 46, Batch 25/32 : Loss = 0.02564660646021366\n",
      "Epoch 46, Batch 26/32 : Loss = 0.011223310604691505\n",
      "Epoch 46, Batch 27/32 : Loss = 0.020332254469394684\n",
      "Epoch 46, Batch 28/32 : Loss = 0.006166035309433937\n",
      "Epoch 46, Batch 29/32 : Loss = 0.021990597248077393\n",
      "Epoch 46, Batch 30/32 : Loss = 0.005303751677274704\n",
      "Epoch 46, Batch 31/32 : Loss = 0.1252792626619339\n",
      "Epoch 46 finished in 0.043631382783253986 minutes\n",
      "Epoch 46 training_loss = 0.02576272189617157\n",
      "----{--BB-----Y----RR----aa---yy---h----#----22--->>----E----44----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "--2---p----:--m------x---a---zz---n----@-----CC----y---%%-----%----- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "--------33-----\\-----$----->>------SS-----\\----MM--------ii--B------ => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "--.---c----O------w------uu---``--u----.--RR----{---3----\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 46 val_loss = 0.6292022466659546, word_accuracy = 0.63\n",
      "Epoch 47, Batch 0/32 : Loss = 0.1488022804260254\n",
      "Epoch 47, Batch 1/32 : Loss = 0.03196733817458153\n",
      "Epoch 47, Batch 2/32 : Loss = 0.014024805277585983\n",
      "Epoch 47, Batch 3/32 : Loss = 0.03451285883784294\n",
      "Epoch 47, Batch 4/32 : Loss = 0.0037450974341481924\n",
      "Epoch 47, Batch 5/32 : Loss = 0.02226797305047512\n",
      "Epoch 47, Batch 6/32 : Loss = 0.003424837486818433\n",
      "Epoch 47, Batch 7/32 : Loss = 0.010885076597332954\n",
      "Epoch 47, Batch 8/32 : Loss = 0.032644350081682205\n",
      "Epoch 47, Batch 9/32 : Loss = 0.003902859054505825\n",
      "Epoch 47, Batch 10/32 : Loss = 0.009815085679292679\n",
      "Epoch 47, Batch 11/32 : Loss = 0.00724452780559659\n",
      "Epoch 47, Batch 12/32 : Loss = 0.02562083676457405\n",
      "Epoch 47, Batch 13/32 : Loss = 0.021746506914496422\n",
      "Epoch 47, Batch 14/32 : Loss = 0.009855195879936218\n",
      "Epoch 47, Batch 15/32 : Loss = 0.03463255241513252\n",
      "Epoch 47, Batch 16/32 : Loss = 0.006758755072951317\n",
      "Epoch 47, Batch 17/32 : Loss = 0.00442797876894474\n",
      "Epoch 47, Batch 18/32 : Loss = 0.042393602430820465\n",
      "Epoch 47, Batch 19/32 : Loss = 0.011535940691828728\n",
      "Epoch 47, Batch 20/32 : Loss = 0.10507374256849289\n",
      "Epoch 47, Batch 21/32 : Loss = 0.0038196309469640255\n",
      "Epoch 47, Batch 22/32 : Loss = 0.009743870235979557\n",
      "Epoch 47, Batch 23/32 : Loss = 0.008336452767252922\n",
      "Epoch 47, Batch 24/32 : Loss = 0.005448414012789726\n",
      "Epoch 47, Batch 25/32 : Loss = 0.003241690807044506\n",
      "Epoch 47, Batch 26/32 : Loss = 0.011592712253332138\n",
      "Epoch 47, Batch 27/32 : Loss = 0.006534511223435402\n",
      "Epoch 47, Batch 28/32 : Loss = 0.01749686524271965\n",
      "Epoch 47, Batch 29/32 : Loss = 0.016131531447172165\n",
      "Epoch 47, Batch 30/32 : Loss = 0.0033219438046216965\n",
      "Epoch 47, Batch 31/32 : Loss = 0.00888133980333805\n",
      "Epoch 47 finished in 0.042545509338378903 minutes\n",
      "Epoch 47 training_loss = 0.021592289209365845\n",
      "-----c----R-----;--99----y----?----2----dd---ii---------{--!!---- => cR;9y?2di{!, Ground Truth is cR;9y?2diO{!\n",
      "----55----9----g----mm-------J---uu---xx---c---X---.--d----\\\\---- => 59gmJuxcX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "---{--BB-----Y----R-----a----y---h---#-----2---->----EE---44----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "-----0-----JJ---!!--(--;;--AA-----3----,--''-))--rr---r----7----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 47 val_loss = 0.5829914212226868, word_accuracy = 0.69\n",
      "Epoch 48, Batch 0/32 : Loss = 0.021971525624394417\n",
      "Epoch 48, Batch 1/32 : Loss = 0.006432804279029369\n",
      "Epoch 48, Batch 2/32 : Loss = 0.01534263789653778\n",
      "Epoch 48, Batch 3/32 : Loss = 0.025415807962417603\n",
      "Epoch 48, Batch 4/32 : Loss = 0.008305681869387627\n",
      "Epoch 48, Batch 5/32 : Loss = 0.007908668369054794\n",
      "Epoch 48, Batch 6/32 : Loss = 0.02690420299768448\n",
      "Epoch 48, Batch 7/32 : Loss = 0.005556041840463877\n",
      "Epoch 48, Batch 8/32 : Loss = 0.01822773925960064\n",
      "Epoch 48, Batch 9/32 : Loss = 0.08216318488121033\n",
      "Epoch 48, Batch 10/32 : Loss = 0.007397335022687912\n",
      "Epoch 48, Batch 11/32 : Loss = 0.040579333901405334\n",
      "Epoch 48, Batch 12/32 : Loss = 0.022294720634818077\n",
      "Epoch 48, Batch 13/32 : Loss = 0.0049464222975075245\n",
      "Epoch 48, Batch 14/32 : Loss = 0.009433857165277004\n",
      "Epoch 48, Batch 15/32 : Loss = 0.031297467648983\n",
      "Epoch 48, Batch 16/32 : Loss = 0.0037825845647603273\n",
      "Epoch 48, Batch 17/32 : Loss = 0.0042340862564742565\n",
      "Epoch 48, Batch 18/32 : Loss = 0.009375500492751598\n",
      "Epoch 48, Batch 19/32 : Loss = 0.014696354046463966\n",
      "Epoch 48, Batch 20/32 : Loss = 0.017392028123140335\n",
      "Epoch 48, Batch 21/32 : Loss = 0.05772846192121506\n",
      "Epoch 48, Batch 22/32 : Loss = 0.005495086777955294\n",
      "Epoch 48, Batch 23/32 : Loss = 0.008815202862024307\n",
      "Epoch 48, Batch 24/32 : Loss = 0.00994997750967741\n",
      "Epoch 48, Batch 25/32 : Loss = 0.08729830384254456\n",
      "Epoch 48, Batch 26/32 : Loss = 0.035398952662944794\n",
      "Epoch 48, Batch 27/32 : Loss = 0.002710394561290741\n",
      "Epoch 48, Batch 28/32 : Loss = 0.012258065864443779\n",
      "Epoch 48, Batch 29/32 : Loss = 0.016088135540485382\n",
      "Epoch 48, Batch 30/32 : Loss = 0.004711753223091364\n",
      "Epoch 48, Batch 31/32 : Loss = 0.005666372366249561\n",
      "Epoch 48 finished in 0.04344122409820557 minutes\n",
      "Epoch 48 training_loss = 0.02007455565035343\n",
      "----CC-----D-----E----gg----m------\"---m-------F---<<-----Q----88---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "---2---pp---::-mm------x---a----z---nn----@@-----C----yy---%------%%------ => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "---555---->>----:--*----Y----A-----'--O-----DD-----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----kk----OO------//--,,--yy----c----**----1-----P----}}---#-----BB------- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 48 val_loss = 0.6274551153182983, word_accuracy = 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Batch 0/32 : Loss = 0.003646902274340391\n",
      "Epoch 49, Batch 1/32 : Loss = 0.01492699421942234\n",
      "Epoch 49, Batch 2/32 : Loss = 0.0033985725603997707\n",
      "Epoch 49, Batch 3/32 : Loss = 0.0039033072534948587\n",
      "Epoch 49, Batch 4/32 : Loss = 0.01036953553557396\n",
      "Epoch 49, Batch 5/32 : Loss = 0.04089135676622391\n",
      "Epoch 49, Batch 6/32 : Loss = 0.026826195418834686\n",
      "Epoch 49, Batch 7/32 : Loss = 0.010076514445245266\n",
      "Epoch 49, Batch 8/32 : Loss = 0.004615632817149162\n",
      "Epoch 49, Batch 9/32 : Loss = 0.007368146907538176\n",
      "Epoch 49, Batch 10/32 : Loss = 0.0026864707469940186\n",
      "Epoch 49, Batch 11/32 : Loss = 0.0037090517580509186\n",
      "Epoch 49, Batch 12/32 : Loss = 0.0031075631268322468\n",
      "Epoch 49, Batch 13/32 : Loss = 0.007015001028776169\n",
      "Epoch 49, Batch 14/32 : Loss = 0.055939931422472\n",
      "Epoch 49, Batch 15/32 : Loss = 0.003327911952510476\n",
      "Epoch 49, Batch 16/32 : Loss = 0.005821590311825275\n",
      "Epoch 49, Batch 17/32 : Loss = 0.015004139393568039\n",
      "Epoch 49, Batch 18/32 : Loss = 0.0035868072882294655\n",
      "Epoch 49, Batch 19/32 : Loss = 0.0035994546487927437\n",
      "Epoch 49, Batch 20/32 : Loss = 0.003907717764377594\n",
      "Epoch 49, Batch 21/32 : Loss = 0.00341402436606586\n",
      "Epoch 49, Batch 22/32 : Loss = 0.015388507395982742\n",
      "Epoch 49, Batch 23/32 : Loss = 0.0033184296917170286\n",
      "Epoch 49, Batch 24/32 : Loss = 0.004263028968125582\n",
      "Epoch 49, Batch 25/32 : Loss = 0.00480648735538125\n",
      "Epoch 49, Batch 26/32 : Loss = 0.0937955379486084\n",
      "Epoch 49, Batch 27/32 : Loss = 0.01722913235425949\n",
      "Epoch 49, Batch 28/32 : Loss = 0.014997733756899834\n",
      "Epoch 49, Batch 29/32 : Loss = 0.11771002411842346\n",
      "Epoch 49, Batch 30/32 : Loss = 0.0024516235571354628\n",
      "Epoch 49, Batch 31/32 : Loss = 0.007943905889987946\n",
      "Epoch 49 finished in 0.04331525961558024 minutes\n",
      "Epoch 49 training_loss = 0.016452893614768982\n",
      "----X----7----0---j---@-----SS---ZZ----L---4-----C----m------MM--------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---------3------\\-----$------>-------SS-----\\-----M---------i--BB------- => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "---kk----$-----I--F-----DD-----e-----hh----]---k-----0----\\\\---X-------- => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---WW------=----22---++---EE----11--nn----TT---X----r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 49 val_loss = 0.6666815876960754, word_accuracy = 0.65\n",
      "Epoch 50, Batch 0/32 : Loss = 0.006122119724750519\n",
      "Epoch 50, Batch 1/32 : Loss = 0.0031156265176832676\n",
      "Epoch 50, Batch 2/32 : Loss = 0.0037982952781021595\n",
      "Epoch 50, Batch 3/32 : Loss = 0.007229749578982592\n",
      "Epoch 50, Batch 4/32 : Loss = 0.00937537383288145\n",
      "Epoch 50, Batch 5/32 : Loss = 0.002817363478243351\n",
      "Epoch 50, Batch 6/32 : Loss = 0.0030687283724546432\n",
      "Epoch 50, Batch 7/32 : Loss = 0.003432496450841427\n",
      "Epoch 50, Batch 8/32 : Loss = 0.003296120557934046\n",
      "Epoch 50, Batch 9/32 : Loss = 0.00277286721393466\n",
      "Epoch 50, Batch 10/32 : Loss = 0.00683549465611577\n",
      "Epoch 50, Batch 11/32 : Loss = 0.00827821809798479\n",
      "Epoch 50, Batch 12/32 : Loss = 0.006461527198553085\n",
      "Epoch 50, Batch 13/32 : Loss = 0.0036949373316019773\n",
      "Epoch 50, Batch 14/32 : Loss = 0.003929371479898691\n",
      "Epoch 50, Batch 15/32 : Loss = 0.0030445652082562447\n",
      "Epoch 50, Batch 16/32 : Loss = 0.005913751199841499\n",
      "Epoch 50, Batch 17/32 : Loss = 0.01439032144844532\n",
      "Epoch 50, Batch 18/32 : Loss = 0.005197014659643173\n",
      "Epoch 50, Batch 19/32 : Loss = 0.004560819827020168\n",
      "Epoch 50, Batch 20/32 : Loss = 0.0030598826706409454\n",
      "Epoch 50, Batch 21/32 : Loss = 0.0033778450451791286\n",
      "Epoch 50, Batch 22/32 : Loss = 0.002792798215523362\n",
      "Epoch 50, Batch 23/32 : Loss = 0.0024677268229424953\n",
      "Epoch 50, Batch 24/32 : Loss = 0.003001788631081581\n",
      "Epoch 50, Batch 25/32 : Loss = 0.21545064449310303\n",
      "Epoch 50, Batch 26/32 : Loss = 0.0025467677041888237\n",
      "Epoch 50, Batch 27/32 : Loss = 0.008136462420225143\n",
      "Epoch 50, Batch 28/32 : Loss = 0.08903879672288895\n",
      "Epoch 50, Batch 29/32 : Loss = 0.03632190078496933\n",
      "Epoch 50, Batch 30/32 : Loss = 0.007381777744740248\n",
      "Epoch 50, Batch 31/32 : Loss = 0.004881925880908966\n",
      "Epoch 50 finished in 0.043235071500142414 minutes\n",
      "Epoch 50 training_loss = 0.015470564365386963\n",
      "----k----$----|--'-,--9----Y---N-----WW-----mm------T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---B-----.--Y----l--W------66----F----hh----X---'--Y----2----- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "--.--cc---O------w-----u----`--u----.-RR----{--33---\"--##----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----0-----JJ---!--((--;--AA-----33---,,-'--)---r---rr---77---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 50 val_loss = 0.6310385465621948, word_accuracy = 0.68\n",
      "Epoch 51, Batch 0/32 : Loss = 0.030371537432074547\n",
      "Epoch 51, Batch 1/32 : Loss = 0.002408404368907213\n",
      "Epoch 51, Batch 2/32 : Loss = 0.006529947742819786\n",
      "Epoch 51, Batch 3/32 : Loss = 0.00539474468678236\n",
      "Epoch 51, Batch 4/32 : Loss = 0.00915883295238018\n",
      "Epoch 51, Batch 5/32 : Loss = 0.005586931016296148\n",
      "Epoch 51, Batch 6/32 : Loss = 0.010394054464995861\n",
      "Epoch 51, Batch 7/32 : Loss = 0.03519250452518463\n",
      "Epoch 51, Batch 8/32 : Loss = 0.003981976769864559\n",
      "Epoch 51, Batch 9/32 : Loss = 0.015089934691786766\n",
      "Epoch 51, Batch 10/32 : Loss = 0.011301605962216854\n",
      "Epoch 51, Batch 11/32 : Loss = 0.2632630467414856\n",
      "Epoch 51, Batch 12/32 : Loss = 0.007249891757965088\n",
      "Epoch 51, Batch 13/32 : Loss = 0.08955402672290802\n",
      "Epoch 51, Batch 14/32 : Loss = 0.002706365194171667\n",
      "Epoch 51, Batch 15/32 : Loss = 0.006495430134236813\n",
      "Epoch 51, Batch 16/32 : Loss = 0.029754972085356712\n",
      "Epoch 51, Batch 17/32 : Loss = 0.015342749655246735\n",
      "Epoch 51, Batch 18/32 : Loss = 0.006669850088655949\n",
      "Epoch 51, Batch 19/32 : Loss = 0.007338843774050474\n",
      "Epoch 51, Batch 20/32 : Loss = 0.007452904246747494\n",
      "Epoch 51, Batch 21/32 : Loss = 0.005567362066358328\n",
      "Epoch 51, Batch 22/32 : Loss = 0.043005961924791336\n",
      "Epoch 51, Batch 23/32 : Loss = 0.004165441729128361\n",
      "Epoch 51, Batch 24/32 : Loss = 0.0032576145604252815\n",
      "Epoch 51, Batch 25/32 : Loss = 0.14257420599460602\n",
      "Epoch 51, Batch 26/32 : Loss = 0.005123730283230543\n",
      "Epoch 51, Batch 27/32 : Loss = 0.005472536198794842\n",
      "Epoch 51, Batch 28/32 : Loss = 0.018769970163702965\n",
      "Epoch 51, Batch 29/32 : Loss = 0.004426208324730396\n",
      "Epoch 51, Batch 30/32 : Loss = 0.004573657643049955\n",
      "Epoch 51, Batch 31/32 : Loss = 0.10816387087106705\n",
      "Epoch 51 finished in 0.043555744489034015 minutes\n",
      "Epoch 51 training_loss = 0.02639986388385296\n",
      "---C----DD----E---gg---mm----\"\"--m------F---<----QQ----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "---/---MM------oo----E----^----3----x----/---&----66----XX------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---:------3-----\\---$$---->>-----S-----\\----M-------ii-B-------- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "--------3-----\\----$----->>------S-----\\-\\--MM-------ii-BB------ => -3\\$>S\\\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 51 val_loss = 0.619772732257843, word_accuracy = 0.71\n",
      "Epoch 52, Batch 0/32 : Loss = 0.005588970147073269\n",
      "Epoch 52, Batch 1/32 : Loss = 0.0805978998541832\n",
      "Epoch 52, Batch 2/32 : Loss = 0.019845185801386833\n",
      "Epoch 52, Batch 3/32 : Loss = 0.005721474997699261\n",
      "Epoch 52, Batch 4/32 : Loss = 0.006561023648828268\n",
      "Epoch 52, Batch 5/32 : Loss = 0.011454625055193901\n",
      "Epoch 52, Batch 6/32 : Loss = 0.006103510037064552\n",
      "Epoch 52, Batch 7/32 : Loss = 0.006344563327729702\n",
      "Epoch 52, Batch 8/32 : Loss = 0.052613139152526855\n",
      "Epoch 52, Batch 9/32 : Loss = 0.022859273478388786\n",
      "Epoch 52, Batch 10/32 : Loss = 0.007252017967402935\n",
      "Epoch 52, Batch 11/32 : Loss = 0.003463407512754202\n",
      "Epoch 52, Batch 12/32 : Loss = 0.004083313047885895\n",
      "Epoch 52, Batch 13/32 : Loss = 0.007974755018949509\n",
      "Epoch 52, Batch 14/32 : Loss = 0.1890045404434204\n",
      "Epoch 52, Batch 15/32 : Loss = 0.00942973978817463\n",
      "Epoch 52, Batch 16/32 : Loss = 0.005168087314814329\n",
      "Epoch 52, Batch 17/32 : Loss = 0.003343414980918169\n",
      "Epoch 52, Batch 18/32 : Loss = 0.1426718384027481\n",
      "Epoch 52, Batch 19/32 : Loss = 0.0025471553672105074\n",
      "Epoch 52, Batch 20/32 : Loss = 0.027032213285565376\n",
      "Epoch 52, Batch 21/32 : Loss = 0.040877815335989\n",
      "Epoch 52, Batch 22/32 : Loss = 0.0851258784532547\n",
      "Epoch 52, Batch 23/32 : Loss = 0.005003361497074366\n",
      "Epoch 52, Batch 24/32 : Loss = 0.002707959618419409\n",
      "Epoch 52, Batch 25/32 : Loss = 0.005706381052732468\n",
      "Epoch 52, Batch 26/32 : Loss = 0.00611052755266428\n",
      "Epoch 52, Batch 27/32 : Loss = 0.07461182028055191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Batch 28/32 : Loss = 0.03621332347393036\n",
      "Epoch 52, Batch 29/32 : Loss = 0.006159237585961819\n",
      "Epoch 52, Batch 30/32 : Loss = 0.007893625646829605\n",
      "Epoch 52, Batch 31/32 : Loss = 0.027532879263162613\n",
      "Epoch 52 finished in 0.04515417019526164 minutes\n",
      "Epoch 52 training_loss = 0.028707200661301613\n",
      "---dd---!--NN----rr--A---jj-**--$----3---hh---55---n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "--{--B----Y----R---aa---y--hh--#----2--->----E---4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----r--]--t--4--------^---W-------Q---44---->----g----- => r]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---kk---O-----/--,--y---c---*---1----P---}--##---B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 52 val_loss = 0.6181474328041077, word_accuracy = 0.6\n",
      "Epoch 53, Batch 0/32 : Loss = 0.030946478247642517\n",
      "Epoch 53, Batch 1/32 : Loss = 0.015551330521702766\n",
      "Epoch 53, Batch 2/32 : Loss = 0.005607552360743284\n",
      "Epoch 53, Batch 3/32 : Loss = 0.02415458671748638\n",
      "Epoch 53, Batch 4/32 : Loss = 0.005544564221054316\n",
      "Epoch 53, Batch 5/32 : Loss = 0.011915860697627068\n",
      "Epoch 53, Batch 6/32 : Loss = 0.004475703462958336\n",
      "Epoch 53, Batch 7/32 : Loss = 0.003155668266117573\n",
      "Epoch 53, Batch 8/32 : Loss = 0.012798232957720757\n",
      "Epoch 53, Batch 9/32 : Loss = 0.003069471102207899\n",
      "Epoch 53, Batch 10/32 : Loss = 0.0030116226989775896\n",
      "Epoch 53, Batch 11/32 : Loss = 0.010811292566359043\n",
      "Epoch 53, Batch 12/32 : Loss = 0.007316300645470619\n",
      "Epoch 53, Batch 13/32 : Loss = 0.037844959646463394\n",
      "Epoch 53, Batch 14/32 : Loss = 0.005790063180029392\n",
      "Epoch 53, Batch 15/32 : Loss = 0.0025759979616850615\n",
      "Epoch 53, Batch 16/32 : Loss = 0.003546284744516015\n",
      "Epoch 53, Batch 17/32 : Loss = 0.003962444141507149\n",
      "Epoch 53, Batch 18/32 : Loss = 0.002446301281452179\n",
      "Epoch 53, Batch 19/32 : Loss = 0.011998039670288563\n",
      "Epoch 53, Batch 20/32 : Loss = 0.003754467237740755\n",
      "Epoch 53, Batch 21/32 : Loss = 0.0031613940373063087\n",
      "Epoch 53, Batch 22/32 : Loss = 0.03542722761631012\n",
      "Epoch 53, Batch 23/32 : Loss = 0.002415597205981612\n",
      "Epoch 53, Batch 24/32 : Loss = 0.006196252070367336\n",
      "Epoch 53, Batch 25/32 : Loss = 0.007741205859929323\n",
      "Epoch 53, Batch 26/32 : Loss = 0.03235596790909767\n",
      "Epoch 53, Batch 27/32 : Loss = 0.0025680584367364645\n",
      "Epoch 53, Batch 28/32 : Loss = 0.040645211935043335\n",
      "Epoch 53, Batch 29/32 : Loss = 0.005362290423363447\n",
      "Epoch 53, Batch 30/32 : Loss = 0.021440647542476654\n",
      "Epoch 53, Batch 31/32 : Loss = 0.06470699608325958\n",
      "Epoch 53 finished in 0.044526724020640056 minutes\n",
      "Epoch 53 training_loss = 0.012070024386048317\n",
      "---{--BB------Y----R-----a-----y---h----#----22---->-----E----44----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "---2----p----:--m-----X---a---z---nn---@@-----C----y---%------%------ => 2p:mXazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "------QQ------3-----g----II--z----#-----Y----:---]--qq-----+----**--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----\"----]--t---44----------^^---W---------Q-----44----->------g----- => \"]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 53 val_loss = 0.6451787948608398, word_accuracy = 0.64\n",
      "Epoch 54, Batch 0/32 : Loss = 0.005222545936703682\n",
      "Epoch 54, Batch 1/32 : Loss = 0.0036706281825900078\n",
      "Epoch 54, Batch 2/32 : Loss = 0.0021077431738376617\n",
      "Epoch 54, Batch 3/32 : Loss = 0.006561797112226486\n",
      "Epoch 54, Batch 4/32 : Loss = 0.002160058356821537\n",
      "Epoch 54, Batch 5/32 : Loss = 0.0029364856891334057\n",
      "Epoch 54, Batch 6/32 : Loss = 0.0023173829540610313\n",
      "Epoch 54, Batch 7/32 : Loss = 0.007010056637227535\n",
      "Epoch 54, Batch 8/32 : Loss = 0.007445089984685183\n",
      "Epoch 54, Batch 9/32 : Loss = 0.009277720004320145\n",
      "Epoch 54, Batch 10/32 : Loss = 0.018165748566389084\n",
      "Epoch 54, Batch 11/32 : Loss = 0.003918306902050972\n",
      "Epoch 54, Batch 12/32 : Loss = 0.0027981912717223167\n",
      "Epoch 54, Batch 13/32 : Loss = 0.003920810297131538\n",
      "Epoch 54, Batch 14/32 : Loss = 0.008148347027599812\n",
      "Epoch 54, Batch 15/32 : Loss = 0.0022480313200503588\n",
      "Epoch 54, Batch 16/32 : Loss = 0.004826700314879417\n",
      "Epoch 54, Batch 17/32 : Loss = 0.019575022161006927\n",
      "Epoch 54, Batch 18/32 : Loss = 0.003177060279995203\n",
      "Epoch 54, Batch 19/32 : Loss = 0.010203990153968334\n",
      "Epoch 54, Batch 20/32 : Loss = 0.002795251552015543\n",
      "Epoch 54, Batch 21/32 : Loss = 0.009139874018728733\n",
      "Epoch 54, Batch 22/32 : Loss = 0.016095761209726334\n",
      "Epoch 54, Batch 23/32 : Loss = 0.0018777140649035573\n",
      "Epoch 54, Batch 24/32 : Loss = 0.004389820154756308\n",
      "Epoch 54, Batch 25/32 : Loss = 0.002405670238658786\n",
      "Epoch 54, Batch 26/32 : Loss = 0.0017172887455672026\n",
      "Epoch 54, Batch 27/32 : Loss = 0.004534909501671791\n",
      "Epoch 54, Batch 28/32 : Loss = 0.010827932506799698\n",
      "Epoch 54, Batch 29/32 : Loss = 0.019683802500367165\n",
      "Epoch 54, Batch 30/32 : Loss = 0.0027337984647601843\n",
      "Epoch 54, Batch 31/32 : Loss = 0.006395407021045685\n",
      "Epoch 54 finished in 0.04463006258010864 minutes\n",
      "Epoch 54 training_loss = 0.006512224208563566\n",
      "---BB-----.---Y-----l--W--------66-----F----hh-----XX---'---Y-----2------ => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----0----Q----66----<<---<<----(---T----N-----5---==---P----(----------- => 0Q6<<(TN5=P(, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----W-------=----2----+----E----11---n----TT----X---r---C-----a---------- => W=2+E1nTXrCa, Ground Truth is W=2+E1nTXrCan\n",
      "---kk----$-----I--F-----DD-----ee-----hh----]--kk-----0----\\\\---X-------- => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 54 val_loss = 0.6402097940444946, word_accuracy = 0.72\n",
      "Epoch 55, Batch 0/32 : Loss = 0.005137619562447071\n",
      "Epoch 55, Batch 1/32 : Loss = 0.02407051995396614\n",
      "Epoch 55, Batch 2/32 : Loss = 0.0034009632654488087\n",
      "Epoch 55, Batch 3/32 : Loss = 0.005123670678585768\n",
      "Epoch 55, Batch 4/32 : Loss = 0.02246609702706337\n",
      "Epoch 55, Batch 5/32 : Loss = 0.03702311962842941\n",
      "Epoch 55, Batch 6/32 : Loss = 0.009553218260407448\n",
      "Epoch 55, Batch 7/32 : Loss = 0.0023490898311138153\n",
      "Epoch 55, Batch 8/32 : Loss = 0.0018689630087465048\n",
      "Epoch 55, Batch 9/32 : Loss = 0.0019662054255604744\n",
      "Epoch 55, Batch 10/32 : Loss = 0.013224710710346699\n",
      "Epoch 55, Batch 11/32 : Loss = 0.004395226016640663\n",
      "Epoch 55, Batch 12/32 : Loss = 0.002619580365717411\n",
      "Epoch 55, Batch 13/32 : Loss = 0.005305632948875427\n",
      "Epoch 55, Batch 14/32 : Loss = 0.005747361574321985\n",
      "Epoch 55, Batch 15/32 : Loss = 0.005650727543979883\n",
      "Epoch 55, Batch 16/32 : Loss = 0.06834498792886734\n",
      "Epoch 55, Batch 17/32 : Loss = 0.026529746130108833\n",
      "Epoch 55, Batch 18/32 : Loss = 0.0038769515231251717\n",
      "Epoch 55, Batch 19/32 : Loss = 0.00785893015563488\n",
      "Epoch 55, Batch 20/32 : Loss = 0.008891550824046135\n",
      "Epoch 55, Batch 21/32 : Loss = 0.0040235030464828014\n",
      "Epoch 55, Batch 22/32 : Loss = 0.003148494753986597\n",
      "Epoch 55, Batch 23/32 : Loss = 0.0033549568615853786\n",
      "Epoch 55, Batch 24/32 : Loss = 0.017571112141013145\n",
      "Epoch 55, Batch 25/32 : Loss = 0.009893465787172318\n",
      "Epoch 55, Batch 26/32 : Loss = 0.016710616648197174\n",
      "Epoch 55, Batch 27/32 : Loss = 0.004364402499049902\n",
      "Epoch 55, Batch 28/32 : Loss = 0.002283631358295679\n",
      "Epoch 55, Batch 29/32 : Loss = 0.0029017915949225426\n",
      "Epoch 55, Batch 30/32 : Loss = 0.0017677408177405596\n",
      "Epoch 55, Batch 31/32 : Loss = 0.007701206486672163\n",
      "Epoch 55 finished in 0.04607597986857096 minutes\n",
      "Epoch 55 training_loss = 0.010679108090698719\n",
      "---JJ--;---q----+---//--zz---y---U-----%%-----UU----11---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "---k---$----|---'-,--9----Y---NN-----W------m------TT---8------ => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---55--->>---:--*---Y---AA---'--O----DD----*--#-----O----g----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---dd---:---X----9----ee----aa----F--,--88--------V----R------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 55 val_loss = 0.6629456877708435, word_accuracy = 0.63\n",
      "Epoch 56, Batch 0/32 : Loss = 0.002003643661737442\n",
      "Epoch 56, Batch 1/32 : Loss = 0.0056647672317922115\n",
      "Epoch 56, Batch 2/32 : Loss = 0.0030623346101492643\n",
      "Epoch 56, Batch 3/32 : Loss = 0.03169115260243416\n",
      "Epoch 56, Batch 4/32 : Loss = 0.00931764580309391\n",
      "Epoch 56, Batch 5/32 : Loss = 0.03486844152212143\n",
      "Epoch 56, Batch 6/32 : Loss = 0.007227750960737467\n",
      "Epoch 56, Batch 7/32 : Loss = 0.0017372367437928915\n",
      "Epoch 56, Batch 8/32 : Loss = 0.0027464053127914667\n",
      "Epoch 56, Batch 9/32 : Loss = 0.0021358560770750046\n",
      "Epoch 56, Batch 10/32 : Loss = 0.008918749168515205\n",
      "Epoch 56, Batch 11/32 : Loss = 0.0027398397214710712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Batch 12/32 : Loss = 0.0034431456588208675\n",
      "Epoch 56, Batch 13/32 : Loss = 0.004194664303213358\n",
      "Epoch 56, Batch 14/32 : Loss = 0.0032498473301529884\n",
      "Epoch 56, Batch 15/32 : Loss = 0.020714199170470238\n",
      "Epoch 56, Batch 16/32 : Loss = 0.01325424574315548\n",
      "Epoch 56, Batch 17/32 : Loss = 0.004318361636251211\n",
      "Epoch 56, Batch 18/32 : Loss = 0.05812352895736694\n",
      "Epoch 56, Batch 19/32 : Loss = 0.009965667501091957\n",
      "Epoch 56, Batch 20/32 : Loss = 0.011685779318213463\n",
      "Epoch 56, Batch 21/32 : Loss = 0.0073767779394984245\n",
      "Epoch 56, Batch 22/32 : Loss = 0.023244574666023254\n",
      "Epoch 56, Batch 23/32 : Loss = 0.003326362231746316\n",
      "Epoch 56, Batch 24/32 : Loss = 0.09752857685089111\n",
      "Epoch 56, Batch 25/32 : Loss = 0.037845056504011154\n",
      "Epoch 56, Batch 26/32 : Loss = 0.09052979946136475\n",
      "Epoch 56, Batch 27/32 : Loss = 0.0028832638636231422\n",
      "Epoch 56, Batch 28/32 : Loss = 0.003443952649831772\n",
      "Epoch 56, Batch 29/32 : Loss = 0.004965629428625107\n",
      "Epoch 56, Batch 30/32 : Loss = 0.005740782245993614\n",
      "Epoch 56, Batch 31/32 : Loss = 0.10790546238422394\n",
      "Epoch 56 finished in 0.04507142702738444 minutes\n",
      "Epoch 56 training_loss = 0.017074251547455788\n",
      "-----Q------3----gg---II-zz---#-----Y---:--]]--qq----+----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----z----0----\"----GG-----/----~~----$-----c----1----j--t----- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---0-----JJ---!--(--;;--A-----33----,--'-))--r---rr---7------- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----J--;---q----+---/---z---yy---U----%%-----UU----11---x---_- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 56 val_loss = 0.627153754234314, word_accuracy = 0.68\n",
      "Epoch 57, Batch 0/32 : Loss = 0.007593459449708462\n",
      "Epoch 57, Batch 1/32 : Loss = 0.005544345825910568\n",
      "Epoch 57, Batch 2/32 : Loss = 0.0029661147855222225\n",
      "Epoch 57, Batch 3/32 : Loss = 0.0022128387354314327\n",
      "Epoch 57, Batch 4/32 : Loss = 0.0028428002260625362\n",
      "Epoch 57, Batch 5/32 : Loss = 0.009331424720585346\n",
      "Epoch 57, Batch 6/32 : Loss = 0.056449100375175476\n",
      "Epoch 57, Batch 7/32 : Loss = 0.005342846270650625\n",
      "Epoch 57, Batch 8/32 : Loss = 0.0097559979185462\n",
      "Epoch 57, Batch 9/32 : Loss = 0.007068050093948841\n",
      "Epoch 57, Batch 10/32 : Loss = 0.01418466679751873\n",
      "Epoch 57, Batch 11/32 : Loss = 0.005867445841431618\n",
      "Epoch 57, Batch 12/32 : Loss = 0.006147944368422031\n",
      "Epoch 57, Batch 13/32 : Loss = 0.003954262938350439\n",
      "Epoch 57, Batch 14/32 : Loss = 0.018075335770845413\n",
      "Epoch 57, Batch 15/32 : Loss = 0.005514147691428661\n",
      "Epoch 57, Batch 16/32 : Loss = 0.007882560603320599\n",
      "Epoch 57, Batch 17/32 : Loss = 0.007249322719871998\n",
      "Epoch 57, Batch 18/32 : Loss = 0.003643166273832321\n",
      "Epoch 57, Batch 19/32 : Loss = 0.03131909668445587\n",
      "Epoch 57, Batch 20/32 : Loss = 0.0028980709612369537\n",
      "Epoch 57, Batch 21/32 : Loss = 0.00310794310644269\n",
      "Epoch 57, Batch 22/32 : Loss = 0.012751616537570953\n",
      "Epoch 57, Batch 23/32 : Loss = 0.004461209289729595\n",
      "Epoch 57, Batch 24/32 : Loss = 0.010107258334755898\n",
      "Epoch 57, Batch 25/32 : Loss = 0.004514190834015608\n",
      "Epoch 57, Batch 26/32 : Loss = 0.14282982051372528\n",
      "Epoch 57, Batch 27/32 : Loss = 0.07941383123397827\n",
      "Epoch 57, Batch 28/32 : Loss = 0.011382650583982468\n",
      "Epoch 57, Batch 29/32 : Loss = 0.004568080417811871\n",
      "Epoch 57, Batch 30/32 : Loss = 0.0056410664692521095\n",
      "Epoch 57, Batch 31/32 : Loss = 0.003909579012542963\n",
      "Epoch 57 finished in 0.04659113089243571 minutes\n",
      "Epoch 57 training_loss = 0.015907127410173416\n",
      "-----z-----0-----\"-----GG------//----~~-----$------c------1----j---t------ => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---J---;---qq----++----/---zz---y----U------%%------UU------1----x---__--- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "-----W-------=----2----+----E-----1----n----T----X----r---C----a---I------ => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "------Y-----W--------]--i--\\----|----MM-------<-----MM------88----8------- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 57 val_loss = 0.6184348464012146, word_accuracy = 0.72\n",
      "Epoch 58, Batch 0/32 : Loss = 0.010675538331270218\n",
      "Epoch 58, Batch 1/32 : Loss = 0.003288818523287773\n",
      "Epoch 58, Batch 2/32 : Loss = 0.045123688876628876\n",
      "Epoch 58, Batch 3/32 : Loss = 0.008198734372854233\n",
      "Epoch 58, Batch 4/32 : Loss = 0.046000588685274124\n",
      "Epoch 58, Batch 5/32 : Loss = 0.019605791196227074\n",
      "Epoch 58, Batch 6/32 : Loss = 0.0025956006720662117\n",
      "Epoch 58, Batch 7/32 : Loss = 0.009207974188029766\n",
      "Epoch 58, Batch 8/32 : Loss = 0.011135077103972435\n",
      "Epoch 58, Batch 9/32 : Loss = 0.007495464291423559\n",
      "Epoch 58, Batch 10/32 : Loss = 0.008638308383524418\n",
      "Epoch 58, Batch 11/32 : Loss = 0.0033927750773727894\n",
      "Epoch 58, Batch 12/32 : Loss = 0.004094983451068401\n",
      "Epoch 58, Batch 13/32 : Loss = 0.0021953615359961987\n",
      "Epoch 58, Batch 14/32 : Loss = 0.005838098004460335\n",
      "Epoch 58, Batch 15/32 : Loss = 0.0030224244110286236\n",
      "Epoch 58, Batch 16/32 : Loss = 0.08219705522060394\n",
      "Epoch 58, Batch 17/32 : Loss = 0.0036602052859961987\n",
      "Epoch 58, Batch 18/32 : Loss = 0.0021978924050927162\n",
      "Epoch 58, Batch 19/32 : Loss = 0.0043936180882155895\n",
      "Epoch 58, Batch 20/32 : Loss = 0.004027244634926319\n",
      "Epoch 58, Batch 21/32 : Loss = 0.04219100996851921\n",
      "Epoch 58, Batch 22/32 : Loss = 0.0036701259668916464\n",
      "Epoch 58, Batch 23/32 : Loss = 0.013801593333482742\n",
      "Epoch 58, Batch 24/32 : Loss = 0.004410061985254288\n",
      "Epoch 58, Batch 25/32 : Loss = 0.002650853944942355\n",
      "Epoch 58, Batch 26/32 : Loss = 0.005308860447257757\n",
      "Epoch 58, Batch 27/32 : Loss = 0.002334082731977105\n",
      "Epoch 58, Batch 28/32 : Loss = 0.0029426689725369215\n",
      "Epoch 58, Batch 29/32 : Loss = 0.007266764063388109\n",
      "Epoch 58, Batch 30/32 : Loss = 0.002207213081419468\n",
      "Epoch 58, Batch 31/32 : Loss = 0.012635952793061733\n",
      "Epoch 58 finished in 0.04470183849334717 minutes\n",
      "Epoch 58 training_loss = 0.012059371918439865\n",
      "--88-----K-----l---Z-----55----pp-----$-----a-----}---w-------- => 8KlZ5p$a}w, Ground Truth is 8KIZ5p$a}w,\n",
      "---J--;--qq----+---//--zz--yy---U-----%------U-----1---x---__-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "-------33-----\\----$----->>-----SS-----\\----M-------i--BB------ => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "--------]--t---44---e-----^---W--------Q-----44---->>----g----- => ]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 58 val_loss = 0.6298243403434753, word_accuracy = 0.68\n",
      "Epoch 59, Batch 0/32 : Loss = 0.02259104885160923\n",
      "Epoch 59, Batch 1/32 : Loss = 0.042451318353414536\n",
      "Epoch 59, Batch 2/32 : Loss = 0.0030125880148261786\n",
      "Epoch 59, Batch 3/32 : Loss = 0.0073744505643844604\n",
      "Epoch 59, Batch 4/32 : Loss = 0.03836644068360329\n",
      "Epoch 59, Batch 5/32 : Loss = 0.02441626414656639\n",
      "Epoch 59, Batch 6/32 : Loss = 0.005010784137994051\n",
      "Epoch 59, Batch 7/32 : Loss = 0.0037966750096529722\n",
      "Epoch 59, Batch 8/32 : Loss = 0.0019258924294263124\n",
      "Epoch 59, Batch 9/32 : Loss = 0.0026901811361312866\n",
      "Epoch 59, Batch 10/32 : Loss = 0.0019618067890405655\n",
      "Epoch 59, Batch 11/32 : Loss = 0.019551821053028107\n",
      "Epoch 59, Batch 12/32 : Loss = 0.007732379250228405\n",
      "Epoch 59, Batch 13/32 : Loss = 0.03650868684053421\n",
      "Epoch 59, Batch 14/32 : Loss = 0.0031784053426235914\n",
      "Epoch 59, Batch 15/32 : Loss = 0.008957019075751305\n",
      "Epoch 59, Batch 16/32 : Loss = 0.017499331384897232\n",
      "Epoch 59, Batch 17/32 : Loss = 0.00709508266299963\n",
      "Epoch 59, Batch 18/32 : Loss = 0.0029010935686528683\n",
      "Epoch 59, Batch 19/32 : Loss = 0.012749743647873402\n",
      "Epoch 59, Batch 20/32 : Loss = 0.0027912892401218414\n",
      "Epoch 59, Batch 21/32 : Loss = 0.028685441240668297\n",
      "Epoch 59, Batch 22/32 : Loss = 0.0023545746225863695\n",
      "Epoch 59, Batch 23/32 : Loss = 0.0031155329197645187\n",
      "Epoch 59, Batch 24/32 : Loss = 0.0035849735140800476\n",
      "Epoch 59, Batch 25/32 : Loss = 0.008828740566968918\n",
      "Epoch 59, Batch 26/32 : Loss = 0.025314250960946083\n",
      "Epoch 59, Batch 27/32 : Loss = 0.0023008473217487335\n",
      "Epoch 59, Batch 28/32 : Loss = 0.0029473062604665756\n",
      "Epoch 59, Batch 29/32 : Loss = 0.01055155135691166\n",
      "Epoch 59, Batch 30/32 : Loss = 0.007536169607192278\n",
      "Epoch 59, Batch 31/32 : Loss = 0.002311278600245714\n",
      "Epoch 59 finished in 0.04400388399759928 minutes\n",
      "Epoch 59 training_loss = 0.01182555966079235\n",
      "---##-----G-----9----E----I--=----hh---55---##----2----JJ--))--kk---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-..--c-----O------w------u----``--u----.--RR----{{--33---\"\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----5---->>----:--*---Y----A----'--O-----DD----*---#-----O-----g----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---55---->>---:---*---Y----A----'--O-----DD-----*--##----OO----gg---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 59 val_loss = 0.692410409450531, word_accuracy = 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Batch 0/32 : Loss = 0.006988401524722576\n",
      "Epoch 60, Batch 1/32 : Loss = 0.013453532010316849\n",
      "Epoch 60, Batch 2/32 : Loss = 0.004059795290231705\n",
      "Epoch 60, Batch 3/32 : Loss = 0.07245497405529022\n",
      "Epoch 60, Batch 4/32 : Loss = 0.004599135369062424\n",
      "Epoch 60, Batch 5/32 : Loss = 0.0193107221275568\n",
      "Epoch 60, Batch 6/32 : Loss = 0.0067779263481497765\n",
      "Epoch 60, Batch 7/32 : Loss = 0.0022796085104346275\n",
      "Epoch 60, Batch 8/32 : Loss = 0.003476344048976898\n",
      "Epoch 60, Batch 9/32 : Loss = 0.004451531916856766\n",
      "Epoch 60, Batch 10/32 : Loss = 0.003996545448899269\n",
      "Epoch 60, Batch 11/32 : Loss = 0.034597910940647125\n",
      "Epoch 60, Batch 12/32 : Loss = 0.007679803762584925\n",
      "Epoch 60, Batch 13/32 : Loss = 0.00415241252630949\n",
      "Epoch 60, Batch 14/32 : Loss = 0.0038104678969830275\n",
      "Epoch 60, Batch 15/32 : Loss = 0.013495544902980328\n",
      "Epoch 60, Batch 16/32 : Loss = 0.004386307206004858\n",
      "Epoch 60, Batch 17/32 : Loss = 0.010711414739489555\n",
      "Epoch 60, Batch 18/32 : Loss = 0.008488585241138935\n",
      "Epoch 60, Batch 19/32 : Loss = 0.0018241317011415958\n",
      "Epoch 60, Batch 20/32 : Loss = 0.0023399239871650934\n",
      "Epoch 60, Batch 21/32 : Loss = 0.0022504283115267754\n",
      "Epoch 60, Batch 22/32 : Loss = 0.005943695083260536\n",
      "Epoch 60, Batch 23/32 : Loss = 0.008310778066515923\n",
      "Epoch 60, Batch 24/32 : Loss = 0.001815187744796276\n",
      "Epoch 60, Batch 25/32 : Loss = 0.006450803019106388\n",
      "Epoch 60, Batch 26/32 : Loss = 0.0022198930382728577\n",
      "Epoch 60, Batch 27/32 : Loss = 0.0037385704927146435\n",
      "Epoch 60, Batch 28/32 : Loss = 0.001612494932487607\n",
      "Epoch 60, Batch 29/32 : Loss = 0.0025903130881488323\n",
      "Epoch 60, Batch 30/32 : Loss = 0.004009624011814594\n",
      "Epoch 60, Batch 31/32 : Loss = 0.0013725581811740994\n",
      "Epoch 60 finished in 0.044057186444600424 minutes\n",
      "Epoch 60 training_loss = 0.00875336118042469\n",
      "---XX---7---00--j--@@----S----Z---L---4----C----m------M-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "-----Q------3-----g----I--zz---#-----Y----:--]---q-----+----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "-----8-----K-----l--ZZ----55----pp----$$----a-----}---w-----,--- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---C----DD----E---gg---mm-----\"--m------F---<<---QQ----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 60 val_loss = 0.6589235067367554, word_accuracy = 0.68\n",
      "Epoch 61, Batch 0/32 : Loss = 0.0024250135757029057\n",
      "Epoch 61, Batch 1/32 : Loss = 0.0017778435721993446\n",
      "Epoch 61, Batch 2/32 : Loss = 0.002314438112080097\n",
      "Epoch 61, Batch 3/32 : Loss = 0.029239997267723083\n",
      "Epoch 61, Batch 4/32 : Loss = 0.006930414587259293\n",
      "Epoch 61, Batch 5/32 : Loss = 0.004074288997799158\n",
      "Epoch 61, Batch 6/32 : Loss = 0.0034577574115246534\n",
      "Epoch 61, Batch 7/32 : Loss = 0.002921678591519594\n",
      "Epoch 61, Batch 8/32 : Loss = 0.001431040233001113\n",
      "Epoch 61, Batch 9/32 : Loss = 0.0016881253104656935\n",
      "Epoch 61, Batch 10/32 : Loss = 0.0017031182069331408\n",
      "Epoch 61, Batch 11/32 : Loss = 0.0015122334007173777\n",
      "Epoch 61, Batch 12/32 : Loss = 0.0022929091937839985\n",
      "Epoch 61, Batch 13/32 : Loss = 0.0017119328258559108\n",
      "Epoch 61, Batch 14/32 : Loss = 0.0031293451320379972\n",
      "Epoch 61, Batch 15/32 : Loss = 0.0014958834508433938\n",
      "Epoch 61, Batch 16/32 : Loss = 0.0013691193889826536\n",
      "Epoch 61, Batch 17/32 : Loss = 0.002353288233280182\n",
      "Epoch 61, Batch 18/32 : Loss = 0.005051929969340563\n",
      "Epoch 61, Batch 19/32 : Loss = 0.00467664934694767\n",
      "Epoch 61, Batch 20/32 : Loss = 0.01277889683842659\n",
      "Epoch 61, Batch 21/32 : Loss = 0.0017077598022297025\n",
      "Epoch 61, Batch 22/32 : Loss = 0.009685073047876358\n",
      "Epoch 61, Batch 23/32 : Loss = 0.0022279471158981323\n",
      "Epoch 61, Batch 24/32 : Loss = 0.0016097468324005604\n",
      "Epoch 61, Batch 25/32 : Loss = 0.001546574174426496\n",
      "Epoch 61, Batch 26/32 : Loss = 0.18944063782691956\n",
      "Epoch 61, Batch 27/32 : Loss = 0.0014555554371327162\n",
      "Epoch 61, Batch 28/32 : Loss = 0.006523628253489733\n",
      "Epoch 61, Batch 29/32 : Loss = 0.002993048634380102\n",
      "Epoch 61, Batch 30/32 : Loss = 0.017634816467761993\n",
      "Epoch 61, Batch 31/32 : Loss = 0.15797469019889832\n",
      "Epoch 61 finished in 0.04323975642522176 minutes\n",
      "Epoch 61 training_loss = 0.011209880001842976\n",
      "---r----GGG-----I---TT-----#------3------P-------Q-------w-------'--i----- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "------dd-----:--XX----99-----e-----aa-----F---,--88---------V-----R------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----C-----DD----E----gg----m------\"---mm------F----<<---QQ-----8---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----O-------c-----++-----bb-----I---\"----bb-----66-----..---Q------------- => Oc+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 61 val_loss = 0.6719224452972412, word_accuracy = 0.64\n",
      "Epoch 62, Batch 0/32 : Loss = 0.002397928386926651\n",
      "Epoch 62, Batch 1/32 : Loss = 0.008833575993776321\n",
      "Epoch 62, Batch 2/32 : Loss = 0.009364880621433258\n",
      "Epoch 62, Batch 3/32 : Loss = 0.012055642902851105\n",
      "Epoch 62, Batch 4/32 : Loss = 0.00370139186270535\n",
      "Epoch 62, Batch 5/32 : Loss = 0.008025964722037315\n",
      "Epoch 62, Batch 6/32 : Loss = 0.008069067262113094\n",
      "Epoch 62, Batch 7/32 : Loss = 0.009351025335490704\n",
      "Epoch 62, Batch 8/32 : Loss = 0.0078226113691926\n",
      "Epoch 62, Batch 9/32 : Loss = 0.0037103453651070595\n",
      "Epoch 62, Batch 10/32 : Loss = 0.003965877927839756\n",
      "Epoch 62, Batch 11/32 : Loss = 0.009710056707262993\n",
      "Epoch 62, Batch 12/32 : Loss = 0.004605192691087723\n",
      "Epoch 62, Batch 13/32 : Loss = 0.006304578389972448\n",
      "Epoch 62, Batch 14/32 : Loss = 0.004723070655018091\n",
      "Epoch 62, Batch 15/32 : Loss = 0.010108813643455505\n",
      "Epoch 62, Batch 16/32 : Loss = 0.01091552060097456\n",
      "Epoch 62, Batch 17/32 : Loss = 0.021631715819239616\n",
      "Epoch 62, Batch 18/32 : Loss = 0.08060013502836227\n",
      "Epoch 62, Batch 19/32 : Loss = 0.0034233226906508207\n",
      "Epoch 62, Batch 20/32 : Loss = 0.021442877128720284\n",
      "Epoch 62, Batch 21/32 : Loss = 0.004139369353652\n",
      "Epoch 62, Batch 22/32 : Loss = 0.11504274606704712\n",
      "Epoch 62, Batch 23/32 : Loss = 0.0029993001371622086\n",
      "Epoch 62, Batch 24/32 : Loss = 0.00211752369068563\n",
      "Epoch 62, Batch 25/32 : Loss = 0.0018966945353895426\n",
      "Epoch 62, Batch 26/32 : Loss = 0.0024826284497976303\n",
      "Epoch 62, Batch 27/32 : Loss = 0.00221996009349823\n",
      "Epoch 62, Batch 28/32 : Loss = 0.0024333354085683823\n",
      "Epoch 62, Batch 29/32 : Loss = 0.0014512515626847744\n",
      "Epoch 62, Batch 30/32 : Loss = 0.006076090969145298\n",
      "Epoch 62, Batch 31/32 : Loss = 0.020022377371788025\n",
      "Epoch 62 finished in 0.04196062485376994 minutes\n",
      "Epoch 62 training_loss = 0.012662660330533981\n",
      "----J---;--q----+---/---z---yy---U----%%-----UU----11---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "-----kk---$----l-FF---DD----ee---hh---]---k---0----\\--XX------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----z----0----\"\"---GG-----/----~~----$-----c----1----j--t----- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----4----r--{{--%%-----/---'--)--w------&----N------+----P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 62 val_loss = 0.6914240121841431, word_accuracy = 0.65\n",
      "Epoch 63, Batch 0/32 : Loss = 0.009076555259525776\n",
      "Epoch 63, Batch 1/32 : Loss = 0.008071469143033028\n",
      "Epoch 63, Batch 2/32 : Loss = 0.0064663090743124485\n",
      "Epoch 63, Batch 3/32 : Loss = 0.004781696014106274\n",
      "Epoch 63, Batch 4/32 : Loss = 0.00254144542850554\n",
      "Epoch 63, Batch 5/32 : Loss = 0.11454976350069046\n",
      "Epoch 63, Batch 6/32 : Loss = 0.002907976508140564\n",
      "Epoch 63, Batch 7/32 : Loss = 0.0028395913541316986\n",
      "Epoch 63, Batch 8/32 : Loss = 0.016605565324425697\n",
      "Epoch 63, Batch 9/32 : Loss = 0.07491591572761536\n",
      "Epoch 63, Batch 10/32 : Loss = 0.0017149221384897828\n",
      "Epoch 63, Batch 11/32 : Loss = 0.0024527739733457565\n",
      "Epoch 63, Batch 12/32 : Loss = 0.010517402552068233\n",
      "Epoch 63, Batch 13/32 : Loss = 0.002321978798136115\n",
      "Epoch 63, Batch 14/32 : Loss = 0.0014415105106309056\n",
      "Epoch 63, Batch 15/32 : Loss = 0.0014229670632630587\n",
      "Epoch 63, Batch 16/32 : Loss = 0.0021481825970113277\n",
      "Epoch 63, Batch 17/32 : Loss = 0.002295759040862322\n",
      "Epoch 63, Batch 18/32 : Loss = 0.004200110677629709\n",
      "Epoch 63, Batch 19/32 : Loss = 0.0018676198087632656\n",
      "Epoch 63, Batch 20/32 : Loss = 0.0017580928979441524\n",
      "Epoch 63, Batch 21/32 : Loss = 0.007436034269630909\n",
      "Epoch 63, Batch 22/32 : Loss = 0.002548759337514639\n",
      "Epoch 63, Batch 23/32 : Loss = 0.006720871664583683\n",
      "Epoch 63, Batch 24/32 : Loss = 0.0018670368008315563\n",
      "Epoch 63, Batch 25/32 : Loss = 0.01734604500234127\n",
      "Epoch 63, Batch 26/32 : Loss = 0.006872573867440224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Batch 27/32 : Loss = 0.002700964454561472\n",
      "Epoch 63, Batch 28/32 : Loss = 0.0017287163063883781\n",
      "Epoch 63, Batch 29/32 : Loss = 0.003703117836266756\n",
      "Epoch 63, Batch 30/32 : Loss = 0.0036336160264909267\n",
      "Epoch 63, Batch 31/32 : Loss = 0.0032586632296442986\n",
      "Epoch 63 finished in 0.04334304332733154 minutes\n",
      "Epoch 63 training_loss = 0.010597996413707733\n",
      "----55-----9-----g-----mm--------J----uu----x-----C----X---.--dd-----\\---- => 59gmJuxCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "----+-----:--z-----77---88-----dd-----S-----v----5-----S-----JJ---BB------ => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "-----k-----OO------/---,---y----cc----*-----1----PP----}---##-----B------- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "-----W-------=----2----++---E-----1----n----T----X----r--CC----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 63 val_loss = 0.6460874080657959, word_accuracy = 0.66\n",
      "Epoch 64, Batch 0/32 : Loss = 0.009211347438395023\n",
      "Epoch 64, Batch 1/32 : Loss = 0.0038348459638655186\n",
      "Epoch 64, Batch 2/32 : Loss = 0.0018817017553374171\n",
      "Epoch 64, Batch 3/32 : Loss = 0.0012121321633458138\n",
      "Epoch 64, Batch 4/32 : Loss = 0.001971911173313856\n",
      "Epoch 64, Batch 5/32 : Loss = 0.0027792342007160187\n",
      "Epoch 64, Batch 6/32 : Loss = 0.0016566712874919176\n",
      "Epoch 64, Batch 7/32 : Loss = 0.004369428381323814\n",
      "Epoch 64, Batch 8/32 : Loss = 0.03261507302522659\n",
      "Epoch 64, Batch 9/32 : Loss = 0.0023649234790354967\n",
      "Epoch 64, Batch 10/32 : Loss = 0.00195728219114244\n",
      "Epoch 64, Batch 11/32 : Loss = 0.03977663442492485\n",
      "Epoch 64, Batch 12/32 : Loss = 0.0020870869047939777\n",
      "Epoch 64, Batch 13/32 : Loss = 0.0014020672533661127\n",
      "Epoch 64, Batch 14/32 : Loss = 0.025904301553964615\n",
      "Epoch 64, Batch 15/32 : Loss = 0.018953215330839157\n",
      "Epoch 64, Batch 16/32 : Loss = 0.00357972364872694\n",
      "Epoch 64, Batch 17/32 : Loss = 0.0022066915407776833\n",
      "Epoch 64, Batch 18/32 : Loss = 0.002211430110037327\n",
      "Epoch 64, Batch 19/32 : Loss = 0.020555678755044937\n",
      "Epoch 64, Batch 20/32 : Loss = 0.001386932097375393\n",
      "Epoch 64, Batch 21/32 : Loss = 0.042357437312603\n",
      "Epoch 64, Batch 22/32 : Loss = 0.001661496702581644\n",
      "Epoch 64, Batch 23/32 : Loss = 0.002433898625895381\n",
      "Epoch 64, Batch 24/32 : Loss = 0.017450831830501556\n",
      "Epoch 64, Batch 25/32 : Loss = 0.003096154425293207\n",
      "Epoch 64, Batch 26/32 : Loss = 0.002050128299742937\n",
      "Epoch 64, Batch 27/32 : Loss = 0.003511423710733652\n",
      "Epoch 64, Batch 28/32 : Loss = 0.002296749036759138\n",
      "Epoch 64, Batch 29/32 : Loss = 0.006253370549529791\n",
      "Epoch 64, Batch 30/32 : Loss = 0.022444453090429306\n",
      "Epoch 64, Batch 31/32 : Loss = 0.0021073066163808107\n",
      "Epoch 64 finished in 0.042983571688334145 minutes\n",
      "Epoch 64 training_loss = 0.009180325083434582\n",
      "---oo---\"---}--!--B-----rr--&&----99---;-`---O-----ww----}----- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "--2---p---:--m-----x---a---z---n----@-----C---yy--%%-----%----- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----k----$----|--'-,--9----Y----N-----W------m------T----8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "--JJ--;--qq----+---/---z---y---UU-----%-----UU-----1---x---__-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 64 val_loss = 0.624344527721405, word_accuracy = 0.7\n",
      "Epoch 65, Batch 0/32 : Loss = 0.0028148076962679625\n",
      "Epoch 65, Batch 1/32 : Loss = 0.001980881905183196\n",
      "Epoch 65, Batch 2/32 : Loss = 0.0043386127799749374\n",
      "Epoch 65, Batch 3/32 : Loss = 0.008974033407866955\n",
      "Epoch 65, Batch 4/32 : Loss = 0.0023432865273207426\n",
      "Epoch 65, Batch 5/32 : Loss = 0.001568927546031773\n",
      "Epoch 65, Batch 6/32 : Loss = 0.006893326062709093\n",
      "Epoch 65, Batch 7/32 : Loss = 0.00130925711710006\n",
      "Epoch 65, Batch 8/32 : Loss = 0.0012353817000985146\n",
      "Epoch 65, Batch 9/32 : Loss = 0.0014535619411617517\n",
      "Epoch 65, Batch 10/32 : Loss = 0.001618394278921187\n",
      "Epoch 65, Batch 11/32 : Loss = 0.0014274036511778831\n",
      "Epoch 65, Batch 12/32 : Loss = 0.002918725833296776\n",
      "Epoch 65, Batch 13/32 : Loss = 0.005152000114321709\n",
      "Epoch 65, Batch 14/32 : Loss = 0.014103108085691929\n",
      "Epoch 65, Batch 15/32 : Loss = 0.004257420543581247\n",
      "Epoch 65, Batch 16/32 : Loss = 0.009117752313613892\n",
      "Epoch 65, Batch 17/32 : Loss = 0.0023474651388823986\n",
      "Epoch 65, Batch 18/32 : Loss = 0.005135855637490749\n",
      "Epoch 65, Batch 19/32 : Loss = 0.011135228909552097\n",
      "Epoch 65, Batch 20/32 : Loss = 0.002513402607291937\n",
      "Epoch 65, Batch 21/32 : Loss = 0.0035984981805086136\n",
      "Epoch 65, Batch 22/32 : Loss = 0.003951840102672577\n",
      "Epoch 65, Batch 23/32 : Loss = 0.011826242320239544\n",
      "Epoch 65, Batch 24/32 : Loss = 0.0013599299127236009\n",
      "Epoch 65, Batch 25/32 : Loss = 0.0011634825496003032\n",
      "Epoch 65, Batch 26/32 : Loss = 0.0012904941104352474\n",
      "Epoch 65, Batch 27/32 : Loss = 0.003168083494529128\n",
      "Epoch 65, Batch 28/32 : Loss = 0.040893517434597015\n",
      "Epoch 65, Batch 29/32 : Loss = 0.0027326378040015697\n",
      "Epoch 65, Batch 30/32 : Loss = 0.004305409267544746\n",
      "Epoch 65, Batch 31/32 : Loss = 0.002514210529625416\n",
      "Epoch 65 finished in 0.04335699478785197 minutes\n",
      "Epoch 65 training_loss = 0.005373276770114899\n",
      "----C-----D-----E----g----m------\"--mm------F---<<----Q-----8---2----- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----\"---]---t---44----e------^----W--------Q-------44---->>-----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---0------cc----+------bb-----I--\"-----b------6----..---Q------------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "-----88-----KK-----l--ZZ-----55-----p-----$$----aa-----}---ww------,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "Epoch 65 val_loss = 0.6733526587486267, word_accuracy = 0.7\n",
      "Epoch 66, Batch 0/32 : Loss = 0.003770571667701006\n",
      "Epoch 66, Batch 1/32 : Loss = 0.0014600679278373718\n",
      "Epoch 66, Batch 2/32 : Loss = 0.010901889763772488\n",
      "Epoch 66, Batch 3/32 : Loss = 0.002112724119797349\n",
      "Epoch 66, Batch 4/32 : Loss = 0.0031665973365306854\n",
      "Epoch 66, Batch 5/32 : Loss = 0.0010583486873656511\n",
      "Epoch 66, Batch 6/32 : Loss = 0.06818424165248871\n",
      "Epoch 66, Batch 7/32 : Loss = 0.0018376436783000827\n",
      "Epoch 66, Batch 8/32 : Loss = 0.0018925517797470093\n",
      "Epoch 66, Batch 9/32 : Loss = 0.008304934948682785\n",
      "Epoch 66, Batch 10/32 : Loss = 0.005829146131873131\n",
      "Epoch 66, Batch 11/32 : Loss = 0.001946442760527134\n",
      "Epoch 66, Batch 12/32 : Loss = 0.0033873070497065783\n",
      "Epoch 66, Batch 13/32 : Loss = 0.041837818920612335\n",
      "Epoch 66, Batch 14/32 : Loss = 0.0015201239148154855\n",
      "Epoch 66, Batch 15/32 : Loss = 0.004841371905058622\n",
      "Epoch 66, Batch 16/32 : Loss = 0.01864919811487198\n",
      "Epoch 66, Batch 17/32 : Loss = 0.0012747339205816388\n",
      "Epoch 66, Batch 18/32 : Loss = 0.004741215147078037\n",
      "Epoch 66, Batch 19/32 : Loss = 0.013597394339740276\n",
      "Epoch 66, Batch 20/32 : Loss = 0.0026028575375676155\n",
      "Epoch 66, Batch 21/32 : Loss = 0.04893186688423157\n",
      "Epoch 66, Batch 22/32 : Loss = 0.0018267870182171464\n",
      "Epoch 66, Batch 23/32 : Loss = 0.003548050532117486\n",
      "Epoch 66, Batch 24/32 : Loss = 0.01361366081982851\n",
      "Epoch 66, Batch 25/32 : Loss = 0.0021381943952292204\n",
      "Epoch 66, Batch 26/32 : Loss = 0.07980647683143616\n",
      "Epoch 66, Batch 27/32 : Loss = 0.0022849873639643192\n",
      "Epoch 66, Batch 28/32 : Loss = 0.0016095747705549002\n",
      "Epoch 66, Batch 29/32 : Loss = 0.0016115889884531498\n",
      "Epoch 66, Batch 30/32 : Loss = 0.018144015222787857\n",
      "Epoch 66, Batch 31/32 : Loss = 0.0008431980386376381\n",
      "Epoch 66 finished in 0.042950924237569174 minutes\n",
      "Epoch 66 training_loss = 0.012097599916160107\n",
      "-----0-----JJ----!--((---;---A------33----,--''--)---rr---r----77----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "-----cc----RR----;---9-----y-----?----2-----d----ii---O------{---!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----JJ--;;--qq----+----/----z---y----U------%-------U------1---x------ => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "----X----7----0---j---@-----S----Z----L---4-----C----mm-----MM-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 66 val_loss = 0.6735139489173889, word_accuracy = 0.74\n",
      "Epoch 67, Batch 0/32 : Loss = 0.0037323166616261005\n",
      "Epoch 67, Batch 1/32 : Loss = 0.0020587220788002014\n",
      "Epoch 67, Batch 2/32 : Loss = 0.002536026295274496\n",
      "Epoch 67, Batch 3/32 : Loss = 0.0734555721282959\n",
      "Epoch 67, Batch 4/32 : Loss = 0.011123282834887505\n",
      "Epoch 67, Batch 5/32 : Loss = 0.0287289060652256\n",
      "Epoch 67, Batch 6/32 : Loss = 0.004116089548915625\n",
      "Epoch 67, Batch 7/32 : Loss = 0.006100860890001059\n",
      "Epoch 67, Batch 8/32 : Loss = 0.002433546120300889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Batch 9/32 : Loss = 0.011371927335858345\n",
      "Epoch 67, Batch 10/32 : Loss = 0.03105738013982773\n",
      "Epoch 67, Batch 11/32 : Loss = 0.08490338176488876\n",
      "Epoch 67, Batch 12/32 : Loss = 0.003837304189801216\n",
      "Epoch 67, Batch 13/32 : Loss = 0.002381739905104041\n",
      "Epoch 67, Batch 14/32 : Loss = 0.002083907136693597\n",
      "Epoch 67, Batch 15/32 : Loss = 0.04058394208550453\n",
      "Epoch 67, Batch 16/32 : Loss = 0.08037535846233368\n",
      "Epoch 67, Batch 17/32 : Loss = 0.04039604589343071\n",
      "Epoch 67, Batch 18/32 : Loss = 0.003143844660371542\n",
      "Epoch 67, Batch 19/32 : Loss = 0.0028282352723181248\n",
      "Epoch 67, Batch 20/32 : Loss = 0.003228437388315797\n",
      "Epoch 67, Batch 21/32 : Loss = 0.007709365338087082\n",
      "Epoch 67, Batch 22/32 : Loss = 0.0032141157425940037\n",
      "Epoch 67, Batch 23/32 : Loss = 0.0038891469594091177\n",
      "Epoch 67, Batch 24/32 : Loss = 0.002677339129149914\n",
      "Epoch 67, Batch 25/32 : Loss = 0.008993957191705704\n",
      "Epoch 67, Batch 26/32 : Loss = 0.012972616590559483\n",
      "Epoch 67, Batch 27/32 : Loss = 0.0035333498381078243\n",
      "Epoch 67, Batch 28/32 : Loss = 0.0026233945973217487\n",
      "Epoch 67, Batch 29/32 : Loss = 0.01867828145623207\n",
      "Epoch 67, Batch 30/32 : Loss = 0.003523000981658697\n",
      "Epoch 67, Batch 31/32 : Loss = 0.004071327392011881\n",
      "Epoch 67 finished in 0.041568164030710855 minutes\n",
      "Epoch 67 training_loss = 0.016346998512744904\n",
      "----W-------=----2----+----E-----1---n-----T----X---r---C-----a----m----- => W=2+E1nTXrCam, Ground Truth is W=2+E1nTXrCan\n",
      "---/----MM--------o-----E-----^----3-----x----//----&-----6-----XX------- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----C-----DD----E----gg----m------\"---m------FF---<<----Q-----8---22----- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      ":--------3------\\-----$------>>-------S------\\-----M---------i---B------- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 67 val_loss = 0.6598130464553833, word_accuracy = 0.64\n",
      "Epoch 68, Batch 0/32 : Loss = 0.0028732982464134693\n",
      "Epoch 68, Batch 1/32 : Loss = 0.0017219972796738148\n",
      "Epoch 68, Batch 2/32 : Loss = 0.002616515848785639\n",
      "Epoch 68, Batch 3/32 : Loss = 0.004199269227683544\n",
      "Epoch 68, Batch 4/32 : Loss = 0.003064572112634778\n",
      "Epoch 68, Batch 5/32 : Loss = 0.009114792570471764\n",
      "Epoch 68, Batch 6/32 : Loss = 0.0052701858803629875\n",
      "Epoch 68, Batch 7/32 : Loss = 0.050634026527404785\n",
      "Epoch 68, Batch 8/32 : Loss = 0.004153526853770018\n",
      "Epoch 68, Batch 9/32 : Loss = 0.025610214099287987\n",
      "Epoch 68, Batch 10/32 : Loss = 0.0028749778866767883\n",
      "Epoch 68, Batch 11/32 : Loss = 0.027879003435373306\n",
      "Epoch 68, Batch 12/32 : Loss = 0.013306956738233566\n",
      "Epoch 68, Batch 13/32 : Loss = 0.013144341297447681\n",
      "Epoch 68, Batch 14/32 : Loss = 0.002580805914476514\n",
      "Epoch 68, Batch 15/32 : Loss = 0.09181205183267593\n",
      "Epoch 68, Batch 16/32 : Loss = 0.0035802219063043594\n",
      "Epoch 68, Batch 17/32 : Loss = 0.0022147519048303366\n",
      "Epoch 68, Batch 18/32 : Loss = 0.006747199688106775\n",
      "Epoch 68, Batch 19/32 : Loss = 0.0026971623301506042\n",
      "Epoch 68, Batch 20/32 : Loss = 0.002587323309853673\n",
      "Epoch 68, Batch 21/32 : Loss = 0.0029085855931043625\n",
      "Epoch 68, Batch 22/32 : Loss = 0.002249701414257288\n",
      "Epoch 68, Batch 23/32 : Loss = 0.018892474472522736\n",
      "Epoch 68, Batch 24/32 : Loss = 0.009584574028849602\n",
      "Epoch 68, Batch 25/32 : Loss = 0.002212350955232978\n",
      "Epoch 68, Batch 26/32 : Loss = 0.022672194987535477\n",
      "Epoch 68, Batch 27/32 : Loss = 0.08646935969591141\n",
      "Epoch 68, Batch 28/32 : Loss = 0.003934259060770273\n",
      "Epoch 68, Batch 29/32 : Loss = 0.0022007902152836323\n",
      "Epoch 68, Batch 30/32 : Loss = 0.0028925128281116486\n",
      "Epoch 68, Batch 31/32 : Loss = 0.06991749256849289\n",
      "Epoch 68 finished in 0.04276092847188314 minutes\n",
      "Epoch 68 training_loss = 0.014182799495756626\n",
      "----k---$----|--'-,--9---Y----N----W------m------T---8---- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----k---$---II-F----DD----e---hh---]--k----0---\\\\--X------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----8----K-----l-ZZ----55----p----$----aa---}}--w-----,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---k---$---|--'',--9----Y---N-----W------m-----TT---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 68 val_loss = 0.6360381841659546, word_accuracy = 0.59\n",
      "Epoch 69, Batch 0/32 : Loss = 0.003632304724305868\n",
      "Epoch 69, Batch 1/32 : Loss = 0.003913705237209797\n",
      "Epoch 69, Batch 2/32 : Loss = 0.004205948673188686\n",
      "Epoch 69, Batch 3/32 : Loss = 0.003334582317620516\n",
      "Epoch 69, Batch 4/32 : Loss = 0.0038555869832634926\n",
      "Epoch 69, Batch 5/32 : Loss = 0.004692660644650459\n",
      "Epoch 69, Batch 6/32 : Loss = 0.043334852904081345\n",
      "Epoch 69, Batch 7/32 : Loss = 0.002329764422029257\n",
      "Epoch 69, Batch 8/32 : Loss = 0.0018531951354816556\n",
      "Epoch 69, Batch 9/32 : Loss = 0.006352167576551437\n",
      "Epoch 69, Batch 10/32 : Loss = 0.0031880487222224474\n",
      "Epoch 69, Batch 11/32 : Loss = 0.02377636916935444\n",
      "Epoch 69, Batch 12/32 : Loss = 0.16825301945209503\n",
      "Epoch 69, Batch 13/32 : Loss = 0.002309514442458749\n",
      "Epoch 69, Batch 14/32 : Loss = 0.004145471379160881\n",
      "Epoch 69, Batch 15/32 : Loss = 0.008275222033262253\n",
      "Epoch 69, Batch 16/32 : Loss = 0.13450846076011658\n",
      "Epoch 69, Batch 17/32 : Loss = 0.002877775812521577\n",
      "Epoch 69, Batch 18/32 : Loss = 0.0033096126280725002\n",
      "Epoch 69, Batch 19/32 : Loss = 0.003412737278267741\n",
      "Epoch 69, Batch 20/32 : Loss = 0.03550254553556442\n",
      "Epoch 69, Batch 21/32 : Loss = 0.002003228757530451\n",
      "Epoch 69, Batch 22/32 : Loss = 0.06010821834206581\n",
      "Epoch 69, Batch 23/32 : Loss = 0.6028165221214294\n",
      "Epoch 69, Batch 24/32 : Loss = 0.003552778158336878\n",
      "Epoch 69, Batch 25/32 : Loss = 0.002364818938076496\n",
      "Epoch 69, Batch 26/32 : Loss = 0.013139952905476093\n",
      "Epoch 69, Batch 27/32 : Loss = 0.0077918111346662045\n",
      "Epoch 69, Batch 28/32 : Loss = 0.17428289353847504\n",
      "Epoch 69, Batch 29/32 : Loss = 0.028547843918204308\n",
      "Epoch 69, Batch 30/32 : Loss = 0.00332931661978364\n",
      "Epoch 69, Batch 31/32 : Loss = 0.009862549602985382\n",
      "Epoch 69 finished in 0.04349526166915894 minutes\n",
      "Epoch 69 training_loss = 0.04389506205916405\n",
      "----W------=-----2----+----E-----1---n----TT----X---r---C-----a---------- => W=2+E1nTXrCa, Ground Truth is W=2+E1nTXrCan\n",
      "----X----77---0---j---@@-----S----Z----L----4----CC----m-------M--------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---kk---$$----|---'--,--9-----Y----NN------W-------m-------TT----8------- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "-----5-----9-----g-----mm-------JJ----u-----x----C----X---..--d-----\\---- => 59gmJuxCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 69 val_loss = 0.5748984217643738, word_accuracy = 0.65\n",
      "Epoch 70, Batch 0/32 : Loss = 0.016535364091396332\n",
      "Epoch 70, Batch 1/32 : Loss = 0.003697737352922559\n",
      "Epoch 70, Batch 2/32 : Loss = 0.011830532923340797\n",
      "Epoch 70, Batch 3/32 : Loss = 0.02043233811855316\n",
      "Epoch 70, Batch 4/32 : Loss = 0.020946063101291656\n",
      "Epoch 70, Batch 5/32 : Loss = 0.010790146887302399\n",
      "Epoch 70, Batch 6/32 : Loss = 0.03644281625747681\n",
      "Epoch 70, Batch 7/32 : Loss = 0.09640134871006012\n",
      "Epoch 70, Batch 8/32 : Loss = 0.006814961321651936\n",
      "Epoch 70, Batch 9/32 : Loss = 0.008961650542914867\n",
      "Epoch 70, Batch 10/32 : Loss = 0.022281313315033913\n",
      "Epoch 70, Batch 11/32 : Loss = 0.012150745838880539\n",
      "Epoch 70, Batch 12/32 : Loss = 0.016410082578659058\n",
      "Epoch 70, Batch 13/32 : Loss = 0.013677876442670822\n",
      "Epoch 70, Batch 14/32 : Loss = 0.0070966556668281555\n",
      "Epoch 70, Batch 15/32 : Loss = 0.12772899866104126\n",
      "Epoch 70, Batch 16/32 : Loss = 0.03449207916855812\n",
      "Epoch 70, Batch 17/32 : Loss = 0.012539182789623737\n",
      "Epoch 70, Batch 18/32 : Loss = 0.0076561253517866135\n",
      "Epoch 70, Batch 19/32 : Loss = 0.008324566297233105\n",
      "Epoch 70, Batch 20/32 : Loss = 0.058967143297195435\n",
      "Epoch 70, Batch 21/32 : Loss = 0.012964630499482155\n",
      "Epoch 70, Batch 22/32 : Loss = 0.00461065536364913\n",
      "Epoch 70, Batch 23/32 : Loss = 0.010978254489600658\n",
      "Epoch 70, Batch 24/32 : Loss = 0.00347082014195621\n",
      "Epoch 70, Batch 25/32 : Loss = 0.003193090669810772\n",
      "Epoch 70, Batch 26/32 : Loss = 0.016431566327810287\n",
      "Epoch 70, Batch 27/32 : Loss = 0.009622897952795029\n",
      "Epoch 70, Batch 28/32 : Loss = 0.005173671990633011\n",
      "Epoch 70, Batch 29/32 : Loss = 0.0036534208338707685\n",
      "Epoch 70, Batch 30/32 : Loss = 0.016911949962377548\n",
      "Epoch 70, Batch 31/32 : Loss = 0.014306316152215004\n",
      "Epoch 70 finished in 0.043741703033447266 minutes\n",
      "Epoch 70 training_loss = 0.020657891407608986\n",
      "-----QQ-----3-----g----I--z---#-----Y---:--]---q----+----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----OO-----c----+-----bb----I--\"---bb----66----.---Q--------- => Oc+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "--.--c----O-----ww-----u---`--uu---.--R----{--33---\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "---JJ--;--qq----+---/---z---y---U-----%%-----U-----1---x---.- => J;q+/zyU%U1x., Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 70 val_loss = 0.6460299491882324, word_accuracy = 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Batch 0/32 : Loss = 0.1962113380432129\n",
      "Epoch 71, Batch 1/32 : Loss = 0.007832552306354046\n",
      "Epoch 71, Batch 2/32 : Loss = 0.1902206987142563\n",
      "Epoch 71, Batch 3/32 : Loss = 0.007345476653426886\n",
      "Epoch 71, Batch 4/32 : Loss = 0.00409979373216629\n",
      "Epoch 71, Batch 5/32 : Loss = 0.0036756389308720827\n",
      "Epoch 71, Batch 6/32 : Loss = 0.008405270986258984\n",
      "Epoch 71, Batch 7/32 : Loss = 0.004155768547207117\n",
      "Epoch 71, Batch 8/32 : Loss = 0.00360582722350955\n",
      "Epoch 71, Batch 9/32 : Loss = 0.010282028466463089\n",
      "Epoch 71, Batch 10/32 : Loss = 0.011287211440503597\n",
      "Epoch 71, Batch 11/32 : Loss = 0.004464009776711464\n",
      "Epoch 71, Batch 12/32 : Loss = 0.0066812303848564625\n",
      "Epoch 71, Batch 13/32 : Loss = 0.008741937577724457\n",
      "Epoch 71, Batch 14/32 : Loss = 0.006777389906346798\n",
      "Epoch 71, Batch 15/32 : Loss = 0.007497039623558521\n",
      "Epoch 71, Batch 16/32 : Loss = 0.008080173283815384\n",
      "Epoch 71, Batch 17/32 : Loss = 0.006860609631985426\n",
      "Epoch 71, Batch 18/32 : Loss = 0.017654918134212494\n",
      "Epoch 71, Batch 19/32 : Loss = 0.004265433177351952\n",
      "Epoch 71, Batch 20/32 : Loss = 0.07351553440093994\n",
      "Epoch 71, Batch 21/32 : Loss = 0.01414710097014904\n",
      "Epoch 71, Batch 22/32 : Loss = 0.005178737919777632\n",
      "Epoch 71, Batch 23/32 : Loss = 0.025545645505189896\n",
      "Epoch 71, Batch 24/32 : Loss = 0.0034856866113841534\n",
      "Epoch 71, Batch 25/32 : Loss = 0.01809876412153244\n",
      "Epoch 71, Batch 26/32 : Loss = 0.0065477387979626656\n",
      "Epoch 71, Batch 27/32 : Loss = 0.049707990139722824\n",
      "Epoch 71, Batch 28/32 : Loss = 0.013071754947304726\n",
      "Epoch 71, Batch 29/32 : Loss = 0.006030450575053692\n",
      "Epoch 71, Batch 30/32 : Loss = 0.0028110092971473932\n",
      "Epoch 71, Batch 31/32 : Loss = 0.0029176934622228146\n",
      "Epoch 71 finished in 0.043371673425038657 minutes\n",
      "Epoch 71 training_loss = 0.023667452856898308\n",
      "----4----r--{---%%-----/--\"\"-))--w-----&&----N------+---PP--- => 4r{%/\")w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---{--B-----Y----R----a----y---h---##---22--->>----E----4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----Y----W------]]-i--\\---|---MM------<<----M------8----8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "---#----GG----9----E---I-=----hh---5---#----2----J---)--k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 71 val_loss = 0.6366317868232727, word_accuracy = 0.65\n",
      "Epoch 72, Batch 0/32 : Loss = 0.03804425150156021\n",
      "Epoch 72, Batch 1/32 : Loss = 0.011065227910876274\n",
      "Epoch 72, Batch 2/32 : Loss = 0.004433367867022753\n",
      "Epoch 72, Batch 3/32 : Loss = 0.003625433426350355\n",
      "Epoch 72, Batch 4/32 : Loss = 0.016306811943650246\n",
      "Epoch 72, Batch 5/32 : Loss = 0.00426172511652112\n",
      "Epoch 72, Batch 6/32 : Loss = 0.031592510640621185\n",
      "Epoch 72, Batch 7/32 : Loss = 0.0028117927722632885\n",
      "Epoch 72, Batch 8/32 : Loss = 0.0022481014020740986\n",
      "Epoch 72, Batch 9/32 : Loss = 0.013061879202723503\n",
      "Epoch 72, Batch 10/32 : Loss = 0.018957361578941345\n",
      "Epoch 72, Batch 11/32 : Loss = 0.0049856314435601234\n",
      "Epoch 72, Batch 12/32 : Loss = 0.012491018511354923\n",
      "Epoch 72, Batch 13/32 : Loss = 0.02023392543196678\n",
      "Epoch 72, Batch 14/32 : Loss = 0.01841815933585167\n",
      "Epoch 72, Batch 15/32 : Loss = 0.012195596471428871\n",
      "Epoch 72, Batch 16/32 : Loss = 0.009605973027646542\n",
      "Epoch 72, Batch 17/32 : Loss = 0.005188952200114727\n",
      "Epoch 72, Batch 18/32 : Loss = 0.012109716422855854\n",
      "Epoch 72, Batch 19/32 : Loss = 0.06672387570142746\n",
      "Epoch 72, Batch 20/32 : Loss = 0.00992775708436966\n",
      "Epoch 72, Batch 21/32 : Loss = 0.007316894829273224\n",
      "Epoch 72, Batch 22/32 : Loss = 0.002315023448318243\n",
      "Epoch 72, Batch 23/32 : Loss = 0.004947768524289131\n",
      "Epoch 72, Batch 24/32 : Loss = 0.006621053908020258\n",
      "Epoch 72, Batch 25/32 : Loss = 0.06136832386255264\n",
      "Epoch 72, Batch 26/32 : Loss = 0.004890057258307934\n",
      "Epoch 72, Batch 27/32 : Loss = 0.08150607347488403\n",
      "Epoch 72, Batch 28/32 : Loss = 0.0030694007873535156\n",
      "Epoch 72, Batch 29/32 : Loss = 0.018858499825000763\n",
      "Epoch 72, Batch 30/32 : Loss = 0.005607446189969778\n",
      "Epoch 72, Batch 31/32 : Loss = 0.012496120296418667\n",
      "Epoch 72 finished in 0.044894119103749595 minutes\n",
      "Epoch 72 training_loss = 0.016589608043432236\n",
      "---oo---\"---}--!--BB----r---&&----99---;-`---O-----w-----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----5----99---gg----m------JJ---u----x---c---x---.-dd----\\---- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "--B-----.--Y----I--W-------66----F----hh----X---''-Y----22---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----dd----:--X----9----e-----a----F---,--8--------V----R------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 72 val_loss = 0.6601130366325378, word_accuracy = 0.66\n",
      "Epoch 73, Batch 0/32 : Loss = 0.0045593236573040485\n",
      "Epoch 73, Batch 1/32 : Loss = 0.010769153013825417\n",
      "Epoch 73, Batch 2/32 : Loss = 0.03914833813905716\n",
      "Epoch 73, Batch 3/32 : Loss = 0.004220705479383469\n",
      "Epoch 73, Batch 4/32 : Loss = 0.0030716846231371164\n",
      "Epoch 73, Batch 5/32 : Loss = 0.032076962292194366\n",
      "Epoch 73, Batch 6/32 : Loss = 0.004951310344040394\n",
      "Epoch 73, Batch 7/32 : Loss = 0.0022935711313039064\n",
      "Epoch 73, Batch 8/32 : Loss = 0.014747037552297115\n",
      "Epoch 73, Batch 9/32 : Loss = 0.03420758992433548\n",
      "Epoch 73, Batch 10/32 : Loss = 0.003712927922606468\n",
      "Epoch 73, Batch 11/32 : Loss = 0.12198402732610703\n",
      "Epoch 73, Batch 12/32 : Loss = 0.004202906507998705\n",
      "Epoch 73, Batch 13/32 : Loss = 0.0032990616746246815\n",
      "Epoch 73, Batch 14/32 : Loss = 0.003128266427665949\n",
      "Epoch 73, Batch 15/32 : Loss = 0.010350339114665985\n",
      "Epoch 73, Batch 16/32 : Loss = 0.003839419689029455\n",
      "Epoch 73, Batch 17/32 : Loss = 0.029861032962799072\n",
      "Epoch 73, Batch 18/32 : Loss = 0.002766021993011236\n",
      "Epoch 73, Batch 19/32 : Loss = 0.0032841877546161413\n",
      "Epoch 73, Batch 20/32 : Loss = 0.0027136632706969976\n",
      "Epoch 73, Batch 21/32 : Loss = 0.011271409690380096\n",
      "Epoch 73, Batch 22/32 : Loss = 0.0050183129496872425\n",
      "Epoch 73, Batch 23/32 : Loss = 0.040807053446769714\n",
      "Epoch 73, Batch 24/32 : Loss = 0.01517315674573183\n",
      "Epoch 73, Batch 25/32 : Loss = 0.008295423351228237\n",
      "Epoch 73, Batch 26/32 : Loss = 0.00859153177589178\n",
      "Epoch 73, Batch 27/32 : Loss = 0.0035112593322992325\n",
      "Epoch 73, Batch 28/32 : Loss = 0.004715633578598499\n",
      "Epoch 73, Batch 29/32 : Loss = 0.0027939029969274998\n",
      "Epoch 73, Batch 30/32 : Loss = 0.0036761886440217495\n",
      "Epoch 73, Batch 31/32 : Loss = 0.007969730533659458\n",
      "Epoch 73 finished in 0.042422008514404294 minutes\n",
      "Epoch 73 training_loss = 0.014266266487538815\n",
      "----00----JJ---!--((--;---A-----3-----,-'--)---rr---r---77---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---dd---:--XX----9----e-----a-----F--,--8--------V----RR------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "---B-----.--Y----l--W------66----F----hh---XX---'--Y----2----- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "------Q-----33----gg---I--z----#----YY---:--]---q----++---**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 73 val_loss = 0.654887855052948, word_accuracy = 0.57\n",
      "Epoch 74, Batch 0/32 : Loss = 0.011144755408167839\n",
      "Epoch 74, Batch 1/32 : Loss = 0.004717004019767046\n",
      "Epoch 74, Batch 2/32 : Loss = 0.10182173550128937\n",
      "Epoch 74, Batch 3/32 : Loss = 0.006338069215416908\n",
      "Epoch 74, Batch 4/32 : Loss = 0.002513511572033167\n",
      "Epoch 74, Batch 5/32 : Loss = 0.0027691079303622246\n",
      "Epoch 74, Batch 6/32 : Loss = 0.001778963953256607\n",
      "Epoch 74, Batch 7/32 : Loss = 0.005454341880977154\n",
      "Epoch 74, Batch 8/32 : Loss = 0.015075908042490482\n",
      "Epoch 74, Batch 9/32 : Loss = 0.003130909986793995\n",
      "Epoch 74, Batch 10/32 : Loss = 0.0025884625501930714\n",
      "Epoch 74, Batch 11/32 : Loss = 0.002097922842949629\n",
      "Epoch 74, Batch 12/32 : Loss = 0.0023736078292131424\n",
      "Epoch 74, Batch 13/32 : Loss = 0.0030541105661541224\n",
      "Epoch 74, Batch 14/32 : Loss = 0.0023675765842199326\n",
      "Epoch 74, Batch 15/32 : Loss = 0.0016387514770030975\n",
      "Epoch 74, Batch 16/32 : Loss = 0.002823659684509039\n",
      "Epoch 74, Batch 17/32 : Loss = 0.024528605863451958\n",
      "Epoch 74, Batch 18/32 : Loss = 0.0026896707713603973\n",
      "Epoch 74, Batch 19/32 : Loss = 0.012243720702826977\n",
      "Epoch 74, Batch 20/32 : Loss = 0.0022619557566940784\n",
      "Epoch 74, Batch 21/32 : Loss = 0.003210834925994277\n",
      "Epoch 74, Batch 22/32 : Loss = 0.01773587055504322\n",
      "Epoch 74, Batch 23/32 : Loss = 0.008977396413683891\n",
      "Epoch 74, Batch 24/32 : Loss = 0.005594667978584766\n",
      "Epoch 74, Batch 25/32 : Loss = 0.11707179993391037\n",
      "Epoch 74, Batch 26/32 : Loss = 0.09386913478374481\n",
      "Epoch 74, Batch 27/32 : Loss = 0.0030927476473152637\n",
      "Epoch 74, Batch 28/32 : Loss = 0.006083880551159382\n",
      "Epoch 74, Batch 29/32 : Loss = 0.009964109398424625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Batch 30/32 : Loss = 0.019558362662792206\n",
      "Epoch 74, Batch 31/32 : Loss = 0.004330148454755545\n",
      "Epoch 74 finished in 0.04539973735809326 minutes\n",
      "Epoch 74 training_loss = 0.016035739332437515\n",
      "----#-----G-----99---EE----I-=-----h----5----##---22----JJ--))--k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---!-------3-----\\\\----$----->>------S-----\\----M--------ii--B-------- => !-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----X----7----0---j---@@----S----Z----L---44----C----mm-----M--------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "-----#-----GG----9----E----I-=----hh----5---##---22----J---)--kk------ => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 74 val_loss = 0.6522765159606934, word_accuracy = 0.67\n",
      "Epoch 75, Batch 0/32 : Loss = 0.002921447856351733\n",
      "Epoch 75, Batch 1/32 : Loss = 0.0038990387693047523\n",
      "Epoch 75, Batch 2/32 : Loss = 0.002646208740770817\n",
      "Epoch 75, Batch 3/32 : Loss = 0.018104566261172295\n",
      "Epoch 75, Batch 4/32 : Loss = 0.002339258324354887\n",
      "Epoch 75, Batch 5/32 : Loss = 0.0024625572841614485\n",
      "Epoch 75, Batch 6/32 : Loss = 0.0050536952912807465\n",
      "Epoch 75, Batch 7/32 : Loss = 0.0038740960881114006\n",
      "Epoch 75, Batch 8/32 : Loss = 0.004473371431231499\n",
      "Epoch 75, Batch 9/32 : Loss = 0.005450120661407709\n",
      "Epoch 75, Batch 10/32 : Loss = 0.022877678275108337\n",
      "Epoch 75, Batch 11/32 : Loss = 0.007838981226086617\n",
      "Epoch 75, Batch 12/32 : Loss = 0.008786620572209358\n",
      "Epoch 75, Batch 13/32 : Loss = 0.00730097945779562\n",
      "Epoch 75, Batch 14/32 : Loss = 0.01701340824365616\n",
      "Epoch 75, Batch 15/32 : Loss = 0.33817988634109497\n",
      "Epoch 75, Batch 16/32 : Loss = 0.0034788777120411396\n",
      "Epoch 75, Batch 17/32 : Loss = 0.0036077466793358326\n",
      "Epoch 75, Batch 18/32 : Loss = 0.02501390501856804\n",
      "Epoch 75, Batch 19/32 : Loss = 0.002950610127300024\n",
      "Epoch 75, Batch 20/32 : Loss = 0.0017494424246251583\n",
      "Epoch 75, Batch 21/32 : Loss = 0.0056105125695466995\n",
      "Epoch 75, Batch 22/32 : Loss = 0.1350703090429306\n",
      "Epoch 75, Batch 23/32 : Loss = 0.0226333886384964\n",
      "Epoch 75, Batch 24/32 : Loss = 0.0394841693341732\n",
      "Epoch 75, Batch 25/32 : Loss = 0.017616940662264824\n",
      "Epoch 75, Batch 26/32 : Loss = 0.0058434344828128815\n",
      "Epoch 75, Batch 27/32 : Loss = 0.018513230606913567\n",
      "Epoch 75, Batch 28/32 : Loss = 0.057349324226379395\n",
      "Epoch 75, Batch 29/32 : Loss = 0.024384979158639908\n",
      "Epoch 75, Batch 30/32 : Loss = 0.017600353807210922\n",
      "Epoch 75, Batch 31/32 : Loss = 0.017886817455291748\n",
      "Epoch 75 finished in 0.04259562095006307 minutes\n",
      "Epoch 75 training_loss = 0.02687116153538227\n",
      "--2---p---:--mm----x---a---zz--n----@@----C---yy--%%-----%----- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----##----G----9----E---I-=----h---55---#---2----J---)--k------ => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-----<<----v-----O------T---`---4----VV---[[--0o-------QQ------ => <vOT`4V[0o-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---J--;---q----+---|---zz--y----U-----%------U-----1---x---__-- => J;q+|zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 75 val_loss = 0.6304835081100464, word_accuracy = 0.61\n",
      "Epoch 76, Batch 0/32 : Loss = 0.004379433114081621\n",
      "Epoch 76, Batch 1/32 : Loss = 0.002821394708007574\n",
      "Epoch 76, Batch 2/32 : Loss = 0.0060499259270727634\n",
      "Epoch 76, Batch 3/32 : Loss = 0.04558386281132698\n",
      "Epoch 76, Batch 4/32 : Loss = 0.037804991006851196\n",
      "Epoch 76, Batch 5/32 : Loss = 0.006049451418220997\n",
      "Epoch 76, Batch 6/32 : Loss = 0.006326516158878803\n",
      "Epoch 76, Batch 7/32 : Loss = 0.060556113719940186\n",
      "Epoch 76, Batch 8/32 : Loss = 0.02845163084566593\n",
      "Epoch 76, Batch 9/32 : Loss = 0.029888339340686798\n",
      "Epoch 76, Batch 10/32 : Loss = 0.04582379758358002\n",
      "Epoch 76, Batch 11/32 : Loss = 0.02246180921792984\n",
      "Epoch 76, Batch 12/32 : Loss = 0.01061943918466568\n",
      "Epoch 76, Batch 13/32 : Loss = 0.009285550564527512\n",
      "Epoch 76, Batch 14/32 : Loss = 0.011342309415340424\n",
      "Epoch 76, Batch 15/32 : Loss = 0.028554243966937065\n",
      "Epoch 76, Batch 16/32 : Loss = 0.05141739174723625\n",
      "Epoch 76, Batch 17/32 : Loss = 0.016605615615844727\n",
      "Epoch 76, Batch 18/32 : Loss = 0.02021070569753647\n",
      "Epoch 76, Batch 19/32 : Loss = 0.003897792659699917\n",
      "Epoch 76, Batch 20/32 : Loss = 0.0066723935306072235\n",
      "Epoch 76, Batch 21/32 : Loss = 0.008061982691287994\n",
      "Epoch 76, Batch 22/32 : Loss = 0.008570561185479164\n",
      "Epoch 76, Batch 23/32 : Loss = 0.10114701837301254\n",
      "Epoch 76, Batch 24/32 : Loss = 0.01387888751924038\n",
      "Epoch 76, Batch 25/32 : Loss = 0.00890974048525095\n",
      "Epoch 76, Batch 26/32 : Loss = 0.035863328725099564\n",
      "Epoch 76, Batch 27/32 : Loss = 0.022204989567399025\n",
      "Epoch 76, Batch 28/32 : Loss = 0.008742810226976871\n",
      "Epoch 76, Batch 29/32 : Loss = 0.02899077907204628\n",
      "Epoch 76, Batch 30/32 : Loss = 0.0037662226241081953\n",
      "Epoch 76, Batch 31/32 : Loss = 0.004767808597534895\n",
      "Epoch 76 finished in 0.0422165830930074 minutes\n",
      "Epoch 76 training_loss = 0.022346505895256996\n",
      "---C----DD----E----g---mm-----\"--mm-----F---<<----Q----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----d----!---N-----rr--A----j---*---$$---33----hh---5-----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "----dd----:--XX----9----e----aa----F--,,-88--------V----R------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----kk----O-----//--,--y----c----*----1---PP----}--#-----B------ => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 76 val_loss = 0.703608512878418, word_accuracy = 0.72\n",
      "Epoch 77, Batch 0/32 : Loss = 0.0034439903683960438\n",
      "Epoch 77, Batch 1/32 : Loss = 0.0038229392375797033\n",
      "Epoch 77, Batch 2/32 : Loss = 0.004091604147106409\n",
      "Epoch 77, Batch 3/32 : Loss = 0.012529426254332066\n",
      "Epoch 77, Batch 4/32 : Loss = 0.0074442727491259575\n",
      "Epoch 77, Batch 5/32 : Loss = 0.002542979083955288\n",
      "Epoch 77, Batch 6/32 : Loss = 0.002776835346594453\n",
      "Epoch 77, Batch 7/32 : Loss = 0.0032828697003424168\n",
      "Epoch 77, Batch 8/32 : Loss = 0.0019531181314960122\n",
      "Epoch 77, Batch 9/32 : Loss = 0.003018120303750038\n",
      "Epoch 77, Batch 10/32 : Loss = 0.0034447789657860994\n",
      "Epoch 77, Batch 11/32 : Loss = 0.012710705399513245\n",
      "Epoch 77, Batch 12/32 : Loss = 0.007654245011508465\n",
      "Epoch 77, Batch 13/32 : Loss = 0.0026210290379822254\n",
      "Epoch 77, Batch 14/32 : Loss = 0.005463497247546911\n",
      "Epoch 77, Batch 15/32 : Loss = 0.002078187884762883\n",
      "Epoch 77, Batch 16/32 : Loss = 0.014496473595499992\n",
      "Epoch 77, Batch 17/32 : Loss = 0.003988207317888737\n",
      "Epoch 77, Batch 18/32 : Loss = 0.0028991331346333027\n",
      "Epoch 77, Batch 19/32 : Loss = 0.00891843531280756\n",
      "Epoch 77, Batch 20/32 : Loss = 0.0018796245567500591\n",
      "Epoch 77, Batch 21/32 : Loss = 0.01537054218351841\n",
      "Epoch 77, Batch 22/32 : Loss = 0.003501231549307704\n",
      "Epoch 77, Batch 23/32 : Loss = 0.05134814232587814\n",
      "Epoch 77, Batch 24/32 : Loss = 0.05093453824520111\n",
      "Epoch 77, Batch 25/32 : Loss = 0.013362094759941101\n",
      "Epoch 77, Batch 26/32 : Loss = 0.0035011745058000088\n",
      "Epoch 77, Batch 27/32 : Loss = 0.0015838166000321507\n",
      "Epoch 77, Batch 28/32 : Loss = 0.006083084736019373\n",
      "Epoch 77, Batch 29/32 : Loss = 0.0035070800222456455\n",
      "Epoch 77, Batch 30/32 : Loss = 0.01084057055413723\n",
      "Epoch 77, Batch 31/32 : Loss = 0.05119645595550537\n",
      "Epoch 77 finished in 0.04285873174667358 minutes\n",
      "Epoch 77 training_loss = 0.008915414102375507\n",
      "----d----!--N-----rr--A----j--*---$$---3----hh---5----nn-- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---<<----v----O-----TT---`--4----V----[---0-------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "--##----G----99---E---I-=----h---5----#---2----J--))--k--- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---o---\"\"--}}-!--B-----r---&----9---;-``--O-----w----}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 77 val_loss = 0.6708246469497681, word_accuracy = 0.7\n",
      "Epoch 78, Batch 0/32 : Loss = 0.003477589227259159\n",
      "Epoch 78, Batch 1/32 : Loss = 0.001673762104474008\n",
      "Epoch 78, Batch 2/32 : Loss = 0.0019432485569268465\n",
      "Epoch 78, Batch 3/32 : Loss = 0.003966824151575565\n",
      "Epoch 78, Batch 4/32 : Loss = 0.0027316417545080185\n",
      "Epoch 78, Batch 5/32 : Loss = 0.003421992529183626\n",
      "Epoch 78, Batch 6/32 : Loss = 0.0029809889383614063\n",
      "Epoch 78, Batch 7/32 : Loss = 0.00198739324696362\n",
      "Epoch 78, Batch 8/32 : Loss = 0.0017714024288579822\n",
      "Epoch 78, Batch 9/32 : Loss = 0.0022829179652035236\n",
      "Epoch 78, Batch 10/32 : Loss = 0.005399053916335106\n",
      "Epoch 78, Batch 11/32 : Loss = 0.0018452481599524617\n",
      "Epoch 78, Batch 12/32 : Loss = 0.0015592740382999182\n",
      "Epoch 78, Batch 13/32 : Loss = 0.08670113235712051\n",
      "Epoch 78, Batch 14/32 : Loss = 0.0160731952637434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Batch 15/32 : Loss = 0.005688142031431198\n",
      "Epoch 78, Batch 16/32 : Loss = 0.0016408950323238969\n",
      "Epoch 78, Batch 17/32 : Loss = 0.0028552513103932142\n",
      "Epoch 78, Batch 18/32 : Loss = 0.0022492478601634502\n",
      "Epoch 78, Batch 19/32 : Loss = 0.019891828298568726\n",
      "Epoch 78, Batch 20/32 : Loss = 0.002409040927886963\n",
      "Epoch 78, Batch 21/32 : Loss = 0.0022587734274566174\n",
      "Epoch 78, Batch 22/32 : Loss = 0.01461707428097725\n",
      "Epoch 78, Batch 23/32 : Loss = 0.03867414966225624\n",
      "Epoch 78, Batch 24/32 : Loss = 0.018710797652602196\n",
      "Epoch 78, Batch 25/32 : Loss = 0.0013118063798174262\n",
      "Epoch 78, Batch 26/32 : Loss = 0.011374257504940033\n",
      "Epoch 78, Batch 27/32 : Loss = 0.002171859610825777\n",
      "Epoch 78, Batch 28/32 : Loss = 0.0030891902279108763\n",
      "Epoch 78, Batch 29/32 : Loss = 0.0064298370853066444\n",
      "Epoch 78, Batch 30/32 : Loss = 0.009598703123629093\n",
      "Epoch 78, Batch 31/32 : Loss = 0.0021259624045342207\n",
      "Epoch 78 finished in 0.04205273787180583 minutes\n",
      "Epoch 78 training_loss = 0.009029792621731758\n",
      "-----c---R----;--9---yy---?---2----dd---i--O----{{-!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "--{{-BB----Y---RR---a---y---h---#---2--->>---E---4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----cc---R----;-99---y---?---2----dd--ii--O----{--!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---0-----c---++----b----I-\"\"--bb----6---..--QQ-------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 78 val_loss = 0.6941241025924683, word_accuracy = 0.66\n",
      "Epoch 79, Batch 0/32 : Loss = 0.009730231016874313\n",
      "Epoch 79, Batch 1/32 : Loss = 0.0043522268533706665\n",
      "Epoch 79, Batch 2/32 : Loss = 0.0212362390011549\n",
      "Epoch 79, Batch 3/32 : Loss = 0.003715421538800001\n",
      "Epoch 79, Batch 4/32 : Loss = 0.00334261916577816\n",
      "Epoch 79, Batch 5/32 : Loss = 0.00238047749735415\n",
      "Epoch 79, Batch 6/32 : Loss = 0.008586050942540169\n",
      "Epoch 79, Batch 7/32 : Loss = 0.024849634617567062\n",
      "Epoch 79, Batch 8/32 : Loss = 0.009832595475018024\n",
      "Epoch 79, Batch 9/32 : Loss = 0.002848571864888072\n",
      "Epoch 79, Batch 10/32 : Loss = 0.0017718699527904391\n",
      "Epoch 79, Batch 11/32 : Loss = 0.0016275106463581324\n",
      "Epoch 79, Batch 12/32 : Loss = 0.0018590646795928478\n",
      "Epoch 79, Batch 13/32 : Loss = 0.003113572485744953\n",
      "Epoch 79, Batch 14/32 : Loss = 0.022254182025790215\n",
      "Epoch 79, Batch 15/32 : Loss = 0.0026033888570964336\n",
      "Epoch 79, Batch 16/32 : Loss = 0.004190369509160519\n",
      "Epoch 79, Batch 17/32 : Loss = 0.008907792158424854\n",
      "Epoch 79, Batch 18/32 : Loss = 0.005784627981483936\n",
      "Epoch 79, Batch 19/32 : Loss = 0.003582300618290901\n",
      "Epoch 79, Batch 20/32 : Loss = 0.004226868972182274\n",
      "Epoch 79, Batch 21/32 : Loss = 0.022697828710079193\n",
      "Epoch 79, Batch 22/32 : Loss = 0.0018491486553102732\n",
      "Epoch 79, Batch 23/32 : Loss = 0.006077036261558533\n",
      "Epoch 79, Batch 24/32 : Loss = 0.002757712034508586\n",
      "Epoch 79, Batch 25/32 : Loss = 0.009000614285469055\n",
      "Epoch 79, Batch 26/32 : Loss = 0.0023874768521636724\n",
      "Epoch 79, Batch 27/32 : Loss = 0.02057029865682125\n",
      "Epoch 79, Batch 28/32 : Loss = 0.004313766956329346\n",
      "Epoch 79, Batch 29/32 : Loss = 0.002205418888479471\n",
      "Epoch 79, Batch 30/32 : Loss = 0.0015113904373720288\n",
      "Epoch 79, Batch 31/32 : Loss = 0.012385353446006775\n",
      "Epoch 79 finished in 0.04132325251897176 minutes\n",
      "Epoch 79 training_loss = 0.0072518703527748585\n",
      "----#-----00----[--x----)---R-----8---ii--P----ww-----)--- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "----\"--]--tt--4--------^---WW-------Q-----4--->-----g----- => \"]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "-----Y---WW-----]--ii-\\--|---MM-----<<---MM-----8---8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "---JJ--;--q----+---|---z---y---U----%%-----U-----1---x---- => J;q+|zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 79 val_loss = 0.681047260761261, word_accuracy = 0.69\n",
      "Epoch 80, Batch 0/32 : Loss = 0.004191395360976458\n",
      "Epoch 80, Batch 1/32 : Loss = 0.015535334125161171\n",
      "Epoch 80, Batch 2/32 : Loss = 0.003622638527303934\n",
      "Epoch 80, Batch 3/32 : Loss = 0.0016377390129491687\n",
      "Epoch 80, Batch 4/32 : Loss = 0.0016829479718580842\n",
      "Epoch 80, Batch 5/32 : Loss = 0.0017249856609851122\n",
      "Epoch 80, Batch 6/32 : Loss = 0.007675240747630596\n",
      "Epoch 80, Batch 7/32 : Loss = 0.01101046521216631\n",
      "Epoch 80, Batch 8/32 : Loss = 0.0056130969896912575\n",
      "Epoch 80, Batch 9/32 : Loss = 0.002269473159685731\n",
      "Epoch 80, Batch 10/32 : Loss = 0.009665019810199738\n",
      "Epoch 80, Batch 11/32 : Loss = 0.0036243912763893604\n",
      "Epoch 80, Batch 12/32 : Loss = 0.0013313645031303167\n",
      "Epoch 80, Batch 13/32 : Loss = 0.004027967341244221\n",
      "Epoch 80, Batch 14/32 : Loss = 0.0025723623111844063\n",
      "Epoch 80, Batch 15/32 : Loss = 0.0024638138711452484\n",
      "Epoch 80, Batch 16/32 : Loss = 0.0043309396132826805\n",
      "Epoch 80, Batch 17/32 : Loss = 0.012799945659935474\n",
      "Epoch 80, Batch 18/32 : Loss = 0.002210881095379591\n",
      "Epoch 80, Batch 19/32 : Loss = 0.0017748211976140738\n",
      "Epoch 80, Batch 20/32 : Loss = 0.01849256455898285\n",
      "Epoch 80, Batch 21/32 : Loss = 0.002574326004832983\n",
      "Epoch 80, Batch 22/32 : Loss = 0.003011982422322035\n",
      "Epoch 80, Batch 23/32 : Loss = 0.006844620686024427\n",
      "Epoch 80, Batch 24/32 : Loss = 0.0017076436197385192\n",
      "Epoch 80, Batch 25/32 : Loss = 0.001875495188869536\n",
      "Epoch 80, Batch 26/32 : Loss = 0.001419516745954752\n",
      "Epoch 80, Batch 27/32 : Loss = 0.00330402422696352\n",
      "Epoch 80, Batch 28/32 : Loss = 0.0374530665576458\n",
      "Epoch 80, Batch 29/32 : Loss = 0.0011279694736003876\n",
      "Epoch 80, Batch 30/32 : Loss = 0.0010540623916313052\n",
      "Epoch 80, Batch 31/32 : Loss = 0.001967565156519413\n",
      "Epoch 80 finished in 0.040645698706309 minutes\n",
      "Epoch 80 training_loss = 0.005747020710259676\n",
      "----d----!--N-----r---A----j--*---$----3---hh----5----n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---<<----v----O------T---`---4----v----[--0--------Q------ => <vOT`4v[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---\"---]--t---4--------^^---W-------Q-----44--->>----g---- => \"]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---JJ--;--q----+---|---z---y---U----%%-----U-----1--xx---- => J;q+|zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 80 val_loss = 0.662230908870697, word_accuracy = 0.7\n",
      "Epoch 81, Batch 0/32 : Loss = 0.0015965066850185394\n",
      "Epoch 81, Batch 1/32 : Loss = 0.00354738999158144\n",
      "Epoch 81, Batch 2/32 : Loss = 0.0016737249679863453\n",
      "Epoch 81, Batch 3/32 : Loss = 0.003942456096410751\n",
      "Epoch 81, Batch 4/32 : Loss = 0.002372289542108774\n",
      "Epoch 81, Batch 5/32 : Loss = 0.001705889473669231\n",
      "Epoch 81, Batch 6/32 : Loss = 0.0015074721304699779\n",
      "Epoch 81, Batch 7/32 : Loss = 0.028371132910251617\n",
      "Epoch 81, Batch 8/32 : Loss = 0.005971329752355814\n",
      "Epoch 81, Batch 9/32 : Loss = 0.002641459461301565\n",
      "Epoch 81, Batch 10/32 : Loss = 0.0016046009259298444\n",
      "Epoch 81, Batch 11/32 : Loss = 0.00523937214165926\n",
      "Epoch 81, Batch 12/32 : Loss = 0.0025162436068058014\n",
      "Epoch 81, Batch 13/32 : Loss = 0.0015401270939037204\n",
      "Epoch 81, Batch 14/32 : Loss = 0.0024387966841459274\n",
      "Epoch 81, Batch 15/32 : Loss = 0.002106525469571352\n",
      "Epoch 81, Batch 16/32 : Loss = 0.089169941842556\n",
      "Epoch 81, Batch 17/32 : Loss = 0.0023234612308442593\n",
      "Epoch 81, Batch 18/32 : Loss = 0.00463861832395196\n",
      "Epoch 81, Batch 19/32 : Loss = 0.0028652630280703306\n",
      "Epoch 81, Batch 20/32 : Loss = 0.0035243076272308826\n",
      "Epoch 81, Batch 21/32 : Loss = 0.01459911186248064\n",
      "Epoch 81, Batch 22/32 : Loss = 0.005897331051528454\n",
      "Epoch 81, Batch 23/32 : Loss = 0.0025068733375519514\n",
      "Epoch 81, Batch 24/32 : Loss = 0.0033621019683778286\n",
      "Epoch 81, Batch 25/32 : Loss = 0.0022068440448492765\n",
      "Epoch 81, Batch 26/32 : Loss = 0.004881688859313726\n",
      "Epoch 81, Batch 27/32 : Loss = 0.001756633399054408\n",
      "Epoch 81, Batch 28/32 : Loss = 0.12425132095813751\n",
      "Epoch 81, Batch 29/32 : Loss = 0.001793205738067627\n",
      "Epoch 81, Batch 30/32 : Loss = 0.0023209163919091225\n",
      "Epoch 81, Batch 31/32 : Loss = 0.028632882982492447\n",
      "Epoch 81 finished in 0.041188641389211016 minutes\n",
      "Epoch 81 training_loss = 0.010873960331082344\n",
      "----{---B-----Y-----R----aa----y---h----#----2----->----E-----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----YY----W--------]--i--\\---||---MM-------<-----MM------8----88---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "---r----GG-----I---TT----#-----33-----P------QQ------w------''-i---- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "----#----GG-----9----E----I--=----h----5----#----2----JJ---)--kk---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 81 val_loss = 0.6607493162155151, word_accuracy = 0.62\n",
      "Epoch 82, Batch 0/32 : Loss = 0.002173159271478653\n",
      "Epoch 82, Batch 1/32 : Loss = 0.0036091560032218695\n",
      "Epoch 82, Batch 2/32 : Loss = 0.0026205210015177727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Batch 3/32 : Loss = 0.0022954437881708145\n",
      "Epoch 82, Batch 4/32 : Loss = 0.006021813955157995\n",
      "Epoch 82, Batch 5/32 : Loss = 0.0013948046835139394\n",
      "Epoch 82, Batch 6/32 : Loss = 0.0036358421202749014\n",
      "Epoch 82, Batch 7/32 : Loss = 0.0685746893286705\n",
      "Epoch 82, Batch 8/32 : Loss = 0.013843965716660023\n",
      "Epoch 82, Batch 9/32 : Loss = 0.0014274141285568476\n",
      "Epoch 82, Batch 10/32 : Loss = 0.0011653242399916053\n",
      "Epoch 82, Batch 11/32 : Loss = 0.004124609287828207\n",
      "Epoch 82, Batch 12/32 : Loss = 0.003734809346497059\n",
      "Epoch 82, Batch 13/32 : Loss = 0.001349030528217554\n",
      "Epoch 82, Batch 14/32 : Loss = 0.0014903631526976824\n",
      "Epoch 82, Batch 15/32 : Loss = 0.002709111664444208\n",
      "Epoch 82, Batch 16/32 : Loss = 0.002503388561308384\n",
      "Epoch 82, Batch 17/32 : Loss = 0.0022135148756206036\n",
      "Epoch 82, Batch 18/32 : Loss = 0.004154664929956198\n",
      "Epoch 82, Batch 19/32 : Loss = 0.010430263355374336\n",
      "Epoch 82, Batch 20/32 : Loss = 0.0015550593379884958\n",
      "Epoch 82, Batch 21/32 : Loss = 0.0009064581245183945\n",
      "Epoch 82, Batch 22/32 : Loss = 0.0012255142210051417\n",
      "Epoch 82, Batch 23/32 : Loss = 0.009823473170399666\n",
      "Epoch 82, Batch 24/32 : Loss = 0.001374578569084406\n",
      "Epoch 82, Batch 25/32 : Loss = 0.0012406869791448116\n",
      "Epoch 82, Batch 26/32 : Loss = 0.03814712539315224\n",
      "Epoch 82, Batch 27/32 : Loss = 0.029057541862130165\n",
      "Epoch 82, Batch 28/32 : Loss = 0.04014138504862785\n",
      "Epoch 82, Batch 29/32 : Loss = 0.0014122073771432042\n",
      "Epoch 82, Batch 30/32 : Loss = 0.01494203507900238\n",
      "Epoch 82, Batch 31/32 : Loss = 0.0059937466867268085\n",
      "Epoch 82 finished in 0.04371804396311442 minutes\n",
      "Epoch 82 training_loss = 0.008997499011456966\n",
      "---#----GG-----9---EE---I--=----h----5---##---2----JJ---)--k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-----k---$$----|--',,--9----Y----N-----W------mm------T----8---- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---55---->---:--*----Y---A----'--O-----D----*---#----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---0-----cc----+-----bb-----I--\"---bb-----6----..---Q----------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 82 val_loss = 0.7052229642868042, word_accuracy = 0.63\n",
      "Epoch 83, Batch 0/32 : Loss = 0.0029454093892127275\n",
      "Epoch 83, Batch 1/32 : Loss = 0.004038374871015549\n",
      "Epoch 83, Batch 2/32 : Loss = 0.039884600788354874\n",
      "Epoch 83, Batch 3/32 : Loss = 0.0021213185973465443\n",
      "Epoch 83, Batch 4/32 : Loss = 0.0021798887755721807\n",
      "Epoch 83, Batch 5/32 : Loss = 0.014399884268641472\n",
      "Epoch 83, Batch 6/32 : Loss = 0.0021500145085155964\n",
      "Epoch 83, Batch 7/32 : Loss = 0.0190346147865057\n",
      "Epoch 83, Batch 8/32 : Loss = 0.00461119320243597\n",
      "Epoch 83, Batch 9/32 : Loss = 0.01894655078649521\n",
      "Epoch 83, Batch 10/32 : Loss = 0.002136471215635538\n",
      "Epoch 83, Batch 11/32 : Loss = 0.0030115984845906496\n",
      "Epoch 83, Batch 12/32 : Loss = 0.0019829263910651207\n",
      "Epoch 83, Batch 13/32 : Loss = 0.012218855321407318\n",
      "Epoch 83, Batch 14/32 : Loss = 0.014231419190764427\n",
      "Epoch 83, Batch 15/32 : Loss = 0.009803494438529015\n",
      "Epoch 83, Batch 16/32 : Loss = 0.0011540958657860756\n",
      "Epoch 83, Batch 17/32 : Loss = 0.0028795818798244\n",
      "Epoch 83, Batch 18/32 : Loss = 0.0019729905761778355\n",
      "Epoch 83, Batch 19/32 : Loss = 0.003612749744206667\n",
      "Epoch 83, Batch 20/32 : Loss = 0.007323451340198517\n",
      "Epoch 83, Batch 21/32 : Loss = 0.0027092937380075455\n",
      "Epoch 83, Batch 22/32 : Loss = 0.002916195197030902\n",
      "Epoch 83, Batch 23/32 : Loss = 0.004186877049505711\n",
      "Epoch 83, Batch 24/32 : Loss = 0.004987245425581932\n",
      "Epoch 83, Batch 25/32 : Loss = 0.046595413237810135\n",
      "Epoch 83, Batch 26/32 : Loss = 0.003649669699370861\n",
      "Epoch 83, Batch 27/32 : Loss = 0.008535018190741539\n",
      "Epoch 83, Batch 28/32 : Loss = 0.00413413904607296\n",
      "Epoch 83, Batch 29/32 : Loss = 0.0022139716893434525\n",
      "Epoch 83, Batch 30/32 : Loss = 0.012016269378364086\n",
      "Epoch 83, Batch 31/32 : Loss = 0.0016656549414619803\n",
      "Epoch 83 finished in 0.04449498256047567 minutes\n",
      "Epoch 83 training_loss = 0.008443108759820461\n",
      "----4---rr--{--%%-----/--'--)--w-----&----NN-----+----P--- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---d----:--X----9----e----a----F--,,-8-------VV---RR------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      ":------3-----\\---$$---->>-----S-----\\----M------ii-B------ => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----k----O-----/--,--y---cc---*---1----P---}--##---BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 83 val_loss = 0.6920279264450073, word_accuracy = 0.71\n",
      "Epoch 84, Batch 0/32 : Loss = 0.007721595000475645\n",
      "Epoch 84, Batch 1/32 : Loss = 0.00264621339738369\n",
      "Epoch 84, Batch 2/32 : Loss = 0.006810253951698542\n",
      "Epoch 84, Batch 3/32 : Loss = 0.001840503653511405\n",
      "Epoch 84, Batch 4/32 : Loss = 0.0019666405860334635\n",
      "Epoch 84, Batch 5/32 : Loss = 0.00635102204978466\n",
      "Epoch 84, Batch 6/32 : Loss = 0.01138436608016491\n",
      "Epoch 84, Batch 7/32 : Loss = 0.021700819954276085\n",
      "Epoch 84, Batch 8/32 : Loss = 0.0016571531305089593\n",
      "Epoch 84, Batch 9/32 : Loss = 0.002828218974173069\n",
      "Epoch 84, Batch 10/32 : Loss = 0.004152918234467506\n",
      "Epoch 84, Batch 11/32 : Loss = 0.0023089791648089886\n",
      "Epoch 84, Batch 12/32 : Loss = 0.00109206628985703\n",
      "Epoch 84, Batch 13/32 : Loss = 0.003248842665925622\n",
      "Epoch 84, Batch 14/32 : Loss = 0.04966342821717262\n",
      "Epoch 84, Batch 15/32 : Loss = 0.011150844395160675\n",
      "Epoch 84, Batch 16/32 : Loss = 0.2628788948059082\n",
      "Epoch 84, Batch 17/32 : Loss = 0.001253879745490849\n",
      "Epoch 84, Batch 18/32 : Loss = 0.0025450449902564287\n",
      "Epoch 84, Batch 19/32 : Loss = 0.024499960243701935\n",
      "Epoch 84, Batch 20/32 : Loss = 0.009909182786941528\n",
      "Epoch 84, Batch 21/32 : Loss = 0.0013780940789729357\n",
      "Epoch 84, Batch 22/32 : Loss = 0.01000942476093769\n",
      "Epoch 84, Batch 23/32 : Loss = 0.003781659994274378\n",
      "Epoch 84, Batch 24/32 : Loss = 0.0433674231171608\n",
      "Epoch 84, Batch 25/32 : Loss = 0.03181067854166031\n",
      "Epoch 84, Batch 26/32 : Loss = 0.012895869091153145\n",
      "Epoch 84, Batch 27/32 : Loss = 0.003860363271087408\n",
      "Epoch 84, Batch 28/32 : Loss = 0.11917056143283844\n",
      "Epoch 84, Batch 29/32 : Loss = 0.0024920287542045116\n",
      "Epoch 84, Batch 30/32 : Loss = 0.009783661924302578\n",
      "Epoch 84, Batch 31/32 : Loss = 0.004424634389579296\n",
      "Epoch 84 finished in 0.0424927274386088 minutes\n",
      "Epoch 84 training_loss = 0.021741803735494614\n",
      "-----0----Q----66---<<----<<---(---TT---N-----5---=----P----(----------- => 0Q6<<(TN5=P(, Ground Truth is 0Q6<<(TN5=P(m\n",
      "-----/----MM-------o----EE----^----3-----x----/----&----66-----X-------- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---WW------=----2----++---EE----1----n----TT---XX---r---C-----a---I----- => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "---rr---GGG-----I---TT----#------3------PP------Q------ww------'--i----- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "Epoch 84 val_loss = 0.6779996156692505, word_accuracy = 0.57\n",
      "Epoch 85, Batch 0/32 : Loss = 0.002573242411017418\n",
      "Epoch 85, Batch 1/32 : Loss = 0.002640525111928582\n",
      "Epoch 85, Batch 2/32 : Loss = 0.004256923217326403\n",
      "Epoch 85, Batch 3/32 : Loss = 0.0016062483191490173\n",
      "Epoch 85, Batch 4/32 : Loss = 0.005219719372689724\n",
      "Epoch 85, Batch 5/32 : Loss = 0.012976729311048985\n",
      "Epoch 85, Batch 6/32 : Loss = 0.0031367428600788116\n",
      "Epoch 85, Batch 7/32 : Loss = 0.012269529514014721\n",
      "Epoch 85, Batch 8/32 : Loss = 0.005381180439144373\n",
      "Epoch 85, Batch 9/32 : Loss = 0.08311663568019867\n",
      "Epoch 85, Batch 10/32 : Loss = 0.0034128364641219378\n",
      "Epoch 85, Batch 11/32 : Loss = 0.0030959644354879856\n",
      "Epoch 85, Batch 12/32 : Loss = 0.0061021726578474045\n",
      "Epoch 85, Batch 13/32 : Loss = 0.012776137329638004\n",
      "Epoch 85, Batch 14/32 : Loss = 0.03825266286730766\n",
      "Epoch 85, Batch 15/32 : Loss = 0.002339091384783387\n",
      "Epoch 85, Batch 16/32 : Loss = 0.004305388778448105\n",
      "Epoch 85, Batch 17/32 : Loss = 0.08092039823532104\n",
      "Epoch 85, Batch 18/32 : Loss = 0.002513545099645853\n",
      "Epoch 85, Batch 19/32 : Loss = 0.0022127379197627306\n",
      "Epoch 85, Batch 20/32 : Loss = 0.003297496121376753\n",
      "Epoch 85, Batch 21/32 : Loss = 0.0064300051890313625\n",
      "Epoch 85, Batch 22/32 : Loss = 0.004234361462295055\n",
      "Epoch 85, Batch 23/32 : Loss = 0.0035072830505669117\n",
      "Epoch 85, Batch 24/32 : Loss = 0.021275095641613007\n",
      "Epoch 85, Batch 25/32 : Loss = 0.031146978959441185\n",
      "Epoch 85, Batch 26/32 : Loss = 0.01603923924267292\n",
      "Epoch 85, Batch 27/32 : Loss = 0.004816930275410414\n",
      "Epoch 85, Batch 28/32 : Loss = 0.001756146433763206\n",
      "Epoch 85, Batch 29/32 : Loss = 0.04236052185297012\n",
      "Epoch 85, Batch 30/32 : Loss = 0.00332829263061285\n",
      "Epoch 85, Batch 31/32 : Loss = 0.5714757442474365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 finished in 0.04291137456893921 minutes\n",
      "Epoch 85 training_loss = 0.016023622825741768\n",
      "----/---MM-----o----E---^---3----x---/--&----6----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----<----v----O-----T---`---4----V---[---0-------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "-;------3----\\---$$---->>-----S----\\\\---MM-----ii--B----- => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----dd---!--NN----r---A---j--**--$$---3----h----5----n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 85 val_loss = 0.6645870208740234, word_accuracy = 0.7\n",
      "Epoch 86, Batch 0/32 : Loss = 0.0031817001290619373\n",
      "Epoch 86, Batch 1/32 : Loss = 0.005356575362384319\n",
      "Epoch 86, Batch 2/32 : Loss = 0.006153594236820936\n",
      "Epoch 86, Batch 3/32 : Loss = 0.0027484246529638767\n",
      "Epoch 86, Batch 4/32 : Loss = 0.0017099416581913829\n",
      "Epoch 86, Batch 5/32 : Loss = 0.004542353563010693\n",
      "Epoch 86, Batch 6/32 : Loss = 0.014982658438384533\n",
      "Epoch 86, Batch 7/32 : Loss = 0.002227430697530508\n",
      "Epoch 86, Batch 8/32 : Loss = 0.014500726014375687\n",
      "Epoch 86, Batch 9/32 : Loss = 0.002892611315473914\n",
      "Epoch 86, Batch 10/32 : Loss = 0.017270194366574287\n",
      "Epoch 86, Batch 11/32 : Loss = 0.004051155876368284\n",
      "Epoch 86, Batch 12/32 : Loss = 0.008644161745905876\n",
      "Epoch 86, Batch 13/32 : Loss = 0.007058538496494293\n",
      "Epoch 86, Batch 14/32 : Loss = 0.003193238517269492\n",
      "Epoch 86, Batch 15/32 : Loss = 0.003637524787336588\n",
      "Epoch 86, Batch 16/32 : Loss = 0.010591772384941578\n",
      "Epoch 86, Batch 17/32 : Loss = 0.016000255942344666\n",
      "Epoch 86, Batch 18/32 : Loss = 0.004966264590620995\n",
      "Epoch 86, Batch 19/32 : Loss = 0.019612712785601616\n",
      "Epoch 86, Batch 20/32 : Loss = 0.0026368710678070784\n",
      "Epoch 86, Batch 21/32 : Loss = 0.0026223305612802505\n",
      "Epoch 86, Batch 22/32 : Loss = 0.10234232246875763\n",
      "Epoch 86, Batch 23/32 : Loss = 0.0023867867421358824\n",
      "Epoch 86, Batch 24/32 : Loss = 0.0030418322421610355\n",
      "Epoch 86, Batch 25/32 : Loss = 0.005186666734516621\n",
      "Epoch 86, Batch 26/32 : Loss = 0.016353033483028412\n",
      "Epoch 86, Batch 27/32 : Loss = 0.1297157257795334\n",
      "Epoch 86, Batch 28/32 : Loss = 0.005319083109498024\n",
      "Epoch 86, Batch 29/32 : Loss = 0.0035237448755651712\n",
      "Epoch 86, Batch 30/32 : Loss = 0.06427793204784393\n",
      "Epoch 86, Batch 31/32 : Loss = 0.015652015805244446\n",
      "Epoch 86 finished in 0.04179981549580892 minutes\n",
      "Epoch 86 training_loss = 0.01582922600209713\n",
      "---BB-----..--YY----l--WW--------6-----FF----h------X----''--Y-----2------ => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----C-----DD----E-----g----m------\"\"--mm------F---<<-----Q----88---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "------dd-----:--XX-----9-----e-----a------F---,--88---------V-----R------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----7---n-----DD-----P-----n----t---w------d----\\----Q-----aa----R-------- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 86 val_loss = 0.6382565498352051, word_accuracy = 0.64\n",
      "Epoch 87, Batch 0/32 : Loss = 0.0024858186952769756\n",
      "Epoch 87, Batch 1/32 : Loss = 0.0061784484423696995\n",
      "Epoch 87, Batch 2/32 : Loss = 0.004068488720804453\n",
      "Epoch 87, Batch 3/32 : Loss = 0.01124839298427105\n",
      "Epoch 87, Batch 4/32 : Loss = 0.0043441723100841045\n",
      "Epoch 87, Batch 5/32 : Loss = 0.023671533912420273\n",
      "Epoch 87, Batch 6/32 : Loss = 0.005238683428615332\n",
      "Epoch 87, Batch 7/32 : Loss = 0.23041009902954102\n",
      "Epoch 87, Batch 8/32 : Loss = 0.023162705823779106\n",
      "Epoch 87, Batch 9/32 : Loss = 0.016080670058727264\n",
      "Epoch 87, Batch 10/32 : Loss = 0.0032119157258421183\n",
      "Epoch 87, Batch 11/32 : Loss = 0.013761152513325214\n",
      "Epoch 87, Batch 12/32 : Loss = 0.0233769491314888\n",
      "Epoch 87, Batch 13/32 : Loss = 0.003382694674655795\n",
      "Epoch 87, Batch 14/32 : Loss = 0.004839505068957806\n",
      "Epoch 87, Batch 15/32 : Loss = 0.0020288333762437105\n",
      "Epoch 87, Batch 16/32 : Loss = 0.030354227870702744\n",
      "Epoch 87, Batch 17/32 : Loss = 0.010642909444868565\n",
      "Epoch 87, Batch 18/32 : Loss = 0.012660403735935688\n",
      "Epoch 87, Batch 19/32 : Loss = 0.01624084822833538\n",
      "Epoch 87, Batch 20/32 : Loss = 0.008655989542603493\n",
      "Epoch 87, Batch 21/32 : Loss = 0.0040916576981544495\n",
      "Epoch 87, Batch 22/32 : Loss = 0.0027713696472346783\n",
      "Epoch 87, Batch 23/32 : Loss = 0.005328169092535973\n",
      "Epoch 87, Batch 24/32 : Loss = 0.00903613306581974\n",
      "Epoch 87, Batch 25/32 : Loss = 0.003624430624768138\n",
      "Epoch 87, Batch 26/32 : Loss = 0.004280583001673222\n",
      "Epoch 87, Batch 27/32 : Loss = 0.007024683989584446\n",
      "Epoch 87, Batch 28/32 : Loss = 0.003273752983659506\n",
      "Epoch 87, Batch 29/32 : Loss = 0.007280379068106413\n",
      "Epoch 87, Batch 30/32 : Loss = 0.0064012520015239716\n",
      "Epoch 87, Batch 31/32 : Loss = 0.0026959015522152185\n",
      "Epoch 87 finished in 0.043381377061208086 minutes\n",
      "Epoch 87 training_loss = 0.016369279474020004\n",
      "-----++---'---z----7----8-----d-----S-----v---5-----S----JJ----B----- => +'z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----8-----KK-----l---Z-----55-----p-----$-----aa----}---ww------,---- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---dd----:---X-----9-----e-----aa----FF--,,--8--------VV----RR------- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "--.--cc----O------w------u----``--u----.--RR-----{--33---\"\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 87 val_loss = 0.6592435836791992, word_accuracy = 0.71\n",
      "Epoch 88, Batch 0/32 : Loss = 0.0031205180566757917\n",
      "Epoch 88, Batch 1/32 : Loss = 0.0032840059138834476\n",
      "Epoch 88, Batch 2/32 : Loss = 0.005031367763876915\n",
      "Epoch 88, Batch 3/32 : Loss = 0.0014942577108740807\n",
      "Epoch 88, Batch 4/32 : Loss = 0.00570323783904314\n",
      "Epoch 88, Batch 5/32 : Loss = 0.002546522533521056\n",
      "Epoch 88, Batch 6/32 : Loss = 0.0018119483720511198\n",
      "Epoch 88, Batch 7/32 : Loss = 0.0038145012222230434\n",
      "Epoch 88, Batch 8/32 : Loss = 0.00402661319822073\n",
      "Epoch 88, Batch 9/32 : Loss = 0.0024556906428188086\n",
      "Epoch 88, Batch 10/32 : Loss = 0.0037603324744850397\n",
      "Epoch 88, Batch 11/32 : Loss = 0.0017843316309154034\n",
      "Epoch 88, Batch 12/32 : Loss = 0.005166868679225445\n",
      "Epoch 88, Batch 13/32 : Loss = 0.003000258933752775\n",
      "Epoch 88, Batch 14/32 : Loss = 0.002878598403185606\n",
      "Epoch 88, Batch 15/32 : Loss = 0.003763715038076043\n",
      "Epoch 88, Batch 16/32 : Loss = 0.0037971925921738148\n",
      "Epoch 88, Batch 17/32 : Loss = 0.0036933475639671087\n",
      "Epoch 88, Batch 18/32 : Loss = 0.06389254331588745\n",
      "Epoch 88, Batch 19/32 : Loss = 0.001519283396191895\n",
      "Epoch 88, Batch 20/32 : Loss = 0.0015616228338330984\n",
      "Epoch 88, Batch 21/32 : Loss = 0.003391603473573923\n",
      "Epoch 88, Batch 22/32 : Loss = 0.0043416330590844154\n",
      "Epoch 88, Batch 23/32 : Loss = 0.019550489261746407\n",
      "Epoch 88, Batch 24/32 : Loss = 0.004115888383239508\n",
      "Epoch 88, Batch 25/32 : Loss = 0.006017151288688183\n",
      "Epoch 88, Batch 26/32 : Loss = 0.0020485487766563892\n",
      "Epoch 88, Batch 27/32 : Loss = 0.0011235240381211042\n",
      "Epoch 88, Batch 28/32 : Loss = 0.003705008188262582\n",
      "Epoch 88, Batch 29/32 : Loss = 0.001427493873052299\n",
      "Epoch 88, Batch 30/32 : Loss = 0.06099661812186241\n",
      "Epoch 88, Batch 31/32 : Loss = 0.007589319720864296\n",
      "Epoch 88 finished in 0.04447211821873983 minutes\n",
      "Epoch 88 training_loss = 0.007575048133730888\n",
      "----dd---!!--NN-----r---AA---jj--*---$$----3----hh----5----mm--- => d!NrAj*$3h5m, Ground Truth is d!NrAj*$3h5n\n",
      "------QQ-----3-----g----I--z----#-----Y----:--]--qq----+----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---55---->---:--*----Y---A----'--O-----D----*---#-----O----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---JJ--;;--q-----+---/----z--yy---U-----%%-----UU-----1---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 88 val_loss = 0.6596340537071228, word_accuracy = 0.71\n",
      "Epoch 89, Batch 0/32 : Loss = 0.002234347630292177\n",
      "Epoch 89, Batch 1/32 : Loss = 0.0007219568360596895\n",
      "Epoch 89, Batch 2/32 : Loss = 0.001822321442887187\n",
      "Epoch 89, Batch 3/32 : Loss = 0.042786382138729095\n",
      "Epoch 89, Batch 4/32 : Loss = 0.002111503155902028\n",
      "Epoch 89, Batch 5/32 : Loss = 0.0012207869440317154\n",
      "Epoch 89, Batch 6/32 : Loss = 0.0010553591419011354\n",
      "Epoch 89, Batch 7/32 : Loss = 0.0031877076253294945\n",
      "Epoch 89, Batch 8/32 : Loss = 0.014357887208461761\n",
      "Epoch 89, Batch 9/32 : Loss = 0.0028991946019232273\n",
      "Epoch 89, Batch 10/32 : Loss = 0.0030189678072929382\n",
      "Epoch 89, Batch 11/32 : Loss = 0.013581756502389908\n",
      "Epoch 89, Batch 12/32 : Loss = 0.006053468212485313\n",
      "Epoch 89, Batch 13/32 : Loss = 0.0018264495301991701\n",
      "Epoch 89, Batch 14/32 : Loss = 0.005539700388908386\n",
      "Epoch 89, Batch 15/32 : Loss = 0.0031797545962035656\n",
      "Epoch 89, Batch 16/32 : Loss = 0.0020108455792069435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Batch 17/32 : Loss = 0.016815846785902977\n",
      "Epoch 89, Batch 18/32 : Loss = 0.004041990730911493\n",
      "Epoch 89, Batch 19/32 : Loss = 0.0011677888687700033\n",
      "Epoch 89, Batch 20/32 : Loss = 0.02943049557507038\n",
      "Epoch 89, Batch 21/32 : Loss = 0.00119158870074898\n",
      "Epoch 89, Batch 22/32 : Loss = 0.0026053027249872684\n",
      "Epoch 89, Batch 23/32 : Loss = 0.002119577955454588\n",
      "Epoch 89, Batch 24/32 : Loss = 0.007417204324156046\n",
      "Epoch 89, Batch 25/32 : Loss = 0.001784157706424594\n",
      "Epoch 89, Batch 26/32 : Loss = 0.0032658299896866083\n",
      "Epoch 89, Batch 27/32 : Loss = 0.003960396628826857\n",
      "Epoch 89, Batch 28/32 : Loss = 0.024194421246647835\n",
      "Epoch 89, Batch 29/32 : Loss = 0.005070737563073635\n",
      "Epoch 89, Batch 30/32 : Loss = 0.0011724643409252167\n",
      "Epoch 89, Batch 31/32 : Loss = 0.4403693377971649\n",
      "Epoch 89 finished in 0.04286861817042033 minutes\n",
      "Epoch 89 training_loss = 0.008574853651225567\n",
      "-----Q------3-----g----l--z----#-----YY---::--]---q-----+----*--- => Q3glz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "--..--c-----O----ww-----u----`--u----.--R-----{--3---\"\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----QQ-----3-----gg----I--z---##-----Y---:---]---q----++----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----55----9----gg----m------JJ---uu---Xx---C---X---..-dd---\\----- => 59gmJuXxCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 89 val_loss = 0.6669179201126099, word_accuracy = 0.72\n",
      "Epoch 90, Batch 0/32 : Loss = 0.0012539827730506659\n",
      "Epoch 90, Batch 1/32 : Loss = 0.0014931842451915145\n",
      "Epoch 90, Batch 2/32 : Loss = 0.005978478118777275\n",
      "Epoch 90, Batch 3/32 : Loss = 0.0011764786904677749\n",
      "Epoch 90, Batch 4/32 : Loss = 0.0010107180569320917\n",
      "Epoch 90, Batch 5/32 : Loss = 0.003506867215037346\n",
      "Epoch 90, Batch 6/32 : Loss = 0.004993025213479996\n",
      "Epoch 90, Batch 7/32 : Loss = 0.00301387719810009\n",
      "Epoch 90, Batch 8/32 : Loss = 0.0028322399593889713\n",
      "Epoch 90, Batch 9/32 : Loss = 0.0014405858237296343\n",
      "Epoch 90, Batch 10/32 : Loss = 0.00242608948610723\n",
      "Epoch 90, Batch 11/32 : Loss = 0.00188755476847291\n",
      "Epoch 90, Batch 12/32 : Loss = 0.001378444954752922\n",
      "Epoch 90, Batch 13/32 : Loss = 0.004968637600541115\n",
      "Epoch 90, Batch 14/32 : Loss = 0.0015552747063338757\n",
      "Epoch 90, Batch 15/32 : Loss = 0.001631818595342338\n",
      "Epoch 90, Batch 16/32 : Loss = 0.001476967940106988\n",
      "Epoch 90, Batch 17/32 : Loss = 0.006875640247017145\n",
      "Epoch 90, Batch 18/32 : Loss = 0.003627052064985037\n",
      "Epoch 90, Batch 19/32 : Loss = 0.0015245452523231506\n",
      "Epoch 90, Batch 20/32 : Loss = 0.001081195194274187\n",
      "Epoch 90, Batch 21/32 : Loss = 0.0008895957726053894\n",
      "Epoch 90, Batch 22/32 : Loss = 0.00383908674120903\n",
      "Epoch 90, Batch 23/32 : Loss = 0.05428377166390419\n",
      "Epoch 90, Batch 24/32 : Loss = 0.00277567096054554\n",
      "Epoch 90, Batch 25/32 : Loss = 0.0009332645568065345\n",
      "Epoch 90, Batch 26/32 : Loss = 0.0015236452454701066\n",
      "Epoch 90, Batch 27/32 : Loss = 0.0018403433496132493\n",
      "Epoch 90, Batch 28/32 : Loss = 0.004739832598716021\n",
      "Epoch 90, Batch 29/32 : Loss = 0.002540772082284093\n",
      "Epoch 90, Batch 30/32 : Loss = 0.015593711286783218\n",
      "Epoch 90, Batch 31/32 : Loss = 0.0008221614989452064\n",
      "Epoch 90 finished in 0.04402757485707601 minutes\n",
      "Epoch 90 training_loss = 0.004632774740457535\n",
      "---c----R-----;--9----y---?----2----dd---ii--O-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----k---$$--ll-F----DD----e---h----]--k---0---\\\\--X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----c----R----;--9----y----?---2----dd---i---O----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      ":------3-----\\---$$---->------S-----V----M------iiBB----- => :-3\\$>SVMiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 90 val_loss = 0.6878992319107056, word_accuracy = 0.65\n",
      "Epoch 91, Batch 0/32 : Loss = 0.0011834644246846437\n",
      "Epoch 91, Batch 1/32 : Loss = 0.012159844860434532\n",
      "Epoch 91, Batch 2/32 : Loss = 0.0013496167957782745\n",
      "Epoch 91, Batch 3/32 : Loss = 0.0012990778777748346\n",
      "Epoch 91, Batch 4/32 : Loss = 0.012963377870619297\n",
      "Epoch 91, Batch 5/32 : Loss = 0.0015003583393990993\n",
      "Epoch 91, Batch 6/32 : Loss = 0.0013748056953772902\n",
      "Epoch 91, Batch 7/32 : Loss = 0.0009591280249878764\n",
      "Epoch 91, Batch 8/32 : Loss = 0.0023835496976971626\n",
      "Epoch 91, Batch 9/32 : Loss = 0.008080879226326942\n",
      "Epoch 91, Batch 10/32 : Loss = 0.0007311701774597168\n",
      "Epoch 91, Batch 11/32 : Loss = 0.0006647236878052354\n",
      "Epoch 91, Batch 12/32 : Loss = 0.002252334263175726\n",
      "Epoch 91, Batch 13/32 : Loss = 0.001942046103067696\n",
      "Epoch 91, Batch 14/32 : Loss = 0.008281268179416656\n",
      "Epoch 91, Batch 15/32 : Loss = 0.0007679857080802321\n",
      "Epoch 91, Batch 16/32 : Loss = 0.0008035823702812195\n",
      "Epoch 91, Batch 17/32 : Loss = 0.0017912868643179536\n",
      "Epoch 91, Batch 18/32 : Loss = 0.0034271215554326773\n",
      "Epoch 91, Batch 19/32 : Loss = 0.0015186341479420662\n",
      "Epoch 91, Batch 20/32 : Loss = 0.0035794214345514774\n",
      "Epoch 91, Batch 21/32 : Loss = 0.0027272123843431473\n",
      "Epoch 91, Batch 22/32 : Loss = 0.014307970181107521\n",
      "Epoch 91, Batch 23/32 : Loss = 0.0010265475139021873\n",
      "Epoch 91, Batch 24/32 : Loss = 0.015132025815546513\n",
      "Epoch 91, Batch 25/32 : Loss = 0.0023836279287934303\n",
      "Epoch 91, Batch 26/32 : Loss = 0.010043916292488575\n",
      "Epoch 91, Batch 27/32 : Loss = 0.0014147304464131594\n",
      "Epoch 91, Batch 28/32 : Loss = 0.0025998600758612156\n",
      "Epoch 91, Batch 29/32 : Loss = 0.011869723908603191\n",
      "Epoch 91, Batch 30/32 : Loss = 0.000749452505260706\n",
      "Epoch 91, Batch 31/32 : Loss = 0.003685613628476858\n",
      "Epoch 91 finished in 0.04243799845377604 minutes\n",
      "Epoch 91 training_loss = 0.004232271108776331\n",
      "-----<<----v----OO-----TT---`--44----V----[[--00-------QQ------ => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----4----r---{---%------/--''-)--ww------&----N------+----P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---J--;---q----+---/---z---y----U----%%-----UU-----1---x---__-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "----\"---]--t---4---------^---WW-------Q-----44---->-----g------ => \"]t4^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 91 val_loss = 0.706452488899231, word_accuracy = 0.68\n",
      "Epoch 92, Batch 0/32 : Loss = 0.0014095810474827886\n",
      "Epoch 92, Batch 1/32 : Loss = 0.004083207342773676\n",
      "Epoch 92, Batch 2/32 : Loss = 0.0009621725766919553\n",
      "Epoch 92, Batch 3/32 : Loss = 0.0010009128600358963\n",
      "Epoch 92, Batch 4/32 : Loss = 0.0013609282905235887\n",
      "Epoch 92, Batch 5/32 : Loss = 0.001752135343849659\n",
      "Epoch 92, Batch 6/32 : Loss = 0.001084103249013424\n",
      "Epoch 92, Batch 7/32 : Loss = 0.012185071595013142\n",
      "Epoch 92, Batch 8/32 : Loss = 0.004887023009359837\n",
      "Epoch 92, Batch 9/32 : Loss = 0.0010163160040974617\n",
      "Epoch 92, Batch 10/32 : Loss = 0.02270342782139778\n",
      "Epoch 92, Batch 11/32 : Loss = 0.0025970712304115295\n",
      "Epoch 92, Batch 12/32 : Loss = 0.011690251529216766\n",
      "Epoch 92, Batch 13/32 : Loss = 0.0021532857790589333\n",
      "Epoch 92, Batch 14/32 : Loss = 0.00347705977037549\n",
      "Epoch 92, Batch 15/32 : Loss = 0.0068345139734447\n",
      "Epoch 92, Batch 16/32 : Loss = 0.0013917038450017571\n",
      "Epoch 92, Batch 17/32 : Loss = 0.005600525997579098\n",
      "Epoch 92, Batch 18/32 : Loss = 0.018544049933552742\n",
      "Epoch 92, Batch 19/32 : Loss = 0.0040431213565170765\n",
      "Epoch 92, Batch 20/32 : Loss = 0.018317455425858498\n",
      "Epoch 92, Batch 21/32 : Loss = 0.0026945958379656076\n",
      "Epoch 92, Batch 22/32 : Loss = 0.0010563938412815332\n",
      "Epoch 92, Batch 23/32 : Loss = 0.0029839796479791403\n",
      "Epoch 92, Batch 24/32 : Loss = 0.0012564739445224404\n",
      "Epoch 92, Batch 25/32 : Loss = 0.009458429180085659\n",
      "Epoch 92, Batch 26/32 : Loss = 0.005041469354182482\n",
      "Epoch 92, Batch 27/32 : Loss = 0.010640416294336319\n",
      "Epoch 92, Batch 28/32 : Loss = 0.003348379395902157\n",
      "Epoch 92, Batch 29/32 : Loss = 0.0025288239121437073\n",
      "Epoch 92, Batch 30/32 : Loss = 0.007381140720099211\n",
      "Epoch 92, Batch 31/32 : Loss = 0.04079149290919304\n",
      "Epoch 92 finished in 0.06618870099385579 minutes\n",
      "Epoch 92 training_loss = 0.005737604107707739\n",
      "---k----$---I--F----DD----e----hh---]--kk---0----\\---X------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----++---:-zz---77---8----d----S----v---5----S---JJ---B----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "------Q-----3----gg---II-z---#-----Y---::--]--q----+----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----##---GG----9---E---I-=---hh---5---#---2---JJ--)--k------ => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 92 val_loss = 0.7204262614250183, word_accuracy = 0.73\n",
      "Epoch 93, Batch 0/32 : Loss = 0.03533552587032318\n",
      "Epoch 93, Batch 1/32 : Loss = 0.021623607724905014\n",
      "Epoch 93, Batch 2/32 : Loss = 0.01749567873775959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Batch 3/32 : Loss = 0.0012730190064758062\n",
      "Epoch 93, Batch 4/32 : Loss = 0.002050351584330201\n",
      "Epoch 93, Batch 5/32 : Loss = 0.0009614721639081836\n",
      "Epoch 93, Batch 6/32 : Loss = 0.002599226078018546\n",
      "Epoch 93, Batch 7/32 : Loss = 0.004552524071186781\n",
      "Epoch 93, Batch 8/32 : Loss = 0.0022130950819700956\n",
      "Epoch 93, Batch 9/32 : Loss = 0.0014742190251126885\n",
      "Epoch 93, Batch 10/32 : Loss = 0.0008341155480593443\n",
      "Epoch 93, Batch 11/32 : Loss = 0.008251705206930637\n",
      "Epoch 93, Batch 12/32 : Loss = 0.0011901558609679341\n",
      "Epoch 93, Batch 13/32 : Loss = 0.0014414165634661913\n",
      "Epoch 93, Batch 14/32 : Loss = 0.0020477711223065853\n",
      "Epoch 93, Batch 15/32 : Loss = 0.0020790761336684227\n",
      "Epoch 93, Batch 16/32 : Loss = 0.017234764993190765\n",
      "Epoch 93, Batch 17/32 : Loss = 0.0012695988407358527\n",
      "Epoch 93, Batch 18/32 : Loss = 0.0013075408060103655\n",
      "Epoch 93, Batch 19/32 : Loss = 0.004592408426105976\n",
      "Epoch 93, Batch 20/32 : Loss = 0.0009591610869392753\n",
      "Epoch 93, Batch 21/32 : Loss = 0.0008430809248238802\n",
      "Epoch 93, Batch 22/32 : Loss = 0.000931803195271641\n",
      "Epoch 93, Batch 23/32 : Loss = 0.01113245077431202\n",
      "Epoch 93, Batch 24/32 : Loss = 0.017100870609283447\n",
      "Epoch 93, Batch 25/32 : Loss = 0.0016241330886259675\n",
      "Epoch 93, Batch 26/32 : Loss = 0.0028900750912725925\n",
      "Epoch 93, Batch 27/32 : Loss = 0.01899578422307968\n",
      "Epoch 93, Batch 28/32 : Loss = 0.00678028492256999\n",
      "Epoch 93, Batch 29/32 : Loss = 0.001280863187275827\n",
      "Epoch 93, Batch 30/32 : Loss = 0.002631319221109152\n",
      "Epoch 93, Batch 31/32 : Loss = 0.0009697852656245232\n",
      "Epoch 93 finished in 0.04270084698994955 minutes\n",
      "Epoch 93 training_loss = 0.00626886123791337\n",
      "----kk----$$----l--FF----DD------e-----h-----]--kk-----0----\\----X-------- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----55---->----::--*----Y----A----'---O-----DD----*---#------O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "--.---C-----O------w-------u----`---u----..--RR-----{---3----\"----#------- => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----W-------=----2----++---E-----1----n----T----X----r---C----a----I----- => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 93 val_loss = 0.719277024269104, word_accuracy = 0.71\n",
      "Epoch 94, Batch 0/32 : Loss = 0.004894155077636242\n",
      "Epoch 94, Batch 1/32 : Loss = 0.005049741826951504\n",
      "Epoch 94, Batch 2/32 : Loss = 0.001003679004497826\n",
      "Epoch 94, Batch 3/32 : Loss = 0.004112794529646635\n",
      "Epoch 94, Batch 4/32 : Loss = 0.003991757519543171\n",
      "Epoch 94, Batch 5/32 : Loss = 0.002378277014940977\n",
      "Epoch 94, Batch 6/32 : Loss = 0.001484819920733571\n",
      "Epoch 94, Batch 7/32 : Loss = 0.00110559759195894\n",
      "Epoch 94, Batch 8/32 : Loss = 0.01068020984530449\n",
      "Epoch 94, Batch 9/32 : Loss = 0.0034299022518098354\n",
      "Epoch 94, Batch 10/32 : Loss = 0.0023830297868698835\n",
      "Epoch 94, Batch 11/32 : Loss = 0.0014372470322996378\n",
      "Epoch 94, Batch 12/32 : Loss = 0.0010022214846685529\n",
      "Epoch 94, Batch 13/32 : Loss = 0.001407517702318728\n",
      "Epoch 94, Batch 14/32 : Loss = 0.0019808984361588955\n",
      "Epoch 94, Batch 15/32 : Loss = 0.0011894248891621828\n",
      "Epoch 94, Batch 16/32 : Loss = 0.0012346924049779773\n",
      "Epoch 94, Batch 17/32 : Loss = 0.002830471843481064\n",
      "Epoch 94, Batch 18/32 : Loss = 0.04634419083595276\n",
      "Epoch 94, Batch 19/32 : Loss = 0.001727074384689331\n",
      "Epoch 94, Batch 20/32 : Loss = 0.0057883248664438725\n",
      "Epoch 94, Batch 21/32 : Loss = 0.0019189559388905764\n",
      "Epoch 94, Batch 22/32 : Loss = 0.0022229517344385386\n",
      "Epoch 94, Batch 23/32 : Loss = 0.014955340884625912\n",
      "Epoch 94, Batch 24/32 : Loss = 0.002988427644595504\n",
      "Epoch 94, Batch 25/32 : Loss = 0.0015465167816728354\n",
      "Epoch 94, Batch 26/32 : Loss = 0.0014845408732071519\n",
      "Epoch 94, Batch 27/32 : Loss = 0.002237612148746848\n",
      "Epoch 94, Batch 28/32 : Loss = 0.0021544061601161957\n",
      "Epoch 94, Batch 29/32 : Loss = 0.0033359392546117306\n",
      "Epoch 94, Batch 30/32 : Loss = 0.002610554452985525\n",
      "Epoch 94, Batch 31/32 : Loss = 0.0033951853401958942\n",
      "Epoch 94 finished in 0.04378589391708374 minutes\n",
      "Epoch 94 training_loss = 0.0045409053564071655\n",
      "----##------0-----[----x----))---R------88----ii--PP-----w-------))-- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "---2---pp---:--m------x---a---z---n-----@-----C----y---%%-----%%----- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----\"\"---]--tt--44-----e----^---WW---------Q-----44---->------g------ => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "--.---c----OO-----w------uu---`---u----.--RR----{{--33---\"\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 94 val_loss = 0.7461957931518555, word_accuracy = 0.66\n",
      "Epoch 95, Batch 0/32 : Loss = 0.002762002870440483\n",
      "Epoch 95, Batch 1/32 : Loss = 0.0017433019820600748\n",
      "Epoch 95, Batch 2/32 : Loss = 0.004513924475759268\n",
      "Epoch 95, Batch 3/32 : Loss = 0.005420705769211054\n",
      "Epoch 95, Batch 4/32 : Loss = 0.005987251177430153\n",
      "Epoch 95, Batch 5/32 : Loss = 0.0015017673140391707\n",
      "Epoch 95, Batch 6/32 : Loss = 0.0018676049076020718\n",
      "Epoch 95, Batch 7/32 : Loss = 0.049728795886039734\n",
      "Epoch 95, Batch 8/32 : Loss = 0.00496060773730278\n",
      "Epoch 95, Batch 9/32 : Loss = 0.0017755184089764953\n",
      "Epoch 95, Batch 10/32 : Loss = 0.0018989710370078683\n",
      "Epoch 95, Batch 11/32 : Loss = 0.0014709091046825051\n",
      "Epoch 95, Batch 12/32 : Loss = 0.0007571304449811578\n",
      "Epoch 95, Batch 13/32 : Loss = 0.002171663101762533\n",
      "Epoch 95, Batch 14/32 : Loss = 0.007027719635516405\n",
      "Epoch 95, Batch 15/32 : Loss = 0.0009434083476662636\n",
      "Epoch 95, Batch 16/32 : Loss = 0.0016890265978872776\n",
      "Epoch 95, Batch 17/32 : Loss = 0.0015368671156466007\n",
      "Epoch 95, Batch 18/32 : Loss = 0.0015479519497603178\n",
      "Epoch 95, Batch 19/32 : Loss = 0.0019177342765033245\n",
      "Epoch 95, Batch 20/32 : Loss = 0.0031925546936690807\n",
      "Epoch 95, Batch 21/32 : Loss = 0.0008105484885163605\n",
      "Epoch 95, Batch 22/32 : Loss = 0.011403553187847137\n",
      "Epoch 95, Batch 23/32 : Loss = 0.0018010674975812435\n",
      "Epoch 95, Batch 24/32 : Loss = 0.000830034026876092\n",
      "Epoch 95, Batch 25/32 : Loss = 0.0018811782356351614\n",
      "Epoch 95, Batch 26/32 : Loss = 0.0031669398304075003\n",
      "Epoch 95, Batch 27/32 : Loss = 0.0006786152953281999\n",
      "Epoch 95, Batch 28/32 : Loss = 0.0011299619218334556\n",
      "Epoch 95, Batch 29/32 : Loss = 0.002018432365730405\n",
      "Epoch 95, Batch 30/32 : Loss = 0.004197738599032164\n",
      "Epoch 95, Batch 31/32 : Loss = 0.0028762277215719223\n",
      "Epoch 95 finished in 0.04348864555358887 minutes\n",
      "Epoch 95 training_loss = 0.004263229202479124\n",
      "-----Y----W------]--ii-\\---|---MM------<----MM-----8----8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----<<----v----O------T----`--44----V----[---0--------Q------ => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---B-----.--Y----l--W-------6----F----h-----X---'--Y----2----- => B.YlW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----Q-----3-----gg---l--z----#-----Y----:--]---q-----+----*-- => Q3glz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 95 val_loss = 0.7249544858932495, word_accuracy = 0.71\n",
      "Epoch 96, Batch 0/32 : Loss = 0.0012201691279187799\n",
      "Epoch 96, Batch 1/32 : Loss = 0.0015739352675154805\n",
      "Epoch 96, Batch 2/32 : Loss = 0.001151537406258285\n",
      "Epoch 96, Batch 3/32 : Loss = 0.0019659092649817467\n",
      "Epoch 96, Batch 4/32 : Loss = 0.000686928047798574\n",
      "Epoch 96, Batch 5/32 : Loss = 0.0011565180029720068\n",
      "Epoch 96, Batch 6/32 : Loss = 0.012369303032755852\n",
      "Epoch 96, Batch 7/32 : Loss = 0.0006483343313448131\n",
      "Epoch 96, Batch 8/32 : Loss = 0.0015368593158200383\n",
      "Epoch 96, Batch 9/32 : Loss = 0.000863577937707305\n",
      "Epoch 96, Batch 10/32 : Loss = 0.0012591991107910872\n",
      "Epoch 96, Batch 11/32 : Loss = 0.0017311166739091277\n",
      "Epoch 96, Batch 12/32 : Loss = 0.0017273176927119493\n",
      "Epoch 96, Batch 13/32 : Loss = 0.000977486139163375\n",
      "Epoch 96, Batch 14/32 : Loss = 0.0007436856394633651\n",
      "Epoch 96, Batch 15/32 : Loss = 0.0008626698981970549\n",
      "Epoch 96, Batch 16/32 : Loss = 0.0009503282490186393\n",
      "Epoch 96, Batch 17/32 : Loss = 0.0007860466721467674\n",
      "Epoch 96, Batch 18/32 : Loss = 0.010118769481778145\n",
      "Epoch 96, Batch 19/32 : Loss = 0.0017913375049829483\n",
      "Epoch 96, Batch 20/32 : Loss = 0.0008214589324779809\n",
      "Epoch 96, Batch 21/32 : Loss = 0.003096925560384989\n",
      "Epoch 96, Batch 22/32 : Loss = 0.0028267106972634792\n",
      "Epoch 96, Batch 23/32 : Loss = 0.0012892589438706636\n",
      "Epoch 96, Batch 24/32 : Loss = 0.001141444663517177\n",
      "Epoch 96, Batch 25/32 : Loss = 0.0007955464534461498\n",
      "Epoch 96, Batch 26/32 : Loss = 0.001446135574951768\n",
      "Epoch 96, Batch 27/32 : Loss = 0.0033200799953192472\n",
      "Epoch 96, Batch 28/32 : Loss = 0.008938425220549107\n",
      "Epoch 96, Batch 29/32 : Loss = 0.002285459078848362\n",
      "Epoch 96, Batch 30/32 : Loss = 0.002321478445082903\n",
      "Epoch 96, Batch 31/32 : Loss = 0.0003889898653142154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 finished in 0.044500919183095296 minutes\n",
      "Epoch 96 training_loss = 0.002327793510630727\n",
      "-----c----R-----;--9----y----?---22----d----i--OO-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---0-----c----+-----b-----I-\"\"---b-----6----.---QQ---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "--88----KK----ll--Z-----55----p-----$$----a-----}---w------, => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----##---GG----9---E---I-=---hh---5---#---2---JJ--))-k------ => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 96 val_loss = 0.7331629991531372, word_accuracy = 0.7\n",
      "Epoch 97, Batch 0/32 : Loss = 0.0008191640372388065\n",
      "Epoch 97, Batch 1/32 : Loss = 0.003299647942185402\n",
      "Epoch 97, Batch 2/32 : Loss = 0.0015393649227917194\n",
      "Epoch 97, Batch 3/32 : Loss = 0.004914905875921249\n",
      "Epoch 97, Batch 4/32 : Loss = 0.0009909968357533216\n",
      "Epoch 97, Batch 5/32 : Loss = 0.017180874943733215\n",
      "Epoch 97, Batch 6/32 : Loss = 0.0005602086894214153\n",
      "Epoch 97, Batch 7/32 : Loss = 0.0016568077262490988\n",
      "Epoch 97, Batch 8/32 : Loss = 0.0005095452070236206\n",
      "Epoch 97, Batch 9/32 : Loss = 0.0015371280023828149\n",
      "Epoch 97, Batch 10/32 : Loss = 0.00045107625192031264\n",
      "Epoch 97, Batch 11/32 : Loss = 0.0015580379404127598\n",
      "Epoch 97, Batch 12/32 : Loss = 0.0016027664532884955\n",
      "Epoch 97, Batch 13/32 : Loss = 0.001181453000754118\n",
      "Epoch 97, Batch 14/32 : Loss = 0.0004310995282139629\n",
      "Epoch 97, Batch 15/32 : Loss = 0.00187266047578305\n",
      "Epoch 97, Batch 16/32 : Loss = 0.0019391501555219293\n",
      "Epoch 97, Batch 17/32 : Loss = 0.0017971330089494586\n",
      "Epoch 97, Batch 18/32 : Loss = 0.0007884219521656632\n",
      "Epoch 97, Batch 19/32 : Loss = 0.0009795849910005927\n",
      "Epoch 97, Batch 20/32 : Loss = 0.0038555115461349487\n",
      "Epoch 97, Batch 21/32 : Loss = 0.0008767424733377993\n",
      "Epoch 97, Batch 22/32 : Loss = 0.0006943029584363103\n",
      "Epoch 97, Batch 23/32 : Loss = 0.0006863768212497234\n",
      "Epoch 97, Batch 24/32 : Loss = 0.012991504743695259\n",
      "Epoch 97, Batch 25/32 : Loss = 0.0011376718757674098\n",
      "Epoch 97, Batch 26/32 : Loss = 0.0006053858669474721\n",
      "Epoch 97, Batch 27/32 : Loss = 0.0006895689875818789\n",
      "Epoch 97, Batch 28/32 : Loss = 0.0018740991363301873\n",
      "Epoch 97, Batch 29/32 : Loss = 0.0005444573471322656\n",
      "Epoch 97, Batch 30/32 : Loss = 0.0012655078899115324\n",
      "Epoch 97, Batch 31/32 : Loss = 0.03077496960759163\n",
      "Epoch 97 finished in 0.04225773016611735 minutes\n",
      "Epoch 97 training_loss = 0.0023992941714823246\n",
      "----{---B-----Y-----R----aa----y---h----#----2----->----E-----44---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----+----:--z----77----8----dd-----S-----v---5-----S----JJ----B----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----55----99----g-----m-------J----u----x----c----x---.--d----\\----- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "--.---c----OO-----ww-----uu---``--u----.--RR----{{--33---\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 97 val_loss = 0.7227096557617188, word_accuracy = 0.69\n",
      "Epoch 98, Batch 0/32 : Loss = 0.0035198419354856014\n",
      "Epoch 98, Batch 1/32 : Loss = 0.0008101187413558364\n",
      "Epoch 98, Batch 2/32 : Loss = 0.0004905181122012436\n",
      "Epoch 98, Batch 3/32 : Loss = 0.000660973135381937\n",
      "Epoch 98, Batch 4/32 : Loss = 0.0008671085815876722\n",
      "Epoch 98, Batch 5/32 : Loss = 0.0005507804453372955\n",
      "Epoch 98, Batch 6/32 : Loss = 0.0007774808909744024\n",
      "Epoch 98, Batch 7/32 : Loss = 0.009551827795803547\n",
      "Epoch 98, Batch 8/32 : Loss = 0.0010183792328462005\n",
      "Epoch 98, Batch 9/32 : Loss = 0.00046928797382861376\n",
      "Epoch 98, Batch 10/32 : Loss = 0.001424498506821692\n",
      "Epoch 98, Batch 11/32 : Loss = 0.001059717615135014\n",
      "Epoch 98, Batch 12/32 : Loss = 0.0017718869494274259\n",
      "Epoch 98, Batch 13/32 : Loss = 0.0006247055134736001\n",
      "Epoch 98, Batch 14/32 : Loss = 0.0010996609926223755\n",
      "Epoch 98, Batch 15/32 : Loss = 0.0006641036598011851\n",
      "Epoch 98, Batch 16/32 : Loss = 0.0011741463094949722\n",
      "Epoch 98, Batch 17/32 : Loss = 0.006681704428046942\n",
      "Epoch 98, Batch 18/32 : Loss = 0.0004503134696278721\n",
      "Epoch 98, Batch 19/32 : Loss = 0.0012511068489402533\n",
      "Epoch 98, Batch 20/32 : Loss = 0.002041909843683243\n",
      "Epoch 98, Batch 21/32 : Loss = 0.002015260513871908\n",
      "Epoch 98, Batch 22/32 : Loss = 0.0015886423643678427\n",
      "Epoch 98, Batch 23/32 : Loss = 0.0431361123919487\n",
      "Epoch 98, Batch 24/32 : Loss = 0.0006320934044197202\n",
      "Epoch 98, Batch 25/32 : Loss = 0.02274571731686592\n",
      "Epoch 98, Batch 26/32 : Loss = 0.001978558488190174\n",
      "Epoch 98, Batch 27/32 : Loss = 0.0016749755013734102\n",
      "Epoch 98, Batch 28/32 : Loss = 0.0009838332189247012\n",
      "Epoch 98, Batch 29/32 : Loss = 0.001169679337181151\n",
      "Epoch 98, Batch 30/32 : Loss = 0.002670691581442952\n",
      "Epoch 98, Batch 31/32 : Loss = 0.001274910755455494\n",
      "Epoch 98 finished in 0.0423007607460022 minutes\n",
      "Epoch 98 training_loss = 0.0037177507765591145\n",
      "-----0----JJ--!!--(--;--A-----3----,-''-)---r--rr--77---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----k----O-----/--,--y---c---*---11---P----}--#----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "---:-----3----\\\\--$$---->-----S----\\----M-----ii-BB------ => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----\"--]--t--44---ee---^---W-------Q----44--->-----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 98 val_loss = 0.7194345593452454, word_accuracy = 0.7\n",
      "Epoch 99, Batch 0/32 : Loss = 0.020389679819345474\n",
      "Epoch 99, Batch 1/32 : Loss = 0.0008780560456216335\n",
      "Epoch 99, Batch 2/32 : Loss = 0.09371677041053772\n",
      "Epoch 99, Batch 3/32 : Loss = 0.001076852553524077\n",
      "Epoch 99, Batch 4/32 : Loss = 0.0015707057900726795\n",
      "Epoch 99, Batch 5/32 : Loss = 0.0009514989797025919\n",
      "Epoch 99, Batch 6/32 : Loss = 0.00144969392567873\n",
      "Epoch 99, Batch 7/32 : Loss = 0.0016909699188545346\n",
      "Epoch 99, Batch 8/32 : Loss = 0.0008313143043778837\n",
      "Epoch 99, Batch 9/32 : Loss = 0.023780101910233498\n",
      "Epoch 99, Batch 10/32 : Loss = 0.00522603141143918\n",
      "Epoch 99, Batch 11/32 : Loss = 0.06821160763502121\n",
      "Epoch 99, Batch 12/32 : Loss = 0.001183257787488401\n",
      "Epoch 99, Batch 13/32 : Loss = 0.0006563868373632431\n",
      "Epoch 99, Batch 14/32 : Loss = 0.0016153600299730897\n",
      "Epoch 99, Batch 15/32 : Loss = 0.0012594449799507856\n",
      "Epoch 99, Batch 16/32 : Loss = 0.0031408551149070263\n",
      "Epoch 99, Batch 17/32 : Loss = 0.002373900031670928\n",
      "Epoch 99, Batch 18/32 : Loss = 0.0013124067336320877\n",
      "Epoch 99, Batch 19/32 : Loss = 0.008703745901584625\n",
      "Epoch 99, Batch 20/32 : Loss = 0.003321033902466297\n",
      "Epoch 99, Batch 21/32 : Loss = 0.006402403581887484\n",
      "Epoch 99, Batch 22/32 : Loss = 0.003766751615330577\n",
      "Epoch 99, Batch 23/32 : Loss = 0.0015548127703368664\n",
      "Epoch 99, Batch 24/32 : Loss = 0.0034157459158450365\n",
      "Epoch 99, Batch 25/32 : Loss = 0.0007165895076468587\n",
      "Epoch 99, Batch 26/32 : Loss = 0.0012418830301612616\n",
      "Epoch 99, Batch 27/32 : Loss = 0.0009232418378815055\n",
      "Epoch 99, Batch 28/32 : Loss = 0.0014399928040802479\n",
      "Epoch 99, Batch 29/32 : Loss = 0.0010944325476884842\n",
      "Epoch 99, Batch 30/32 : Loss = 0.001574014313519001\n",
      "Epoch 99, Batch 31/32 : Loss = 0.0010962486267089844\n",
      "Epoch 99 finished in 0.043114391962687175 minutes\n",
      "Epoch 99 training_loss = 0.008533544838428497\n",
      "----0-----JJ---!--(---;--AA-----3----,--'--)---r---r---77----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----J--;;--q----+----/--z---yy---U----%%-----UU----11---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "----k----$----|--'-,--9----Y----N----WW------m------T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----z----O----\"\"----G-----/----~-----$-----c----11---j--t----- => zO\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 99 val_loss = 0.8133980631828308, word_accuracy = 0.58\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(epochs_num):\n",
    "    \n",
    "    tick = time.time()\n",
    "    train_loss = train(crnn, criterion, optimizer, logger, \n",
    "                       train_dataloder, batch_size, epoch)\n",
    "    tock = time.time()\n",
    "    \n",
    "    print(f'Epoch {epoch} finished in {(tock - tick) / 60} minutes')\n",
    "    print(f'Epoch {epoch} training_loss = {train_loss}')\n",
    "    \n",
    "    if epoch % val_each == 0:\n",
    "        val_loss, val_accurcy = val(crnn, criterion, logger, val_dataloder,\n",
    "                                    epoch, batch_size)\n",
    "        print(f'Epoch {epoch} val_loss = {val_loss}, word_accuracy = {val_accurcy}')\n",
    "            \n",
    "        # save best checkpoint\n",
    "        if best_acc <= val_accurcy:\n",
    "            best_acc = val_accurcy\n",
    "            checkpoint = {'input_hight':32,\n",
    "                          'output_size':len(alphapet)+1,\n",
    "                          'alphapet':alphapet,\n",
    "                          'train_transforms':train_transforms,\n",
    "                          'optim_dic':optimizer.state_dict(),\n",
    "                          'state_dic':crnn.state_dict(),\n",
    "                          'epoch':epoch\n",
    "                         }\n",
    "            torch.save(checkpoint,'checkpoints/best_checkpoint.pth')\n",
    "    \n",
    "    # save last epoch\n",
    "    checkpoint = {'input_hight':32,\n",
    "                  'output_size':len(alphapet)+1,\n",
    "                  'alphapet':alphapet,\n",
    "                  'train_transforms':train_transforms,\n",
    "                  'optim_dic':optimizer.state_dict(),\n",
    "                  'state_dic':crnn.state_dict(),\n",
    "                  'epoch':epoch\n",
    "                 }\n",
    "    torch.save(checkpoint,'checkpoints/last_checkpoint.pth')\n",
    "    \n",
    "print(f'the best accurcay is {best_acc}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
