{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"password_trainv3.csv\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    char_ststistics = {}\n",
    "    for i, row in enumerate(reader):\n",
    "        if(i == 0): continue\n",
    "        for c in row[1]:\n",
    "            if c in char_ststistics: char_ststistics[c] += 1\n",
    "            else: char_ststistics[c] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 66\n",
      "\" 72\n",
      "# 114\n",
      "$ 99\n",
      "% 66\n",
      "& 63\n",
      "' 84\n",
      "( 54\n",
      ") 66\n",
      "* 96\n",
      "+ 54\n",
      ", 72\n",
      "- 84\n",
      ". 81\n",
      "/ 57\n",
      "0 114\n",
      "1 120\n",
      "2 114\n",
      "3 105\n",
      "4 138\n",
      "5 111\n",
      "6 114\n",
      "7 99\n",
      "8 90\n",
      "9 105\n",
      ": 57\n",
      "; 69\n",
      "< 75\n",
      "= 84\n",
      "> 72\n",
      "? 66\n",
      "@ 60\n",
      "A 54\n",
      "B 78\n",
      "C 84\n",
      "D 75\n",
      "E 84\n",
      "F 69\n",
      "G 84\n",
      "H 54\n",
      "I 87\n",
      "J 78\n",
      "K 39\n",
      "L 72\n",
      "M 69\n",
      "N 78\n",
      "O 75\n",
      "P 75\n",
      "Q 66\n",
      "R 81\n",
      "S 81\n",
      "T 99\n",
      "U 42\n",
      "V 69\n",
      "W 96\n",
      "X 75\n",
      "Y 69\n",
      "Z 72\n",
      "[ 60\n",
      "\\ 63\n",
      "] 87\n",
      "^ 84\n",
      "_ 54\n",
      "` 81\n",
      "a 93\n",
      "b 69\n",
      "c 63\n",
      "d 96\n",
      "e 60\n",
      "f 72\n",
      "g 87\n",
      "h 87\n",
      "i 81\n",
      "j 66\n",
      "k 66\n",
      "l 45\n",
      "m 111\n",
      "n 114\n",
      "o 63\n",
      "p 72\n",
      "q 69\n",
      "r 69\n",
      "s 48\n",
      "t 69\n",
      "u 60\n",
      "v 63\n",
      "w 81\n",
      "x 60\n",
      "y 66\n",
      "z 63\n",
      "{ 57\n",
      "| 63\n",
      "} 81\n",
      "~ 60\n",
      "min value 39\n",
      "max value 138\n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "alphapet = ''\n",
    "for c in sorted(char_ststistics):\n",
    "    print(c, char_ststistics[c])\n",
    "    values.append(char_ststistics[c])\n",
    "    alphapet += c\n",
    "\n",
    "print(f'min value {min(values)}\\nmax value {max(values)}')\n",
    "print (alphapet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, sampler, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Passwords_data(Dataset):\n",
    "    def __init__ (self, csv_Path, imgs_path, transformers = None):\n",
    "        with open(csv_Path, 'r') as csv_file:\n",
    "            # images names and labels in matching indices\n",
    "            reader = csv.reader(csv_file)\n",
    "            self.imgs = []\n",
    "            self.lables = []\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0: continue\n",
    "                self.imgs.append(row[0])\n",
    "                self.lables.append(row[1])\n",
    "                \n",
    "            self.imgs_file = imgs_path\n",
    "            self.transformers = transformers\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # image\n",
    "        img_path = self.imgs_file + '/' + self.imgs[index]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        #print(img.size)\n",
    "        \n",
    "        # image augmentation\n",
    "        if self.transformers != None:\n",
    "            img = self.transformers(img)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # lable\n",
    "        lable = self.lables[index]\n",
    "        \n",
    "        return (img_path, img, lable)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeNormalize (object):\n",
    "    def __init__(self, img_size):\n",
    "        self.img_size = img_size # imgH, imgW\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img = transforms.Resize(size=self.img_size)(transforms.ToPILImage()(img))\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img.sub_(0.5).div_(0.5) # normalize gray scale\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignBatch(object):\n",
    "    def __init__(self, imgH = 32, imgW = 100, keep_ratio = True, min_ratio = 1, padding = False):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio = keep_ratio\n",
    "        self.min_ratio = min_ratio\n",
    "        self.padding = padding\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        img_paths, imgs, lables = zip(*batch)\n",
    "        imgH = self.imgH\n",
    "        imgW = self.imgW\n",
    "        \n",
    "        if(self.keep_ratio):\n",
    "            max_ratio = 0\n",
    "            for img in imgs:\n",
    "                #print(img.shape)\n",
    "                _, h, w = img.shape\n",
    "                if max_ratio < (w/h): max_ratio = (w/h)\n",
    "            if self.padding:\n",
    "                imgs_padded = []\n",
    "                for i, img in enumerate(imgs):\n",
    "                    _, h, w = img.shape\n",
    "                    w_new = h * max_ratio\n",
    "                    pad = int((w_new - w) / 2)\n",
    "                    #print(img.shape)\n",
    "                    imgs_padded.append(F.pad(img, (pad, pad), \"constant\"))\n",
    "                imgs = imgs_padded\n",
    "            \n",
    "            imgW = int(max_ratio * imgH)\n",
    "            \n",
    "        resizer = ResizeNormalize((imgH, imgW))\n",
    "        imgs = [resizer(img).unsqueeze(0) for img in imgs]\n",
    "        imgs = torch.cat(imgs, 0)\n",
    "        return img_paths, imgs, lables\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingSampler(sampler.Sampler):\n",
    "    \n",
    "    def __init__ (self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.number_sampels = len(data_source)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__ (self):\n",
    "        n_batches = len(self)//self.batch_size\n",
    "        tail = len(self) % self.batch_size\n",
    "        index = torch.LongTensor(len(self)).fill_(0)\n",
    "        for i in range(n_batches):\n",
    "            random_start = random.randint(0, len(self)-self.batch_size)\n",
    "            b_indx = random_start + torch.arange(0, self.batch_size)\n",
    "            index[i*self.batch_size : (i+1) * self.batch_size] = b_indx\n",
    "        if tail:\n",
    "            random_start = random.randint(0, len(self)-tail)\n",
    "            b_indx = random_start + torch.arange(0, tail-1)\n",
    "            index[n_batches * self.batch_size:] = b_indx\n",
    "            \n",
    "        return iter(index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.number_sampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<6S>z9X=j|\n",
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<6S>z9X=j|\n",
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<6S>z9X=j|\n",
      "torch.Size([1, 32, 228])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n}HG0oh+^|<%\n"
     ]
    }
   ],
   "source": [
    "dataset = Passwords_data('password_trainv3.csv', 'train_passwordv3')\n",
    "\n",
    "sampler = MatchingSampler(dataset, 4)\n",
    "align = AlignBatch()\n",
    "dataloder = DataLoader(dataset, batch_size=4, collate_fn=align, shuffle= False)\n",
    "\n",
    "img_pathes, imgs, lables = next(iter(dataloder))\n",
    "\n",
    "for sample in zip(img_pathes, imgs, lables):\n",
    "    #print(sample)\n",
    "    img_path, img, lable = sample\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.squeeze(0).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDrictionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BiDrictionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        t_steps, b_size, h_num = recurrent.shape\n",
    "        recurrent = recurrent.view(t_steps*b_size, h_num) # prepare the linear layer input\n",
    "        \n",
    "        output = self.embedding(recurrent)\n",
    "        output = output.view(t_steps, b_size, -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, inFilt, outFilt, kerSiz, padSiz, strideSiz, b_n = False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(inFilt, outFilt, kerSiz, padSiz, strideSiz)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, inChannal, nClasses, nHidden, nLSTMs = 2):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH == 32 , 'the image input hight must be 32'\n",
    "        \n",
    "        ks = [3, 3, 3, 3, 3, 3, 2] # kernal Size\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0] # padding\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1] # stride\n",
    "        fn = [64, 128, 256, 256, 512, 512, 512] # filters number\n",
    "        \n",
    "        cnn = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        def conv_layer(layNum, b_n = False):\n",
    "            nIn = inChannal if layNum == 0 else fn[layNum-1]\n",
    "            nOut = fn[layNum]\n",
    "            # Conv Layer\n",
    "            cnn.add_module(f'conv{layNum}', nn.Conv2d(nIn, nOut, ks[layNum], ss[layNum], ps[layNum]))\n",
    "            # btach normalization\n",
    "            if b_n:\n",
    "                cnn.add_module(f'batchnorm{layNum}', nn.BatchNorm2d(nOut))\n",
    "            # non Linearity (ReLU)\n",
    "            cnn.add_module(f'relu{layNum}', nn.ReLU(inplace=True))\n",
    "            \n",
    "        # Cnn Arch\n",
    "        conv_layer(0)\n",
    "        cnn.add_module(f'pooling{0}', nn.MaxPool2d(2, 2))  # 64 x 16\n",
    "        conv_layer(1)\n",
    "        cnn.add_module(f'pooling{1}', nn.MaxPool2d(2, 2))  # 128 x 8\n",
    "        conv_layer(2, b_n=True)\n",
    "        conv_layer(3)\n",
    "        # the irregular shape of stride and padding beacause of the shape of some char like (i, ..)\n",
    "        cnn.add_module(f'pooling{2}', nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 256 x 4\n",
    "        conv_layer(4, b_n=True)\n",
    "        conv_layer(5)\n",
    "        cnn.add_module(f'pooling{3}', nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 512 x 2\n",
    "        conv_layer(6, b_n=True) #  512 x 1\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        # Rnn Arch\n",
    "        rnn = nn.Sequential(\n",
    "            BiDrictionalLSTM(512, nHidden, nHidden),\n",
    "            BiDrictionalLSTM(nHidden, nHidden, nClasses)\n",
    "        )\n",
    "\n",
    "        self.rnn = rnn\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # cnn pass\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.shape\n",
    "        \n",
    "        assert h == 1, 'the hight after cnn must equal 1'\n",
    "        \n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1) # sequance, batch, features\n",
    "        \n",
    "        # rnn pass \n",
    "        rnn = self.rnn(conv)\n",
    "        \n",
    "        output = self.softmax(rnn)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN(32, 1, 37, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.load_state_dict(torch.load('crnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model state dict\n",
    "st = crnn.state_dict()\n",
    "for name in st:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint state dic\n",
    "st = torch.load('crnn.pth')\n",
    "for name,k in st.items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "        \"\"\"Log scalar variables.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value) for tag, value in tag_value_pairs])\n",
    "        self.writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label String Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class strLabelConverter(object):\n",
    "    \"\"\"Convert between str and label.\n",
    "\n",
    "    NOTE:\n",
    "        Insert `blank` to the alphabet for CTC.\n",
    "\n",
    "    Args:\n",
    "        alphabet (str): set of the possible characters.\n",
    "        ignore_case (bool, default=True): whether or not to ignore all of the case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, ignore_case=True):\n",
    "        self._ignore_case = ignore_case\n",
    "        if self._ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        self.alphabet = alphabet + '-'  # for `-1` index\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(alphabet):\n",
    "            # NOTE: 0 is reserved for 'blank' required by wrap_ctc\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Support batch or single str.\n",
    "\n",
    "        Args:\n",
    "            text (str or list of str): texts to convert.\n",
    "\n",
    "        Returns:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [\n",
    "                self.dict[char.lower() if self._ignore_case else char]\n",
    "                for char in text\n",
    "            ]\n",
    "            length = [len(text)]\n",
    "        elif isinstance(text, collections.abc.Iterable):\n",
    "            length = [len(s) for s in text]\n",
    "            text = ''.join(text)\n",
    "            text, _ = self.encode(text)\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode(self, t, length, raw=False):\n",
    "        \"\"\"Decode encoded texts back into strs.\n",
    "\n",
    "        Args:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: when the texts and its length does not match.\n",
    "\n",
    "        Returns:\n",
    "            text (str or list of str): texts to convert.\n",
    "        \"\"\"\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(), length)\n",
    "            if raw:\n",
    "                return ''.join([self.alphabet[i - 1] for i in t]), [i - 1 for i in t]\n",
    "            else:\n",
    "                char_list = []\n",
    "                lables_list = []\n",
    "                for i in range(length):\n",
    "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
    "                        char_list.append(self.alphabet[t[i] - 1])\n",
    "                        lables_list.append(t[i] - 1)\n",
    "                return ''.join(char_list), lables_list\n",
    "        else:\n",
    "            # batch mode\n",
    "            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(t.numel(), length.sum())\n",
    "            texts = []\n",
    "            lables = []\n",
    "            index = 0\n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                text, lable = self.decode(\n",
    "                        t[index:index + l], torch.IntTensor([l]), raw=raw)\n",
    "                texts.append(text)\n",
    "                lables.append(lable)\n",
    "                index += l\n",
    "            return texts, lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, translate=(.03,.03))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, scale=(.95,1.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, shear=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(degrees=3, expand=True), \n",
    "                                       transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomPerspective()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Passwords_data('password_trainv3.csv', 'train_passwordv3', transformers=train_transforms)\n",
    "\n",
    "sampler = MatchingSampler(dataset, 4)\n",
    "align = AlignBatch()\n",
    "dataloder = DataLoader(dataset, batch_size=4, collate_fn=align, shuffle= True)\n",
    "\n",
    "img_pathes, imgs, lables = next(iter(dataloder))\n",
    "#print(converter.encode(lables))\n",
    "\n",
    "for sample in zip(img_pathes, imgs, lables):\n",
    "    #print(sample)\n",
    "    img_path, img, lable = sample\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.squeeze(0).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect cuda device? True\n",
      "number of classes id 94 + blank\n"
     ]
    }
   ],
   "source": [
    "# training variables \n",
    "epochs_num = 100\n",
    "batch_size = 16\n",
    "cuda = torch.cuda.is_available()\n",
    "n_workers = 4\n",
    "nClasses = len(char_ststistics) + 1\n",
    "inChannels = 1\n",
    "imgH = 32\n",
    "nHidden = 256\n",
    "lr = .001\n",
    "test_display = 4\n",
    "val_each = 1\n",
    "use_pretrained = True\n",
    "pre_trained = 'crnn.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'detect cuda device? {cuda}')\n",
    "print(f'number of classes id {nClasses-1} + blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=0, translate=(.03,.03)),\n",
    "            transforms.RandomAffine(degrees=0, scale=(.95,1.05)),\n",
    "            transforms.RandomAffine(degrees=0, shear=20),\n",
    "            transforms.RandomRotation(degrees=3, expand=True)]),\n",
    "        transforms.ColorJitter(brightness=.3, contrast=.3, saturation=.3)],  p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Passwords_data('password_train.csv', 'train_passwordv3', transformers=train_transforms)\n",
    "\n",
    "#sampler = MatchingSampler(dataset, batch_size)\n",
    "align = AlignBatch()\n",
    "train_dataloder = DataLoader(train_dataset, batch_size=batch_size, collate_fn=align, \n",
    "                       shuffle= True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Passwords_data('password_train.csv', 'train_passwordv3')\n",
    "\n",
    "#sampler = MatchingSampler(dataset, batch_size)\n",
    "align = AlignBatch()\n",
    "val_dataloder = DataLoader(train_dataset, batch_size=batch_size, collate_fn=align, \n",
    "                       shuffle= True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = strLabelConverter(alphapet, ignore_case=False)\n",
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu0): ReLU(inplace)\n",
      "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): ReLU(inplace)\n",
      "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace)\n",
      "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu3): ReLU(inplace)\n",
      "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace)\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu5): ReLU(inplace)\n",
      "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu6): ReLU(inplace)\n",
      "  )\n",
      "  (rnn): Sequential(\n",
      "    (0): BiDrictionalLSTM(\n",
      "      (rnn): LSTM(512, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): BiDrictionalLSTM(\n",
      "      (rnn): LSTM(256, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=95, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "crnn = CRNN(imgH, inChannels, nClasses, nHidden)\n",
    "crnn.apply(weights_init)\n",
    "if use_pretrained :\n",
    "    model_dict = crnn.state_dict() # state of the current model\n",
    "    pretrained_dict = torch.load(pre_trained) # state of the pretrained model\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k != 'rnn.1.embedding.weight' and k != 'rnn.1.embedding.bias'} # remove the classifier from the state\n",
    "    classifier_dict = {k: v for k, v in model_dict.items() if k == 'rnn.1.embedding.weight' or k == 'rnn.1.embedding.bias'} # get the classifier weight from new model\n",
    "    pretrained_dict.update(classifier_dict) # update without classifier\n",
    "    crnn.load_state_dict(pretrained_dict)\n",
    "    \n",
    "print(crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda :\n",
    "    crnn = crnn.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(crnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logger file\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logger = Logger('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, logger, train_dataloder, batch_size, epoch_num):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    samples_num = 0\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(train_dataloder):\n",
    "        batches_done = len(train_dataloder) * epoch_num + batch_i\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device))\n",
    "        targets, lenghts = converter.encode(targets)\n",
    "        targets = Variable(targets.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        # pass to the network\n",
    "        preds = model(imgs)\n",
    "        preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "        \n",
    "        # loss\n",
    "        #print(preds_size.shape)\n",
    "        loss = criterion(preds, targets.cpu(), preds_size, t_lens)\n",
    "        epoch_loss += loss * imgs.shape[0]\n",
    "        logger.scalar_summary('loss_batches', loss, batches_done)\n",
    "        print(f'Epoch {epoch_num}, Batch {batch_i}/{len(train_dataloder)} : Loss = {loss}')\n",
    "        \n",
    "        # optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    epoch_loss /= samples_num\n",
    "    logger.scalar_summary('loss_epochs', epoch_loss, epoch_num)\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, criterion, logger, val_dataloder, epoch_num, batch_size=16, max_itre=100):\n",
    "    model.eval()\n",
    "    \n",
    "    nCorrect_words = 0\n",
    "    #nCorrect_chars = 0\n",
    "    val_loss = 0\n",
    "    samples_num = 0\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(val_dataloder):\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device), requires_grad=False)\n",
    "        targets_encoded, lenghts = converter.encode(targets)\n",
    "        targets_encoded = Variable(targets_encoded.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # pass to the network\n",
    "            preds = model(imgs)\n",
    "            preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(preds, targets_encoded.cpu(), preds_size, t_lens)\n",
    "            val_loss += loss * imgs.shape[0]\n",
    "            \n",
    "            # get the nework prediction\n",
    "            _, preds = preds.max(2)\n",
    "            preds = preds.transpose(1,0).contiguous().view(-1)\n",
    "            words_preds, lables_preds = converter.decode(preds, preds_size)\n",
    "            \n",
    "            for word_pred, target in zip(words_preds, targets):\n",
    "                if word_pred == target:\n",
    "                    nCorrect_words += 1\n",
    "    \n",
    "    # display some of the network prediction\n",
    "    row_preds, _ = converter.decode(preds, preds_size, raw=True)[:test_display]\n",
    "\n",
    "    for row_pred, word_pred, gt in zip(row_preds, words_preds, targets):\n",
    "        print(f'{row_pred} => {word_pred}, Ground Truth is {gt}')\n",
    "    \n",
    "    #compute loss and accurcy\n",
    "    word_accurcy = nCorrect_words / samples_num\n",
    "    val_loss /= samples_num\n",
    "    logger.scalar_summary('val_loss', val_loss, epoch_num)\n",
    "    logger.scalar_summary('val_WordAccurcy', word_accurcy, epoch_num)\n",
    "    \n",
    "    return val_loss, word_accurcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0/32 : Loss = 19.610260009765625\n",
      "Epoch 0, Batch 1/32 : Loss = 21.16632652282715\n",
      "Epoch 0, Batch 2/32 : Loss = 19.838205337524414\n",
      "Epoch 0, Batch 3/32 : Loss = 19.917219161987305\n",
      "Epoch 0, Batch 4/32 : Loss = 20.068241119384766\n",
      "Epoch 0, Batch 5/32 : Loss = 18.35588264465332\n",
      "Epoch 0, Batch 6/32 : Loss = 16.09850311279297\n",
      "Epoch 0, Batch 7/32 : Loss = 14.059656143188477\n",
      "Epoch 0, Batch 8/32 : Loss = 13.752017974853516\n",
      "Epoch 0, Batch 9/32 : Loss = 10.350309371948242\n",
      "Epoch 0, Batch 10/32 : Loss = 8.150835990905762\n",
      "Epoch 0, Batch 11/32 : Loss = 6.024013996124268\n",
      "Epoch 0, Batch 12/32 : Loss = 4.972746849060059\n",
      "Epoch 0, Batch 13/32 : Loss = 4.749800682067871\n",
      "Epoch 0, Batch 14/32 : Loss = 5.021124839782715\n",
      "Epoch 0, Batch 15/32 : Loss = 5.356621742248535\n",
      "Epoch 0, Batch 16/32 : Loss = 5.650416374206543\n",
      "Epoch 0, Batch 17/32 : Loss = 5.926567077636719\n",
      "Epoch 0, Batch 18/32 : Loss = 5.952961444854736\n",
      "Epoch 0, Batch 19/32 : Loss = 5.917829513549805\n",
      "Epoch 0, Batch 20/32 : Loss = 5.719517707824707\n",
      "Epoch 0, Batch 21/32 : Loss = 5.589520454406738\n",
      "Epoch 0, Batch 22/32 : Loss = 5.346749782562256\n",
      "Epoch 0, Batch 23/32 : Loss = 5.076338768005371\n",
      "Epoch 0, Batch 24/32 : Loss = 4.8856306076049805\n",
      "Epoch 0, Batch 25/32 : Loss = 4.746244430541992\n",
      "Epoch 0, Batch 26/32 : Loss = 4.717957496643066\n",
      "Epoch 0, Batch 27/32 : Loss = 4.78378438949585\n",
      "Epoch 0, Batch 28/32 : Loss = 4.7783613204956055\n",
      "Epoch 0, Batch 29/32 : Loss = 4.834011077880859\n",
      "Epoch 0, Batch 30/32 : Loss = 4.907083034515381\n",
      "Epoch 0, Batch 31/32 : Loss = 4.749537467956543\n",
      "Epoch 0 finished in 0.04026882648468018 minutes\n",
      "Epoch 0 training_loss = 9.218262672424316\n",
      "---------------------------------------------------------- => , Ground Truth is 9Od\\=wqpI$VV\n",
      "---------------------------------------------------------- => , Ground Truth is ZWi3w\"Cv|BQ\n",
      "Epoch 0 val_loss = 4.744897365570068, word_accuracy = 0.0\n",
      "Epoch 1, Batch 0/32 : Loss = 4.778485298156738\n",
      "Epoch 1, Batch 1/32 : Loss = 4.755269527435303\n",
      "Epoch 1, Batch 2/32 : Loss = 4.698902130126953\n",
      "Epoch 1, Batch 3/32 : Loss = 4.745770454406738\n",
      "Epoch 1, Batch 4/32 : Loss = 4.6349873542785645\n",
      "Epoch 1, Batch 5/32 : Loss = 4.637944221496582\n",
      "Epoch 1, Batch 6/32 : Loss = 4.646753311157227\n",
      "Epoch 1, Batch 7/32 : Loss = 4.733882427215576\n",
      "Epoch 1, Batch 8/32 : Loss = 4.691531181335449\n",
      "Epoch 1, Batch 9/32 : Loss = 4.663727760314941\n",
      "Epoch 1, Batch 10/32 : Loss = 4.651385307312012\n",
      "Epoch 1, Batch 11/32 : Loss = 4.654200553894043\n",
      "Epoch 1, Batch 12/32 : Loss = 4.615005016326904\n",
      "Epoch 1, Batch 13/32 : Loss = 4.654205322265625\n",
      "Epoch 1, Batch 14/32 : Loss = 4.710179805755615\n",
      "Epoch 1, Batch 15/32 : Loss = 4.697958946228027\n",
      "Epoch 1, Batch 16/32 : Loss = 4.5932297706604\n",
      "Epoch 1, Batch 17/32 : Loss = 4.603625297546387\n",
      "Epoch 1, Batch 18/32 : Loss = 4.632111072540283\n",
      "Epoch 1, Batch 19/32 : Loss = 4.713850021362305\n",
      "Epoch 1, Batch 20/32 : Loss = 4.617743968963623\n",
      "Epoch 1, Batch 21/32 : Loss = 4.623035907745361\n",
      "Epoch 1, Batch 22/32 : Loss = 4.607726097106934\n",
      "Epoch 1, Batch 23/32 : Loss = 4.647380828857422\n",
      "Epoch 1, Batch 24/32 : Loss = 4.73952579498291\n",
      "Epoch 1, Batch 25/32 : Loss = 4.582968235015869\n",
      "Epoch 1, Batch 26/32 : Loss = 4.598893642425537\n",
      "Epoch 1, Batch 27/32 : Loss = 4.606191635131836\n",
      "Epoch 1, Batch 28/32 : Loss = 4.625396728515625\n",
      "Epoch 1, Batch 29/32 : Loss = 4.577217102050781\n",
      "Epoch 1, Batch 30/32 : Loss = 4.585015773773193\n",
      "Epoch 1, Batch 31/32 : Loss = 4.609143257141113\n",
      "Epoch 1 finished in 0.040796101093292236 minutes\n",
      "Epoch 1 training_loss = 4.655429363250732\n",
      "------------------------------------------------------------ => , Ground Truth is TPug7\\n'?k^\n",
      "------------------------------------------------------------ => , Ground Truth is rAm\"1E'tP>J{D\n",
      "Epoch 1 val_loss = 4.5464324951171875, word_accuracy = 0.0\n",
      "Epoch 2, Batch 0/32 : Loss = 4.531296253204346\n",
      "Epoch 2, Batch 1/32 : Loss = 4.496330261230469\n",
      "Epoch 2, Batch 2/32 : Loss = 4.560053825378418\n",
      "Epoch 2, Batch 3/32 : Loss = 4.559390544891357\n",
      "Epoch 2, Batch 4/32 : Loss = 4.522505760192871\n",
      "Epoch 2, Batch 5/32 : Loss = 4.516317367553711\n",
      "Epoch 2, Batch 6/32 : Loss = 4.542410850524902\n",
      "Epoch 2, Batch 7/32 : Loss = 4.519242286682129\n",
      "Epoch 2, Batch 8/32 : Loss = 4.4955925941467285\n",
      "Epoch 2, Batch 9/32 : Loss = 4.509483337402344\n",
      "Epoch 2, Batch 10/32 : Loss = 4.471419811248779\n",
      "Epoch 2, Batch 11/32 : Loss = 4.471031188964844\n",
      "Epoch 2, Batch 12/32 : Loss = 4.494998931884766\n",
      "Epoch 2, Batch 13/32 : Loss = 4.510457992553711\n",
      "Epoch 2, Batch 14/32 : Loss = 4.443821907043457\n",
      "Epoch 2, Batch 15/32 : Loss = 4.574087142944336\n",
      "Epoch 2, Batch 16/32 : Loss = 4.4939374923706055\n",
      "Epoch 2, Batch 17/32 : Loss = 4.458458423614502\n",
      "Epoch 2, Batch 18/32 : Loss = 4.5205278396606445\n",
      "Epoch 2, Batch 19/32 : Loss = 4.4278411865234375\n",
      "Epoch 2, Batch 20/32 : Loss = 4.376712322235107\n",
      "Epoch 2, Batch 21/32 : Loss = 4.375775337219238\n",
      "Epoch 2, Batch 22/32 : Loss = 4.3343658447265625\n",
      "Epoch 2, Batch 23/32 : Loss = 4.422729015350342\n",
      "Epoch 2, Batch 24/32 : Loss = 4.349863529205322\n",
      "Epoch 2, Batch 25/32 : Loss = 4.397516250610352\n",
      "Epoch 2, Batch 26/32 : Loss = 4.27597713470459\n",
      "Epoch 2, Batch 27/32 : Loss = 4.411561489105225\n",
      "Epoch 2, Batch 28/32 : Loss = 4.36574125289917\n",
      "Epoch 2, Batch 29/32 : Loss = 4.337288856506348\n",
      "Epoch 2, Batch 30/32 : Loss = 4.291046142578125\n",
      "Epoch 2, Batch 31/32 : Loss = 4.16011905670166\n",
      "Epoch 2 finished in 0.04008466402689616 minutes\n",
      "Epoch 2 training_loss = 4.452298641204834\n",
      "------------------------------------------------------- => , Ground Truth is 9neVt4?-wW=\n",
      "------------------------------------------------------- => , Ground Truth is TPug7\\n'?k^\n",
      "Epoch 2 val_loss = 4.226965427398682, word_accuracy = 0.0\n",
      "Epoch 3, Batch 0/32 : Loss = 4.199681282043457\n",
      "Epoch 3, Batch 1/32 : Loss = 4.235893249511719\n",
      "Epoch 3, Batch 2/32 : Loss = 4.152153015136719\n",
      "Epoch 3, Batch 3/32 : Loss = 4.236870288848877\n",
      "Epoch 3, Batch 4/32 : Loss = 4.119251251220703\n",
      "Epoch 3, Batch 5/32 : Loss = 4.162259101867676\n",
      "Epoch 3, Batch 6/32 : Loss = 4.097809314727783\n",
      "Epoch 3, Batch 7/32 : Loss = 4.076776504516602\n",
      "Epoch 3, Batch 8/32 : Loss = 4.153436660766602\n",
      "Epoch 3, Batch 9/32 : Loss = 4.086685657501221\n",
      "Epoch 3, Batch 10/32 : Loss = 4.038054466247559\n",
      "Epoch 3, Batch 11/32 : Loss = 3.9308664798736572\n",
      "Epoch 3, Batch 12/32 : Loss = 4.089718341827393\n",
      "Epoch 3, Batch 13/32 : Loss = 4.022326946258545\n",
      "Epoch 3, Batch 14/32 : Loss = 3.9620110988616943\n",
      "Epoch 3, Batch 15/32 : Loss = 3.9224746227264404\n",
      "Epoch 3, Batch 16/32 : Loss = 3.8993191719055176\n",
      "Epoch 3, Batch 17/32 : Loss = 3.943814754486084\n",
      "Epoch 3, Batch 18/32 : Loss = 3.8043155670166016\n",
      "Epoch 3, Batch 19/32 : Loss = 3.783696413040161\n",
      "Epoch 3, Batch 20/32 : Loss = 3.8063912391662598\n",
      "Epoch 3, Batch 21/32 : Loss = 3.832460403442383\n",
      "Epoch 3, Batch 22/32 : Loss = 3.722541570663452\n",
      "Epoch 3, Batch 23/32 : Loss = 3.682098150253296\n",
      "Epoch 3, Batch 24/32 : Loss = 3.6303319931030273\n",
      "Epoch 3, Batch 25/32 : Loss = 3.571239948272705\n",
      "Epoch 3, Batch 26/32 : Loss = 3.712117910385132\n",
      "Epoch 3, Batch 27/32 : Loss = 3.6237168312072754\n",
      "Epoch 3, Batch 28/32 : Loss = 3.425541400909424\n",
      "Epoch 3, Batch 29/32 : Loss = 3.5181994438171387\n",
      "Epoch 3, Batch 30/32 : Loss = 3.4304795265197754\n",
      "Epoch 3, Batch 31/32 : Loss = 3.098031520843506\n",
      "Epoch 3 finished in 0.04012119770050049 minutes\n",
      "Epoch 3 training_loss = 3.8958969116210938\n",
      "--------------------------GG----------------------------------------- => G, Ground Truth is 6#\"bGfVwLhm\n",
      "--------------------------------------------------------------------- => , Ground Truth is WmFHU&2|;F27D\n",
      "Epoch 3 val_loss = 3.3123388290405273, word_accuracy = 0.0\n",
      "Epoch 4, Batch 0/32 : Loss = 3.30715274810791\n",
      "Epoch 4, Batch 1/32 : Loss = 3.341905117034912\n",
      "Epoch 4, Batch 2/32 : Loss = 3.321812629699707\n",
      "Epoch 4, Batch 3/32 : Loss = 3.1471381187438965\n",
      "Epoch 4, Batch 4/32 : Loss = 3.3212480545043945\n",
      "Epoch 4, Batch 5/32 : Loss = 3.2642853260040283\n",
      "Epoch 4, Batch 6/32 : Loss = 3.236881971359253\n",
      "Epoch 4, Batch 7/32 : Loss = 2.871783971786499\n",
      "Epoch 4, Batch 8/32 : Loss = 2.8418312072753906\n",
      "Epoch 4, Batch 9/32 : Loss = 2.996326446533203\n",
      "Epoch 4, Batch 10/32 : Loss = 2.8675713539123535\n",
      "Epoch 4, Batch 11/32 : Loss = 2.7983286380767822\n",
      "Epoch 4, Batch 12/32 : Loss = 2.739680290222168\n",
      "Epoch 4, Batch 13/32 : Loss = 2.8505053520202637\n",
      "Epoch 4, Batch 14/32 : Loss = 2.7527670860290527\n",
      "Epoch 4, Batch 15/32 : Loss = 2.4972152709960938\n",
      "Epoch 4, Batch 16/32 : Loss = 2.7233664989471436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 17/32 : Loss = 2.5552821159362793\n",
      "Epoch 4, Batch 18/32 : Loss = 2.424297571182251\n",
      "Epoch 4, Batch 19/32 : Loss = 2.7277517318725586\n",
      "Epoch 4, Batch 20/32 : Loss = 2.3791983127593994\n",
      "Epoch 4, Batch 21/32 : Loss = 2.387735366821289\n",
      "Epoch 4, Batch 22/32 : Loss = 2.3702149391174316\n",
      "Epoch 4, Batch 23/32 : Loss = 2.6557981967926025\n",
      "Epoch 4, Batch 24/32 : Loss = 2.1713461875915527\n",
      "Epoch 4, Batch 25/32 : Loss = 2.512227773666382\n",
      "Epoch 4, Batch 26/32 : Loss = 2.3508734703063965\n",
      "Epoch 4, Batch 27/32 : Loss = 2.28956937789917\n",
      "Epoch 4, Batch 28/32 : Loss = 2.1982340812683105\n",
      "Epoch 4, Batch 29/32 : Loss = 2.0634965896606445\n",
      "Epoch 4, Batch 30/32 : Loss = 2.091888904571533\n",
      "Epoch 4, Batch 31/32 : Loss = 2.140155792236328\n",
      "Epoch 4 finished in 0.03985077937444051 minutes\n",
      "Epoch 4 training_loss = 2.709244728088379\n",
      "--------$---g---------------4---6---k--------p------------ => $g46kp, Ground Truth is $$gAh/46kHp0=\n",
      "G----------f--EE-----Z---ee-------------------4-----Z----- => GfEZe4Z, Ground Truth is GfEze#!^'4Z\n",
      "Epoch 4 val_loss = 1.9530433416366577, word_accuracy = 0.0\n",
      "Epoch 5, Batch 0/32 : Loss = 2.114065408706665\n",
      "Epoch 5, Batch 1/32 : Loss = 2.0878422260284424\n",
      "Epoch 5, Batch 2/32 : Loss = 1.707195520401001\n",
      "Epoch 5, Batch 3/32 : Loss = 2.1072607040405273\n",
      "Epoch 5, Batch 4/32 : Loss = 1.9446580410003662\n",
      "Epoch 5, Batch 5/32 : Loss = 1.63076651096344\n",
      "Epoch 5, Batch 6/32 : Loss = 1.9373493194580078\n",
      "Epoch 5, Batch 7/32 : Loss = 1.8071714639663696\n",
      "Epoch 5, Batch 8/32 : Loss = 1.5592942237854004\n",
      "Epoch 5, Batch 9/32 : Loss = 1.6570959091186523\n",
      "Epoch 5, Batch 10/32 : Loss = 1.6440011262893677\n",
      "Epoch 5, Batch 11/32 : Loss = 1.5043058395385742\n",
      "Epoch 5, Batch 12/32 : Loss = 1.3670356273651123\n",
      "Epoch 5, Batch 13/32 : Loss = 1.4501991271972656\n",
      "Epoch 5, Batch 14/32 : Loss = 1.874640703201294\n",
      "Epoch 5, Batch 15/32 : Loss = 1.268918514251709\n",
      "Epoch 5, Batch 16/32 : Loss = 1.4796441793441772\n",
      "Epoch 5, Batch 17/32 : Loss = 1.2669475078582764\n",
      "Epoch 5, Batch 18/32 : Loss = 1.253116250038147\n",
      "Epoch 5, Batch 19/32 : Loss = 1.2707805633544922\n",
      "Epoch 5, Batch 20/32 : Loss = 1.272157907485962\n",
      "Epoch 5, Batch 21/32 : Loss = 1.1727983951568604\n",
      "Epoch 5, Batch 22/32 : Loss = 1.256643533706665\n",
      "Epoch 5, Batch 23/32 : Loss = 1.5235828161239624\n",
      "Epoch 5, Batch 24/32 : Loss = 0.9892687797546387\n",
      "Epoch 5, Batch 25/32 : Loss = 0.8999283313751221\n",
      "Epoch 5, Batch 26/32 : Loss = 1.4789519309997559\n",
      "Epoch 5, Batch 27/32 : Loss = 1.6442533731460571\n",
      "Epoch 5, Batch 28/32 : Loss = 1.0909910202026367\n",
      "Epoch 5, Batch 29/32 : Loss = 1.0068100690841675\n",
      "Epoch 5, Batch 30/32 : Loss = 0.8241660594940186\n",
      "Epoch 5, Batch 31/32 : Loss = 0.8030757904052734\n",
      "Epoch 5 finished in 0.039975472291310626 minutes\n",
      "Epoch 5 training_loss = 1.484087586402893\n",
      "E------J--m-----3-----------RR---$---------- => EJm3R$, Ground Truth is EJm3[]|R$[h\n",
      "-----B------&----e-------J---------99---P--- => B&eJ9P, Ground Truth is fB`&e|J)o9P\n",
      "Epoch 5 val_loss = 0.9943920373916626, word_accuracy = 0.0321285140562249\n",
      "Epoch 6, Batch 0/32 : Loss = 0.8712865710258484\n",
      "Epoch 6, Batch 1/32 : Loss = 0.8320332765579224\n",
      "Epoch 6, Batch 2/32 : Loss = 1.0102459192276\n",
      "Epoch 6, Batch 3/32 : Loss = 1.1345014572143555\n",
      "Epoch 6, Batch 4/32 : Loss = 0.8194395303726196\n",
      "Epoch 6, Batch 5/32 : Loss = 0.8458527326583862\n",
      "Epoch 6, Batch 6/32 : Loss = 0.6864299774169922\n",
      "Epoch 6, Batch 7/32 : Loss = 0.7943707704544067\n",
      "Epoch 6, Batch 8/32 : Loss = 0.8699756264686584\n",
      "Epoch 6, Batch 9/32 : Loss = 0.6873177289962769\n",
      "Epoch 6, Batch 10/32 : Loss = 1.028111457824707\n",
      "Epoch 6, Batch 11/32 : Loss = 1.0298936367034912\n",
      "Epoch 6, Batch 12/32 : Loss = 0.574255108833313\n",
      "Epoch 6, Batch 13/32 : Loss = 0.8660458326339722\n",
      "Epoch 6, Batch 14/32 : Loss = 0.956962525844574\n",
      "Epoch 6, Batch 15/32 : Loss = 1.140059232711792\n",
      "Epoch 6, Batch 16/32 : Loss = 0.424044132232666\n",
      "Epoch 6, Batch 17/32 : Loss = 0.5295346975326538\n",
      "Epoch 6, Batch 18/32 : Loss = 0.4954787790775299\n",
      "Epoch 6, Batch 19/32 : Loss = 0.5641443729400635\n",
      "Epoch 6, Batch 20/32 : Loss = 0.5072970986366272\n",
      "Epoch 6, Batch 21/32 : Loss = 0.5758140087127686\n",
      "Epoch 6, Batch 22/32 : Loss = 0.5363547801971436\n",
      "Epoch 6, Batch 23/32 : Loss = 0.5477204322814941\n",
      "Epoch 6, Batch 24/32 : Loss = 0.8167130947113037\n",
      "Epoch 6, Batch 25/32 : Loss = 0.6492631435394287\n",
      "Epoch 6, Batch 26/32 : Loss = 0.5346975326538086\n",
      "Epoch 6, Batch 27/32 : Loss = 0.5806812047958374\n",
      "Epoch 6, Batch 28/32 : Loss = 1.0001808404922485\n",
      "Epoch 6, Batch 29/32 : Loss = 0.7875951528549194\n",
      "Epoch 6, Batch 30/32 : Loss = 0.4464558959007263\n",
      "Epoch 6, Batch 31/32 : Loss = 0.394647479057312\n",
      "Epoch 6 finished in 0.04069598118464152 minutes\n",
      "Epoch 6 training_loss = 0.745127260684967\n",
      "}-------)--I--<-----Y---ff--0----G----=----->----H----ZZ---- => })I<Yf0G=>HZ, Ground Truth is })I<Yf0G=>HZ\n",
      "#--------T----RR----7---n----*---77---D----yy---K---l--2---- => #TR7n*7DyKl2, Ground Truth is #TR7n*7DyKl2\n",
      "Epoch 6 val_loss = 0.5094054937362671, word_accuracy = 0.5301204819277109\n",
      "Epoch 7, Batch 0/32 : Loss = 0.6401955485343933\n",
      "Epoch 7, Batch 1/32 : Loss = 0.32874664664268494\n",
      "Epoch 7, Batch 2/32 : Loss = 0.8099756240844727\n",
      "Epoch 7, Batch 3/32 : Loss = 0.532443642616272\n",
      "Epoch 7, Batch 4/32 : Loss = 0.4183483123779297\n",
      "Epoch 7, Batch 5/32 : Loss = 0.6146501302719116\n",
      "Epoch 7, Batch 6/32 : Loss = 0.4164313077926636\n",
      "Epoch 7, Batch 7/32 : Loss = 0.35136982798576355\n",
      "Epoch 7, Batch 8/32 : Loss = 0.5792064070701599\n",
      "Epoch 7, Batch 9/32 : Loss = 0.29243749380111694\n",
      "Epoch 7, Batch 10/32 : Loss = 0.3214535415172577\n",
      "Epoch 7, Batch 11/32 : Loss = 0.5820269584655762\n",
      "Epoch 7, Batch 12/32 : Loss = 0.5211321115493774\n",
      "Epoch 7, Batch 13/32 : Loss = 0.3062750995159149\n",
      "Epoch 7, Batch 14/32 : Loss = 0.3187254071235657\n",
      "Epoch 7, Batch 15/32 : Loss = 0.5527862906455994\n",
      "Epoch 7, Batch 16/32 : Loss = 0.5900324583053589\n",
      "Epoch 7, Batch 17/32 : Loss = 0.257451593875885\n",
      "Epoch 7, Batch 18/32 : Loss = 0.30938759446144104\n",
      "Epoch 7, Batch 19/32 : Loss = 0.27971628308296204\n",
      "Epoch 7, Batch 20/32 : Loss = 0.3001718819141388\n",
      "Epoch 7, Batch 21/32 : Loss = 0.32359063625335693\n",
      "Epoch 7, Batch 22/32 : Loss = 0.31377142667770386\n",
      "Epoch 7, Batch 23/32 : Loss = 0.5932235717773438\n",
      "Epoch 7, Batch 24/32 : Loss = 0.6036441922187805\n",
      "Epoch 7, Batch 25/32 : Loss = 0.5689945220947266\n",
      "Epoch 7, Batch 26/32 : Loss = 0.216096892952919\n",
      "Epoch 7, Batch 27/32 : Loss = 0.28209659457206726\n",
      "Epoch 7, Batch 28/32 : Loss = 0.18326149880886078\n",
      "Epoch 7, Batch 29/32 : Loss = 0.2788273096084595\n",
      "Epoch 7, Batch 30/32 : Loss = 0.2060013711452484\n",
      "Epoch 7, Batch 31/32 : Loss = 0.7435278296470642\n",
      "Epoch 7 finished in 0.03993619680404663 minutes\n",
      "Epoch 7 training_loss = 0.41720205545425415\n",
      "p-------}---A---z---|---O-----J--f--g----t--1---d--- => p}Az|OJfgt1d, Ground Truth is p}Az|OJfgt1d\n",
      "G----------4------U----k--d---2---E---y--99--u---)-- => G4Ukd2Ey9u), Ground Truth is GN4{Ukd2Ey9u)\n",
      "Epoch 7 val_loss = 0.29552945494651794, word_accuracy = 0.7771084337349398\n",
      "Epoch 8, Batch 0/32 : Loss = 0.1682729423046112\n",
      "Epoch 8, Batch 1/32 : Loss = 0.1826915740966797\n",
      "Epoch 8, Batch 2/32 : Loss = 0.19910705089569092\n",
      "Epoch 8, Batch 3/32 : Loss = 0.24821794033050537\n",
      "Epoch 8, Batch 4/32 : Loss = 0.4056360125541687\n",
      "Epoch 8, Batch 5/32 : Loss = 0.23330824077129364\n",
      "Epoch 8, Batch 6/32 : Loss = 0.20308451354503632\n",
      "Epoch 8, Batch 7/32 : Loss = 0.21288681030273438\n",
      "Epoch 8, Batch 8/32 : Loss = 0.3864418566226959\n",
      "Epoch 8, Batch 9/32 : Loss = 0.4809088110923767\n",
      "Epoch 8, Batch 10/32 : Loss = 0.19044779241085052\n",
      "Epoch 8, Batch 11/32 : Loss = 0.7094486951828003\n",
      "Epoch 8, Batch 12/32 : Loss = 0.7073676586151123\n",
      "Epoch 8, Batch 13/32 : Loss = 0.40965360403060913\n",
      "Epoch 8, Batch 14/32 : Loss = 0.13755062222480774\n",
      "Epoch 8, Batch 15/32 : Loss = 0.18323269486427307\n",
      "Epoch 8, Batch 16/32 : Loss = 0.20147651433944702\n",
      "Epoch 8, Batch 17/32 : Loss = 0.18700957298278809\n",
      "Epoch 8, Batch 18/32 : Loss = 0.357552707195282\n",
      "Epoch 8, Batch 19/32 : Loss = 0.3725707232952118\n",
      "Epoch 8, Batch 20/32 : Loss = 0.37417250871658325\n",
      "Epoch 8, Batch 21/32 : Loss = 0.36016911268234253\n",
      "Epoch 8, Batch 22/32 : Loss = 0.19601333141326904\n",
      "Epoch 8, Batch 23/32 : Loss = 0.13973768055438995\n",
      "Epoch 8, Batch 24/32 : Loss = 0.23165756464004517\n",
      "Epoch 8, Batch 25/32 : Loss = 0.2257309854030609\n",
      "Epoch 8, Batch 26/32 : Loss = 0.15957683324813843\n",
      "Epoch 8, Batch 27/32 : Loss = 0.14996890723705292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 28/32 : Loss = 0.1403288096189499\n",
      "Epoch 8, Batch 29/32 : Loss = 0.12423238903284073\n",
      "Epoch 8, Batch 30/32 : Loss = 0.1491430252790451\n",
      "Epoch 8, Batch 31/32 : Loss = 1.29090416431427\n",
      "Epoch 8 finished in 0.04023329416910807 minutes\n",
      "Epoch 8 training_loss = 0.27595049142837524\n",
      "f------66-----U------(---KK------S-----q--------!--55----;-- => f6U(KSq-!5;, Ground Truth is f6U(KSq-!5;\n",
      "n--------^---*---55---i-LL---++---3----#----Z----PP----0---- => n^*5iL+3#ZP0, Ground Truth is n^*5iL+3#ZPO\n",
      "Epoch 8 val_loss = 0.19491487741470337, word_accuracy = 0.8493975903614458\n",
      "Epoch 9, Batch 0/32 : Loss = 0.3144795894622803\n",
      "Epoch 9, Batch 1/32 : Loss = 0.14291512966156006\n",
      "Epoch 9, Batch 2/32 : Loss = 0.23911243677139282\n",
      "Epoch 9, Batch 3/32 : Loss = 0.12628278136253357\n",
      "Epoch 9, Batch 4/32 : Loss = 0.15839390456676483\n",
      "Epoch 9, Batch 5/32 : Loss = 0.10601982474327087\n",
      "Epoch 9, Batch 6/32 : Loss = 0.11433203518390656\n",
      "Epoch 9, Batch 7/32 : Loss = 0.15381571650505066\n",
      "Epoch 9, Batch 8/32 : Loss = 0.18851353228092194\n",
      "Epoch 9, Batch 9/32 : Loss = 0.3316357135772705\n",
      "Epoch 9, Batch 10/32 : Loss = 0.2780017852783203\n",
      "Epoch 9, Batch 11/32 : Loss = 0.3402574062347412\n",
      "Epoch 9, Batch 12/32 : Loss = 0.19255533814430237\n",
      "Epoch 9, Batch 13/32 : Loss = 0.5130632519721985\n",
      "Epoch 9, Batch 14/32 : Loss = 0.1540619432926178\n",
      "Epoch 9, Batch 15/32 : Loss = 0.12412448972463608\n",
      "Epoch 9, Batch 16/32 : Loss = 0.08584561944007874\n",
      "Epoch 9, Batch 17/32 : Loss = 0.133706733584404\n",
      "Epoch 9, Batch 18/32 : Loss = 0.22922579944133759\n",
      "Epoch 9, Batch 19/32 : Loss = 0.28600239753723145\n",
      "Epoch 9, Batch 20/32 : Loss = 0.10743964463472366\n",
      "Epoch 9, Batch 21/32 : Loss = 0.10122627019882202\n",
      "Epoch 9, Batch 22/32 : Loss = 0.15801741182804108\n",
      "Epoch 9, Batch 23/32 : Loss = 0.07762372493743896\n",
      "Epoch 9, Batch 24/32 : Loss = 0.23289445042610168\n",
      "Epoch 9, Batch 25/32 : Loss = 0.11757762730121613\n",
      "Epoch 9, Batch 26/32 : Loss = 0.24788710474967957\n",
      "Epoch 9, Batch 27/32 : Loss = 0.10539381206035614\n",
      "Epoch 9, Batch 28/32 : Loss = 0.2699061632156372\n",
      "Epoch 9, Batch 29/32 : Loss = 0.15723824501037598\n",
      "Epoch 9, Batch 30/32 : Loss = 0.1001979410648346\n",
      "Epoch 9, Batch 31/32 : Loss = 0.4917053282260895\n",
      "Epoch 9 finished in 0.03869995673497518 minutes\n",
      "Epoch 9 training_loss = 0.1911393404006958\n",
      "O-----------]---P-----n-----]---]--kk----f--;--77----XX---66----I-- => O]Pn]]kf;7X6I, Ground Truth is O]Pn]]kf;7X6I\n",
      "}-------zz---@------~---j--N-----FF---55----C----!--9----1---n----- => }z@~jNF5C!91n, Ground Truth is }z@~jNF5C!91n\n",
      "Epoch 9 val_loss = 0.14873725175857544, word_accuracy = 0.8714859437751004\n",
      "Epoch 10, Batch 0/32 : Loss = 0.25322800874710083\n",
      "Epoch 10, Batch 1/32 : Loss = 0.11410954594612122\n",
      "Epoch 10, Batch 2/32 : Loss = 0.09845200181007385\n",
      "Epoch 10, Batch 3/32 : Loss = 0.19680926203727722\n",
      "Epoch 10, Batch 4/32 : Loss = 0.06675797700881958\n",
      "Epoch 10, Batch 5/32 : Loss = 0.08931250125169754\n",
      "Epoch 10, Batch 6/32 : Loss = 0.1642216444015503\n",
      "Epoch 10, Batch 7/32 : Loss = 0.06238269433379173\n",
      "Epoch 10, Batch 8/32 : Loss = 0.09546162188053131\n",
      "Epoch 10, Batch 9/32 : Loss = 0.24517390131950378\n",
      "Epoch 10, Batch 10/32 : Loss = 0.08979268372058868\n",
      "Epoch 10, Batch 11/32 : Loss = 0.09729761630296707\n",
      "Epoch 10, Batch 12/32 : Loss = 0.12382529675960541\n",
      "Epoch 10, Batch 13/32 : Loss = 0.07017698138952255\n",
      "Epoch 10, Batch 14/32 : Loss = 0.07885529100894928\n",
      "Epoch 10, Batch 15/32 : Loss = 0.2976239323616028\n",
      "Epoch 10, Batch 16/32 : Loss = 0.1939905285835266\n",
      "Epoch 10, Batch 17/32 : Loss = 0.08881697058677673\n",
      "Epoch 10, Batch 18/32 : Loss = 0.07281738519668579\n",
      "Epoch 10, Batch 19/32 : Loss = 0.08952335268259048\n",
      "Epoch 10, Batch 20/32 : Loss = 0.11460088193416595\n",
      "Epoch 10, Batch 21/32 : Loss = 0.2651859223842621\n",
      "Epoch 10, Batch 22/32 : Loss = 0.11520587652921677\n",
      "Epoch 10, Batch 23/32 : Loss = 0.1569882333278656\n",
      "Epoch 10, Batch 24/32 : Loss = 0.16978071630001068\n",
      "Epoch 10, Batch 25/32 : Loss = 0.17787374556064606\n",
      "Epoch 10, Batch 26/32 : Loss = 0.20126019418239594\n",
      "Epoch 10, Batch 27/32 : Loss = 0.16641536355018616\n",
      "Epoch 10, Batch 28/32 : Loss = 0.06892499327659607\n",
      "Epoch 10, Batch 29/32 : Loss = 0.15559740364551544\n",
      "Epoch 10, Batch 30/32 : Loss = 0.11749821156263351\n",
      "Epoch 10, Batch 31/32 : Loss = 0.05666346102952957\n",
      "Epoch 10 finished in 0.04021073182423909 minutes\n",
      "Epoch 10 training_loss = 0.1383146345615387\n",
      "v-------%------MM------1--;---$--l-j--t--W------jj-- => v%M1;$ljtWj, Ground Truth is v%M1;$ljtWIj\n",
      "7------DD----~~--i--2---0----$---.-r--<<---JJ--LL--- => 7D~i20$.r<JL, Ground Truth is 7D~i20$.r<JL\n",
      "Epoch 10 val_loss = 0.11320576816797256, word_accuracy = 0.9096385542168675\n",
      "Epoch 11, Batch 0/32 : Loss = 0.062631756067276\n",
      "Epoch 11, Batch 1/32 : Loss = 0.07098748534917831\n",
      "Epoch 11, Batch 2/32 : Loss = 0.06730574369430542\n",
      "Epoch 11, Batch 3/32 : Loss = 0.15170346200466156\n",
      "Epoch 11, Batch 4/32 : Loss = 0.12755054235458374\n",
      "Epoch 11, Batch 5/32 : Loss = 0.1303679645061493\n",
      "Epoch 11, Batch 6/32 : Loss = 0.1808864176273346\n",
      "Epoch 11, Batch 7/32 : Loss = 0.17000481486320496\n",
      "Epoch 11, Batch 8/32 : Loss = 0.15042488276958466\n",
      "Epoch 11, Batch 9/32 : Loss = 0.10973377525806427\n",
      "Epoch 11, Batch 10/32 : Loss = 0.15775664150714874\n",
      "Epoch 11, Batch 11/32 : Loss = 0.06769837439060211\n",
      "Epoch 11, Batch 12/32 : Loss = 0.06423316895961761\n",
      "Epoch 11, Batch 13/32 : Loss = 0.12659889459609985\n",
      "Epoch 11, Batch 14/32 : Loss = 0.09603756666183472\n",
      "Epoch 11, Batch 15/32 : Loss = 0.31103965640068054\n",
      "Epoch 11, Batch 16/32 : Loss = 0.07595236599445343\n",
      "Epoch 11, Batch 17/32 : Loss = 0.04627447575330734\n",
      "Epoch 11, Batch 18/32 : Loss = 0.06374997645616531\n",
      "Epoch 11, Batch 19/32 : Loss = 0.05966842919588089\n",
      "Epoch 11, Batch 20/32 : Loss = 0.045180000364780426\n",
      "Epoch 11, Batch 21/32 : Loss = 0.052410371601581573\n",
      "Epoch 11, Batch 22/32 : Loss = 0.06896067410707474\n",
      "Epoch 11, Batch 23/32 : Loss = 0.07742328196763992\n",
      "Epoch 11, Batch 24/32 : Loss = 0.22559520602226257\n",
      "Epoch 11, Batch 25/32 : Loss = 0.13044726848602295\n",
      "Epoch 11, Batch 26/32 : Loss = 0.0815892219543457\n",
      "Epoch 11, Batch 27/32 : Loss = 0.10676021873950958\n",
      "Epoch 11, Batch 28/32 : Loss = 0.14686580002307892\n",
      "Epoch 11, Batch 29/32 : Loss = 0.06169750168919563\n",
      "Epoch 11, Batch 30/32 : Loss = 0.08943939954042435\n",
      "Epoch 11, Batch 31/32 : Loss = 0.03847804665565491\n",
      "Epoch 11 finished in 0.03899390697479248 minutes\n",
      "Epoch 11 training_loss = 0.10865174233913422\n",
      "G--------J---m------\\---F----2---(---r--8----*----=-- => GJm\\F2(r8*=, Ground Truth is GJm\\F2(r8*=\n",
      "v------v--!--DD---h----EE---c---G-----%------2---4--- => vv!DhEcG%24, Ground Truth is vv!DhEcG%24\n",
      "Epoch 11 val_loss = 0.09049489349126816, word_accuracy = 0.9236947791164659\n",
      "Epoch 12, Batch 0/32 : Loss = 0.0626247301697731\n",
      "Epoch 12, Batch 1/32 : Loss = 0.1322922706604004\n",
      "Epoch 12, Batch 2/32 : Loss = 0.08700548857450485\n",
      "Epoch 12, Batch 3/32 : Loss = 0.07695884257555008\n",
      "Epoch 12, Batch 4/32 : Loss = 0.09656435251235962\n",
      "Epoch 12, Batch 5/32 : Loss = 0.09335993230342865\n",
      "Epoch 12, Batch 6/32 : Loss = 0.09507578611373901\n",
      "Epoch 12, Batch 7/32 : Loss = 0.03511286526918411\n",
      "Epoch 12, Batch 8/32 : Loss = 0.039057210087776184\n",
      "Epoch 12, Batch 9/32 : Loss = 0.10145476460456848\n",
      "Epoch 12, Batch 10/32 : Loss = 0.04243491590023041\n",
      "Epoch 12, Batch 11/32 : Loss = 0.04945562779903412\n",
      "Epoch 12, Batch 12/32 : Loss = 0.034433599561452866\n",
      "Epoch 12, Batch 13/32 : Loss = 0.14035357534885406\n",
      "Epoch 12, Batch 14/32 : Loss = 0.07040415704250336\n",
      "Epoch 12, Batch 15/32 : Loss = 0.06629982590675354\n",
      "Epoch 12, Batch 16/32 : Loss = 0.0750967413187027\n",
      "Epoch 12, Batch 17/32 : Loss = 0.05447506532073021\n",
      "Epoch 12, Batch 18/32 : Loss = 0.04730406403541565\n",
      "Epoch 12, Batch 19/32 : Loss = 0.17864860594272614\n",
      "Epoch 12, Batch 20/32 : Loss = 0.1468157023191452\n",
      "Epoch 12, Batch 21/32 : Loss = 0.09697926789522171\n",
      "Epoch 12, Batch 22/32 : Loss = 0.15703117847442627\n",
      "Epoch 12, Batch 23/32 : Loss = 0.03419695422053337\n",
      "Epoch 12, Batch 24/32 : Loss = 0.09487924724817276\n",
      "Epoch 12, Batch 25/32 : Loss = 0.040660008788108826\n",
      "Epoch 12, Batch 26/32 : Loss = 0.062255967408418655\n",
      "Epoch 12, Batch 27/32 : Loss = 0.08940306305885315\n",
      "Epoch 12, Batch 28/32 : Loss = 0.18635806441307068\n",
      "Epoch 12, Batch 29/32 : Loss = 0.22496217489242554\n",
      "Epoch 12, Batch 30/32 : Loss = 0.03522877395153046\n",
      "Epoch 12, Batch 31/32 : Loss = 0.06362377852201462\n",
      "Epoch 12 finished in 0.04002918799718221 minutes\n",
      "Epoch 12 training_loss = 0.0885184034705162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p------#-----@-----c----V---0---!--o---\"---1---x---|--mm-------- => p#@cV0!o\"1x|m, Ground Truth is p#@cV0!o\"1x|m\n",
      "V--------4----m-----HH--------O----t--o--->>---W-----;--1--33--- => V4mH-Oto>W;13, Ground Truth is V4mH-Oto>W;13\n",
      "Epoch 12 val_loss = 0.07688577473163605, word_accuracy = 0.9096385542168675\n",
      "Epoch 13, Batch 0/32 : Loss = 0.056882381439208984\n",
      "Epoch 13, Batch 1/32 : Loss = 0.041528329253196716\n",
      "Epoch 13, Batch 2/32 : Loss = 0.3783855140209198\n",
      "Epoch 13, Batch 3/32 : Loss = 0.050487592816352844\n",
      "Epoch 13, Batch 4/32 : Loss = 0.0409623458981514\n",
      "Epoch 13, Batch 5/32 : Loss = 0.10144537687301636\n",
      "Epoch 13, Batch 6/32 : Loss = 0.05255229026079178\n",
      "Epoch 13, Batch 7/32 : Loss = 0.06614335626363754\n",
      "Epoch 13, Batch 8/32 : Loss = 0.12827885150909424\n",
      "Epoch 13, Batch 9/32 : Loss = 0.06496576964855194\n",
      "Epoch 13, Batch 10/32 : Loss = 0.0393630787730217\n",
      "Epoch 13, Batch 11/32 : Loss = 0.06200292706489563\n",
      "Epoch 13, Batch 12/32 : Loss = 0.044265661388635635\n",
      "Epoch 13, Batch 13/32 : Loss = 0.04611847549676895\n",
      "Epoch 13, Batch 14/32 : Loss = 0.11227096617221832\n",
      "Epoch 13, Batch 15/32 : Loss = 0.04594196751713753\n",
      "Epoch 13, Batch 16/32 : Loss = 0.08931414783000946\n",
      "Epoch 13, Batch 17/32 : Loss = 0.04358517378568649\n",
      "Epoch 13, Batch 18/32 : Loss = 0.050661928951740265\n",
      "Epoch 13, Batch 19/32 : Loss = 0.03455790504813194\n",
      "Epoch 13, Batch 20/32 : Loss = 0.04857750982046127\n",
      "Epoch 13, Batch 21/32 : Loss = 0.11940263211727142\n",
      "Epoch 13, Batch 22/32 : Loss = 0.04585009068250656\n",
      "Epoch 13, Batch 23/32 : Loss = 0.09040296077728271\n",
      "Epoch 13, Batch 24/32 : Loss = 0.13438349962234497\n",
      "Epoch 13, Batch 25/32 : Loss = 0.07835422456264496\n",
      "Epoch 13, Batch 26/32 : Loss = 0.09156377613544464\n",
      "Epoch 13, Batch 27/32 : Loss = 0.07997570186853409\n",
      "Epoch 13, Batch 28/32 : Loss = 0.17175288498401642\n",
      "Epoch 13, Batch 29/32 : Loss = 0.1476849913597107\n",
      "Epoch 13, Batch 30/32 : Loss = 0.08750885725021362\n",
      "Epoch 13, Batch 31/32 : Loss = 0.04102205112576485\n",
      "Epoch 13 finished in 0.039300270875295 minutes\n",
      "Epoch 13 training_loss = 0.08515015244483948\n",
      "T-------P----u---g----7---\\--n---'-?---k--^^- => TPug7\\n'?k^, Ground Truth is TPug7\\n'?k^\n",
      "S-------[---K---\"---d----N----33---99--y---`- => S[K\"dN39y`, Ground Truth is S[K\"dN39y`*\n",
      "Epoch 13 val_loss = 0.06498175114393234, word_accuracy = 0.9417670682730924\n",
      "Epoch 14, Batch 0/32 : Loss = 0.08752638101577759\n",
      "Epoch 14, Batch 1/32 : Loss = 0.06420155614614487\n",
      "Epoch 14, Batch 2/32 : Loss = 0.057258184999227524\n",
      "Epoch 14, Batch 3/32 : Loss = 0.034683823585510254\n",
      "Epoch 14, Batch 4/32 : Loss = 0.02871069684624672\n",
      "Epoch 14, Batch 5/32 : Loss = 0.07226991653442383\n",
      "Epoch 14, Batch 6/32 : Loss = 0.05766137316823006\n",
      "Epoch 14, Batch 7/32 : Loss = 0.06292217969894409\n",
      "Epoch 14, Batch 8/32 : Loss = 0.19392578303813934\n",
      "Epoch 14, Batch 9/32 : Loss = 0.17796245217323303\n",
      "Epoch 14, Batch 10/32 : Loss = 0.046782687306404114\n",
      "Epoch 14, Batch 11/32 : Loss = 0.14671920239925385\n",
      "Epoch 14, Batch 12/32 : Loss = 0.18273137509822845\n",
      "Epoch 14, Batch 13/32 : Loss = 0.028644666075706482\n",
      "Epoch 14, Batch 14/32 : Loss = 0.08877493441104889\n",
      "Epoch 14, Batch 15/32 : Loss = 0.11394304037094116\n",
      "Epoch 14, Batch 16/32 : Loss = 0.030796660110354424\n",
      "Epoch 14, Batch 17/32 : Loss = 0.0329199954867363\n",
      "Epoch 14, Batch 18/32 : Loss = 0.061811648309230804\n",
      "Epoch 14, Batch 19/32 : Loss = 0.049028050154447556\n",
      "Epoch 14, Batch 20/32 : Loss = 0.0418328158557415\n",
      "Epoch 14, Batch 21/32 : Loss = 0.030649980530142784\n",
      "Epoch 14, Batch 22/32 : Loss = 0.08997230231761932\n",
      "Epoch 14, Batch 23/32 : Loss = 0.05422220379114151\n",
      "Epoch 14, Batch 24/32 : Loss = 0.1053236797451973\n",
      "Epoch 14, Batch 25/32 : Loss = 0.04624953866004944\n",
      "Epoch 14, Batch 26/32 : Loss = 0.03968826308846474\n",
      "Epoch 14, Batch 27/32 : Loss = 0.045371685177087784\n",
      "Epoch 14, Batch 28/32 : Loss = 0.054830003529787064\n",
      "Epoch 14, Batch 29/32 : Loss = 0.1260528713464737\n",
      "Epoch 14, Batch 30/32 : Loss = 0.1565142720937729\n",
      "Epoch 14, Batch 31/32 : Loss = 0.5373311042785645\n",
      "Epoch 14 finished in 0.04215110143025716 minutes\n",
      "Epoch 14 training_loss = 0.07958710193634033\n",
      "(--------*---#----NN-----E-----=---;--1---(---u----33---#----<------ => (*#NE=;1(u3#<, Ground Truth is (*#NE=;1(u3#<\n",
      "0--------55---66---hh---__---u----~----<----OO-----U-----J---.-0---- => 056h_u~<OUJ.0, Ground Truth is 056h_u~<OUJ.0\n",
      "Epoch 14 val_loss = 0.0586698092520237, word_accuracy = 0.9377510040160643\n",
      "Epoch 15, Batch 0/32 : Loss = 0.036224156618118286\n",
      "Epoch 15, Batch 1/32 : Loss = 0.0673753172159195\n",
      "Epoch 15, Batch 2/32 : Loss = 0.05678863823413849\n",
      "Epoch 15, Batch 3/32 : Loss = 0.05494270101189613\n",
      "Epoch 15, Batch 4/32 : Loss = 0.031856585294008255\n",
      "Epoch 15, Batch 5/32 : Loss = 0.059327494353055954\n",
      "Epoch 15, Batch 6/32 : Loss = 0.03014559857547283\n",
      "Epoch 15, Batch 7/32 : Loss = 0.033764615654945374\n",
      "Epoch 15, Batch 8/32 : Loss = 0.034056149423122406\n",
      "Epoch 15, Batch 9/32 : Loss = 0.11024511605501175\n",
      "Epoch 15, Batch 10/32 : Loss = 0.028813211247324944\n",
      "Epoch 15, Batch 11/32 : Loss = 0.020484238862991333\n",
      "Epoch 15, Batch 12/32 : Loss = 0.06299765408039093\n",
      "Epoch 15, Batch 13/32 : Loss = 0.15829893946647644\n",
      "Epoch 15, Batch 14/32 : Loss = 0.05014459043741226\n",
      "Epoch 15, Batch 15/32 : Loss = 0.02338910475373268\n",
      "Epoch 15, Batch 16/32 : Loss = 0.3082727789878845\n",
      "Epoch 15, Batch 17/32 : Loss = 0.05430283397436142\n",
      "Epoch 15, Batch 18/32 : Loss = 0.03344465792179108\n",
      "Epoch 15, Batch 19/32 : Loss = 0.030178895220160484\n",
      "Epoch 15, Batch 20/32 : Loss = 0.12437443435192108\n",
      "Epoch 15, Batch 21/32 : Loss = 0.030520278960466385\n",
      "Epoch 15, Batch 22/32 : Loss = 0.10321767628192902\n",
      "Epoch 15, Batch 23/32 : Loss = 0.023250266909599304\n",
      "Epoch 15, Batch 24/32 : Loss = 0.021157018840312958\n",
      "Epoch 15, Batch 25/32 : Loss = 0.0613250806927681\n",
      "Epoch 15, Batch 26/32 : Loss = 0.06453754752874374\n",
      "Epoch 15, Batch 27/32 : Loss = 0.023763006553053856\n",
      "Epoch 15, Batch 28/32 : Loss = 0.08428440988063812\n",
      "Epoch 15, Batch 29/32 : Loss = 0.05713695287704468\n",
      "Epoch 15, Batch 30/32 : Loss = 0.027330346405506134\n",
      "Epoch 15, Batch 31/32 : Loss = 0.033835120499134064\n",
      "Epoch 15 finished in 0.03963292837142944 minutes\n",
      "Epoch 15 training_loss = 0.06137123331427574\n",
      "W----------L----7---rr---Y---f---a----n---:--M------;--l--T----- => WL7rYfan:M;lT, Ground Truth is WL7rYfan:M;lT\n",
      "d----------4----4----f---n----SS----z---v---[--{--ii--A----f---- => d44fnSzv[{iAf, Ground Truth is d44fnSzv[{iAf\n",
      "Epoch 15 val_loss = 0.04974176362156868, word_accuracy = 0.9497991967871486\n",
      "Epoch 16, Batch 0/32 : Loss = 0.023549340665340424\n",
      "Epoch 16, Batch 1/32 : Loss = 0.07574450969696045\n",
      "Epoch 16, Batch 2/32 : Loss = 0.07614609599113464\n",
      "Epoch 16, Batch 3/32 : Loss = 0.029330743476748466\n",
      "Epoch 16, Batch 4/32 : Loss = 0.039030201733112335\n",
      "Epoch 16, Batch 5/32 : Loss = 0.025417186319828033\n",
      "Epoch 16, Batch 6/32 : Loss = 0.021046554669737816\n",
      "Epoch 16, Batch 7/32 : Loss = 0.02674848958849907\n",
      "Epoch 16, Batch 8/32 : Loss = 0.024629678577184677\n",
      "Epoch 16, Batch 9/32 : Loss = 0.12597234547138214\n",
      "Epoch 16, Batch 10/32 : Loss = 0.02700287103652954\n",
      "Epoch 16, Batch 11/32 : Loss = 0.07215623557567596\n",
      "Epoch 16, Batch 12/32 : Loss = 0.08941803872585297\n",
      "Epoch 16, Batch 13/32 : Loss = 0.10995826870203018\n",
      "Epoch 16, Batch 14/32 : Loss = 0.028894467279314995\n",
      "Epoch 16, Batch 15/32 : Loss = 0.02509644627571106\n",
      "Epoch 16, Batch 16/32 : Loss = 0.01890604756772518\n",
      "Epoch 16, Batch 17/32 : Loss = 0.02242720127105713\n",
      "Epoch 16, Batch 18/32 : Loss = 0.021930329501628876\n",
      "Epoch 16, Batch 19/32 : Loss = 0.019937757402658463\n",
      "Epoch 16, Batch 20/32 : Loss = 0.215262308716774\n",
      "Epoch 16, Batch 21/32 : Loss = 0.025530997663736343\n",
      "Epoch 16, Batch 22/32 : Loss = 0.020846756175160408\n",
      "Epoch 16, Batch 23/32 : Loss = 0.04605089873075485\n",
      "Epoch 16, Batch 24/32 : Loss = 0.027790645137429237\n",
      "Epoch 16, Batch 25/32 : Loss = 0.10545863211154938\n",
      "Epoch 16, Batch 26/32 : Loss = 0.06650199741125107\n",
      "Epoch 16, Batch 27/32 : Loss = 0.14255867898464203\n",
      "Epoch 16, Batch 28/32 : Loss = 0.03234438598155975\n",
      "Epoch 16, Batch 29/32 : Loss = 0.08594836294651031\n",
      "Epoch 16, Batch 30/32 : Loss = 0.021591920405626297\n",
      "Epoch 16, Batch 31/32 : Loss = 0.4227220416069031\n",
      "Epoch 16 finished in 0.040078977743784584 minutes\n",
      "Epoch 16 training_loss = 0.05609859153628349\n",
      "^------Q-----U-----S----*---2---,-yy---c----qq---t--->--- => ^QUS*2,ycqt>, Ground Truth is ^QUS*2,yCqt>\n",
      "~------T---WW-----d---1---^---=--PP---d---0---}--+---\\--- => ~TWd1^=Pd0}+\\, Ground Truth is ~TWd1^=Pd0}+\\\n",
      "Epoch 16 val_loss = 0.04782276973128319, word_accuracy = 0.9397590361445783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 0/32 : Loss = 0.07801835238933563\n",
      "Epoch 17, Batch 1/32 : Loss = 0.0553760826587677\n",
      "Epoch 17, Batch 2/32 : Loss = 0.04319000616669655\n",
      "Epoch 17, Batch 3/32 : Loss = 0.17324572801589966\n",
      "Epoch 17, Batch 4/32 : Loss = 0.07396084070205688\n",
      "Epoch 17, Batch 5/32 : Loss = 0.022697385400533676\n",
      "Epoch 17, Batch 6/32 : Loss = 0.0342891626060009\n",
      "Epoch 17, Batch 7/32 : Loss = 0.03275661915540695\n",
      "Epoch 17, Batch 8/32 : Loss = 0.03814217820763588\n",
      "Epoch 17, Batch 9/32 : Loss = 0.022198237478733063\n",
      "Epoch 17, Batch 10/32 : Loss = 0.027904998511075974\n",
      "Epoch 17, Batch 11/32 : Loss = 0.01927957683801651\n",
      "Epoch 17, Batch 12/32 : Loss = 0.041376784443855286\n",
      "Epoch 17, Batch 13/32 : Loss = 0.03286003693938255\n",
      "Epoch 17, Batch 14/32 : Loss = 0.06817516684532166\n",
      "Epoch 17, Batch 15/32 : Loss = 0.10638922452926636\n",
      "Epoch 17, Batch 16/32 : Loss = 0.06890788674354553\n",
      "Epoch 17, Batch 17/32 : Loss = 0.034170281141996384\n",
      "Epoch 17, Batch 18/32 : Loss = 0.019268985837697983\n",
      "Epoch 17, Batch 19/32 : Loss = 0.023738278076052666\n",
      "Epoch 17, Batch 20/32 : Loss = 0.021440818905830383\n",
      "Epoch 17, Batch 21/32 : Loss = 0.04650164395570755\n",
      "Epoch 17, Batch 22/32 : Loss = 0.10530500113964081\n",
      "Epoch 17, Batch 23/32 : Loss = 0.015441304072737694\n",
      "Epoch 17, Batch 24/32 : Loss = 0.06891733407974243\n",
      "Epoch 17, Batch 25/32 : Loss = 0.3673422038555145\n",
      "Epoch 17, Batch 26/32 : Loss = 0.3030724823474884\n",
      "Epoch 17, Batch 27/32 : Loss = 0.01692967303097248\n",
      "Epoch 17, Batch 28/32 : Loss = 0.03529171645641327\n",
      "Epoch 17, Batch 29/32 : Loss = 0.0764736756682396\n",
      "Epoch 17, Batch 30/32 : Loss = 0.031153090298175812\n",
      "Epoch 17, Batch 31/32 : Loss = 0.021285049617290497\n",
      "Epoch 17 finished in 0.04161599079767863 minutes\n",
      "Epoch 17 training_loss = 0.06767792254686356\n",
      "r----u---_---&---c---g---]-8----@----I-`-T---VV----- => ru_&cg]8@I`TV, Ground Truth is ru_&cg]8@I`TV\n",
      "Q---------v--2----C---j--*--^---q----W-----`-gg---,- => Qv2Cj*^qW`g,, Ground Truth is Qv2Cj*^qW`g,\n",
      "Epoch 17 val_loss = 0.05968131124973297, word_accuracy = 0.9337349397590361\n",
      "Epoch 18, Batch 0/32 : Loss = 0.08490761369466782\n",
      "Epoch 18, Batch 1/32 : Loss = 0.12388180196285248\n",
      "Epoch 18, Batch 2/32 : Loss = 0.03175368160009384\n",
      "Epoch 18, Batch 3/32 : Loss = 0.02590528316795826\n",
      "Epoch 18, Batch 4/32 : Loss = 0.1181093379855156\n",
      "Epoch 18, Batch 5/32 : Loss = 0.034540995955467224\n",
      "Epoch 18, Batch 6/32 : Loss = 0.02667556330561638\n",
      "Epoch 18, Batch 7/32 : Loss = 0.030786490067839622\n",
      "Epoch 18, Batch 8/32 : Loss = 0.027789073064923286\n",
      "Epoch 18, Batch 9/32 : Loss = 0.21212391555309296\n",
      "Epoch 18, Batch 10/32 : Loss = 0.026717938482761383\n",
      "Epoch 18, Batch 11/32 : Loss = 0.028389165177941322\n",
      "Epoch 18, Batch 12/32 : Loss = 0.020310211926698685\n",
      "Epoch 18, Batch 13/32 : Loss = 0.025655632838606834\n",
      "Epoch 18, Batch 14/32 : Loss = 0.01935550570487976\n",
      "Epoch 18, Batch 15/32 : Loss = 0.033896952867507935\n",
      "Epoch 18, Batch 16/32 : Loss = 0.055133141577243805\n",
      "Epoch 18, Batch 17/32 : Loss = 0.04662931710481644\n",
      "Epoch 18, Batch 18/32 : Loss = 0.07306496798992157\n",
      "Epoch 18, Batch 19/32 : Loss = 0.0412873737514019\n",
      "Epoch 18, Batch 20/32 : Loss = 0.02063540741801262\n",
      "Epoch 18, Batch 21/32 : Loss = 0.07307744026184082\n",
      "Epoch 18, Batch 22/32 : Loss = 0.10013821721076965\n",
      "Epoch 18, Batch 23/32 : Loss = 0.022072356194257736\n",
      "Epoch 18, Batch 24/32 : Loss = 0.15591546893119812\n",
      "Epoch 18, Batch 25/32 : Loss = 0.11501634865999222\n",
      "Epoch 18, Batch 26/32 : Loss = 0.0197812020778656\n",
      "Epoch 18, Batch 27/32 : Loss = 0.15922662615776062\n",
      "Epoch 18, Batch 28/32 : Loss = 0.031946778297424316\n",
      "Epoch 18, Batch 29/32 : Loss = 0.028749341145157814\n",
      "Epoch 18, Batch 30/32 : Loss = 0.02667929045855999\n",
      "Epoch 18, Batch 31/32 : Loss = 0.011287234723567963\n",
      "Epoch 18 finished in 0.04051057895024617 minutes\n",
      "Epoch 18 training_loss = 0.059166695922613144\n",
      "^----TT----a---55----%-----)---/---V----X---R----#----- => ^Ta5%)/VXR#, Ground Truth is ^Ta5%)/VXR#\n",
      "}-----y---L--\"\"--|---F----*---F---mm----bb----/---4---- => }yL\"|F*Fmb/4, Ground Truth is }yL\"|F*Fmb/4\n",
      "Epoch 18 val_loss = 0.043943848460912704, word_accuracy = 0.9357429718875502\n",
      "Epoch 19, Batch 0/32 : Loss = 0.03486952185630798\n",
      "Epoch 19, Batch 1/32 : Loss = 0.029871512204408646\n",
      "Epoch 19, Batch 2/32 : Loss = 0.13083864748477936\n",
      "Epoch 19, Batch 3/32 : Loss = 0.019287344068288803\n",
      "Epoch 19, Batch 4/32 : Loss = 0.058800019323825836\n",
      "Epoch 19, Batch 5/32 : Loss = 0.019122784957289696\n",
      "Epoch 19, Batch 6/32 : Loss = 0.20675094425678253\n",
      "Epoch 19, Batch 7/32 : Loss = 0.0743597224354744\n",
      "Epoch 19, Batch 8/32 : Loss = 0.0540570393204689\n",
      "Epoch 19, Batch 9/32 : Loss = 0.09133302420377731\n",
      "Epoch 19, Batch 10/32 : Loss = 0.06475593149662018\n",
      "Epoch 19, Batch 11/32 : Loss = 0.01815197616815567\n",
      "Epoch 19, Batch 12/32 : Loss = 0.07599502056837082\n",
      "Epoch 19, Batch 13/32 : Loss = 0.024561194702982903\n",
      "Epoch 19, Batch 14/32 : Loss = 0.02466156706213951\n",
      "Epoch 19, Batch 15/32 : Loss = 0.03300795331597328\n",
      "Epoch 19, Batch 16/32 : Loss = 0.017010342329740524\n",
      "Epoch 19, Batch 17/32 : Loss = 0.10166071355342865\n",
      "Epoch 19, Batch 18/32 : Loss = 0.030941952019929886\n",
      "Epoch 19, Batch 19/32 : Loss = 0.03697449713945389\n",
      "Epoch 19, Batch 20/32 : Loss = 0.023916423320770264\n",
      "Epoch 19, Batch 21/32 : Loss = 0.1359500139951706\n",
      "Epoch 19, Batch 22/32 : Loss = 0.08994186669588089\n",
      "Epoch 19, Batch 23/32 : Loss = 0.023570451885461807\n",
      "Epoch 19, Batch 24/32 : Loss = 0.04231390357017517\n",
      "Epoch 19, Batch 25/32 : Loss = 0.10608118772506714\n",
      "Epoch 19, Batch 26/32 : Loss = 0.014297286048531532\n",
      "Epoch 19, Batch 27/32 : Loss = 0.038661159574985504\n",
      "Epoch 19, Batch 28/32 : Loss = 0.07854970544576645\n",
      "Epoch 19, Batch 29/32 : Loss = 0.027779292315244675\n",
      "Epoch 19, Batch 30/32 : Loss = 0.0171210840344429\n",
      "Epoch 19, Batch 31/32 : Loss = 0.5374305248260498\n",
      "Epoch 19 finished in 0.04022397994995117 minutes\n",
      "Epoch 19 training_loss = 0.05822885036468506\n",
      "#------#----/---|---D----I-6----H----55---h----S---kk---- => ##/|DI6H5hSk, Ground Truth is ##/|DI6H5hSk\n",
      "~------T---WW-----d---1---^--==--PP---d---0---}--+---\\--- => ~TWd1^=Pd0}+\\, Ground Truth is ~TWd1^=Pd0}+\\\n",
      "Epoch 19 val_loss = 0.044323209673166275, word_accuracy = 0.9658634538152611\n",
      "Epoch 20, Batch 0/32 : Loss = 0.023054828867316246\n",
      "Epoch 20, Batch 1/32 : Loss = 0.07937745004892349\n",
      "Epoch 20, Batch 2/32 : Loss = 0.05790766328573227\n",
      "Epoch 20, Batch 3/32 : Loss = 0.025452911853790283\n",
      "Epoch 20, Batch 4/32 : Loss = 0.017221789807081223\n",
      "Epoch 20, Batch 5/32 : Loss = 0.0267851073294878\n",
      "Epoch 20, Batch 6/32 : Loss = 0.016403701156377792\n",
      "Epoch 20, Batch 7/32 : Loss = 0.037203628569841385\n",
      "Epoch 20, Batch 8/32 : Loss = 0.020098572596907616\n",
      "Epoch 20, Batch 9/32 : Loss = 0.03032572753727436\n",
      "Epoch 20, Batch 10/32 : Loss = 0.024680985137820244\n",
      "Epoch 20, Batch 11/32 : Loss = 0.023460298776626587\n",
      "Epoch 20, Batch 12/32 : Loss = 0.026778465136885643\n",
      "Epoch 20, Batch 13/32 : Loss = 0.02018386498093605\n",
      "Epoch 20, Batch 14/32 : Loss = 0.04570538550615311\n",
      "Epoch 20, Batch 15/32 : Loss = 0.11641337722539902\n",
      "Epoch 20, Batch 16/32 : Loss = 0.017314430326223373\n",
      "Epoch 20, Batch 17/32 : Loss = 0.019869104027748108\n",
      "Epoch 20, Batch 18/32 : Loss = 0.16006311774253845\n",
      "Epoch 20, Batch 19/32 : Loss = 0.014351222664117813\n",
      "Epoch 20, Batch 20/32 : Loss = 0.06742437183856964\n",
      "Epoch 20, Batch 21/32 : Loss = 0.014224529266357422\n",
      "Epoch 20, Batch 22/32 : Loss = 0.018245449289679527\n",
      "Epoch 20, Batch 23/32 : Loss = 0.015840649604797363\n",
      "Epoch 20, Batch 24/32 : Loss = 0.04945911467075348\n",
      "Epoch 20, Batch 25/32 : Loss = 0.06586748361587524\n",
      "Epoch 20, Batch 26/32 : Loss = 0.08397248387336731\n",
      "Epoch 20, Batch 27/32 : Loss = 0.10431528836488724\n",
      "Epoch 20, Batch 28/32 : Loss = 0.04917588829994202\n",
      "Epoch 20, Batch 29/32 : Loss = 0.03396270051598549\n",
      "Epoch 20, Batch 30/32 : Loss = 0.12903161346912384\n",
      "Epoch 20, Batch 31/32 : Loss = 0.02176756039261818\n",
      "Epoch 20 finished in 0.041005031267801924 minutes\n",
      "Epoch 20 training_loss = 0.046165209263563156\n",
      "y--------3---yy---s--^---W------->---)---G-----9--!!-R----- => y3ys^W>)G9!R, Ground Truth is y3ys^W>)G9!R\n",
      "%---------ee---=----9---\"---#----G---''-l--a---d-----@----- => %e=9\"#G'lad@, Ground Truth is %e=9\"#G'lad@\n",
      "Epoch 20 val_loss = 0.03714305907487869, word_accuracy = 0.9497991967871486\n",
      "Epoch 21, Batch 0/32 : Loss = 0.013026249594986439\n",
      "Epoch 21, Batch 1/32 : Loss = 0.04003859683871269\n",
      "Epoch 21, Batch 2/32 : Loss = 0.021978028118610382\n",
      "Epoch 21, Batch 3/32 : Loss = 0.042908161878585815\n",
      "Epoch 21, Batch 4/32 : Loss = 0.04328698664903641\n",
      "Epoch 21, Batch 5/32 : Loss = 0.0391797311604023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 6/32 : Loss = 0.07969698309898376\n",
      "Epoch 21, Batch 7/32 : Loss = 0.0486239455640316\n",
      "Epoch 21, Batch 8/32 : Loss = 0.1418711543083191\n",
      "Epoch 21, Batch 9/32 : Loss = 0.11123774945735931\n",
      "Epoch 21, Batch 10/32 : Loss = 0.0728241503238678\n",
      "Epoch 21, Batch 11/32 : Loss = 0.015328049659729004\n",
      "Epoch 21, Batch 12/32 : Loss = 0.09813929349184036\n",
      "Epoch 21, Batch 13/32 : Loss = 0.013170985504984856\n",
      "Epoch 21, Batch 14/32 : Loss = 0.02388206496834755\n",
      "Epoch 21, Batch 15/32 : Loss = 0.03685669228434563\n",
      "Epoch 21, Batch 16/32 : Loss = 0.035265788435935974\n",
      "Epoch 21, Batch 17/32 : Loss = 0.014539666473865509\n",
      "Epoch 21, Batch 18/32 : Loss = 0.014411303214728832\n",
      "Epoch 21, Batch 19/32 : Loss = 0.014497226104140282\n",
      "Epoch 21, Batch 20/32 : Loss = 0.11791840195655823\n",
      "Epoch 21, Batch 21/32 : Loss = 0.057754456996917725\n",
      "Epoch 21, Batch 22/32 : Loss = 0.024964675307273865\n",
      "Epoch 21, Batch 23/32 : Loss = 0.01750950887799263\n",
      "Epoch 21, Batch 24/32 : Loss = 0.0629376694560051\n",
      "Epoch 21, Batch 25/32 : Loss = 0.06232853978872299\n",
      "Epoch 21, Batch 26/32 : Loss = 0.01674279384315014\n",
      "Epoch 21, Batch 27/32 : Loss = 0.014913090504705906\n",
      "Epoch 21, Batch 28/32 : Loss = 0.013405710458755493\n",
      "Epoch 21, Batch 29/32 : Loss = 0.017444102093577385\n",
      "Epoch 21, Batch 30/32 : Loss = 0.012386666610836983\n",
      "Epoch 21, Batch 31/32 : Loss = 0.012754421681165695\n",
      "Epoch 21 finished in 0.04093501567840576 minutes\n",
      "Epoch 21 training_loss = 0.043073493987321854\n",
      "^-----}--``--]---2----F----99----D-----e---hh----)--f--- => ^}`]2F9Deh)f, Ground Truth is ^}`]2F9Deh)f\n",
      "<---------C----!--v---:-u---'--=----X---~~---2----J---:- => <C!v:u'=X~2J:, Ground Truth is <C!v:u'=X~2J:\n",
      "Epoch 21 val_loss = 0.04049554467201233, word_accuracy = 0.9578313253012049\n",
      "Epoch 22, Batch 0/32 : Loss = 0.023706868290901184\n",
      "Epoch 22, Batch 1/32 : Loss = 0.021080706268548965\n",
      "Epoch 22, Batch 2/32 : Loss = 0.04820195958018303\n",
      "Epoch 22, Batch 3/32 : Loss = 0.043381258845329285\n",
      "Epoch 22, Batch 4/32 : Loss = 0.031888727098703384\n",
      "Epoch 22, Batch 5/32 : Loss = 0.01217703241854906\n",
      "Epoch 22, Batch 6/32 : Loss = 0.03520379960536957\n",
      "Epoch 22, Batch 7/32 : Loss = 0.04823800176382065\n",
      "Epoch 22, Batch 8/32 : Loss = 0.19046109914779663\n",
      "Epoch 22, Batch 9/32 : Loss = 0.04792754352092743\n",
      "Epoch 22, Batch 10/32 : Loss = 0.021379997953772545\n",
      "Epoch 22, Batch 11/32 : Loss = 0.012998334132134914\n",
      "Epoch 22, Batch 12/32 : Loss = 0.015130758285522461\n",
      "Epoch 22, Batch 13/32 : Loss = 0.010454416275024414\n",
      "Epoch 22, Batch 14/32 : Loss = 0.014076009392738342\n",
      "Epoch 22, Batch 15/32 : Loss = 0.010704094544053078\n",
      "Epoch 22, Batch 16/32 : Loss = 0.03572611138224602\n",
      "Epoch 22, Batch 17/32 : Loss = 0.049598757177591324\n",
      "Epoch 22, Batch 18/32 : Loss = 0.07675100117921829\n",
      "Epoch 22, Batch 19/32 : Loss = 0.010522948578000069\n",
      "Epoch 22, Batch 20/32 : Loss = 0.0314149409532547\n",
      "Epoch 22, Batch 21/32 : Loss = 0.03389982879161835\n",
      "Epoch 22, Batch 22/32 : Loss = 0.014532586559653282\n",
      "Epoch 22, Batch 23/32 : Loss = 0.009618144482374191\n",
      "Epoch 22, Batch 24/32 : Loss = 0.041801098734140396\n",
      "Epoch 22, Batch 25/32 : Loss = 0.07147872447967529\n",
      "Epoch 22, Batch 26/32 : Loss = 0.09342196583747864\n",
      "Epoch 22, Batch 27/32 : Loss = 0.011752349324524403\n",
      "Epoch 22, Batch 28/32 : Loss = 0.013550633564591408\n",
      "Epoch 22, Batch 29/32 : Loss = 0.10849495232105255\n",
      "Epoch 22, Batch 30/32 : Loss = 0.016455180943012238\n",
      "Epoch 22, Batch 31/32 : Loss = 0.014222675934433937\n",
      "Epoch 22 finished in 0.04060108661651611 minutes\n",
      "Epoch 22 training_loss = 0.03880506753921509\n",
      "i-------]----99----66-----p-------B------I---m----------?-----s----]----- => i]96pBIm?s], Ground Truth is i]96pBIm?s]\n",
      "e--------**---%------0----Z----@@-----R----N-----(---U----gg----V---<<--- => e*%0Z@RN(UgV<, Ground Truth is e*%0Z@RN(UgV<\n",
      "Epoch 22 val_loss = 0.03362961485981941, word_accuracy = 0.9618473895582329\n",
      "Epoch 23, Batch 0/32 : Loss = 0.04229077324271202\n",
      "Epoch 23, Batch 1/32 : Loss = 0.011902490630745888\n",
      "Epoch 23, Batch 2/32 : Loss = 0.011080930940806866\n",
      "Epoch 23, Batch 3/32 : Loss = 0.025690529495477676\n",
      "Epoch 23, Batch 4/32 : Loss = 0.07915414869785309\n",
      "Epoch 23, Batch 5/32 : Loss = 0.13479287922382355\n",
      "Epoch 23, Batch 6/32 : Loss = 0.015565515495836735\n",
      "Epoch 23, Batch 7/32 : Loss = 0.032043151557445526\n",
      "Epoch 23, Batch 8/32 : Loss = 0.04892939329147339\n",
      "Epoch 23, Batch 9/32 : Loss = 0.1256750226020813\n",
      "Epoch 23, Batch 10/32 : Loss = 0.0982903391122818\n",
      "Epoch 23, Batch 11/32 : Loss = 0.00988752767443657\n",
      "Epoch 23, Batch 12/32 : Loss = 0.02197101339697838\n",
      "Epoch 23, Batch 13/32 : Loss = 0.01589977741241455\n",
      "Epoch 23, Batch 14/32 : Loss = 0.0698515996336937\n",
      "Epoch 23, Batch 15/32 : Loss = 0.00815156102180481\n",
      "Epoch 23, Batch 16/32 : Loss = 0.014394807629287243\n",
      "Epoch 23, Batch 17/32 : Loss = 0.011773476377129555\n",
      "Epoch 23, Batch 18/32 : Loss = 0.012002406641840935\n",
      "Epoch 23, Batch 19/32 : Loss = 0.032345373183488846\n",
      "Epoch 23, Batch 20/32 : Loss = 0.030453158542513847\n",
      "Epoch 23, Batch 21/32 : Loss = 0.013297799974679947\n",
      "Epoch 23, Batch 22/32 : Loss = 0.020030438899993896\n",
      "Epoch 23, Batch 23/32 : Loss = 0.03223097324371338\n",
      "Epoch 23, Batch 24/32 : Loss = 0.01209934800863266\n",
      "Epoch 23, Batch 25/32 : Loss = 0.014910433441400528\n",
      "Epoch 23, Batch 26/32 : Loss = 0.03060595877468586\n",
      "Epoch 23, Batch 27/32 : Loss = 0.035241108387708664\n",
      "Epoch 23, Batch 28/32 : Loss = 0.023458868265151978\n",
      "Epoch 23, Batch 29/32 : Loss = 0.023577529937028885\n",
      "Epoch 23, Batch 30/32 : Loss = 0.061718061566352844\n",
      "Epoch 23, Batch 31/32 : Loss = 0.04246785119175911\n",
      "Epoch 23 finished in 0.040138852596282956 minutes\n",
      "Epoch 23 training_loss = 0.03613252565264702\n",
      "%--------e----=---99--\"\"--#----G---'-l--a---d-----@------ => %e=9\"#G'lad@, Ground Truth is %e=9\"#G'lad@\n",
      "_------00---{{---V---__---z---m-------s----S----.-0------ => _0{V_zmsS.0, Ground Truth is _0{V_zmsS.0\n",
      "Epoch 23 val_loss = 0.030740395188331604, word_accuracy = 0.9578313253012049\n",
      "Epoch 24, Batch 0/32 : Loss = 0.010166788473725319\n",
      "Epoch 24, Batch 1/32 : Loss = 0.05674700438976288\n",
      "Epoch 24, Batch 2/32 : Loss = 0.01537280436605215\n",
      "Epoch 24, Batch 3/32 : Loss = 0.02014261856675148\n",
      "Epoch 24, Batch 4/32 : Loss = 0.011779947206377983\n",
      "Epoch 24, Batch 5/32 : Loss = 0.0982099324464798\n",
      "Epoch 24, Batch 6/32 : Loss = 0.02159183658659458\n",
      "Epoch 24, Batch 7/32 : Loss = 0.019292118027806282\n",
      "Epoch 24, Batch 8/32 : Loss = 0.028624411672353745\n",
      "Epoch 24, Batch 9/32 : Loss = 0.010532735846936703\n",
      "Epoch 24, Batch 10/32 : Loss = 0.05026598647236824\n",
      "Epoch 24, Batch 11/32 : Loss = 0.02147899940609932\n",
      "Epoch 24, Batch 12/32 : Loss = 0.023519787937402725\n",
      "Epoch 24, Batch 13/32 : Loss = 0.04764755070209503\n",
      "Epoch 24, Batch 14/32 : Loss = 0.012344248592853546\n",
      "Epoch 24, Batch 15/32 : Loss = 0.01224137656390667\n",
      "Epoch 24, Batch 16/32 : Loss = 0.008391251787543297\n",
      "Epoch 24, Batch 17/32 : Loss = 0.049493998289108276\n",
      "Epoch 24, Batch 18/32 : Loss = 0.016088513657450676\n",
      "Epoch 24, Batch 19/32 : Loss = 0.09862558543682098\n",
      "Epoch 24, Batch 20/32 : Loss = 0.05913042277097702\n",
      "Epoch 24, Batch 21/32 : Loss = 0.009654650464653969\n",
      "Epoch 24, Batch 22/32 : Loss = 0.049032993614673615\n",
      "Epoch 24, Batch 23/32 : Loss = 0.04007626697421074\n",
      "Epoch 24, Batch 24/32 : Loss = 0.01671045832335949\n",
      "Epoch 24, Batch 25/32 : Loss = 0.0070202965289354324\n",
      "Epoch 24, Batch 26/32 : Loss = 0.016789592802524567\n",
      "Epoch 24, Batch 27/32 : Loss = 0.013460885733366013\n",
      "Epoch 24, Batch 28/32 : Loss = 0.01913716457784176\n",
      "Epoch 24, Batch 29/32 : Loss = 0.08479539304971695\n",
      "Epoch 24, Batch 30/32 : Loss = 0.028556250035762787\n",
      "Epoch 24, Batch 31/32 : Loss = 0.13126476109027863\n",
      "Epoch 24 finished in 0.03973666429519653 minutes\n",
      "Epoch 24 training_loss = 0.03191421180963516\n",
      ")-----7---xx--E----y---T---J---b----$---7----!-i--- => )7xEyTJb$7!i, Ground Truth is )7xEyTJb$7l,i\n",
      "v-------%%-----MM------1--;--$--lljj-t--W-----I-j-- => v%M1;$ljtWIj, Ground Truth is v%M1;$ljtWIj\n",
      "Epoch 24 val_loss = 0.04017581045627594, word_accuracy = 0.9417670682730924\n",
      "Epoch 25, Batch 0/32 : Loss = 0.03508889675140381\n",
      "Epoch 25, Batch 1/32 : Loss = 0.018583841621875763\n",
      "Epoch 25, Batch 2/32 : Loss = 0.03885616362094879\n",
      "Epoch 25, Batch 3/32 : Loss = 0.022297687828540802\n",
      "Epoch 25, Batch 4/32 : Loss = 0.01168467290699482\n",
      "Epoch 25, Batch 5/32 : Loss = 0.1521618515253067\n",
      "Epoch 25, Batch 6/32 : Loss = 0.1038779765367508\n",
      "Epoch 25, Batch 7/32 : Loss = 0.00920710526406765\n",
      "Epoch 25, Batch 8/32 : Loss = 0.051350437104701996\n",
      "Epoch 25, Batch 9/32 : Loss = 0.02835404872894287\n",
      "Epoch 25, Batch 10/32 : Loss = 0.009269901551306248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 11/32 : Loss = 0.016219746321439743\n",
      "Epoch 25, Batch 12/32 : Loss = 0.02023009955883026\n",
      "Epoch 25, Batch 13/32 : Loss = 0.04325266554951668\n",
      "Epoch 25, Batch 14/32 : Loss = 0.03134904056787491\n",
      "Epoch 25, Batch 15/32 : Loss = 0.012088526040315628\n",
      "Epoch 25, Batch 16/32 : Loss = 0.016588270664215088\n",
      "Epoch 25, Batch 17/32 : Loss = 0.012637777253985405\n",
      "Epoch 25, Batch 18/32 : Loss = 0.029850389808416367\n",
      "Epoch 25, Batch 19/32 : Loss = 0.009953180328011513\n",
      "Epoch 25, Batch 20/32 : Loss = 0.08688323199748993\n",
      "Epoch 25, Batch 21/32 : Loss = 0.042297229170799255\n",
      "Epoch 25, Batch 22/32 : Loss = 0.01097826473414898\n",
      "Epoch 25, Batch 23/32 : Loss = 0.009291746653616428\n",
      "Epoch 25, Batch 24/32 : Loss = 0.011639556847512722\n",
      "Epoch 25, Batch 25/32 : Loss = 0.0071739256381988525\n",
      "Epoch 25, Batch 26/32 : Loss = 0.012655029073357582\n",
      "Epoch 25, Batch 27/32 : Loss = 0.06133433058857918\n",
      "Epoch 25, Batch 28/32 : Loss = 0.02022751234471798\n",
      "Epoch 25, Batch 29/32 : Loss = 0.022443946450948715\n",
      "Epoch 25, Batch 30/32 : Loss = 0.026438534259796143\n",
      "Epoch 25, Batch 31/32 : Loss = 0.01237858273088932\n",
      "Epoch 25 finished in 0.03957704703013102 minutes\n",
      "Epoch 25 training_loss = 0.031672701239585876\n",
      "m-----------W------6---i--a----7---f--=----y---\\--g----MM------ => mW6ia7f=y\\gM, Ground Truth is mW6ia7f=y\\gM\n",
      "$-------$---g-----A---h---/---44--6----k---H----pp---0---==---- => $$gAh/46kHp0=, Ground Truth is $$gAh/46kHp0=\n",
      "Epoch 25 val_loss = 0.03163067623972893, word_accuracy = 0.9678714859437751\n",
      "Epoch 26, Batch 0/32 : Loss = 0.032614387571811676\n",
      "Epoch 26, Batch 1/32 : Loss = 0.03216695040464401\n",
      "Epoch 26, Batch 2/32 : Loss = 0.009654425084590912\n",
      "Epoch 26, Batch 3/32 : Loss = 0.11549343168735504\n",
      "Epoch 26, Batch 4/32 : Loss = 0.009275440126657486\n",
      "Epoch 26, Batch 5/32 : Loss = 0.008438605815172195\n",
      "Epoch 26, Batch 6/32 : Loss = 0.15552136301994324\n",
      "Epoch 26, Batch 7/32 : Loss = 0.06318283081054688\n",
      "Epoch 26, Batch 8/32 : Loss = 0.01312300655990839\n",
      "Epoch 26, Batch 9/32 : Loss = 0.011468833312392235\n",
      "Epoch 26, Batch 10/32 : Loss = 0.051271889358758926\n",
      "Epoch 26, Batch 11/32 : Loss = 0.03451361879706383\n",
      "Epoch 26, Batch 12/32 : Loss = 0.02251351997256279\n",
      "Epoch 26, Batch 13/32 : Loss = 0.011723292991518974\n",
      "Epoch 26, Batch 14/32 : Loss = 0.00611548637971282\n",
      "Epoch 26, Batch 15/32 : Loss = 0.008252529427409172\n",
      "Epoch 26, Batch 16/32 : Loss = 0.01357429102063179\n",
      "Epoch 26, Batch 17/32 : Loss = 0.015058755874633789\n",
      "Epoch 26, Batch 18/32 : Loss = 0.00889111403375864\n",
      "Epoch 26, Batch 19/32 : Loss = 0.009509589523077011\n",
      "Epoch 26, Batch 20/32 : Loss = 0.009430859237909317\n",
      "Epoch 26, Batch 21/32 : Loss = 0.008750959299504757\n",
      "Epoch 26, Batch 22/32 : Loss = 0.020757505670189857\n",
      "Epoch 26, Batch 23/32 : Loss = 0.011949937790632248\n",
      "Epoch 26, Batch 24/32 : Loss = 0.0428878515958786\n",
      "Epoch 26, Batch 25/32 : Loss = 0.03469902276992798\n",
      "Epoch 26, Batch 26/32 : Loss = 0.029292570427060127\n",
      "Epoch 26, Batch 27/32 : Loss = 0.018033906817436218\n",
      "Epoch 26, Batch 28/32 : Loss = 0.03311140462756157\n",
      "Epoch 26, Batch 29/32 : Loss = 0.020726308226585388\n",
      "Epoch 26, Batch 30/32 : Loss = 0.2558131515979767\n",
      "Epoch 26, Batch 31/32 : Loss = 0.18255092203617096\n",
      "Epoch 26 finished in 0.041507073243459064 minutes\n",
      "Epoch 26 training_loss = 0.03664692863821983\n",
      "p-------R---I--N----&&---H----.-m-----~---2---e---I--- => pRIN&H.m~2eI, Ground Truth is pRIN&H.m~2eI\n",
      "P---------7---]---}--R----&----z---@----r--f--~---?--- => P7]}R&z@rf~?, Ground Truth is P7]I}R&z@rf~?\n",
      "Epoch 26 val_loss = 0.023975852876901627, word_accuracy = 0.9698795180722891\n",
      "Epoch 27, Batch 0/32 : Loss = 0.00926157459616661\n",
      "Epoch 27, Batch 1/32 : Loss = 0.054012976586818695\n",
      "Epoch 27, Batch 2/32 : Loss = 0.010167762637138367\n",
      "Epoch 27, Batch 3/32 : Loss = 0.0631752759218216\n",
      "Epoch 27, Batch 4/32 : Loss = 0.04639841988682747\n",
      "Epoch 27, Batch 5/32 : Loss = 0.07569395750761032\n",
      "Epoch 27, Batch 6/32 : Loss = 0.01999039202928543\n",
      "Epoch 27, Batch 7/32 : Loss = 0.014698883518576622\n",
      "Epoch 27, Batch 8/32 : Loss = 0.0190038550645113\n",
      "Epoch 27, Batch 9/32 : Loss = 0.027052124962210655\n",
      "Epoch 27, Batch 10/32 : Loss = 0.010082053020596504\n",
      "Epoch 27, Batch 11/32 : Loss = 0.051902420818805695\n",
      "Epoch 27, Batch 12/32 : Loss = 0.0074904318898916245\n",
      "Epoch 27, Batch 13/32 : Loss = 0.0833226814866066\n",
      "Epoch 27, Batch 14/32 : Loss = 0.05087324604392052\n",
      "Epoch 27, Batch 15/32 : Loss = 0.010219050571322441\n",
      "Epoch 27, Batch 16/32 : Loss = 0.00976287480443716\n",
      "Epoch 27, Batch 17/32 : Loss = 0.008754553273320198\n",
      "Epoch 27, Batch 18/32 : Loss = 0.049108680337667465\n",
      "Epoch 27, Batch 19/32 : Loss = 0.012542972341179848\n",
      "Epoch 27, Batch 20/32 : Loss = 0.03286159038543701\n",
      "Epoch 27, Batch 21/32 : Loss = 0.0147879458963871\n",
      "Epoch 27, Batch 22/32 : Loss = 0.014667926356196404\n",
      "Epoch 27, Batch 23/32 : Loss = 0.045262280851602554\n",
      "Epoch 27, Batch 24/32 : Loss = 0.06415299326181412\n",
      "Epoch 27, Batch 25/32 : Loss = 0.038551945239305496\n",
      "Epoch 27, Batch 26/32 : Loss = 0.006169795524328947\n",
      "Epoch 27, Batch 27/32 : Loss = 0.019683076068758965\n",
      "Epoch 27, Batch 28/32 : Loss = 0.011193308047950268\n",
      "Epoch 27, Batch 29/32 : Loss = 0.06070901080965996\n",
      "Epoch 27, Batch 30/32 : Loss = 0.11350895464420319\n",
      "Epoch 27, Batch 31/32 : Loss = 0.013836207799613476\n",
      "Epoch 27 finished in 0.040490579605102536 minutes\n",
      "Epoch 27 training_loss = 0.03395317122340202\n",
      "X-------+---\"--8----ww---MM-----R---#----p----C---L---i-o---- => X+\"8wMR#pCLio, Ground Truth is X+\"8wMR#pCLio\n",
      "?-----*---QQ----<---33---MM---->----c---W------D---00---F---- => ?*Q<3M>cWD0F, Ground Truth is ?*Q<3M>cWD0F\n",
      "Epoch 27 val_loss = 0.021194512024521828, word_accuracy = 0.9738955823293173\n",
      "Epoch 28, Batch 0/32 : Loss = 0.009284645318984985\n",
      "Epoch 28, Batch 1/32 : Loss = 0.027197061106562614\n",
      "Epoch 28, Batch 2/32 : Loss = 0.009348124265670776\n",
      "Epoch 28, Batch 3/32 : Loss = 0.02158980444073677\n",
      "Epoch 28, Batch 4/32 : Loss = 0.011829407885670662\n",
      "Epoch 28, Batch 5/32 : Loss = 0.02731148712337017\n",
      "Epoch 28, Batch 6/32 : Loss = 0.010083641856908798\n",
      "Epoch 28, Batch 7/32 : Loss = 0.039469700306653976\n",
      "Epoch 28, Batch 8/32 : Loss = 0.0073007275350391865\n",
      "Epoch 28, Batch 9/32 : Loss = 0.04904334247112274\n",
      "Epoch 28, Batch 10/32 : Loss = 0.013384784571826458\n",
      "Epoch 28, Batch 11/32 : Loss = 0.07556124776601791\n",
      "Epoch 28, Batch 12/32 : Loss = 0.009778291918337345\n",
      "Epoch 28, Batch 13/32 : Loss = 0.0073808771558105946\n",
      "Epoch 28, Batch 14/32 : Loss = 0.017568636685609818\n",
      "Epoch 28, Batch 15/32 : Loss = 0.0781731829047203\n",
      "Epoch 28, Batch 16/32 : Loss = 0.01233559288084507\n",
      "Epoch 28, Batch 17/32 : Loss = 0.02944653108716011\n",
      "Epoch 28, Batch 18/32 : Loss = 0.01656290888786316\n",
      "Epoch 28, Batch 19/32 : Loss = 0.008834166452288628\n",
      "Epoch 28, Batch 20/32 : Loss = 0.0474906861782074\n",
      "Epoch 28, Batch 21/32 : Loss = 0.017202312126755714\n",
      "Epoch 28, Batch 22/32 : Loss = 0.036031968891620636\n",
      "Epoch 28, Batch 23/32 : Loss = 0.017886560410261154\n",
      "Epoch 28, Batch 24/32 : Loss = 0.011353923939168453\n",
      "Epoch 28, Batch 25/32 : Loss = 0.034599266946315765\n",
      "Epoch 28, Batch 26/32 : Loss = 0.022344741970300674\n",
      "Epoch 28, Batch 27/32 : Loss = 0.008905489929020405\n",
      "Epoch 28, Batch 28/32 : Loss = 0.03314601257443428\n",
      "Epoch 28, Batch 29/32 : Loss = 0.01040148176252842\n",
      "Epoch 28, Batch 30/32 : Loss = 0.013706700876355171\n",
      "Epoch 28, Batch 31/32 : Loss = 0.058240316808223724\n",
      "Epoch 28 finished in 0.039078160127003984 minutes\n",
      "Epoch 28 training_loss = 0.023834003135561943\n",
      "a-------9---v--#----c---s----G----|--BB----3---]---- => a9v#csG|B3], Ground Truth is a9v#csG|B3]\n",
      "Y------hh--hh--33--,--??--3--,-'-g---6----O----g---- => Yhh3,?3,'g6Og, Ground Truth is Yhh3,?3,'g6Og\n",
      "Epoch 28 val_loss = 0.02725374884903431, word_accuracy = 0.9558232931726908\n",
      "Epoch 29, Batch 0/32 : Loss = 0.0823545902967453\n",
      "Epoch 29, Batch 1/32 : Loss = 0.00854557752609253\n",
      "Epoch 29, Batch 2/32 : Loss = 0.013764331117272377\n",
      "Epoch 29, Batch 3/32 : Loss = 0.011897354386746883\n",
      "Epoch 29, Batch 4/32 : Loss = 0.010078301653265953\n",
      "Epoch 29, Batch 5/32 : Loss = 0.009869921952486038\n",
      "Epoch 29, Batch 6/32 : Loss = 0.011197768151760101\n",
      "Epoch 29, Batch 7/32 : Loss = 0.01200295239686966\n",
      "Epoch 29, Batch 8/32 : Loss = 0.009551698341965675\n",
      "Epoch 29, Batch 9/32 : Loss = 0.12881919741630554\n",
      "Epoch 29, Batch 10/32 : Loss = 0.008818356320261955\n",
      "Epoch 29, Batch 11/32 : Loss = 0.024179525673389435\n",
      "Epoch 29, Batch 12/32 : Loss = 0.015484924428164959\n",
      "Epoch 29, Batch 13/32 : Loss = 0.007585322484374046\n",
      "Epoch 29, Batch 14/32 : Loss = 0.006312540732324123\n",
      "Epoch 29, Batch 15/32 : Loss = 0.0421280600130558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 16/32 : Loss = 0.04176906868815422\n",
      "Epoch 29, Batch 17/32 : Loss = 0.023320162668824196\n",
      "Epoch 29, Batch 18/32 : Loss = 0.009426401928067207\n",
      "Epoch 29, Batch 19/32 : Loss = 0.008047293871641159\n",
      "Epoch 29, Batch 20/32 : Loss = 0.026049330830574036\n",
      "Epoch 29, Batch 21/32 : Loss = 0.005444326438009739\n",
      "Epoch 29, Batch 22/32 : Loss = 0.0278912503272295\n",
      "Epoch 29, Batch 23/32 : Loss = 0.07444564253091812\n",
      "Epoch 29, Batch 24/32 : Loss = 0.005357431247830391\n",
      "Epoch 29, Batch 25/32 : Loss = 0.021459633484482765\n",
      "Epoch 29, Batch 26/32 : Loss = 0.026408717036247253\n",
      "Epoch 29, Batch 27/32 : Loss = 0.011518936604261398\n",
      "Epoch 29, Batch 28/32 : Loss = 0.024945445358753204\n",
      "Epoch 29, Batch 29/32 : Loss = 0.017840130254626274\n",
      "Epoch 29, Batch 30/32 : Loss = 0.04676049202680588\n",
      "Epoch 29, Batch 31/32 : Loss = 0.03615189716219902\n",
      "Epoch 29 finished in 0.03995502392450968 minutes\n",
      "Epoch 29 training_loss = 0.024989359080791473\n",
      "n----------7----Q-----t---^---P-----p----uu----K----#----]--- => n7Qt^PpuK#], Ground Truth is n7Qt^PpuK#]\n",
      "@-------n----7----\\--B-----\\-l--&----g---:--C-----MM----FF--- => @n7\\B\\l&g:CMF, Ground Truth is @n7\\B\\l&g:CMF\n",
      "Epoch 29 val_loss = 0.02242802456021309, word_accuracy = 0.9759036144578314\n",
      "Epoch 30, Batch 0/32 : Loss = 0.01903480663895607\n",
      "Epoch 30, Batch 1/32 : Loss = 0.02102939411997795\n",
      "Epoch 30, Batch 2/32 : Loss = 0.007112607825547457\n",
      "Epoch 30, Batch 3/32 : Loss = 0.014055324718356133\n",
      "Epoch 30, Batch 4/32 : Loss = 0.016404878348112106\n",
      "Epoch 30, Batch 5/32 : Loss = 0.014014734886586666\n",
      "Epoch 30, Batch 6/32 : Loss = 0.017795534804463387\n",
      "Epoch 30, Batch 7/32 : Loss = 0.006339439190924168\n",
      "Epoch 30, Batch 8/32 : Loss = 0.01943417824804783\n",
      "Epoch 30, Batch 9/32 : Loss = 0.0086970841512084\n",
      "Epoch 30, Batch 10/32 : Loss = 0.011608188971877098\n",
      "Epoch 30, Batch 11/32 : Loss = 0.016726866364479065\n",
      "Epoch 30, Batch 12/32 : Loss = 0.026235615834593773\n",
      "Epoch 30, Batch 13/32 : Loss = 0.0068160733208060265\n",
      "Epoch 30, Batch 14/32 : Loss = 0.009029039181768894\n",
      "Epoch 30, Batch 15/32 : Loss = 0.019947130233049393\n",
      "Epoch 30, Batch 16/32 : Loss = 0.005158032290637493\n",
      "Epoch 30, Batch 17/32 : Loss = 0.006048925220966339\n",
      "Epoch 30, Batch 18/32 : Loss = 0.14331665635108948\n",
      "Epoch 30, Batch 19/32 : Loss = 0.020667796954512596\n",
      "Epoch 30, Batch 20/32 : Loss = 0.021102946251630783\n",
      "Epoch 30, Batch 21/32 : Loss = 0.006686762440949678\n",
      "Epoch 30, Batch 22/32 : Loss = 0.012125988490879536\n",
      "Epoch 30, Batch 23/32 : Loss = 0.005824683234095573\n",
      "Epoch 30, Batch 24/32 : Loss = 0.017191503196954727\n",
      "Epoch 30, Batch 25/32 : Loss = 0.16887688636779785\n",
      "Epoch 30, Batch 26/32 : Loss = 0.02517334744334221\n",
      "Epoch 30, Batch 27/32 : Loss = 0.016734853386878967\n",
      "Epoch 30, Batch 28/32 : Loss = 0.018930761143565178\n",
      "Epoch 30, Batch 29/32 : Loss = 0.006511739455163479\n",
      "Epoch 30, Batch 30/32 : Loss = 0.012986467219889164\n",
      "Epoch 30, Batch 31/32 : Loss = 0.015130847692489624\n",
      "Epoch 30 finished in 0.03964207569758097 minutes\n",
      "Epoch 30 training_loss = 0.02324528805911541\n",
      "@------n---7---\\-B---\\\\--&---gg----C---M----F--- => @n7\\B\\&gCMF, Ground Truth is @n7\\B\\l&g:CMF\n",
      "i-----D----+-------b----W------?--}-``-^--x----- => iD+bW?}`^x, Ground Truth is iD+2bW?}`^x\n",
      "Epoch 30 val_loss = 0.024056993424892426, word_accuracy = 0.9598393574297188\n",
      "Epoch 31, Batch 0/32 : Loss = 0.0170351043343544\n",
      "Epoch 31, Batch 1/32 : Loss = 0.02225884050130844\n",
      "Epoch 31, Batch 2/32 : Loss = 0.011024238541722298\n",
      "Epoch 31, Batch 3/32 : Loss = 0.00863079447299242\n",
      "Epoch 31, Batch 4/32 : Loss = 0.011259080842137337\n",
      "Epoch 31, Batch 5/32 : Loss = 0.01490443293005228\n",
      "Epoch 31, Batch 6/32 : Loss = 0.03406552970409393\n",
      "Epoch 31, Batch 7/32 : Loss = 0.008497610688209534\n",
      "Epoch 31, Batch 8/32 : Loss = 0.009069353342056274\n",
      "Epoch 31, Batch 9/32 : Loss = 0.010806936770677567\n",
      "Epoch 31, Batch 10/32 : Loss = 0.013640021905303001\n",
      "Epoch 31, Batch 11/32 : Loss = 0.04255099594593048\n",
      "Epoch 31, Batch 12/32 : Loss = 0.005554476752877235\n",
      "Epoch 31, Batch 13/32 : Loss = 0.02712239883840084\n",
      "Epoch 31, Batch 14/32 : Loss = 0.004821966402232647\n",
      "Epoch 31, Batch 15/32 : Loss = 0.0057553998194634914\n",
      "Epoch 31, Batch 16/32 : Loss = 0.007706897333264351\n",
      "Epoch 31, Batch 17/32 : Loss = 0.010265237651765347\n",
      "Epoch 31, Batch 18/32 : Loss = 0.004512860905379057\n",
      "Epoch 31, Batch 19/32 : Loss = 0.006837451830506325\n",
      "Epoch 31, Batch 20/32 : Loss = 0.07942906022071838\n",
      "Epoch 31, Batch 21/32 : Loss = 0.03369311988353729\n",
      "Epoch 31, Batch 22/32 : Loss = 0.006514485459774733\n",
      "Epoch 31, Batch 23/32 : Loss = 0.03721751645207405\n",
      "Epoch 31, Batch 24/32 : Loss = 0.029750587418675423\n",
      "Epoch 31, Batch 25/32 : Loss = 0.0068099405616521835\n",
      "Epoch 31, Batch 26/32 : Loss = 0.04256626218557358\n",
      "Epoch 31, Batch 27/32 : Loss = 0.009682392701506615\n",
      "Epoch 31, Batch 28/32 : Loss = 0.011057890020310879\n",
      "Epoch 31, Batch 29/32 : Loss = 0.020896175876259804\n",
      "Epoch 31, Batch 30/32 : Loss = 0.1653137505054474\n",
      "Epoch 31, Batch 31/32 : Loss = 0.016268404200673103\n",
      "Epoch 31 finished in 0.03906592925389608 minutes\n",
      "Epoch 31 training_loss = 0.023173796012997627\n",
      "n------^^--**---5---i--L----++---3---#-----Z----P----OO----- => n^*5iL+3#ZPO, Ground Truth is n^*5iL+3#ZPO\n",
      "E-------]--O-----C---MM----nn--88---5---q---d---'-/--x------ => E]OCMn85qd'/x, Ground Truth is E]OCMn85qd'/x\n",
      "Epoch 31 val_loss = 0.026565589010715485, word_accuracy = 0.9718875502008032\n",
      "Epoch 32, Batch 0/32 : Loss = 0.015351895242929459\n",
      "Epoch 32, Batch 1/32 : Loss = 0.00835986528545618\n",
      "Epoch 32, Batch 2/32 : Loss = 0.08298012614250183\n",
      "Epoch 32, Batch 3/32 : Loss = 0.010536312125623226\n",
      "Epoch 32, Batch 4/32 : Loss = 0.020955964922904968\n",
      "Epoch 32, Batch 5/32 : Loss = 0.016689075157046318\n",
      "Epoch 32, Batch 6/32 : Loss = 0.007389782927930355\n",
      "Epoch 32, Batch 7/32 : Loss = 0.005743585992604494\n",
      "Epoch 32, Batch 8/32 : Loss = 0.010172614827752113\n",
      "Epoch 32, Batch 9/32 : Loss = 0.006768140010535717\n",
      "Epoch 32, Batch 10/32 : Loss = 0.017886610701680183\n",
      "Epoch 32, Batch 11/32 : Loss = 0.014823666773736477\n",
      "Epoch 32, Batch 12/32 : Loss = 0.009179308079183102\n",
      "Epoch 32, Batch 13/32 : Loss = 0.009912856854498386\n",
      "Epoch 32, Batch 14/32 : Loss = 0.018789416179060936\n",
      "Epoch 32, Batch 15/32 : Loss = 0.012695670127868652\n",
      "Epoch 32, Batch 16/32 : Loss = 0.03012968972325325\n",
      "Epoch 32, Batch 17/32 : Loss = 0.005607531405985355\n",
      "Epoch 32, Batch 18/32 : Loss = 0.016898415982723236\n",
      "Epoch 32, Batch 19/32 : Loss = 0.005459492094814777\n",
      "Epoch 32, Batch 20/32 : Loss = 0.04743245989084244\n",
      "Epoch 32, Batch 21/32 : Loss = 0.04440556466579437\n",
      "Epoch 32, Batch 22/32 : Loss = 0.045007698237895966\n",
      "Epoch 32, Batch 23/32 : Loss = 0.011717409826815128\n",
      "Epoch 32, Batch 24/32 : Loss = 0.0049568526446819305\n",
      "Epoch 32, Batch 25/32 : Loss = 0.012023182585835457\n",
      "Epoch 32, Batch 26/32 : Loss = 0.00920257717370987\n",
      "Epoch 32, Batch 27/32 : Loss = 0.02512167952954769\n",
      "Epoch 32, Batch 28/32 : Loss = 0.01385660469532013\n",
      "Epoch 32, Batch 29/32 : Loss = 0.01193187665194273\n",
      "Epoch 32, Batch 30/32 : Loss = 0.005510978400707245\n",
      "Epoch 32, Batch 31/32 : Loss = 0.003989530261605978\n",
      "Epoch 32 finished in 0.0392802357673645 minutes\n",
      "Epoch 32 training_loss = 0.01792757213115692\n",
      "S---------[---KK-----\"----d------N-------3-----9-----y--``--***- => S[K\"dN39y`*, Ground Truth is S[K\"dN39y`*\n",
      "W----------L----77---r---Y---f---a----n---:--MM-----;-ll--T----- => WL7rYfan:M;lT, Ground Truth is WL7rYfan:M;lT\n",
      "Epoch 32 val_loss = 0.015736432746052742, word_accuracy = 0.9779116465863453\n",
      "Epoch 33, Batch 0/32 : Loss = 0.010728375054895878\n",
      "Epoch 33, Batch 1/32 : Loss = 0.009764367714524269\n",
      "Epoch 33, Batch 2/32 : Loss = 0.008351713418960571\n",
      "Epoch 33, Batch 3/32 : Loss = 0.006592503748834133\n",
      "Epoch 33, Batch 4/32 : Loss = 0.006139139179140329\n",
      "Epoch 33, Batch 5/32 : Loss = 0.0066148340702056885\n",
      "Epoch 33, Batch 6/32 : Loss = 0.022865137085318565\n",
      "Epoch 33, Batch 7/32 : Loss = 0.016129575669765472\n",
      "Epoch 33, Batch 8/32 : Loss = 0.012340733781456947\n",
      "Epoch 33, Batch 9/32 : Loss = 0.0049857329577207565\n",
      "Epoch 33, Batch 10/32 : Loss = 0.0074320510029792786\n",
      "Epoch 33, Batch 11/32 : Loss = 0.02763088047504425\n",
      "Epoch 33, Batch 12/32 : Loss = 0.00404901709407568\n",
      "Epoch 33, Batch 13/32 : Loss = 0.01402430608868599\n",
      "Epoch 33, Batch 14/32 : Loss = 0.006247437559068203\n",
      "Epoch 33, Batch 15/32 : Loss = 0.005439172964543104\n",
      "Epoch 33, Batch 16/32 : Loss = 0.0037237650249153376\n",
      "Epoch 33, Batch 17/32 : Loss = 0.013883574865758419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 18/32 : Loss = 0.009085571393370628\n",
      "Epoch 33, Batch 19/32 : Loss = 0.005833026021718979\n",
      "Epoch 33, Batch 20/32 : Loss = 0.011179492808878422\n",
      "Epoch 33, Batch 21/32 : Loss = 0.006071976386010647\n",
      "Epoch 33, Batch 22/32 : Loss = 0.004732733592391014\n",
      "Epoch 33, Batch 23/32 : Loss = 0.00707668624818325\n",
      "Epoch 33, Batch 24/32 : Loss = 0.013389656320214272\n",
      "Epoch 33, Batch 25/32 : Loss = 0.08022483438253403\n",
      "Epoch 33, Batch 26/32 : Loss = 0.007900210097432137\n",
      "Epoch 33, Batch 27/32 : Loss = 0.05555025860667229\n",
      "Epoch 33, Batch 28/32 : Loss = 0.03352990746498108\n",
      "Epoch 33, Batch 29/32 : Loss = 0.004466587211936712\n",
      "Epoch 33, Batch 30/32 : Loss = 0.007072173058986664\n",
      "Epoch 33, Batch 31/32 : Loss = 0.005776832811534405\n",
      "Epoch 33 finished in 0.04014374415079753 minutes\n",
      "Epoch 33 training_loss = 0.01393662765622139\n",
      "~------R----55---MM-----HH----~~---u----@@-----%-----__---&------ => ~R5MH~u@%_&, Ground Truth is ~R5MH~u@%_&\n",
      "h-----------YY----[---o-----E-----]---0-----}---gg----bb-----(--- => hY[oE]0}gb(, Ground Truth is hY[oE]0}gb(\n",
      "Epoch 33 val_loss = 0.019359389320015907, word_accuracy = 0.9678714859437751\n",
      "Epoch 34, Batch 0/32 : Loss = 0.008288874290883541\n",
      "Epoch 34, Batch 1/32 : Loss = 0.006678535602986813\n",
      "Epoch 34, Batch 2/32 : Loss = 0.007088779471814632\n",
      "Epoch 34, Batch 3/32 : Loss = 0.0057346150279045105\n",
      "Epoch 34, Batch 4/32 : Loss = 0.0077024842612445354\n",
      "Epoch 34, Batch 5/32 : Loss = 0.04942300543189049\n",
      "Epoch 34, Batch 6/32 : Loss = 0.007830589078366756\n",
      "Epoch 34, Batch 7/32 : Loss = 0.04801301285624504\n",
      "Epoch 34, Batch 8/32 : Loss = 0.004649242386221886\n",
      "Epoch 34, Batch 9/32 : Loss = 0.010751212015748024\n",
      "Epoch 34, Batch 10/32 : Loss = 0.023169461637735367\n",
      "Epoch 34, Batch 11/32 : Loss = 0.13560602068901062\n",
      "Epoch 34, Batch 12/32 : Loss = 0.008807744830846786\n",
      "Epoch 34, Batch 13/32 : Loss = 0.021639609709382057\n",
      "Epoch 34, Batch 14/32 : Loss = 0.010473214089870453\n",
      "Epoch 34, Batch 15/32 : Loss = 0.00528551172465086\n",
      "Epoch 34, Batch 16/32 : Loss = 0.007247502915561199\n",
      "Epoch 34, Batch 17/32 : Loss = 0.007628876715898514\n",
      "Epoch 34, Batch 18/32 : Loss = 0.005205363035202026\n",
      "Epoch 34, Batch 19/32 : Loss = 0.008751319721341133\n",
      "Epoch 34, Batch 20/32 : Loss = 0.039303284138441086\n",
      "Epoch 34, Batch 21/32 : Loss = 0.008140413090586662\n",
      "Epoch 34, Batch 22/32 : Loss = 0.005454966798424721\n",
      "Epoch 34, Batch 23/32 : Loss = 0.005799534730613232\n",
      "Epoch 34, Batch 24/32 : Loss = 0.005423609167337418\n",
      "Epoch 34, Batch 25/32 : Loss = 0.007812832482159138\n",
      "Epoch 34, Batch 26/32 : Loss = 0.029455017298460007\n",
      "Epoch 34, Batch 27/32 : Loss = 0.06283989548683167\n",
      "Epoch 34, Batch 28/32 : Loss = 0.0057582613080739975\n",
      "Epoch 34, Batch 29/32 : Loss = 0.013597104698419571\n",
      "Epoch 34, Batch 30/32 : Loss = 0.004782393574714661\n",
      "Epoch 34, Batch 31/32 : Loss = 0.009494300931692123\n",
      "Epoch 34 finished in 0.04129439194997152 minutes\n",
      "Epoch 34 training_loss = 0.018619408831000328\n",
      "*-----oo---5----s---L---[--aa---%------*---a----C----DD---``- => *o5sL[a%*aCD`, Ground Truth is *o5sL[a%*aCD`\n",
      "7---------TT----C------S----f--}----?---LL---55---j--K------- => 7TCSf}?L5jK, Ground Truth is 7TCSf}?L5jK\n",
      "Epoch 34 val_loss = 0.01941034197807312, word_accuracy = 0.9779116465863453\n",
      "Epoch 35, Batch 0/32 : Loss = 0.020573856309056282\n",
      "Epoch 35, Batch 1/32 : Loss = 0.020662572234869003\n",
      "Epoch 35, Batch 2/32 : Loss = 0.012914837338030338\n",
      "Epoch 35, Batch 3/32 : Loss = 0.022661171853542328\n",
      "Epoch 35, Batch 4/32 : Loss = 0.005432751961052418\n",
      "Epoch 35, Batch 5/32 : Loss = 0.0059075551107525826\n",
      "Epoch 35, Batch 6/32 : Loss = 0.1188305914402008\n",
      "Epoch 35, Batch 7/32 : Loss = 0.012478789314627647\n",
      "Epoch 35, Batch 8/32 : Loss = 0.005284864455461502\n",
      "Epoch 35, Batch 9/32 : Loss = 0.005189446732401848\n",
      "Epoch 35, Batch 10/32 : Loss = 0.0035830519627779722\n",
      "Epoch 35, Batch 11/32 : Loss = 0.039916619658470154\n",
      "Epoch 35, Batch 12/32 : Loss = 0.014359619468450546\n",
      "Epoch 35, Batch 13/32 : Loss = 0.007869000546634197\n",
      "Epoch 35, Batch 14/32 : Loss = 0.06577449291944504\n",
      "Epoch 35, Batch 15/32 : Loss = 0.10506721585988998\n",
      "Epoch 35, Batch 16/32 : Loss = 0.07870884984731674\n",
      "Epoch 35, Batch 17/32 : Loss = 0.0049505094066262245\n",
      "Epoch 35, Batch 18/32 : Loss = 0.009438756853342056\n",
      "Epoch 35, Batch 19/32 : Loss = 0.005867536645382643\n",
      "Epoch 35, Batch 20/32 : Loss = 0.005815712735056877\n",
      "Epoch 35, Batch 21/32 : Loss = 0.005555725656449795\n",
      "Epoch 35, Batch 22/32 : Loss = 0.005607914179563522\n",
      "Epoch 35, Batch 23/32 : Loss = 0.006982361897826195\n",
      "Epoch 35, Batch 24/32 : Loss = 0.006844025105237961\n",
      "Epoch 35, Batch 25/32 : Loss = 0.009652357548475266\n",
      "Epoch 35, Batch 26/32 : Loss = 0.005412006750702858\n",
      "Epoch 35, Batch 27/32 : Loss = 0.007903036661446095\n",
      "Epoch 35, Batch 28/32 : Loss = 0.005741693079471588\n",
      "Epoch 35, Batch 29/32 : Loss = 0.04421743378043175\n",
      "Epoch 35, Batch 30/32 : Loss = 0.005727373994886875\n",
      "Epoch 35, Batch 31/32 : Loss = 0.009567130357027054\n",
      "Epoch 35 finished in 0.0711875319480896 minutes\n",
      "Epoch 35 training_loss = 0.021722976118326187\n",
      "&--------C-----SS----4---F----J---J---F----n---%%------m-------- => &CS4FJJFn%m, Ground Truth is &CS4FJJFn%m\n",
      "i-------/--88----.-hh----G------9----_--i--kk---x----V----YY---- => i/8.hG9_ikxVY, Ground Truth is i/8.hG9_ikxVY\n",
      "Epoch 35 val_loss = 0.023299168795347214, word_accuracy = 0.9658634538152611\n",
      "Epoch 36, Batch 0/32 : Loss = 0.11448092013597488\n",
      "Epoch 36, Batch 1/32 : Loss = 0.005294335074722767\n",
      "Epoch 36, Batch 2/32 : Loss = 0.003917534835636616\n",
      "Epoch 36, Batch 3/32 : Loss = 0.007188739720731974\n",
      "Epoch 36, Batch 4/32 : Loss = 0.02082093060016632\n",
      "Epoch 36, Batch 5/32 : Loss = 0.059849683195352554\n",
      "Epoch 36, Batch 6/32 : Loss = 0.005518171936273575\n",
      "Epoch 36, Batch 7/32 : Loss = 0.0040624490939080715\n",
      "Epoch 36, Batch 8/32 : Loss = 0.004517718218266964\n",
      "Epoch 36, Batch 9/32 : Loss = 0.053267702460289\n",
      "Epoch 36, Batch 10/32 : Loss = 0.006445683538913727\n",
      "Epoch 36, Batch 11/32 : Loss = 0.005903531797230244\n",
      "Epoch 36, Batch 12/32 : Loss = 0.006371374242007732\n",
      "Epoch 36, Batch 13/32 : Loss = 0.005536177661269903\n",
      "Epoch 36, Batch 14/32 : Loss = 0.00771662313491106\n",
      "Epoch 36, Batch 15/32 : Loss = 0.004553779028356075\n",
      "Epoch 36, Batch 16/32 : Loss = 0.039480723440647125\n",
      "Epoch 36, Batch 17/32 : Loss = 0.008721897378563881\n",
      "Epoch 36, Batch 18/32 : Loss = 0.032256223261356354\n",
      "Epoch 36, Batch 19/32 : Loss = 0.06567331403493881\n",
      "Epoch 36, Batch 20/32 : Loss = 0.0045632184483110905\n",
      "Epoch 36, Batch 21/32 : Loss = 0.006374055985361338\n",
      "Epoch 36, Batch 22/32 : Loss = 0.008629795163869858\n",
      "Epoch 36, Batch 23/32 : Loss = 0.004078096244484186\n",
      "Epoch 36, Batch 24/32 : Loss = 0.020095501095056534\n",
      "Epoch 36, Batch 25/32 : Loss = 0.0047879284247756\n",
      "Epoch 36, Batch 26/32 : Loss = 0.005724250804632902\n",
      "Epoch 36, Batch 27/32 : Loss = 0.006413590162992477\n",
      "Epoch 36, Batch 28/32 : Loss = 0.022563641890883446\n",
      "Epoch 36, Batch 29/32 : Loss = 0.013556682504713535\n",
      "Epoch 36, Batch 30/32 : Loss = 0.008665792644023895\n",
      "Epoch 36, Batch 31/32 : Loss = 0.003038919996470213\n",
      "Epoch 36 finished in 0.03951363960901896 minutes\n",
      "Epoch 36 training_loss = 0.01823003590106964\n",
      "E--------S----w----X---qq---,---3----~--j-LL---- => ESwXq,3~jL, Ground Truth is ESwXq,'3~jL\n",
      "c------DD---j-44--}-ii-C---->---**--e---6---c--- => cDj4}iC>*e6c, Ground Truth is cDj4}iC>*e6c\n",
      "Epoch 36 val_loss = 0.014903180301189423, word_accuracy = 0.9819277108433735\n",
      "Epoch 37, Batch 0/32 : Loss = 0.008723347447812557\n",
      "Epoch 37, Batch 1/32 : Loss = 0.004335931036621332\n",
      "Epoch 37, Batch 2/32 : Loss = 0.005527053028345108\n",
      "Epoch 37, Batch 3/32 : Loss = 0.017554547637701035\n",
      "Epoch 37, Batch 4/32 : Loss = 0.0835515633225441\n",
      "Epoch 37, Batch 5/32 : Loss = 0.010169764049351215\n",
      "Epoch 37, Batch 6/32 : Loss = 0.03785288333892822\n",
      "Epoch 37, Batch 7/32 : Loss = 0.017071446403861046\n",
      "Epoch 37, Batch 8/32 : Loss = 0.005827818065881729\n",
      "Epoch 37, Batch 9/32 : Loss = 0.006510072387754917\n",
      "Epoch 37, Batch 10/32 : Loss = 0.006884999107569456\n",
      "Epoch 37, Batch 11/32 : Loss = 0.04130128398537636\n",
      "Epoch 37, Batch 12/32 : Loss = 0.005632105749100447\n",
      "Epoch 37, Batch 13/32 : Loss = 0.07303166389465332\n",
      "Epoch 37, Batch 14/32 : Loss = 0.018397416919469833\n",
      "Epoch 37, Batch 15/32 : Loss = 0.013159431517124176\n",
      "Epoch 37, Batch 16/32 : Loss = 0.20383773744106293\n",
      "Epoch 37, Batch 17/32 : Loss = 0.015009745955467224\n",
      "Epoch 37, Batch 18/32 : Loss = 0.0058838920667767525\n",
      "Epoch 37, Batch 19/32 : Loss = 0.004322180524468422\n",
      "Epoch 37, Batch 20/32 : Loss = 0.004842309281229973\n",
      "Epoch 37, Batch 21/32 : Loss = 0.00986453890800476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 22/32 : Loss = 0.010249100625514984\n",
      "Epoch 37, Batch 23/32 : Loss = 0.00924828089773655\n",
      "Epoch 37, Batch 24/32 : Loss = 0.02900313213467598\n",
      "Epoch 37, Batch 25/32 : Loss = 0.026114026084542274\n",
      "Epoch 37, Batch 26/32 : Loss = 0.006119413301348686\n",
      "Epoch 37, Batch 27/32 : Loss = 0.010017897933721542\n",
      "Epoch 37, Batch 28/32 : Loss = 0.01808481477200985\n",
      "Epoch 37, Batch 29/32 : Loss = 0.015253018587827682\n",
      "Epoch 37, Batch 30/32 : Loss = 0.01785309612751007\n",
      "Epoch 37, Batch 31/32 : Loss = 0.006480532698333263\n",
      "Epoch 37 finished in 0.03967127799987793 minutes\n",
      "Epoch 37 training_loss = 0.023840786889195442\n",
      "9------)---V---<<----T----G-----\\-;-I--v--99---=---- => 9)V<TG\\;Iv9=, Ground Truth is 9)V<TG\\;Iv9=\n",
      "7-------T----C-----S---f--}---?---L---55--j--K------ => 7TCSf}?L5jK, Ground Truth is 7TCSf}?L5jK\n",
      "Epoch 37 val_loss = 0.03672589361667633, word_accuracy = 0.9678714859437751\n",
      "Epoch 38, Batch 0/32 : Loss = 0.03232376277446747\n",
      "Epoch 38, Batch 1/32 : Loss = 0.006975976750254631\n",
      "Epoch 38, Batch 2/32 : Loss = 0.02489456906914711\n",
      "Epoch 38, Batch 3/32 : Loss = 0.00849514827132225\n",
      "Epoch 38, Batch 4/32 : Loss = 0.05871466174721718\n",
      "Epoch 38, Batch 5/32 : Loss = 0.012383734807372093\n",
      "Epoch 38, Batch 6/32 : Loss = 0.0071576982736587524\n",
      "Epoch 38, Batch 7/32 : Loss = 0.007833901792764664\n",
      "Epoch 38, Batch 8/32 : Loss = 0.0053514898754656315\n",
      "Epoch 38, Batch 9/32 : Loss = 0.010572656989097595\n",
      "Epoch 38, Batch 10/32 : Loss = 0.015424673445522785\n",
      "Epoch 38, Batch 11/32 : Loss = 0.010007081553339958\n",
      "Epoch 38, Batch 12/32 : Loss = 0.017722371965646744\n",
      "Epoch 38, Batch 13/32 : Loss = 0.0067933714017271996\n",
      "Epoch 38, Batch 14/32 : Loss = 0.30589908361434937\n",
      "Epoch 38, Batch 15/32 : Loss = 0.005529549904167652\n",
      "Epoch 38, Batch 16/32 : Loss = 0.007479197811335325\n",
      "Epoch 38, Batch 17/32 : Loss = 0.016113365069031715\n",
      "Epoch 38, Batch 18/32 : Loss = 0.009138323366641998\n",
      "Epoch 38, Batch 19/32 : Loss = 0.006709881126880646\n",
      "Epoch 38, Batch 20/32 : Loss = 0.0048459554091095924\n",
      "Epoch 38, Batch 21/32 : Loss = 0.02157607674598694\n",
      "Epoch 38, Batch 22/32 : Loss = 0.04355957359075546\n",
      "Epoch 38, Batch 23/32 : Loss = 0.005758969113230705\n",
      "Epoch 38, Batch 24/32 : Loss = 0.0077410307712852955\n",
      "Epoch 38, Batch 25/32 : Loss = 0.06036822125315666\n",
      "Epoch 38, Batch 26/32 : Loss = 0.00847445335239172\n",
      "Epoch 38, Batch 27/32 : Loss = 0.006527610123157501\n",
      "Epoch 38, Batch 28/32 : Loss = 0.20563876628875732\n",
      "Epoch 38, Batch 29/32 : Loss = 0.007608418352901936\n",
      "Epoch 38, Batch 30/32 : Loss = 0.017641153186559677\n",
      "Epoch 38, Batch 31/32 : Loss = 0.003269562963396311\n",
      "Epoch 38 finished in 0.03932254711786906 minutes\n",
      "Epoch 38 training_loss = 0.031025517731904984\n",
      "G-------J---mm-----\\--F----2---(--r--8----*---=---- => GJm\\F2(r8*=, Ground Truth is GJm\\F2(r8*=\n",
      "u------D---b----1--OO---I-t-YY--qq--BB---m----[---- => uDb1OItYqBm[, Ground Truth is uDb1OItYqBm[\n",
      "Epoch 38 val_loss = 0.03306761756539345, word_accuracy = 0.9538152610441767\n",
      "Epoch 39, Batch 0/32 : Loss = 0.011395435780286789\n",
      "Epoch 39, Batch 1/32 : Loss = 0.007298883516341448\n",
      "Epoch 39, Batch 2/32 : Loss = 0.0060776593163609505\n",
      "Epoch 39, Batch 3/32 : Loss = 0.01889735646545887\n",
      "Epoch 39, Batch 4/32 : Loss = 0.00553331570699811\n",
      "Epoch 39, Batch 5/32 : Loss = 0.08334069699048996\n",
      "Epoch 39, Batch 6/32 : Loss = 0.02801547199487686\n",
      "Epoch 39, Batch 7/32 : Loss = 0.020547479391098022\n",
      "Epoch 39, Batch 8/32 : Loss = 0.07619237899780273\n",
      "Epoch 39, Batch 9/32 : Loss = 0.043844252824783325\n",
      "Epoch 39, Batch 10/32 : Loss = 0.005841491743922234\n",
      "Epoch 39, Batch 11/32 : Loss = 0.00782885029911995\n",
      "Epoch 39, Batch 12/32 : Loss = 0.027003858238458633\n",
      "Epoch 39, Batch 13/32 : Loss = 0.023734312504529953\n",
      "Epoch 39, Batch 14/32 : Loss = 0.021053384989500046\n",
      "Epoch 39, Batch 15/32 : Loss = 0.02297656610608101\n",
      "Epoch 39, Batch 16/32 : Loss = 0.02941031940281391\n",
      "Epoch 39, Batch 17/32 : Loss = 0.0059343790635466576\n",
      "Epoch 39, Batch 18/32 : Loss = 0.020747173577547073\n",
      "Epoch 39, Batch 19/32 : Loss = 0.0251234769821167\n",
      "Epoch 39, Batch 20/32 : Loss = 0.03227455914020538\n",
      "Epoch 39, Batch 21/32 : Loss = 0.008730912581086159\n",
      "Epoch 39, Batch 22/32 : Loss = 0.09570720046758652\n",
      "Epoch 39, Batch 23/32 : Loss = 0.036464788019657135\n",
      "Epoch 39, Batch 24/32 : Loss = 0.005163858644664288\n",
      "Epoch 39, Batch 25/32 : Loss = 0.010890419594943523\n",
      "Epoch 39, Batch 26/32 : Loss = 0.0055670421570539474\n",
      "Epoch 39, Batch 27/32 : Loss = 0.0060506341978907585\n",
      "Epoch 39, Batch 28/32 : Loss = 0.07157320529222488\n",
      "Epoch 39, Batch 29/32 : Loss = 0.006259238813072443\n",
      "Epoch 39, Batch 30/32 : Loss = 0.014639929868280888\n",
      "Epoch 39, Batch 31/32 : Loss = 0.009343148209154606\n",
      "Epoch 39 finished in 0.042530834674835205 minutes\n",
      "Epoch 39 training_loss = 0.025230085477232933\n",
      "}--------z---@@-----~---j--N-----FF---55----C----!--9----1---n----- => }z@~jNF5C!91n, Ground Truth is }z@~jNF5C!91n\n",
      "#----------}}---?---bb---66----BB----%%-----,-..--N----`---m------- => #}?b6B%,.N`m, Ground Truth is #}?b6B%,.N`m\n",
      "Epoch 39 val_loss = 0.01719038560986519, word_accuracy = 0.9738955823293173\n",
      "Epoch 40, Batch 0/32 : Loss = 0.021797966212034225\n",
      "Epoch 40, Batch 1/32 : Loss = 0.010598061606287956\n",
      "Epoch 40, Batch 2/32 : Loss = 0.0680401548743248\n",
      "Epoch 40, Batch 3/32 : Loss = 0.008472106419503689\n",
      "Epoch 40, Batch 4/32 : Loss = 0.010119302198290825\n",
      "Epoch 40, Batch 5/32 : Loss = 0.01836508698761463\n",
      "Epoch 40, Batch 6/32 : Loss = 0.007338971830904484\n",
      "Epoch 40, Batch 7/32 : Loss = 0.009367827326059341\n",
      "Epoch 40, Batch 8/32 : Loss = 0.020821984857320786\n",
      "Epoch 40, Batch 9/32 : Loss = 0.053730785846710205\n",
      "Epoch 40, Batch 10/32 : Loss = 0.0041722021996974945\n",
      "Epoch 40, Batch 11/32 : Loss = 0.01896379515528679\n",
      "Epoch 40, Batch 12/32 : Loss = 0.005628862418234348\n",
      "Epoch 40, Batch 13/32 : Loss = 0.013695638626813889\n",
      "Epoch 40, Batch 14/32 : Loss = 0.03537355363368988\n",
      "Epoch 40, Batch 15/32 : Loss = 0.006005922332406044\n",
      "Epoch 40, Batch 16/32 : Loss = 0.004346954170614481\n",
      "Epoch 40, Batch 17/32 : Loss = 0.005607803352177143\n",
      "Epoch 40, Batch 18/32 : Loss = 0.02219489775598049\n",
      "Epoch 40, Batch 19/32 : Loss = 0.004738528281450272\n",
      "Epoch 40, Batch 20/32 : Loss = 0.02132142148911953\n",
      "Epoch 40, Batch 21/32 : Loss = 0.028351545333862305\n",
      "Epoch 40, Batch 22/32 : Loss = 0.06104116141796112\n",
      "Epoch 40, Batch 23/32 : Loss = 0.024682363495230675\n",
      "Epoch 40, Batch 24/32 : Loss = 0.004924132023006678\n",
      "Epoch 40, Batch 25/32 : Loss = 0.013024276122450829\n",
      "Epoch 40, Batch 26/32 : Loss = 0.004762023687362671\n",
      "Epoch 40, Batch 27/32 : Loss = 0.01751011796295643\n",
      "Epoch 40, Batch 28/32 : Loss = 0.03344087675213814\n",
      "Epoch 40, Batch 29/32 : Loss = 0.13978061079978943\n",
      "Epoch 40, Batch 30/32 : Loss = 0.06632687151432037\n",
      "Epoch 40, Batch 31/32 : Loss = 0.020129596814513206\n",
      "Epoch 40 finished in 0.039470243453979495 minutes\n",
      "Epoch 40 training_loss = 0.02464456483721733\n",
      "O--------]--PP---n---]]-]--k---f-;;-77---X---6---I--- => O]Pn]]kf;7X6I, Ground Truth is O]Pn]]kf;7X6I\n",
      "}------y---L--\"\"--|---F---*---F---m-----b----/--4---- => }yL\"|F*Fmb/4, Ground Truth is }yL\"|F*Fmb/4\n",
      "Epoch 40 val_loss = 0.014667723327875137, word_accuracy = 0.9759036144578314\n",
      "Epoch 41, Batch 0/32 : Loss = 0.04978499934077263\n",
      "Epoch 41, Batch 1/32 : Loss = 0.005625970661640167\n",
      "Epoch 41, Batch 2/32 : Loss = 0.004729975946247578\n",
      "Epoch 41, Batch 3/32 : Loss = 0.023477399721741676\n",
      "Epoch 41, Batch 4/32 : Loss = 0.009031546302139759\n",
      "Epoch 41, Batch 5/32 : Loss = 0.013533875346183777\n",
      "Epoch 41, Batch 6/32 : Loss = 0.005684279836714268\n",
      "Epoch 41, Batch 7/32 : Loss = 0.0059051807038486\n",
      "Epoch 41, Batch 8/32 : Loss = 0.00924666691571474\n",
      "Epoch 41, Batch 9/32 : Loss = 0.018414005637168884\n",
      "Epoch 41, Batch 10/32 : Loss = 0.005219311453402042\n",
      "Epoch 41, Batch 11/32 : Loss = 0.07364441454410553\n",
      "Epoch 41, Batch 12/32 : Loss = 0.013033950701355934\n",
      "Epoch 41, Batch 13/32 : Loss = 0.005782287567853928\n",
      "Epoch 41, Batch 14/32 : Loss = 0.010139357298612595\n",
      "Epoch 41, Batch 15/32 : Loss = 0.006677162833511829\n",
      "Epoch 41, Batch 16/32 : Loss = 0.008187970146536827\n",
      "Epoch 41, Batch 17/32 : Loss = 0.006166399456560612\n",
      "Epoch 41, Batch 18/32 : Loss = 0.006160052493214607\n",
      "Epoch 41, Batch 19/32 : Loss = 0.032665081322193146\n",
      "Epoch 41, Batch 20/32 : Loss = 0.00808385107666254\n",
      "Epoch 41, Batch 21/32 : Loss = 0.010763701982796192\n",
      "Epoch 41, Batch 22/32 : Loss = 0.023006359115242958\n",
      "Epoch 41, Batch 23/32 : Loss = 0.029308749362826347\n",
      "Epoch 41, Batch 24/32 : Loss = 0.005924772471189499\n",
      "Epoch 41, Batch 25/32 : Loss = 0.00933918822556734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Batch 26/32 : Loss = 0.07660748064517975\n",
      "Epoch 41, Batch 27/32 : Loss = 0.0057440935634076595\n",
      "Epoch 41, Batch 28/32 : Loss = 0.0039126393385231495\n",
      "Epoch 41, Batch 29/32 : Loss = 0.025066552683711052\n",
      "Epoch 41, Batch 30/32 : Loss = 0.019367123022675514\n",
      "Epoch 41, Batch 31/32 : Loss = 0.0037485603243112564\n",
      "Epoch 41 finished in 0.039106754461924235 minutes\n",
      "Epoch 41 training_loss = 0.017050694674253464\n",
      "f-----66----U-----(---K------S----qq-------!--5---;-- => f6U(KSq-!5;, Ground Truth is f6U(KSq-!5;\n",
      "K----------4---_---o---]------z--I-W-------9---s----- => K4_o]-zIW9s, Ground Truth is K4_o]-zIW9s\n",
      "Epoch 41 val_loss = 0.011211655102670193, word_accuracy = 0.9879518072289156\n",
      "Epoch 42, Batch 0/32 : Loss = 0.06776857376098633\n",
      "Epoch 42, Batch 1/32 : Loss = 0.010824788361787796\n",
      "Epoch 42, Batch 2/32 : Loss = 0.006130447145551443\n",
      "Epoch 42, Batch 3/32 : Loss = 0.006721080746501684\n",
      "Epoch 42, Batch 4/32 : Loss = 0.021126097068190575\n",
      "Epoch 42, Batch 5/32 : Loss = 0.004515388049185276\n",
      "Epoch 42, Batch 6/32 : Loss = 0.004068282432854176\n",
      "Epoch 42, Batch 7/32 : Loss = 0.0098978066816926\n",
      "Epoch 42, Batch 8/32 : Loss = 0.0076953349635005\n",
      "Epoch 42, Batch 9/32 : Loss = 0.014862180687487125\n",
      "Epoch 42, Batch 10/32 : Loss = 0.008584794588387012\n",
      "Epoch 42, Batch 11/32 : Loss = 0.007291042245924473\n",
      "Epoch 42, Batch 12/32 : Loss = 0.013291649520397186\n",
      "Epoch 42, Batch 13/32 : Loss = 0.0060925171710550785\n",
      "Epoch 42, Batch 14/32 : Loss = 0.010752101428806782\n",
      "Epoch 42, Batch 15/32 : Loss = 0.005718608386814594\n",
      "Epoch 42, Batch 16/32 : Loss = 0.0058936551213264465\n",
      "Epoch 42, Batch 17/32 : Loss = 0.008844445459544659\n",
      "Epoch 42, Batch 18/32 : Loss = 0.02428256906569004\n",
      "Epoch 42, Batch 19/32 : Loss = 0.0037504234351217747\n",
      "Epoch 42, Batch 20/32 : Loss = 0.10020467638969421\n",
      "Epoch 42, Batch 21/32 : Loss = 0.01066784281283617\n",
      "Epoch 42, Batch 22/32 : Loss = 0.003699625376611948\n",
      "Epoch 42, Batch 23/32 : Loss = 0.020625878125429153\n",
      "Epoch 42, Batch 24/32 : Loss = 0.07015546411275864\n",
      "Epoch 42, Batch 25/32 : Loss = 0.014709259383380413\n",
      "Epoch 42, Batch 26/32 : Loss = 0.004213923588395119\n",
      "Epoch 42, Batch 27/32 : Loss = 0.011553067713975906\n",
      "Epoch 42, Batch 28/32 : Loss = 0.017237083986401558\n",
      "Epoch 42, Batch 29/32 : Loss = 0.05334266275167465\n",
      "Epoch 42, Batch 30/32 : Loss = 0.007654968183487654\n",
      "Epoch 42, Batch 31/32 : Loss = 0.03719412907958031\n",
      "Epoch 42 finished in 0.038981775442759194 minutes\n",
      "Epoch 42 training_loss = 0.018211260437965393\n",
      "G----------e-----4---`---z---}---AA----f--l-I--AA----p----- => Ge4`z}AflIAp, Ground Truth is Ge4`z}AflIAp\n",
      "d-------T---=----G----q---0---gg---u---GG----S---x--3---~~- => dT=Gq0guGSx3~, Ground Truth is dT=Gq0guGSx3~\n",
      "Epoch 42 val_loss = 0.029537323862314224, word_accuracy = 0.9759036144578314\n",
      "Epoch 43, Batch 0/32 : Loss = 0.02393629029393196\n",
      "Epoch 43, Batch 1/32 : Loss = 0.03719504177570343\n",
      "Epoch 43, Batch 2/32 : Loss = 0.13806693255901337\n",
      "Epoch 43, Batch 3/32 : Loss = 0.004270071163773537\n",
      "Epoch 43, Batch 4/32 : Loss = 0.008086212910711765\n",
      "Epoch 43, Batch 5/32 : Loss = 0.007726259063929319\n",
      "Epoch 43, Batch 6/32 : Loss = 0.005179635249078274\n",
      "Epoch 43, Batch 7/32 : Loss = 0.022793691605329514\n",
      "Epoch 43, Batch 8/32 : Loss = 0.008548952639102936\n",
      "Epoch 43, Batch 9/32 : Loss = 0.004518681671470404\n",
      "Epoch 43, Batch 10/32 : Loss = 0.004229075741022825\n",
      "Epoch 43, Batch 11/32 : Loss = 0.00959827285259962\n",
      "Epoch 43, Batch 12/32 : Loss = 0.009123049676418304\n",
      "Epoch 43, Batch 13/32 : Loss = 0.02243540994822979\n",
      "Epoch 43, Batch 14/32 : Loss = 0.005753630772233009\n",
      "Epoch 43, Batch 15/32 : Loss = 0.020183859393000603\n",
      "Epoch 43, Batch 16/32 : Loss = 0.0038221387658268213\n",
      "Epoch 43, Batch 17/32 : Loss = 0.01197815965861082\n",
      "Epoch 43, Batch 18/32 : Loss = 0.007348278537392616\n",
      "Epoch 43, Batch 19/32 : Loss = 0.014529593288898468\n",
      "Epoch 43, Batch 20/32 : Loss = 0.04428139328956604\n",
      "Epoch 43, Batch 21/32 : Loss = 0.07345762848854065\n",
      "Epoch 43, Batch 22/32 : Loss = 0.02622044086456299\n",
      "Epoch 43, Batch 23/32 : Loss = 0.08565549552440643\n",
      "Epoch 43, Batch 24/32 : Loss = 0.009678680449724197\n",
      "Epoch 43, Batch 25/32 : Loss = 0.005695382133126259\n",
      "Epoch 43, Batch 26/32 : Loss = 0.00947167631238699\n",
      "Epoch 43, Batch 27/32 : Loss = 0.0072064390406012535\n",
      "Epoch 43, Batch 28/32 : Loss = 0.004574999213218689\n",
      "Epoch 43, Batch 29/32 : Loss = 0.07591170817613602\n",
      "Epoch 43, Batch 30/32 : Loss = 0.00769193796440959\n",
      "Epoch 43, Batch 31/32 : Loss = 0.006883604917675257\n",
      "Epoch 43 finished in 0.03860063552856445 minutes\n",
      "Epoch 43 training_loss = 0.023133473470807076\n",
      "G------N-----4--{--UU---k---d---2---E---y---9---u---)- => GN4{Ukd2Ey9u), Ground Truth is GN4{Ukd2Ey9u)\n",
      "@----------p----g-----<---!------[[--3---cc--HH------- => @pg<!-[3cH, Ground Truth is @pg<!-,[3cH\n",
      "Epoch 43 val_loss = 0.02073775790631771, word_accuracy = 0.9678714859437751\n",
      "Epoch 44, Batch 0/32 : Loss = 0.02801842801272869\n",
      "Epoch 44, Batch 1/32 : Loss = 0.056607335805892944\n",
      "Epoch 44, Batch 2/32 : Loss = 0.007651740685105324\n",
      "Epoch 44, Batch 3/32 : Loss = 0.004495983012020588\n",
      "Epoch 44, Batch 4/32 : Loss = 0.022862209007143974\n",
      "Epoch 44, Batch 5/32 : Loss = 0.01903345435857773\n",
      "Epoch 44, Batch 6/32 : Loss = 0.01881113275885582\n",
      "Epoch 44, Batch 7/32 : Loss = 0.006921723950654268\n",
      "Epoch 44, Batch 8/32 : Loss = 0.05606431886553764\n",
      "Epoch 44, Batch 9/32 : Loss = 0.010650116950273514\n",
      "Epoch 44, Batch 10/32 : Loss = 0.024471774697303772\n",
      "Epoch 44, Batch 11/32 : Loss = 0.02743823453783989\n",
      "Epoch 44, Batch 12/32 : Loss = 0.011198047548532486\n",
      "Epoch 44, Batch 13/32 : Loss = 0.025451842695474625\n",
      "Epoch 44, Batch 14/32 : Loss = 0.017203439027071\n",
      "Epoch 44, Batch 15/32 : Loss = 0.05699726939201355\n",
      "Epoch 44, Batch 16/32 : Loss = 0.007233598735183477\n",
      "Epoch 44, Batch 17/32 : Loss = 0.005391853861510754\n",
      "Epoch 44, Batch 18/32 : Loss = 0.017743989825248718\n",
      "Epoch 44, Batch 19/32 : Loss = 0.01655031368136406\n",
      "Epoch 44, Batch 20/32 : Loss = 0.010145115666091442\n",
      "Epoch 44, Batch 21/32 : Loss = 0.009289375506341457\n",
      "Epoch 44, Batch 22/32 : Loss = 0.042607344686985016\n",
      "Epoch 44, Batch 23/32 : Loss = 0.04465871676802635\n",
      "Epoch 44, Batch 24/32 : Loss = 0.025578182190656662\n",
      "Epoch 44, Batch 25/32 : Loss = 0.021176420152187347\n",
      "Epoch 44, Batch 26/32 : Loss = 0.008063899353146553\n",
      "Epoch 44, Batch 27/32 : Loss = 0.007222036365419626\n",
      "Epoch 44, Batch 28/32 : Loss = 0.02127756178379059\n",
      "Epoch 44, Batch 29/32 : Loss = 0.05357977747917175\n",
      "Epoch 44, Batch 30/32 : Loss = 0.03206079453229904\n",
      "Epoch 44, Batch 31/32 : Loss = 0.44451937079429626\n",
      "Epoch 44 finished in 0.039370659987131754 minutes\n",
      "Epoch 44 training_loss = 0.02480388805270195\n",
      "[------%---------X--`--mm-----3----4---(--22--F----X---- => [%-X`m34(2FX, Ground Truth is [%-X`m34(2FX\n",
      "7--------;-|---5----W-----h---t--t--V---C----v--o---\\--- => 7;|5WhttVCvo\\, Ground Truth is 7;|5WhttVCvo\\\n",
      "Epoch 44 val_loss = 0.012989799492061138, word_accuracy = 0.9819277108433735\n",
      "Epoch 45, Batch 0/32 : Loss = 0.006476891227066517\n",
      "Epoch 45, Batch 1/32 : Loss = 0.0034442716278135777\n",
      "Epoch 45, Batch 2/32 : Loss = 0.0036927585024386644\n",
      "Epoch 45, Batch 3/32 : Loss = 0.025560442358255386\n",
      "Epoch 45, Batch 4/32 : Loss = 0.021173113957047462\n",
      "Epoch 45, Batch 5/32 : Loss = 0.011820323765277863\n",
      "Epoch 45, Batch 6/32 : Loss = 0.033068228513002396\n",
      "Epoch 45, Batch 7/32 : Loss = 0.007327873259782791\n",
      "Epoch 45, Batch 8/32 : Loss = 0.00795259140431881\n",
      "Epoch 45, Batch 9/32 : Loss = 0.025709837675094604\n",
      "Epoch 45, Batch 10/32 : Loss = 0.011590734124183655\n",
      "Epoch 45, Batch 11/32 : Loss = 0.010223996825516224\n",
      "Epoch 45, Batch 12/32 : Loss = 0.039231348782777786\n",
      "Epoch 45, Batch 13/32 : Loss = 0.00748013099655509\n",
      "Epoch 45, Batch 14/32 : Loss = 0.007213267497718334\n",
      "Epoch 45, Batch 15/32 : Loss = 0.08014474809169769\n",
      "Epoch 45, Batch 16/32 : Loss = 0.00907166674733162\n",
      "Epoch 45, Batch 17/32 : Loss = 0.004049667622894049\n",
      "Epoch 45, Batch 18/32 : Loss = 0.0037794983945786953\n",
      "Epoch 45, Batch 19/32 : Loss = 0.00803492870181799\n",
      "Epoch 45, Batch 20/32 : Loss = 0.07600216567516327\n",
      "Epoch 45, Batch 21/32 : Loss = 0.009470831602811813\n",
      "Epoch 45, Batch 22/32 : Loss = 0.016407886520028114\n",
      "Epoch 45, Batch 23/32 : Loss = 0.005987813696265221\n",
      "Epoch 45, Batch 24/32 : Loss = 0.017680121585726738\n",
      "Epoch 45, Batch 25/32 : Loss = 0.04603899270296097\n",
      "Epoch 45, Batch 26/32 : Loss = 0.0375676266849041\n",
      "Epoch 45, Batch 27/32 : Loss = 0.044774897396564484\n",
      "Epoch 45, Batch 28/32 : Loss = 0.029480384662747383\n",
      "Epoch 45, Batch 29/32 : Loss = 0.02595064602792263\n",
      "Epoch 45, Batch 30/32 : Loss = 0.0041377954185009\n",
      "Epoch 45, Batch 31/32 : Loss = 0.0038477585185319185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 finished in 0.039232869942982994 minutes\n",
      "Epoch 45 training_loss = 0.02059522643685341\n",
      "7---------T----C-----SS---ff-}}---?---L----55--j---K------ => 7TCSf}?L5jK, Ground Truth is 7TCSf}?L5jK\n",
      "6-------#---\"\"--bb----G----f---V---w-----L---h----m------- => 6#\"bGfVwLhm, Ground Truth is 6#\"bGfVwLhm\n",
      "Epoch 45 val_loss = 0.012146107852458954, word_accuracy = 0.9819277108433735\n",
      "Epoch 46, Batch 0/32 : Loss = 0.005688096396625042\n",
      "Epoch 46, Batch 1/32 : Loss = 0.04254947602748871\n",
      "Epoch 46, Batch 2/32 : Loss = 0.022610383108258247\n",
      "Epoch 46, Batch 3/32 : Loss = 0.007775827776640654\n",
      "Epoch 46, Batch 4/32 : Loss = 0.006255289539694786\n",
      "Epoch 46, Batch 5/32 : Loss = 0.0036757448688149452\n",
      "Epoch 46, Batch 6/32 : Loss = 0.036140140146017075\n",
      "Epoch 46, Batch 7/32 : Loss = 0.02710956335067749\n",
      "Epoch 46, Batch 8/32 : Loss = 0.015357332304120064\n",
      "Epoch 46, Batch 9/32 : Loss = 0.025659192353487015\n",
      "Epoch 46, Batch 10/32 : Loss = 0.005584828555583954\n",
      "Epoch 46, Batch 11/32 : Loss = 0.00837804190814495\n",
      "Epoch 46, Batch 12/32 : Loss = 0.0047112153843045235\n",
      "Epoch 46, Batch 13/32 : Loss = 0.004488443955779076\n",
      "Epoch 46, Batch 14/32 : Loss = 0.010288843885064125\n",
      "Epoch 46, Batch 15/32 : Loss = 0.0035412004217505455\n",
      "Epoch 46, Batch 16/32 : Loss = 0.003195947501808405\n",
      "Epoch 46, Batch 17/32 : Loss = 0.014846980571746826\n",
      "Epoch 46, Batch 18/32 : Loss = 0.02581479214131832\n",
      "Epoch 46, Batch 19/32 : Loss = 0.004455101676285267\n",
      "Epoch 46, Batch 20/32 : Loss = 0.01491463091224432\n",
      "Epoch 46, Batch 21/32 : Loss = 0.004383287392556667\n",
      "Epoch 46, Batch 22/32 : Loss = 0.0040636928752064705\n",
      "Epoch 46, Batch 23/32 : Loss = 0.005676619708538055\n",
      "Epoch 46, Batch 24/32 : Loss = 0.004363062791526318\n",
      "Epoch 46, Batch 25/32 : Loss = 0.0065679349936544895\n",
      "Epoch 46, Batch 26/32 : Loss = 0.004034583456814289\n",
      "Epoch 46, Batch 27/32 : Loss = 0.00840184185653925\n",
      "Epoch 46, Batch 28/32 : Loss = 0.002940295496955514\n",
      "Epoch 46, Batch 29/32 : Loss = 0.003938224632292986\n",
      "Epoch 46, Batch 30/32 : Loss = 0.007945137098431587\n",
      "Epoch 46, Batch 31/32 : Loss = 0.004132031928747892\n",
      "Epoch 46 finished in 0.03964409033457438 minutes\n",
      "Epoch 46 training_loss = 0.011112361215054989\n",
      "4--------H----%%----,--]-BB---D----&&---q----q---S------ => 4H%,]BD&qqS, Ground Truth is 4H%,]BD&qqS\n",
      "2------f--55----c---6----TT----+---66----Z----S----{---- => 2f5c6T+6ZS{, Ground Truth is 2f5c6T+6ZS{\n",
      "Epoch 46 val_loss = 0.007895316928625107, word_accuracy = 0.9899598393574297\n",
      "Epoch 47, Batch 0/32 : Loss = 0.015265622176229954\n",
      "Epoch 47, Batch 1/32 : Loss = 0.009870803914964199\n",
      "Epoch 47, Batch 2/32 : Loss = 0.004423202946782112\n",
      "Epoch 47, Batch 3/32 : Loss = 0.023738905787467957\n",
      "Epoch 47, Batch 4/32 : Loss = 0.004917108919471502\n",
      "Epoch 47, Batch 5/32 : Loss = 0.003262123093008995\n",
      "Epoch 47, Batch 6/32 : Loss = 0.007304009981453419\n",
      "Epoch 47, Batch 7/32 : Loss = 0.02449115552008152\n",
      "Epoch 47, Batch 8/32 : Loss = 0.006651738658547401\n",
      "Epoch 47, Batch 9/32 : Loss = 0.01368682086467743\n",
      "Epoch 47, Batch 10/32 : Loss = 0.005069251172244549\n",
      "Epoch 47, Batch 11/32 : Loss = 0.00674286438152194\n",
      "Epoch 47, Batch 12/32 : Loss = 0.006662337109446526\n",
      "Epoch 47, Batch 13/32 : Loss = 0.004649338312447071\n",
      "Epoch 47, Batch 14/32 : Loss = 0.0031561150681227446\n",
      "Epoch 47, Batch 15/32 : Loss = 0.0028669359162449837\n",
      "Epoch 47, Batch 16/32 : Loss = 0.009419497102499008\n",
      "Epoch 47, Batch 17/32 : Loss = 0.006740924436599016\n",
      "Epoch 47, Batch 18/32 : Loss = 0.0032164284493774176\n",
      "Epoch 47, Batch 19/32 : Loss = 0.005832602735608816\n",
      "Epoch 47, Batch 20/32 : Loss = 0.003206404158845544\n",
      "Epoch 47, Batch 21/32 : Loss = 0.004409424029290676\n",
      "Epoch 47, Batch 22/32 : Loss = 0.004805003292858601\n",
      "Epoch 47, Batch 23/32 : Loss = 0.00390461809001863\n",
      "Epoch 47, Batch 24/32 : Loss = 0.0025869891978800297\n",
      "Epoch 47, Batch 25/32 : Loss = 0.004696357063949108\n",
      "Epoch 47, Batch 26/32 : Loss = 0.0038355651777237654\n",
      "Epoch 47, Batch 27/32 : Loss = 0.006398179568350315\n",
      "Epoch 47, Batch 28/32 : Loss = 0.007417676039040089\n",
      "Epoch 47, Batch 29/32 : Loss = 0.0047193123027682304\n",
      "Epoch 47, Batch 30/32 : Loss = 0.024671092629432678\n",
      "Epoch 47, Batch 31/32 : Loss = 0.003460219595581293\n",
      "Epoch 47 finished in 0.0393303116162618 minutes\n",
      "Epoch 47 training_loss = 0.007680350448936224\n",
      "$------~--!--s---DD----9---a---P----s---$---]---- => $~!sD9aPs$], Ground Truth is $~!sD9aPs$]\n",
      "A--------x---0---0---f--$----7---,-O----((--7---- => Ax00f$7,O(7, Ground Truth is Ax00f$7,O(7\n",
      "Epoch 47 val_loss = 0.005870995577424765, word_accuracy = 0.9919678714859438\n",
      "Epoch 48, Batch 0/32 : Loss = 0.006162228062748909\n",
      "Epoch 48, Batch 1/32 : Loss = 0.0029893515165895224\n",
      "Epoch 48, Batch 2/32 : Loss = 0.023763198405504227\n",
      "Epoch 48, Batch 3/32 : Loss = 0.0032107033766806126\n",
      "Epoch 48, Batch 4/32 : Loss = 0.0022597569040954113\n",
      "Epoch 48, Batch 5/32 : Loss = 0.007494636345654726\n",
      "Epoch 48, Batch 6/32 : Loss = 0.0035202885046601295\n",
      "Epoch 48, Batch 7/32 : Loss = 0.012727833352982998\n",
      "Epoch 48, Batch 8/32 : Loss = 0.0022877268493175507\n",
      "Epoch 48, Batch 9/32 : Loss = 0.03107410855591297\n",
      "Epoch 48, Batch 10/32 : Loss = 0.016895974054932594\n",
      "Epoch 48, Batch 11/32 : Loss = 0.0026512346230447292\n",
      "Epoch 48, Batch 12/32 : Loss = 0.02096542716026306\n",
      "Epoch 48, Batch 13/32 : Loss = 0.0045563532039523125\n",
      "Epoch 48, Batch 14/32 : Loss = 0.015772294253110886\n",
      "Epoch 48, Batch 15/32 : Loss = 0.028274010866880417\n",
      "Epoch 48, Batch 16/32 : Loss = 0.0022211179602891207\n",
      "Epoch 48, Batch 17/32 : Loss = 0.006426525767892599\n",
      "Epoch 48, Batch 18/32 : Loss = 0.0031924794893711805\n",
      "Epoch 48, Batch 19/32 : Loss = 0.002881946275010705\n",
      "Epoch 48, Batch 20/32 : Loss = 0.0029953191988170147\n",
      "Epoch 48, Batch 21/32 : Loss = 0.0221851896494627\n",
      "Epoch 48, Batch 22/32 : Loss = 0.0037117234896868467\n",
      "Epoch 48, Batch 23/32 : Loss = 0.010572310537099838\n",
      "Epoch 48, Batch 24/32 : Loss = 0.00290994206443429\n",
      "Epoch 48, Batch 25/32 : Loss = 0.0051214988343417645\n",
      "Epoch 48, Batch 26/32 : Loss = 0.002308663446456194\n",
      "Epoch 48, Batch 27/32 : Loss = 0.002841734793037176\n",
      "Epoch 48, Batch 28/32 : Loss = 0.008003631606698036\n",
      "Epoch 48, Batch 29/32 : Loss = 0.004825357347726822\n",
      "Epoch 48, Batch 30/32 : Loss = 0.0026055339258164167\n",
      "Epoch 48, Batch 31/32 : Loss = 0.003273722715675831\n",
      "Epoch 48 finished in 0.03964900573094686 minutes\n",
      "Epoch 48 training_loss = 0.008604572154581547\n",
      ".----GG-----77---__-------Z----W-------n----W-------BB------ => .-G7_-ZWnWB, Ground Truth is .-G7_-ZWnWB\n",
      "x-------0---,--T---_---TT---2--``-aa---S---HH----$---b------ => x0,T_T2`aSH$b, Ground Truth is x0,T_T2`aSH$b\n",
      "Epoch 48 val_loss = 0.011213652789592743, word_accuracy = 0.9819277108433735\n",
      "Epoch 49, Batch 0/32 : Loss = 0.0076751383021473885\n",
      "Epoch 49, Batch 1/32 : Loss = 0.029427742585539818\n",
      "Epoch 49, Batch 2/32 : Loss = 0.003683013841509819\n",
      "Epoch 49, Batch 3/32 : Loss = 0.010732127353549004\n",
      "Epoch 49, Batch 4/32 : Loss = 0.004070707131177187\n",
      "Epoch 49, Batch 5/32 : Loss = 0.006191474851220846\n",
      "Epoch 49, Batch 6/32 : Loss = 0.003034341149032116\n",
      "Epoch 49, Batch 7/32 : Loss = 0.005339349620044231\n",
      "Epoch 49, Batch 8/32 : Loss = 0.002359120175242424\n",
      "Epoch 49, Batch 9/32 : Loss = 0.0057019926607608795\n",
      "Epoch 49, Batch 10/32 : Loss = 0.0036842175759375095\n",
      "Epoch 49, Batch 11/32 : Loss = 0.0036957436241209507\n",
      "Epoch 49, Batch 12/32 : Loss = 0.0024993084371089935\n",
      "Epoch 49, Batch 13/32 : Loss = 0.012480173259973526\n",
      "Epoch 49, Batch 14/32 : Loss = 0.01817706972360611\n",
      "Epoch 49, Batch 15/32 : Loss = 0.0036574481055140495\n",
      "Epoch 49, Batch 16/32 : Loss = 0.011694561690092087\n",
      "Epoch 49, Batch 17/32 : Loss = 0.002027159323915839\n",
      "Epoch 49, Batch 18/32 : Loss = 0.005778057500720024\n",
      "Epoch 49, Batch 19/32 : Loss = 0.0038279618602246046\n",
      "Epoch 49, Batch 20/32 : Loss = 0.0019281780114397407\n",
      "Epoch 49, Batch 21/32 : Loss = 0.012964929454028606\n",
      "Epoch 49, Batch 22/32 : Loss = 0.002794690430164337\n",
      "Epoch 49, Batch 23/32 : Loss = 0.006361604668200016\n",
      "Epoch 49, Batch 24/32 : Loss = 0.0056927623227238655\n",
      "Epoch 49, Batch 25/32 : Loss = 0.005347205325961113\n",
      "Epoch 49, Batch 26/32 : Loss = 0.01846269518136978\n",
      "Epoch 49, Batch 27/32 : Loss = 0.0027292585000395775\n",
      "Epoch 49, Batch 28/32 : Loss = 0.002829967765137553\n",
      "Epoch 49, Batch 29/32 : Loss = 0.029739465564489365\n",
      "Epoch 49, Batch 30/32 : Loss = 0.00305469729937613\n",
      "Epoch 49, Batch 31/32 : Loss = 0.005966528784483671\n",
      "Epoch 49 finished in 0.03951956828435262 minutes\n",
      "Epoch 49 training_loss = 0.007659050635993481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------ZZ---bb----w----jj-nn---l--1----T---'-*---ZZ--`-- => [Zbwjnl1T'*Z`, Ground Truth is [Zbwjnl1T'*Z`\n",
      "#------------N-----------A--------MM-----------?-------- => #NAM?, Ground Truth is #NAME?\n",
      "Epoch 49 val_loss = 0.012035340070724487, word_accuracy = 0.9799196787148594\n",
      "Epoch 50, Batch 0/32 : Loss = 0.0030342796817421913\n",
      "Epoch 50, Batch 1/32 : Loss = 0.004702065605670214\n",
      "Epoch 50, Batch 2/32 : Loss = 0.004082951694726944\n",
      "Epoch 50, Batch 3/32 : Loss = 0.005898067727684975\n",
      "Epoch 50, Batch 4/32 : Loss = 0.003347193356603384\n",
      "Epoch 50, Batch 5/32 : Loss = 0.003384146373718977\n",
      "Epoch 50, Batch 6/32 : Loss = 0.025025900453329086\n",
      "Epoch 50, Batch 7/32 : Loss = 0.00457946490496397\n",
      "Epoch 50, Batch 8/32 : Loss = 0.005200986284762621\n",
      "Epoch 50, Batch 9/32 : Loss = 0.006023431196808815\n",
      "Epoch 50, Batch 10/32 : Loss = 0.015936817973852158\n",
      "Epoch 50, Batch 11/32 : Loss = 0.04594119265675545\n",
      "Epoch 50, Batch 12/32 : Loss = 0.008331901393830776\n",
      "Epoch 50, Batch 13/32 : Loss = 0.04990652576088905\n",
      "Epoch 50, Batch 14/32 : Loss = 0.006103485822677612\n",
      "Epoch 50, Batch 15/32 : Loss = 0.003777693025767803\n",
      "Epoch 50, Batch 16/32 : Loss = 0.004259616136550903\n",
      "Epoch 50, Batch 17/32 : Loss = 0.01765010878443718\n",
      "Epoch 50, Batch 18/32 : Loss = 0.076627716422081\n",
      "Epoch 50, Batch 19/32 : Loss = 0.005312957800924778\n",
      "Epoch 50, Batch 20/32 : Loss = 0.011000169441103935\n",
      "Epoch 50, Batch 21/32 : Loss = 0.0038159191608428955\n",
      "Epoch 50, Batch 22/32 : Loss = 0.014149538241326809\n",
      "Epoch 50, Batch 23/32 : Loss = 0.010150819085538387\n",
      "Epoch 50, Batch 24/32 : Loss = 0.010793505236506462\n",
      "Epoch 50, Batch 25/32 : Loss = 0.005889316089451313\n",
      "Epoch 50, Batch 26/32 : Loss = 0.024967387318611145\n",
      "Epoch 50, Batch 27/32 : Loss = 0.028972459957003593\n",
      "Epoch 50, Batch 28/32 : Loss = 0.0046300217509269714\n",
      "Epoch 50, Batch 29/32 : Loss = 0.016529390588402748\n",
      "Epoch 50, Batch 30/32 : Loss = 0.006957274861633778\n",
      "Epoch 50, Batch 31/32 : Loss = 0.002520705806091428\n",
      "Epoch 50 finished in 0.03877738316853841 minutes\n",
      "Epoch 50 training_loss = 0.014049713499844074\n",
      ")----m-----PP--bb---W-----y---}--&---k---0---_--)---- => )mPbWy}&k0_), Ground Truth is )mPbWy}&k0_)\n",
      "o--------D---*---w----a---o--B---t-ii-R---]--Q---1--- => oD*waoBtiR]Q1, Ground Truth is OD*waoBtiR]Q1\n",
      "Epoch 50 val_loss = 0.013722604140639305, word_accuracy = 0.9839357429718876\n",
      "Epoch 51, Batch 0/32 : Loss = 0.003500425023958087\n",
      "Epoch 51, Batch 1/32 : Loss = 0.028330504894256592\n",
      "Epoch 51, Batch 2/32 : Loss = 0.007271005772054195\n",
      "Epoch 51, Batch 3/32 : Loss = 0.012578841298818588\n",
      "Epoch 51, Batch 4/32 : Loss = 0.006076047662645578\n",
      "Epoch 51, Batch 5/32 : Loss = 0.02498357743024826\n",
      "Epoch 51, Batch 6/32 : Loss = 0.02395702712237835\n",
      "Epoch 51, Batch 7/32 : Loss = 0.013932695612311363\n",
      "Epoch 51, Batch 8/32 : Loss = 0.005941496230661869\n",
      "Epoch 51, Batch 9/32 : Loss = 0.05287565663456917\n",
      "Epoch 51, Batch 10/32 : Loss = 0.007514511235058308\n",
      "Epoch 51, Batch 11/32 : Loss = 0.004204762168228626\n",
      "Epoch 51, Batch 12/32 : Loss = 0.00600059749558568\n",
      "Epoch 51, Batch 13/32 : Loss = 0.0037273261696100235\n",
      "Epoch 51, Batch 14/32 : Loss = 0.003478881437331438\n",
      "Epoch 51, Batch 15/32 : Loss = 0.002681063488125801\n",
      "Epoch 51, Batch 16/32 : Loss = 0.003111455123871565\n",
      "Epoch 51, Batch 17/32 : Loss = 0.0037559252232313156\n",
      "Epoch 51, Batch 18/32 : Loss = 0.0025876304134726524\n",
      "Epoch 51, Batch 19/32 : Loss = 0.004811297170817852\n",
      "Epoch 51, Batch 20/32 : Loss = 0.013569141738116741\n",
      "Epoch 51, Batch 21/32 : Loss = 0.0033373471815139055\n",
      "Epoch 51, Batch 22/32 : Loss = 0.018356546759605408\n",
      "Epoch 51, Batch 23/32 : Loss = 0.14508013427257538\n",
      "Epoch 51, Batch 24/32 : Loss = 0.03674884885549545\n",
      "Epoch 51, Batch 25/32 : Loss = 0.0033377530053257942\n",
      "Epoch 51, Batch 26/32 : Loss = 0.0036288220435380936\n",
      "Epoch 51, Batch 27/32 : Loss = 0.003926044330000877\n",
      "Epoch 51, Batch 28/32 : Loss = 0.003406886011362076\n",
      "Epoch 51, Batch 29/32 : Loss = 0.009190723299980164\n",
      "Epoch 51, Batch 30/32 : Loss = 0.004630052950233221\n",
      "Epoch 51, Batch 31/32 : Loss = 0.002412803005427122\n",
      "Epoch 51 finished in 0.040338798364003496 minutes\n",
      "Epoch 51 training_loss = 0.014998702332377434\n",
      "Y----------hh---hh----3----,---?---3---,-''--g----6-----O-----gg----- => Yhh3,?3,'g6Og, Ground Truth is Yhh3,?3,'g6Og\n",
      "?------*----Q-----<----3----MM----->-----c---W------DD----0---FF----- => ?*Q<3M>cWD0F, Ground Truth is ?*Q<3M>cWD0F\n",
      "Epoch 51 val_loss = 0.020301679149270058, word_accuracy = 0.9879518072289156\n",
      "Epoch 52, Batch 0/32 : Loss = 0.05401669070124626\n",
      "Epoch 52, Batch 1/32 : Loss = 0.009340362623333931\n",
      "Epoch 52, Batch 2/32 : Loss = 0.004650759045034647\n",
      "Epoch 52, Batch 3/32 : Loss = 0.004327870439738035\n",
      "Epoch 52, Batch 4/32 : Loss = 0.11207685619592667\n",
      "Epoch 52, Batch 5/32 : Loss = 0.0024161487817764282\n",
      "Epoch 52, Batch 6/32 : Loss = 0.005602763965725899\n",
      "Epoch 52, Batch 7/32 : Loss = 0.0029190643690526485\n",
      "Epoch 52, Batch 8/32 : Loss = 0.0058027105405926704\n",
      "Epoch 52, Batch 9/32 : Loss = 0.015877697616815567\n",
      "Epoch 52, Batch 10/32 : Loss = 0.00485384464263916\n",
      "Epoch 52, Batch 11/32 : Loss = 0.0056221941486001015\n",
      "Epoch 52, Batch 12/32 : Loss = 0.0077998582273721695\n",
      "Epoch 52, Batch 13/32 : Loss = 0.005440667271614075\n",
      "Epoch 52, Batch 14/32 : Loss = 0.010208957828581333\n",
      "Epoch 52, Batch 15/32 : Loss = 0.00784999318420887\n",
      "Epoch 52, Batch 16/32 : Loss = 0.004260147921741009\n",
      "Epoch 52, Batch 17/32 : Loss = 0.00289348722435534\n",
      "Epoch 52, Batch 18/32 : Loss = 0.008439142256975174\n",
      "Epoch 52, Batch 19/32 : Loss = 0.03444437310099602\n",
      "Epoch 52, Batch 20/32 : Loss = 0.005241710226982832\n",
      "Epoch 52, Batch 21/32 : Loss = 0.003916849382221699\n",
      "Epoch 52, Batch 22/32 : Loss = 0.01361718401312828\n",
      "Epoch 52, Batch 23/32 : Loss = 0.002792446641251445\n",
      "Epoch 52, Batch 24/32 : Loss = 0.008693983778357506\n",
      "Epoch 52, Batch 25/32 : Loss = 0.0057821013033390045\n",
      "Epoch 52, Batch 26/32 : Loss = 0.0063009439036250114\n",
      "Epoch 52, Batch 27/32 : Loss = 0.0024606299120932817\n",
      "Epoch 52, Batch 28/32 : Loss = 0.0034107642713934183\n",
      "Epoch 52, Batch 29/32 : Loss = 0.0038384830113500357\n",
      "Epoch 52, Batch 30/32 : Loss = 0.002941088518127799\n",
      "Epoch 52, Batch 31/32 : Loss = 0.004601787310093641\n",
      "Epoch 52 finished in 0.0395962913831075 minutes\n",
      "Epoch 52 training_loss = 0.011836626566946507\n",
      "U--------$$---1---\"--f--#----r-``--&----1---W------6--- => U$1\"f#r`&1W6, Ground Truth is U$1\"f#r`&1W6\n",
      "G--------J---mm-----\\\\--F----22--(---r--8---**---=----- => GJm\\F2(r8*=, Ground Truth is GJm\\F2(r8*=\n",
      "Epoch 52 val_loss = 0.008838382549583912, word_accuracy = 0.9899598393574297\n",
      "Epoch 53, Batch 0/32 : Loss = 0.002556957770138979\n",
      "Epoch 53, Batch 1/32 : Loss = 0.0023108450695872307\n",
      "Epoch 53, Batch 2/32 : Loss = 0.008877295069396496\n",
      "Epoch 53, Batch 3/32 : Loss = 0.004412468057125807\n",
      "Epoch 53, Batch 4/32 : Loss = 0.0027539569418877363\n",
      "Epoch 53, Batch 5/32 : Loss = 0.005989070050418377\n",
      "Epoch 53, Batch 6/32 : Loss = 0.035558804869651794\n",
      "Epoch 53, Batch 7/32 : Loss = 0.01864277571439743\n",
      "Epoch 53, Batch 8/32 : Loss = 0.0022586139384657145\n",
      "Epoch 53, Batch 9/32 : Loss = 0.00804483238607645\n",
      "Epoch 53, Batch 10/32 : Loss = 0.029411936178803444\n",
      "Epoch 53, Batch 11/32 : Loss = 0.01608576625585556\n",
      "Epoch 53, Batch 12/32 : Loss = 0.005216613411903381\n",
      "Epoch 53, Batch 13/32 : Loss = 0.007232490926980972\n",
      "Epoch 53, Batch 14/32 : Loss = 0.005466373637318611\n",
      "Epoch 53, Batch 15/32 : Loss = 0.004181230440735817\n",
      "Epoch 53, Batch 16/32 : Loss = 0.002507661236450076\n",
      "Epoch 53, Batch 17/32 : Loss = 0.007817110046744347\n",
      "Epoch 53, Batch 18/32 : Loss = 0.1945694386959076\n",
      "Epoch 53, Batch 19/32 : Loss = 0.013196798972785473\n",
      "Epoch 53, Batch 20/32 : Loss = 0.0023509333841502666\n",
      "Epoch 53, Batch 21/32 : Loss = 0.0019500497728586197\n",
      "Epoch 53, Batch 22/32 : Loss = 0.00471190270036459\n",
      "Epoch 53, Batch 23/32 : Loss = 0.008851498365402222\n",
      "Epoch 53, Batch 24/32 : Loss = 0.011024435050785542\n",
      "Epoch 53, Batch 25/32 : Loss = 0.003873090259730816\n",
      "Epoch 53, Batch 26/32 : Loss = 0.0041350312530994415\n",
      "Epoch 53, Batch 27/32 : Loss = 0.006769781932234764\n",
      "Epoch 53, Batch 28/32 : Loss = 0.007572912611067295\n",
      "Epoch 53, Batch 29/32 : Loss = 0.05263209342956543\n",
      "Epoch 53, Batch 30/32 : Loss = 0.007343009114265442\n",
      "Epoch 53, Batch 31/32 : Loss = 0.07004696875810623\n",
      "Epoch 53 finished in 0.039085630575815836 minutes\n",
      "Epoch 53 training_loss = 0.01596985012292862\n",
      ".-----G------7---__-------Z----W-------n----WW------BB------ => .-G7_-ZWnWB, Ground Truth is .-G7_-ZWnWB\n",
      "d----------4----4---f--n-----S----z--v--[[--{-ii--A---ff---- => d44fnSzv[{iAf, Ground Truth is d44fnSzv[{iAf\n",
      "Epoch 53 val_loss = 0.009418231435120106, word_accuracy = 0.9859437751004017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Batch 0/32 : Loss = 0.008731596171855927\n",
      "Epoch 54, Batch 1/32 : Loss = 0.005935053341090679\n",
      "Epoch 54, Batch 2/32 : Loss = 0.00529771763831377\n",
      "Epoch 54, Batch 3/32 : Loss = 0.033912405371665955\n",
      "Epoch 54, Batch 4/32 : Loss = 0.004938686732202768\n",
      "Epoch 54, Batch 5/32 : Loss = 0.0031809660140424967\n",
      "Epoch 54, Batch 6/32 : Loss = 0.009416580200195312\n",
      "Epoch 54, Batch 7/32 : Loss = 0.007733281701803207\n",
      "Epoch 54, Batch 8/32 : Loss = 0.010174406692385674\n",
      "Epoch 54, Batch 9/32 : Loss = 0.0026932437904179096\n",
      "Epoch 54, Batch 10/32 : Loss = 0.003408984746783972\n",
      "Epoch 54, Batch 11/32 : Loss = 0.002721723634749651\n",
      "Epoch 54, Batch 12/32 : Loss = 0.0045124925673007965\n",
      "Epoch 54, Batch 13/32 : Loss = 0.0046781571581959724\n",
      "Epoch 54, Batch 14/32 : Loss = 0.005267012398689985\n",
      "Epoch 54, Batch 15/32 : Loss = 0.0029919790104031563\n",
      "Epoch 54, Batch 16/32 : Loss = 0.03210350498557091\n",
      "Epoch 54, Batch 17/32 : Loss = 0.004619387909770012\n",
      "Epoch 54, Batch 18/32 : Loss = 0.005527045112103224\n",
      "Epoch 54, Batch 19/32 : Loss = 0.003178603947162628\n",
      "Epoch 54, Batch 20/32 : Loss = 0.0029497812502086163\n",
      "Epoch 54, Batch 21/32 : Loss = 0.003256293246522546\n",
      "Epoch 54, Batch 22/32 : Loss = 0.006286203395575285\n",
      "Epoch 54, Batch 23/32 : Loss = 0.03668231517076492\n",
      "Epoch 54, Batch 24/32 : Loss = 0.003515058197081089\n",
      "Epoch 54, Batch 25/32 : Loss = 0.012887942604720592\n",
      "Epoch 54, Batch 26/32 : Loss = 0.005803240463137627\n",
      "Epoch 54, Batch 27/32 : Loss = 0.002005915157496929\n",
      "Epoch 54, Batch 28/32 : Loss = 0.004968828521668911\n",
      "Epoch 54, Batch 29/32 : Loss = 0.03731394559144974\n",
      "Epoch 54, Batch 30/32 : Loss = 0.008217046037316322\n",
      "Epoch 54, Batch 31/32 : Loss = 0.00454031303524971\n",
      "Epoch 54 finished in 0.03905563751856486 minutes\n",
      "Epoch 54 training_loss = 0.00917194876819849\n",
      "n-------}--H-----G----0---o--hh---+---^--|---<----%------- => n}HG0oh+^|<%, Ground Truth is n}HG0oh+^|<%\n",
      "S--------[---K----\"\"----d-----N------3-----9----y--``--**- => S[K\"dN39y`*, Ground Truth is S[K\"dN39y`*\n",
      "Epoch 54 val_loss = 0.008073155768215656, word_accuracy = 0.9839357429718876\n",
      "Epoch 55, Batch 0/32 : Loss = 0.002354747150093317\n",
      "Epoch 55, Batch 1/32 : Loss = 0.004618654027581215\n",
      "Epoch 55, Batch 2/32 : Loss = 0.0033075588289648294\n",
      "Epoch 55, Batch 3/32 : Loss = 0.007111200597137213\n",
      "Epoch 55, Batch 4/32 : Loss = 0.002129772212356329\n",
      "Epoch 55, Batch 5/32 : Loss = 0.004112827125936747\n",
      "Epoch 55, Batch 6/32 : Loss = 0.005359529051929712\n",
      "Epoch 55, Batch 7/32 : Loss = 0.004357947967946529\n",
      "Epoch 55, Batch 8/32 : Loss = 0.003264468628913164\n",
      "Epoch 55, Batch 9/32 : Loss = 0.0028066730592399836\n",
      "Epoch 55, Batch 10/32 : Loss = 0.009131264872848988\n",
      "Epoch 55, Batch 11/32 : Loss = 0.015433919616043568\n",
      "Epoch 55, Batch 12/32 : Loss = 0.0018229273846372962\n",
      "Epoch 55, Batch 13/32 : Loss = 0.012773905880749226\n",
      "Epoch 55, Batch 14/32 : Loss = 0.012499848380684853\n",
      "Epoch 55, Batch 15/32 : Loss = 0.012742466293275356\n",
      "Epoch 55, Batch 16/32 : Loss = 0.009943823330104351\n",
      "Epoch 55, Batch 17/32 : Loss = 0.0034529301337897778\n",
      "Epoch 55, Batch 18/32 : Loss = 0.04586997255682945\n",
      "Epoch 55, Batch 19/32 : Loss = 0.027134642004966736\n",
      "Epoch 55, Batch 20/32 : Loss = 0.0031072525307536125\n",
      "Epoch 55, Batch 21/32 : Loss = 0.009452391415834427\n",
      "Epoch 55, Batch 22/32 : Loss = 0.00928481388837099\n",
      "Epoch 55, Batch 23/32 : Loss = 0.004299013875424862\n",
      "Epoch 55, Batch 24/32 : Loss = 0.004770778119564056\n",
      "Epoch 55, Batch 25/32 : Loss = 0.022112421691417694\n",
      "Epoch 55, Batch 26/32 : Loss = 0.002402115613222122\n",
      "Epoch 55, Batch 27/32 : Loss = 0.00312799122184515\n",
      "Epoch 55, Batch 28/32 : Loss = 0.005590395070612431\n",
      "Epoch 55, Batch 29/32 : Loss = 0.004694700241088867\n",
      "Epoch 55, Batch 30/32 : Loss = 0.036624401807785034\n",
      "Epoch 55, Batch 31/32 : Loss = 0.010858077555894852\n",
      "Epoch 55 finished in 0.03990431626637777 minutes\n",
      "Epoch 55 training_loss = 0.009543858468532562\n",
      "S---------[[--K----\"\"---dd----N------3----9----y--`** => S[K\"dN39y`*, Ground Truth is S[K\"dN39y`*\n",
      "g--------7---O----n---#--->----F---\\--N----m------1-- => g7On#>F\\Nm1, Ground Truth is g7On#>F\\Nm1\n",
      "Epoch 55 val_loss = 0.00978098064661026, word_accuracy = 0.9839357429718876\n",
      "Epoch 56, Batch 0/32 : Loss = 0.003702746704220772\n",
      "Epoch 56, Batch 1/32 : Loss = 0.005710427649319172\n",
      "Epoch 56, Batch 2/32 : Loss = 0.004072732757776976\n",
      "Epoch 56, Batch 3/32 : Loss = 0.006089494563639164\n",
      "Epoch 56, Batch 4/32 : Loss = 0.0025982647202908993\n",
      "Epoch 56, Batch 5/32 : Loss = 0.010465320199728012\n",
      "Epoch 56, Batch 6/32 : Loss = 0.05411187559366226\n",
      "Epoch 56, Batch 7/32 : Loss = 0.004134831018745899\n",
      "Epoch 56, Batch 8/32 : Loss = 0.0046926224604249\n",
      "Epoch 56, Batch 9/32 : Loss = 0.00686075072735548\n",
      "Epoch 56, Batch 10/32 : Loss = 0.0034038962330669165\n",
      "Epoch 56, Batch 11/32 : Loss = 0.013441651128232479\n",
      "Epoch 56, Batch 12/32 : Loss = 0.013025260530412197\n",
      "Epoch 56, Batch 13/32 : Loss = 0.013321292586624622\n",
      "Epoch 56, Batch 14/32 : Loss = 0.005586707964539528\n",
      "Epoch 56, Batch 15/32 : Loss = 0.00369350821711123\n",
      "Epoch 56, Batch 16/32 : Loss = 0.0032950956374406815\n",
      "Epoch 56, Batch 17/32 : Loss = 0.00639489758759737\n",
      "Epoch 56, Batch 18/32 : Loss = 0.004973190370947123\n",
      "Epoch 56, Batch 19/32 : Loss = 0.0031303507275879383\n",
      "Epoch 56, Batch 20/32 : Loss = 0.005676284432411194\n",
      "Epoch 56, Batch 21/32 : Loss = 0.006750911474227905\n",
      "Epoch 56, Batch 22/32 : Loss = 0.0026176602113991976\n",
      "Epoch 56, Batch 23/32 : Loss = 0.004706454463303089\n",
      "Epoch 56, Batch 24/32 : Loss = 0.011647449806332588\n",
      "Epoch 56, Batch 25/32 : Loss = 0.009151061996817589\n",
      "Epoch 56, Batch 26/32 : Loss = 0.0026441561058163643\n",
      "Epoch 56, Batch 27/32 : Loss = 0.003912057261914015\n",
      "Epoch 56, Batch 28/32 : Loss = 0.009607848711311817\n",
      "Epoch 56, Batch 29/32 : Loss = 0.010581234470009804\n",
      "Epoch 56, Batch 30/32 : Loss = 0.009766755625605583\n",
      "Epoch 56, Batch 31/32 : Loss = 0.4095071852207184\n",
      "Epoch 56 finished in 0.038607625166575114 minutes\n",
      "Epoch 56 training_loss = 0.009669242426753044\n",
      "r----u---__--&---c---g---]-8----@----I---T---VV----- => ru_&cg]8@ITV, Ground Truth is ru_&cg]8@I`TV\n",
      "S------[[--K---\"\"---d----N-----3----9---y---`--*---- => S[K\"dN39y`*, Ground Truth is S[K\"dN39y`*\n",
      "Epoch 56 val_loss = 0.009615563787519932, word_accuracy = 0.9819277108433735\n",
      "Epoch 57, Batch 0/32 : Loss = 0.0021705785766243935\n",
      "Epoch 57, Batch 1/32 : Loss = 0.0039498694241046906\n",
      "Epoch 57, Batch 2/32 : Loss = 0.005860372446477413\n",
      "Epoch 57, Batch 3/32 : Loss = 0.0029500387609004974\n",
      "Epoch 57, Batch 4/32 : Loss = 0.0033962582238018513\n",
      "Epoch 57, Batch 5/32 : Loss = 0.21962441504001617\n",
      "Epoch 57, Batch 6/32 : Loss = 0.003115636296570301\n",
      "Epoch 57, Batch 7/32 : Loss = 0.01666850410401821\n",
      "Epoch 57, Batch 8/32 : Loss = 0.01768413931131363\n",
      "Epoch 57, Batch 9/32 : Loss = 0.004463945981115103\n",
      "Epoch 57, Batch 10/32 : Loss = 0.00785837136209011\n",
      "Epoch 57, Batch 11/32 : Loss = 0.004054440651088953\n",
      "Epoch 57, Batch 12/32 : Loss = 0.006550234742462635\n",
      "Epoch 57, Batch 13/32 : Loss = 0.01132955215871334\n",
      "Epoch 57, Batch 14/32 : Loss = 0.010800279676914215\n",
      "Epoch 57, Batch 15/32 : Loss = 0.3783375322818756\n",
      "Epoch 57, Batch 16/32 : Loss = 0.01366116851568222\n",
      "Epoch 57, Batch 17/32 : Loss = 0.18768931925296783\n",
      "Epoch 57, Batch 18/32 : Loss = 0.004043044056743383\n",
      "Epoch 57, Batch 19/32 : Loss = 0.005806430708616972\n",
      "Epoch 57, Batch 20/32 : Loss = 0.006310929078608751\n",
      "Epoch 57, Batch 21/32 : Loss = 0.005535230506211519\n",
      "Epoch 57, Batch 22/32 : Loss = 0.06888708472251892\n",
      "Epoch 57, Batch 23/32 : Loss = 0.018287811428308487\n",
      "Epoch 57, Batch 24/32 : Loss = 0.00925050862133503\n",
      "Epoch 57, Batch 25/32 : Loss = 0.0034805363975465298\n",
      "Epoch 57, Batch 26/32 : Loss = 0.011620189994573593\n",
      "Epoch 57, Batch 27/32 : Loss = 0.06914426386356354\n",
      "Epoch 57, Batch 28/32 : Loss = 0.005101905204355717\n",
      "Epoch 57, Batch 29/32 : Loss = 0.00666677625849843\n",
      "Epoch 57, Batch 30/32 : Loss = 0.028136776760220528\n",
      "Epoch 57, Batch 31/32 : Loss = 0.009564606472849846\n",
      "Epoch 57 finished in 0.04002037048339844 minutes\n",
      "Epoch 57 training_loss = 0.03674318268895149\n",
      "c--------DD---j--4---}--i--C---->----*---e---6----c--- => cDj4}iC>*e6c, Ground Truth is cDj4}iC>*e6c\n",
      "y-------3---yy--s---^--W------>---)---G----9--!--R---- => y3ys^W>)G9!R, Ground Truth is y3ys^W>)G9!R\n",
      "Epoch 57 val_loss = 0.02590637281537056, word_accuracy = 0.9779116465863453\n",
      "Epoch 58, Batch 0/32 : Loss = 0.005880481097847223\n",
      "Epoch 58, Batch 1/32 : Loss = 0.007727875839918852\n",
      "Epoch 58, Batch 2/32 : Loss = 0.08458582311868668\n",
      "Epoch 58, Batch 3/32 : Loss = 0.009030908346176147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Batch 4/32 : Loss = 0.004839289467781782\n",
      "Epoch 58, Batch 5/32 : Loss = 0.006613275036215782\n",
      "Epoch 58, Batch 6/32 : Loss = 0.004420864395797253\n",
      "Epoch 58, Batch 7/32 : Loss = 0.013475058600306511\n",
      "Epoch 58, Batch 8/32 : Loss = 0.005952258128672838\n",
      "Epoch 58, Batch 9/32 : Loss = 0.0036001517437398434\n",
      "Epoch 58, Batch 10/32 : Loss = 0.008478915318846703\n",
      "Epoch 58, Batch 11/32 : Loss = 0.013897689059376717\n",
      "Epoch 58, Batch 12/32 : Loss = 0.006427803076803684\n",
      "Epoch 58, Batch 13/32 : Loss = 0.006735915318131447\n",
      "Epoch 58, Batch 14/32 : Loss = 0.0033837389200925827\n",
      "Epoch 58, Batch 15/32 : Loss = 0.2437545657157898\n",
      "Epoch 58, Batch 16/32 : Loss = 0.1251290887594223\n",
      "Epoch 58, Batch 17/32 : Loss = 0.02457866072654724\n",
      "Epoch 58, Batch 18/32 : Loss = 0.006585007533431053\n",
      "Epoch 58, Batch 19/32 : Loss = 0.010924413800239563\n",
      "Epoch 58, Batch 20/32 : Loss = 0.07792551070451736\n",
      "Epoch 58, Batch 21/32 : Loss = 0.0494406558573246\n",
      "Epoch 58, Batch 22/32 : Loss = 0.015345939435064793\n",
      "Epoch 58, Batch 23/32 : Loss = 0.015208378434181213\n",
      "Epoch 58, Batch 24/32 : Loss = 0.018619421869516373\n",
      "Epoch 58, Batch 25/32 : Loss = 0.05748910456895828\n",
      "Epoch 58, Batch 26/32 : Loss = 0.04500062018632889\n",
      "Epoch 58, Batch 27/32 : Loss = 0.018041398376226425\n",
      "Epoch 58, Batch 28/32 : Loss = 0.03371562063694\n",
      "Epoch 58, Batch 29/32 : Loss = 0.010167436674237251\n",
      "Epoch 58, Batch 30/32 : Loss = 0.00995571631938219\n",
      "Epoch 58, Batch 31/32 : Loss = 0.09670497477054596\n",
      "Epoch 58 finished in 0.03882056474685669 minutes\n",
      "Epoch 58 training_loss = 0.030811872333288193\n",
      "k------}}--O------T---)---O-----M------~--->>---HH-----2---w------- => k}OT)OM~>H2w, Ground Truth is k}OT)OM~>H2w\n",
      "x---------0-----TT-----C------^^--:---0-----<------aa-----q-----*-- => x0TC^:0<aq*, Ground Truth is x0TC^:0<aq*\n",
      "Epoch 58 val_loss = 0.0448002964258194, word_accuracy = 0.9437751004016064\n",
      "Epoch 59, Batch 0/32 : Loss = 0.020269233733415604\n",
      "Epoch 59, Batch 1/32 : Loss = 0.018270140513777733\n",
      "Epoch 59, Batch 2/32 : Loss = 0.01150871254503727\n",
      "Epoch 59, Batch 3/32 : Loss = 0.008552523329854012\n",
      "Epoch 59, Batch 4/32 : Loss = 0.011081523261964321\n",
      "Epoch 59, Batch 5/32 : Loss = 0.019880782812833786\n",
      "Epoch 59, Batch 6/32 : Loss = 0.010408816859126091\n",
      "Epoch 59, Batch 7/32 : Loss = 0.012703826650977135\n",
      "Epoch 59, Batch 8/32 : Loss = 0.012946544215083122\n",
      "Epoch 59, Batch 9/32 : Loss = 0.008230158127844334\n",
      "Epoch 59, Batch 10/32 : Loss = 0.006275590043514967\n",
      "Epoch 59, Batch 11/32 : Loss = 0.009893786162137985\n",
      "Epoch 59, Batch 12/32 : Loss = 0.12592923641204834\n",
      "Epoch 59, Batch 13/32 : Loss = 0.004173303954303265\n",
      "Epoch 59, Batch 14/32 : Loss = 0.0826466903090477\n",
      "Epoch 59, Batch 15/32 : Loss = 0.05149971321225166\n",
      "Epoch 59, Batch 16/32 : Loss = 0.12934957444667816\n",
      "Epoch 59, Batch 17/32 : Loss = 0.0159306600689888\n",
      "Epoch 59, Batch 18/32 : Loss = 0.013330642133951187\n",
      "Epoch 59, Batch 19/32 : Loss = 0.007704503368586302\n",
      "Epoch 59, Batch 20/32 : Loss = 0.041345980018377304\n",
      "Epoch 59, Batch 21/32 : Loss = 0.005228244699537754\n",
      "Epoch 59, Batch 22/32 : Loss = 0.020572125911712646\n",
      "Epoch 59, Batch 23/32 : Loss = 0.026599014177918434\n",
      "Epoch 59, Batch 24/32 : Loss = 0.011862319894134998\n",
      "Epoch 59, Batch 25/32 : Loss = 0.030101269483566284\n",
      "Epoch 59, Batch 26/32 : Loss = 0.07550666481256485\n",
      "Epoch 59, Batch 27/32 : Loss = 0.20013755559921265\n",
      "Epoch 59, Batch 28/32 : Loss = 0.024532154202461243\n",
      "Epoch 59, Batch 29/32 : Loss = 0.02032359316945076\n",
      "Epoch 59, Batch 30/32 : Loss = 0.02263723500072956\n",
      "Epoch 59, Batch 31/32 : Loss = 0.009565684013068676\n",
      "Epoch 59 finished in 0.03908919890721639 minutes\n",
      "Epoch 59 training_loss = 0.03407639265060425\n",
      "}------z---@-----~---j--N-----F---5----C---!!--9---1---n----- => }z@~jNF5C!91n, Ground Truth is }z@~jNF5C!91n\n",
      "S----------[[---k-----\"----dd----NN------33----99----yy--`*-- => S[k\"dN39y`*, Ground Truth is S[K\"dN39y`*\n",
      "Epoch 59 val_loss = 0.024721955880522728, word_accuracy = 0.9578313253012049\n",
      "Epoch 60, Batch 0/32 : Loss = 0.003992744721472263\n",
      "Epoch 60, Batch 1/32 : Loss = 0.05251748114824295\n",
      "Epoch 60, Batch 2/32 : Loss = 0.06282772123813629\n",
      "Epoch 60, Batch 3/32 : Loss = 0.005673506762832403\n",
      "Epoch 60, Batch 4/32 : Loss = 0.01328399870544672\n",
      "Epoch 60, Batch 5/32 : Loss = 0.015911096706986427\n",
      "Epoch 60, Batch 6/32 : Loss = 0.00677441107109189\n",
      "Epoch 60, Batch 7/32 : Loss = 0.008766863495111465\n",
      "Epoch 60, Batch 8/32 : Loss = 0.027089133858680725\n",
      "Epoch 60, Batch 9/32 : Loss = 0.01617790199816227\n",
      "Epoch 60, Batch 10/32 : Loss = 0.0496450737118721\n",
      "Epoch 60, Batch 11/32 : Loss = 0.012521307915449142\n",
      "Epoch 60, Batch 12/32 : Loss = 0.019407765939831734\n",
      "Epoch 60, Batch 13/32 : Loss = 0.015983380377292633\n",
      "Epoch 60, Batch 14/32 : Loss = 0.0637345090508461\n",
      "Epoch 60, Batch 15/32 : Loss = 0.016178667545318604\n",
      "Epoch 60, Batch 16/32 : Loss = 0.007125989533960819\n",
      "Epoch 60, Batch 17/32 : Loss = 0.005704340524971485\n",
      "Epoch 60, Batch 18/32 : Loss = 0.005132331047207117\n",
      "Epoch 60, Batch 19/32 : Loss = 0.005620627198368311\n",
      "Epoch 60, Batch 20/32 : Loss = 0.006326922681182623\n",
      "Epoch 60, Batch 21/32 : Loss = 0.00407240679487586\n",
      "Epoch 60, Batch 22/32 : Loss = 0.008497343398630619\n",
      "Epoch 60, Batch 23/32 : Loss = 0.01350865513086319\n",
      "Epoch 60, Batch 24/32 : Loss = 0.0052548972889781\n",
      "Epoch 60, Batch 25/32 : Loss = 0.006713945884257555\n",
      "Epoch 60, Batch 26/32 : Loss = 0.008666746318340302\n",
      "Epoch 60, Batch 27/32 : Loss = 0.040965862572193146\n",
      "Epoch 60, Batch 28/32 : Loss = 0.05086567625403404\n",
      "Epoch 60, Batch 29/32 : Loss = 0.004556654486805201\n",
      "Epoch 60, Batch 30/32 : Loss = 0.014711059629917145\n",
      "Epoch 60, Batch 31/32 : Loss = 0.004496927373111248\n",
      "Epoch 60 finished in 0.0390859325726827 minutes\n",
      "Epoch 60 training_loss = 0.018595056608319283\n",
      "~-------T--WW-----d---1---^---=--P---dd--00--}--+---\\--- => ~TWd1^=Pd0}+\\, Ground Truth is ~TWd1^=Pd0}+\\\n",
      "K-------]--C-----G----1---z--B----%-----o---\\--zz--c---- => K]CG1zB%o\\zc, Ground Truth is K]CG1zB%o\\zc\n",
      "Epoch 60 val_loss = 0.015641558915376663, word_accuracy = 0.963855421686747\n",
      "Epoch 61, Batch 0/32 : Loss = 0.003029633779078722\n",
      "Epoch 61, Batch 1/32 : Loss = 0.00712797325104475\n",
      "Epoch 61, Batch 2/32 : Loss = 0.016222180798649788\n",
      "Epoch 61, Batch 3/32 : Loss = 0.028825923800468445\n",
      "Epoch 61, Batch 4/32 : Loss = 0.05660964176058769\n",
      "Epoch 61, Batch 5/32 : Loss = 0.0061109429225325584\n",
      "Epoch 61, Batch 6/32 : Loss = 0.017016157507896423\n",
      "Epoch 61, Batch 7/32 : Loss = 0.015216517262160778\n",
      "Epoch 61, Batch 8/32 : Loss = 0.00818725023418665\n",
      "Epoch 61, Batch 9/32 : Loss = 0.07060442864894867\n",
      "Epoch 61, Batch 10/32 : Loss = 0.009647050872445107\n",
      "Epoch 61, Batch 11/32 : Loss = 0.006441507488489151\n",
      "Epoch 61, Batch 12/32 : Loss = 0.008111614733934402\n",
      "Epoch 61, Batch 13/32 : Loss = 0.009270623326301575\n",
      "Epoch 61, Batch 14/32 : Loss = 0.0045142099261283875\n",
      "Epoch 61, Batch 15/32 : Loss = 0.01606879010796547\n",
      "Epoch 61, Batch 16/32 : Loss = 0.10986994951963425\n",
      "Epoch 61, Batch 17/32 : Loss = 0.005737172439694405\n",
      "Epoch 61, Batch 18/32 : Loss = 0.027233991771936417\n",
      "Epoch 61, Batch 19/32 : Loss = 0.01616813987493515\n",
      "Epoch 61, Batch 20/32 : Loss = 0.10851301997900009\n",
      "Epoch 61, Batch 21/32 : Loss = 0.007661889307200909\n",
      "Epoch 61, Batch 22/32 : Loss = 0.01630191132426262\n",
      "Epoch 61, Batch 23/32 : Loss = 0.015154862776398659\n",
      "Epoch 61, Batch 24/32 : Loss = 0.008447770029306412\n",
      "Epoch 61, Batch 25/32 : Loss = 0.022591006010770798\n",
      "Epoch 61, Batch 26/32 : Loss = 0.019364893436431885\n",
      "Epoch 61, Batch 27/32 : Loss = 0.00645980890840292\n",
      "Epoch 61, Batch 28/32 : Loss = 0.007252159062772989\n",
      "Epoch 61, Batch 29/32 : Loss = 0.006095314398407936\n",
      "Epoch 61, Batch 30/32 : Loss = 0.03369041532278061\n",
      "Epoch 61, Batch 31/32 : Loss = 0.010046510025858879\n",
      "Epoch 61 finished in 0.03982307513554891 minutes\n",
      "Epoch 61 training_loss = 0.022322971373796463\n",
      "G-------JJ---m------\\---F---22---(--rr--8---**---=--- => GJm\\F2(r8*=, Ground Truth is GJm\\F2(r8*=\n",
      "P-------7---]--I-}--R----&---zz---@----r--f--~---?--- => P7]I}R&z@rf~?, Ground Truth is P7]I}R&z@rf~?\n",
      "Epoch 61 val_loss = 0.013804043643176556, word_accuracy = 0.9819277108433735\n",
      "Epoch 62, Batch 0/32 : Loss = 0.04617690294981003\n",
      "Epoch 62, Batch 1/32 : Loss = 0.015728546306490898\n",
      "Epoch 62, Batch 2/32 : Loss = 0.01002957858145237\n",
      "Epoch 62, Batch 3/32 : Loss = 0.011391442269086838\n",
      "Epoch 62, Batch 4/32 : Loss = 0.01670212857425213\n",
      "Epoch 62, Batch 5/32 : Loss = 0.007391727529466152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Batch 6/32 : Loss = 0.011837257072329521\n",
      "Epoch 62, Batch 7/32 : Loss = 0.037620943039655685\n",
      "Epoch 62, Batch 8/32 : Loss = 0.006649806629866362\n",
      "Epoch 62, Batch 9/32 : Loss = 0.006038148887455463\n",
      "Epoch 62, Batch 10/32 : Loss = 0.004762898199260235\n",
      "Epoch 62, Batch 11/32 : Loss = 0.033533647656440735\n",
      "Epoch 62, Batch 12/32 : Loss = 0.004278417211025953\n",
      "Epoch 62, Batch 13/32 : Loss = 0.03304586559534073\n",
      "Epoch 62, Batch 14/32 : Loss = 0.005791589617729187\n",
      "Epoch 62, Batch 15/32 : Loss = 0.008281916379928589\n",
      "Epoch 62, Batch 16/32 : Loss = 0.039500799030065536\n",
      "Epoch 62, Batch 17/32 : Loss = 0.012604077346622944\n",
      "Epoch 62, Batch 18/32 : Loss = 0.0036644884385168552\n",
      "Epoch 62, Batch 19/32 : Loss = 0.004213196691125631\n",
      "Epoch 62, Batch 20/32 : Loss = 0.004616337828338146\n",
      "Epoch 62, Batch 21/32 : Loss = 0.012397327460348606\n",
      "Epoch 62, Batch 22/32 : Loss = 0.004790961742401123\n",
      "Epoch 62, Batch 23/32 : Loss = 0.01745699532330036\n",
      "Epoch 62, Batch 24/32 : Loss = 0.06544018536806107\n",
      "Epoch 62, Batch 25/32 : Loss = 0.032819781452417374\n",
      "Epoch 62, Batch 26/32 : Loss = 0.0037196576595306396\n",
      "Epoch 62, Batch 27/32 : Loss = 0.006676011253148317\n",
      "Epoch 62, Batch 28/32 : Loss = 0.004832233302295208\n",
      "Epoch 62, Batch 29/32 : Loss = 0.00442557642236352\n",
      "Epoch 62, Batch 30/32 : Loss = 0.08883429318666458\n",
      "Epoch 62, Batch 31/32 : Loss = 0.02402864769101143\n",
      "Epoch 62 finished in 0.03977970282236735 minutes\n",
      "Epoch 62 training_loss = 0.018257232382893562\n",
      ")-----m-------PP---bb-----W------yy---}---&-----k---00---___--)---- => )mPbWy}&k0_), Ground Truth is )mPbWy}&k0_)\n",
      "o-------K----EE----G----->----a----I-RR----3---.--w----yy---V------ => oKEG>aIR3.wyV, Ground Truth is oKEG>aIR3.wyV\n",
      "Epoch 62 val_loss = 0.011175526306033134, word_accuracy = 0.9819277108433735\n",
      "Epoch 63, Batch 0/32 : Loss = 0.0043706572614610195\n",
      "Epoch 63, Batch 1/32 : Loss = 0.011833487078547478\n",
      "Epoch 63, Batch 2/32 : Loss = 0.014641374349594116\n",
      "Epoch 63, Batch 3/32 : Loss = 0.003841811791062355\n",
      "Epoch 63, Batch 4/32 : Loss = 0.0030381940305233\n",
      "Epoch 63, Batch 5/32 : Loss = 0.0611477866768837\n",
      "Epoch 63, Batch 6/32 : Loss = 0.005905474536120892\n",
      "Epoch 63, Batch 7/32 : Loss = 0.007028302643448114\n",
      "Epoch 63, Batch 8/32 : Loss = 0.004435836337506771\n",
      "Epoch 63, Batch 9/32 : Loss = 0.01050424762070179\n",
      "Epoch 63, Batch 10/32 : Loss = 0.04586704447865486\n",
      "Epoch 63, Batch 11/32 : Loss = 0.008106792345643044\n",
      "Epoch 63, Batch 12/32 : Loss = 0.005498318932950497\n",
      "Epoch 63, Batch 13/32 : Loss = 0.01679924875497818\n",
      "Epoch 63, Batch 14/32 : Loss = 0.08620814979076385\n",
      "Epoch 63, Batch 15/32 : Loss = 0.009905407205224037\n",
      "Epoch 63, Batch 16/32 : Loss = 0.0106270182877779\n",
      "Epoch 63, Batch 17/32 : Loss = 0.015719225630164146\n",
      "Epoch 63, Batch 18/32 : Loss = 0.00478739757090807\n",
      "Epoch 63, Batch 19/32 : Loss = 0.012211119756102562\n",
      "Epoch 63, Batch 20/32 : Loss = 0.010194866918027401\n",
      "Epoch 63, Batch 21/32 : Loss = 0.0038848172407597303\n",
      "Epoch 63, Batch 22/32 : Loss = 0.005879811942577362\n",
      "Epoch 63, Batch 23/32 : Loss = 0.07304231077432632\n",
      "Epoch 63, Batch 24/32 : Loss = 0.0067665791139006615\n",
      "Epoch 63, Batch 25/32 : Loss = 0.009754525497555733\n",
      "Epoch 63, Batch 26/32 : Loss = 0.0031013023108243942\n",
      "Epoch 63, Batch 27/32 : Loss = 0.010572463274002075\n",
      "Epoch 63, Batch 28/32 : Loss = 0.004660807549953461\n",
      "Epoch 63, Batch 29/32 : Loss = 0.025155656039714813\n",
      "Epoch 63, Batch 30/32 : Loss = 0.003715170780196786\n",
      "Epoch 63, Batch 31/32 : Loss = 0.0037156955804675817\n",
      "Epoch 63 finished in 0.10397226413091024 minutes\n",
      "Epoch 63 training_loss = 0.016053643077611923\n",
      "T-------i-oo---;--{--'-vv------=-------0----4------ => Tio;{'v-=-04, Ground Truth is Tio;{'v-=-04\n",
      ";-----G----S----q---4---f-,,-P---{--.-LL--uu---9--- => ;GSq4f,P{.Lu9, Ground Truth is ;GSq4f,P{.Lu9\n",
      "Epoch 63 val_loss = 0.013198514468967915, word_accuracy = 0.9859437751004017\n",
      "Epoch 64, Batch 0/32 : Loss = 0.003772926051169634\n",
      "Epoch 64, Batch 1/32 : Loss = 0.00867902860045433\n",
      "Epoch 64, Batch 2/32 : Loss = 0.0032567805610597134\n",
      "Epoch 64, Batch 3/32 : Loss = 0.00599745474755764\n",
      "Epoch 64, Batch 4/32 : Loss = 0.0030602654442191124\n",
      "Epoch 64, Batch 5/32 : Loss = 0.008304520510137081\n",
      "Epoch 64, Batch 6/32 : Loss = 0.006748685147613287\n",
      "Epoch 64, Batch 7/32 : Loss = 0.009582720696926117\n",
      "Epoch 64, Batch 8/32 : Loss = 0.027225544676184654\n",
      "Epoch 64, Batch 9/32 : Loss = 0.0044426205568015575\n",
      "Epoch 64, Batch 10/32 : Loss = 0.0055813854560256\n",
      "Epoch 64, Batch 11/32 : Loss = 0.011239508166909218\n",
      "Epoch 64, Batch 12/32 : Loss = 0.004735878203064203\n",
      "Epoch 64, Batch 13/32 : Loss = 0.005587765946984291\n",
      "Epoch 64, Batch 14/32 : Loss = 0.004448495805263519\n",
      "Epoch 64, Batch 15/32 : Loss = 0.08265980333089828\n",
      "Epoch 64, Batch 16/32 : Loss = 0.03158033266663551\n",
      "Epoch 64, Batch 17/32 : Loss = 0.006029973272234201\n",
      "Epoch 64, Batch 18/32 : Loss = 0.0040760338306427\n",
      "Epoch 64, Batch 19/32 : Loss = 0.022371087223291397\n",
      "Epoch 64, Batch 20/32 : Loss = 0.09418533742427826\n",
      "Epoch 64, Batch 21/32 : Loss = 0.002436676062643528\n",
      "Epoch 64, Batch 22/32 : Loss = 0.02682487852871418\n",
      "Epoch 64, Batch 23/32 : Loss = 0.023791160434484482\n",
      "Epoch 64, Batch 24/32 : Loss = 0.00693714153021574\n",
      "Epoch 64, Batch 25/32 : Loss = 0.0136916758492589\n",
      "Epoch 64, Batch 26/32 : Loss = 0.030119407922029495\n",
      "Epoch 64, Batch 27/32 : Loss = 0.033459387719631195\n",
      "Epoch 64, Batch 28/32 : Loss = 0.010754931718111038\n",
      "Epoch 64, Batch 29/32 : Loss = 0.019399510696530342\n",
      "Epoch 64, Batch 30/32 : Loss = 0.011012159287929535\n",
      "Epoch 64, Batch 31/32 : Loss = 0.00988416001200676\n",
      "Epoch 64 finished in 0.0390286127726237 minutes\n",
      "Epoch 64 training_loss = 0.01713184267282486\n",
      "$------~---!-s----D----9---a---PP---s---$---]---- => $~!sD9aPs$], Ground Truth is $~!sD9aPs$]\n",
      "[------Z---bb---ww--jj-n---l--1---T--'-*---Z--``- => [Zbwjnl1T'*Z`, Ground Truth is [Zbwjnl1T'*Z`\n",
      "Epoch 64 val_loss = 0.023048264905810356, word_accuracy = 0.9618473895582329\n",
      "Epoch 65, Batch 0/32 : Loss = 0.00534670427441597\n",
      "Epoch 65, Batch 1/32 : Loss = 0.0037513161078095436\n",
      "Epoch 65, Batch 2/32 : Loss = 0.004058783408254385\n",
      "Epoch 65, Batch 3/32 : Loss = 0.01639692299067974\n",
      "Epoch 65, Batch 4/32 : Loss = 0.011117452755570412\n",
      "Epoch 65, Batch 5/32 : Loss = 0.015365419909358025\n",
      "Epoch 65, Batch 6/32 : Loss = 0.014619736932218075\n",
      "Epoch 65, Batch 7/32 : Loss = 0.035735778510570526\n",
      "Epoch 65, Batch 8/32 : Loss = 0.00928499922156334\n",
      "Epoch 65, Batch 9/32 : Loss = 0.028246916830539703\n",
      "Epoch 65, Batch 10/32 : Loss = 0.05903484299778938\n",
      "Epoch 65, Batch 11/32 : Loss = 0.009230732917785645\n",
      "Epoch 65, Batch 12/32 : Loss = 0.005298798903822899\n",
      "Epoch 65, Batch 13/32 : Loss = 0.008629553020000458\n",
      "Epoch 65, Batch 14/32 : Loss = 0.006496012210845947\n",
      "Epoch 65, Batch 15/32 : Loss = 0.0034917390439659357\n",
      "Epoch 65, Batch 16/32 : Loss = 0.011878187768161297\n",
      "Epoch 65, Batch 17/32 : Loss = 0.014083171263337135\n",
      "Epoch 65, Batch 18/32 : Loss = 0.008427290245890617\n",
      "Epoch 65, Batch 19/32 : Loss = 0.01463458500802517\n",
      "Epoch 65, Batch 20/32 : Loss = 0.02486414834856987\n",
      "Epoch 65, Batch 21/32 : Loss = 0.004558387212455273\n",
      "Epoch 65, Batch 22/32 : Loss = 0.004117797128856182\n",
      "Epoch 65, Batch 23/32 : Loss = 0.02007303759455681\n",
      "Epoch 65, Batch 24/32 : Loss = 0.004147408530116081\n",
      "Epoch 65, Batch 25/32 : Loss = 0.009698837995529175\n",
      "Epoch 65, Batch 26/32 : Loss = 0.041377436369657516\n",
      "Epoch 65, Batch 27/32 : Loss = 0.03099925071001053\n",
      "Epoch 65, Batch 28/32 : Loss = 0.006265698932111263\n",
      "Epoch 65, Batch 29/32 : Loss = 0.005789559334516525\n",
      "Epoch 65, Batch 30/32 : Loss = 0.07720226794481277\n",
      "Epoch 65, Batch 31/32 : Loss = 0.01233408972620964\n",
      "Epoch 65 finished in 0.03916134834289551 minutes\n",
      "Epoch 65 training_loss = 0.016570743173360825\n",
      "}------<---66----S---->----zz--9----X---=---j--|--- => }<6S>z9X=j|, Ground Truth is }<6S>z9X=j|\n",
      "<------55---2----W----'-hh---]-l--Z---$----{--s---- => <52W'h]lZ${s, Ground Truth is <52W'h]lZ${s\n",
      "Epoch 65 val_loss = 0.013935310766100883, word_accuracy = 0.9658634538152611\n",
      "Epoch 66, Batch 0/32 : Loss = 0.03428645804524422\n",
      "Epoch 66, Batch 1/32 : Loss = 0.005105094984173775\n",
      "Epoch 66, Batch 2/32 : Loss = 0.015768110752105713\n",
      "Epoch 66, Batch 3/32 : Loss = 0.005448519252240658\n",
      "Epoch 66, Batch 4/32 : Loss = 0.0032152540516108274\n",
      "Epoch 66, Batch 5/32 : Loss = 0.008839152753353119\n",
      "Epoch 66, Batch 6/32 : Loss = 0.0067016007378697395\n",
      "Epoch 66, Batch 7/32 : Loss = 0.002722230739891529\n",
      "Epoch 66, Batch 8/32 : Loss = 0.008419652469456196\n",
      "Epoch 66, Batch 9/32 : Loss = 0.015706462785601616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Batch 10/32 : Loss = 0.003095865249633789\n",
      "Epoch 66, Batch 11/32 : Loss = 0.006046242546290159\n",
      "Epoch 66, Batch 12/32 : Loss = 0.004601139575242996\n",
      "Epoch 66, Batch 13/32 : Loss = 0.029806673526763916\n",
      "Epoch 66, Batch 14/32 : Loss = 0.019574064761400223\n",
      "Epoch 66, Batch 15/32 : Loss = 0.003328824182972312\n",
      "Epoch 66, Batch 16/32 : Loss = 0.0029420226346701384\n",
      "Epoch 66, Batch 17/32 : Loss = 0.005391966085880995\n",
      "Epoch 66, Batch 18/32 : Loss = 0.004251813981682062\n",
      "Epoch 66, Batch 19/32 : Loss = 0.011159476824104786\n",
      "Epoch 66, Batch 20/32 : Loss = 0.010666845366358757\n",
      "Epoch 66, Batch 21/32 : Loss = 0.002072735223919153\n",
      "Epoch 66, Batch 22/32 : Loss = 0.0107867531478405\n",
      "Epoch 66, Batch 23/32 : Loss = 0.00592767633497715\n",
      "Epoch 66, Batch 24/32 : Loss = 0.02202363684773445\n",
      "Epoch 66, Batch 25/32 : Loss = 0.005076073110103607\n",
      "Epoch 66, Batch 26/32 : Loss = 0.021075639873743057\n",
      "Epoch 66, Batch 27/32 : Loss = 0.0023777508176863194\n",
      "Epoch 66, Batch 28/32 : Loss = 0.007457307539880276\n",
      "Epoch 66, Batch 29/32 : Loss = 0.03895700350403786\n",
      "Epoch 66, Batch 30/32 : Loss = 0.026000304147601128\n",
      "Epoch 66, Batch 31/32 : Loss = 0.014015976339578629\n",
      "Epoch 66 finished in 0.03966655731201172 minutes\n",
      "Epoch 66 training_loss = 0.011263754218816757\n",
      "n-----!--N-----4---f--B----@---jj--R---(--%-----'-w----- => n!N4fB@jR(%'w, Ground Truth is n!N4fB@jR(%'w\n",
      "d---------4---44--ff--n----S----z--v--[--{--i--A---f---- => d44fnSzv[{iAf, Ground Truth is d44fnSzv[{iAf\n",
      "Epoch 66 val_loss = 0.010686731897294521, word_accuracy = 0.9819277108433735\n",
      "Epoch 67, Batch 0/32 : Loss = 0.020788639783859253\n",
      "Epoch 67, Batch 1/32 : Loss = 0.0023277420550584793\n",
      "Epoch 67, Batch 2/32 : Loss = 0.0018665568204596639\n",
      "Epoch 67, Batch 3/32 : Loss = 0.0028099725022912025\n",
      "Epoch 67, Batch 4/32 : Loss = 0.012199066579341888\n",
      "Epoch 67, Batch 5/32 : Loss = 0.0038909243885427713\n",
      "Epoch 67, Batch 6/32 : Loss = 0.018654288724064827\n",
      "Epoch 67, Batch 7/32 : Loss = 0.0035175341181457043\n",
      "Epoch 67, Batch 8/32 : Loss = 0.004161228891462088\n",
      "Epoch 67, Batch 9/32 : Loss = 0.011782320216298103\n",
      "Epoch 67, Batch 10/32 : Loss = 0.004735902883112431\n",
      "Epoch 67, Batch 11/32 : Loss = 0.004935072269290686\n",
      "Epoch 67, Batch 12/32 : Loss = 0.024603117257356644\n",
      "Epoch 67, Batch 13/32 : Loss = 0.0037042591720819473\n",
      "Epoch 67, Batch 14/32 : Loss = 0.0022712815552949905\n",
      "Epoch 67, Batch 15/32 : Loss = 0.002435180125758052\n",
      "Epoch 67, Batch 16/32 : Loss = 0.0023850155994296074\n",
      "Epoch 67, Batch 17/32 : Loss = 0.002170866820961237\n",
      "Epoch 67, Batch 18/32 : Loss = 0.011194070801138878\n",
      "Epoch 67, Batch 19/32 : Loss = 0.005130847916007042\n",
      "Epoch 67, Batch 20/32 : Loss = 0.009958545677363873\n",
      "Epoch 67, Batch 21/32 : Loss = 0.003034053137525916\n",
      "Epoch 67, Batch 22/32 : Loss = 0.012478322722017765\n",
      "Epoch 67, Batch 23/32 : Loss = 0.0031623272225260735\n",
      "Epoch 67, Batch 24/32 : Loss = 0.0033487877808511257\n",
      "Epoch 67, Batch 25/32 : Loss = 0.0022266271989792585\n",
      "Epoch 67, Batch 26/32 : Loss = 0.007112237624824047\n",
      "Epoch 67, Batch 27/32 : Loss = 0.003913475200533867\n",
      "Epoch 67, Batch 28/32 : Loss = 0.00483896816149354\n",
      "Epoch 67, Batch 29/32 : Loss = 0.0028252408374100924\n",
      "Epoch 67, Batch 30/32 : Loss = 0.004609820432960987\n",
      "Epoch 67, Batch 31/32 : Loss = 0.1288895159959793\n",
      "Epoch 67 finished in 0.039567240079243976 minutes\n",
      "Epoch 67 training_loss = 0.007042038720101118\n",
      "n--------^--**--55---i-LL---++--33---#-----Z---P----O-------- => n^*5iL+3#ZPO, Ground Truth is n^*5iL+3#ZPO\n",
      "9-------nn----e----VV---t---4----?-------w-----WW-------=---- => 9neVt4?-wW=, Ground Truth is 9neVt4?-wW=\n",
      "Epoch 67 val_loss = 0.006578871048986912, word_accuracy = 0.9839357429718876\n",
      "Epoch 68, Batch 0/32 : Loss = 0.021972117945551872\n",
      "Epoch 68, Batch 1/32 : Loss = 0.03284984827041626\n",
      "Epoch 68, Batch 2/32 : Loss = 0.0022671115584671497\n",
      "Epoch 68, Batch 3/32 : Loss = 0.006338047794997692\n",
      "Epoch 68, Batch 4/32 : Loss = 0.002426836872473359\n",
      "Epoch 68, Batch 5/32 : Loss = 0.00917208194732666\n",
      "Epoch 68, Batch 6/32 : Loss = 0.00285565247759223\n",
      "Epoch 68, Batch 7/32 : Loss = 0.0116038853302598\n",
      "Epoch 68, Batch 8/32 : Loss = 0.0038072089664638042\n",
      "Epoch 68, Batch 9/32 : Loss = 0.005452520679682493\n",
      "Epoch 68, Batch 10/32 : Loss = 0.022753067314624786\n",
      "Epoch 68, Batch 11/32 : Loss = 0.0029869903810322285\n",
      "Epoch 68, Batch 12/32 : Loss = 0.0021317177452147007\n",
      "Epoch 68, Batch 13/32 : Loss = 0.03372076153755188\n",
      "Epoch 68, Batch 14/32 : Loss = 0.025209974497556686\n",
      "Epoch 68, Batch 15/32 : Loss = 0.0032204322051256895\n",
      "Epoch 68, Batch 16/32 : Loss = 0.04458535835146904\n",
      "Epoch 68, Batch 17/32 : Loss = 0.00522138737142086\n",
      "Epoch 68, Batch 18/32 : Loss = 0.00561606977134943\n",
      "Epoch 68, Batch 19/32 : Loss = 0.0032093371264636517\n",
      "Epoch 68, Batch 20/32 : Loss = 0.002478478942066431\n",
      "Epoch 68, Batch 21/32 : Loss = 0.02330428548157215\n",
      "Epoch 68, Batch 22/32 : Loss = 0.0024142717011272907\n",
      "Epoch 68, Batch 23/32 : Loss = 0.00348661164753139\n",
      "Epoch 68, Batch 24/32 : Loss = 0.002984097693115473\n",
      "Epoch 68, Batch 25/32 : Loss = 0.002744467929005623\n",
      "Epoch 68, Batch 26/32 : Loss = 0.02354789525270462\n",
      "Epoch 68, Batch 27/32 : Loss = 0.004008308053016663\n",
      "Epoch 68, Batch 28/32 : Loss = 0.03552147001028061\n",
      "Epoch 68, Batch 29/32 : Loss = 0.0035524957347661257\n",
      "Epoch 68, Batch 30/32 : Loss = 0.0032102777622640133\n",
      "Epoch 68, Batch 31/32 : Loss = 0.004776836838573217\n",
      "Epoch 68 finished in 0.03895256519317627 minutes\n",
      "Epoch 68 training_loss = 0.011413661763072014\n",
      "ii------DD-----++----2-----b------W--------?---}}--`---^---xx------ => iD+2bW?}`^x, Ground Truth is iD+2bW?}`^x\n",
      "}--------z---@@-----~---j--N------F---55----C----!--9----1---n----- => }z@~jNF5C!91n, Ground Truth is }z@~jNF5C!91n\n",
      "Epoch 68 val_loss = 0.016248777508735657, word_accuracy = 0.9819277108433735\n",
      "Epoch 69, Batch 0/32 : Loss = 0.003903934732079506\n",
      "Epoch 69, Batch 1/32 : Loss = 0.03025725483894348\n",
      "Epoch 69, Batch 2/32 : Loss = 0.0019269922049716115\n",
      "Epoch 69, Batch 3/32 : Loss = 0.003350472077727318\n",
      "Epoch 69, Batch 4/32 : Loss = 0.005452792160212994\n",
      "Epoch 69, Batch 5/32 : Loss = 0.03585823252797127\n",
      "Epoch 69, Batch 6/32 : Loss = 0.0037572397850453854\n",
      "Epoch 69, Batch 7/32 : Loss = 0.007535091135650873\n",
      "Epoch 69, Batch 8/32 : Loss = 0.001566631835885346\n",
      "Epoch 69, Batch 9/32 : Loss = 0.01785200648009777\n",
      "Epoch 69, Batch 10/32 : Loss = 0.0048841554671525955\n",
      "Epoch 69, Batch 11/32 : Loss = 0.0031934231519699097\n",
      "Epoch 69, Batch 12/32 : Loss = 0.004445867612957954\n",
      "Epoch 69, Batch 13/32 : Loss = 0.009218783117830753\n",
      "Epoch 69, Batch 14/32 : Loss = 0.003428651252761483\n",
      "Epoch 69, Batch 15/32 : Loss = 0.0030370126478374004\n",
      "Epoch 69, Batch 16/32 : Loss = 0.02617764100432396\n",
      "Epoch 69, Batch 17/32 : Loss = 0.002540122251957655\n",
      "Epoch 69, Batch 18/32 : Loss = 0.0020049179438501596\n",
      "Epoch 69, Batch 19/32 : Loss = 0.022225182503461838\n",
      "Epoch 69, Batch 20/32 : Loss = 0.021923601627349854\n",
      "Epoch 69, Batch 21/32 : Loss = 0.0027811499312520027\n",
      "Epoch 69, Batch 22/32 : Loss = 0.002062152838334441\n",
      "Epoch 69, Batch 23/32 : Loss = 0.01229976024478674\n",
      "Epoch 69, Batch 24/32 : Loss = 0.0032377501484006643\n",
      "Epoch 69, Batch 25/32 : Loss = 0.005720033310353756\n",
      "Epoch 69, Batch 26/32 : Loss = 0.010125664994120598\n",
      "Epoch 69, Batch 27/32 : Loss = 0.016628660261631012\n",
      "Epoch 69, Batch 28/32 : Loss = 0.002097876276820898\n",
      "Epoch 69, Batch 29/32 : Loss = 0.002653147093951702\n",
      "Epoch 69, Batch 30/32 : Loss = 0.006046767346560955\n",
      "Epoch 69, Batch 31/32 : Loss = 0.0013092480367049575\n",
      "Epoch 69 finished in 0.038272448380788165 minutes\n",
      "Epoch 69 training_loss = 0.008943185210227966\n",
      "N-----------A----XX----7---EE---#--------4----q----n---88----w----22----- => NAX7E#-4qn8w2, Ground Truth is NAX7E#-4qn8w2\n",
      "^----------Q------UU------S----**---22---,---y----C------q-----t-->------ => ^QUS*2,yCqt>, Ground Truth is ^QUS*2,yCqt>\n",
      "Epoch 69 val_loss = 0.010587022639811039, word_accuracy = 0.9859437751004017\n",
      "Epoch 70, Batch 0/32 : Loss = 0.0029668263159692287\n",
      "Epoch 70, Batch 1/32 : Loss = 0.10113967210054398\n",
      "Epoch 70, Batch 2/32 : Loss = 0.002313157543540001\n",
      "Epoch 70, Batch 3/32 : Loss = 0.01343601942062378\n",
      "Epoch 70, Batch 4/32 : Loss = 0.00433538481593132\n",
      "Epoch 70, Batch 5/32 : Loss = 0.007834735326468945\n",
      "Epoch 70, Batch 6/32 : Loss = 0.0029990910552442074\n",
      "Epoch 70, Batch 7/32 : Loss = 0.01841866597533226\n",
      "Epoch 70, Batch 8/32 : Loss = 0.002686919644474983\n",
      "Epoch 70, Batch 9/32 : Loss = 0.001884516212157905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Batch 10/32 : Loss = 0.0028480361215770245\n",
      "Epoch 70, Batch 11/32 : Loss = 0.0028073261491954327\n",
      "Epoch 70, Batch 12/32 : Loss = 0.002890943083912134\n",
      "Epoch 70, Batch 13/32 : Loss = 0.004348406568169594\n",
      "Epoch 70, Batch 14/32 : Loss = 0.002506321296095848\n",
      "Epoch 70, Batch 15/32 : Loss = 0.011870068497955799\n",
      "Epoch 70, Batch 16/32 : Loss = 0.02103899046778679\n",
      "Epoch 70, Batch 17/32 : Loss = 0.002360062673687935\n",
      "Epoch 70, Batch 18/32 : Loss = 0.008741825819015503\n",
      "Epoch 70, Batch 19/32 : Loss = 0.008753573521971703\n",
      "Epoch 70, Batch 20/32 : Loss = 0.00265753036364913\n",
      "Epoch 70, Batch 21/32 : Loss = 0.0019390762317925692\n",
      "Epoch 70, Batch 22/32 : Loss = 0.004836734384298325\n",
      "Epoch 70, Batch 23/32 : Loss = 0.003826085478067398\n",
      "Epoch 70, Batch 24/32 : Loss = 0.0032435967586934566\n",
      "Epoch 70, Batch 25/32 : Loss = 0.0031166323460638523\n",
      "Epoch 70, Batch 26/32 : Loss = 0.0022230567410588264\n",
      "Epoch 70, Batch 27/32 : Loss = 0.004261516500264406\n",
      "Epoch 70, Batch 28/32 : Loss = 0.010389498434960842\n",
      "Epoch 70, Batch 29/32 : Loss = 0.013057559728622437\n",
      "Epoch 70, Batch 30/32 : Loss = 0.0031282089184969664\n",
      "Epoch 70, Batch 31/32 : Loss = 0.36898812651634216\n",
      "Epoch 70 finished in 0.03919550180435181 minutes\n",
      "Epoch 70 training_loss = 0.010441238060593605\n",
      "t------X--:--B----a---JJ--/--[---Y---4---kk--W------|---- => tX:BaJ/[Y4kW|, Ground Truth is tX:BaJ/[Y4kW|\n",
      "O----------]--P----n----]--]--kk--f--;--77---X---6---I--- => O]Pn]]kf;7X6I, Ground Truth is O]Pn]]kf;7X6I\n",
      "Epoch 70 val_loss = 0.005695601459592581, word_accuracy = 0.9939759036144579\n",
      "Epoch 71, Batch 0/32 : Loss = 0.006077226251363754\n",
      "Epoch 71, Batch 1/32 : Loss = 0.0015608856920152903\n",
      "Epoch 71, Batch 2/32 : Loss = 0.002734007779508829\n",
      "Epoch 71, Batch 3/32 : Loss = 0.0029053366743028164\n",
      "Epoch 71, Batch 4/32 : Loss = 0.0014607510529458523\n",
      "Epoch 71, Batch 5/32 : Loss = 0.001835663802921772\n",
      "Epoch 71, Batch 6/32 : Loss = 0.007645695935934782\n",
      "Epoch 71, Batch 7/32 : Loss = 0.005770639516413212\n",
      "Epoch 71, Batch 8/32 : Loss = 0.007325097918510437\n",
      "Epoch 71, Batch 9/32 : Loss = 0.0025807791389524937\n",
      "Epoch 71, Batch 10/32 : Loss = 0.00939530786126852\n",
      "Epoch 71, Batch 11/32 : Loss = 0.009101109579205513\n",
      "Epoch 71, Batch 12/32 : Loss = 0.015452138148248196\n",
      "Epoch 71, Batch 13/32 : Loss = 0.003910117782652378\n",
      "Epoch 71, Batch 14/32 : Loss = 0.0033293659798800945\n",
      "Epoch 71, Batch 15/32 : Loss = 0.0022892530541867018\n",
      "Epoch 71, Batch 16/32 : Loss = 0.003779669525101781\n",
      "Epoch 71, Batch 17/32 : Loss = 0.002863022033125162\n",
      "Epoch 71, Batch 18/32 : Loss = 0.0033515337854623795\n",
      "Epoch 71, Batch 19/32 : Loss = 0.003338647074997425\n",
      "Epoch 71, Batch 20/32 : Loss = 0.0023149922490119934\n",
      "Epoch 71, Batch 21/32 : Loss = 0.0016495783347636461\n",
      "Epoch 71, Batch 22/32 : Loss = 0.0053007276728749275\n",
      "Epoch 71, Batch 23/32 : Loss = 0.0017120755510404706\n",
      "Epoch 71, Batch 24/32 : Loss = 0.002585472771897912\n",
      "Epoch 71, Batch 25/32 : Loss = 0.0021878154948353767\n",
      "Epoch 71, Batch 26/32 : Loss = 0.0028053317219018936\n",
      "Epoch 71, Batch 27/32 : Loss = 0.005047117359936237\n",
      "Epoch 71, Batch 28/32 : Loss = 0.008030684664845467\n",
      "Epoch 71, Batch 29/32 : Loss = 0.008263088762760162\n",
      "Epoch 71, Batch 30/32 : Loss = 0.004892420023679733\n",
      "Epoch 71, Batch 31/32 : Loss = 0.0014880981761962175\n",
      "Epoch 71 finished in 0.03945099910100301 minutes\n",
      "Epoch 71 training_loss = 0.004552018363028765\n",
      "W----------m------F---HH----UU----&----22---|--;--F---22---77--DD---- => WmFHU&2|;F27D, Ground Truth is WmFHU&2|;F27D\n",
      ",----P----11---__---11---a----#----vv---Z---MM------u----*----E------ => ,P1_1a#vZMu*E, Ground Truth is ,P1_1a#vZMu*E\n",
      "Epoch 71 val_loss = 0.005946703255176544, word_accuracy = 0.9899598393574297\n",
      "Epoch 72, Batch 0/32 : Loss = 0.006301673129200935\n",
      "Epoch 72, Batch 1/32 : Loss = 0.0021776589564979076\n",
      "Epoch 72, Batch 2/32 : Loss = 0.011259164661169052\n",
      "Epoch 72, Batch 3/32 : Loss = 0.0016093153972178698\n",
      "Epoch 72, Batch 4/32 : Loss = 0.0036485777236521244\n",
      "Epoch 72, Batch 5/32 : Loss = 0.004290248267352581\n",
      "Epoch 72, Batch 6/32 : Loss = 0.008263621479272842\n",
      "Epoch 72, Batch 7/32 : Loss = 0.006144772283732891\n",
      "Epoch 72, Batch 8/32 : Loss = 0.0024843052960932255\n",
      "Epoch 72, Batch 9/32 : Loss = 0.0015952647663652897\n",
      "Epoch 72, Batch 10/32 : Loss = 0.0021854580845683813\n",
      "Epoch 72, Batch 11/32 : Loss = 0.002448425628244877\n",
      "Epoch 72, Batch 12/32 : Loss = 0.0019335601245984435\n",
      "Epoch 72, Batch 13/32 : Loss = 0.001372663420625031\n",
      "Epoch 72, Batch 14/32 : Loss = 0.005458232015371323\n",
      "Epoch 72, Batch 15/32 : Loss = 0.002050206298008561\n",
      "Epoch 72, Batch 16/32 : Loss = 0.0017677334835752845\n",
      "Epoch 72, Batch 17/32 : Loss = 0.003296351758763194\n",
      "Epoch 72, Batch 18/32 : Loss = 0.00423467718064785\n",
      "Epoch 72, Batch 19/32 : Loss = 0.007872849702835083\n",
      "Epoch 72, Batch 20/32 : Loss = 0.0019728948827832937\n",
      "Epoch 72, Batch 21/32 : Loss = 0.05806148424744606\n",
      "Epoch 72, Batch 22/32 : Loss = 0.005904438905417919\n",
      "Epoch 72, Batch 23/32 : Loss = 0.06379823386669159\n",
      "Epoch 72, Batch 24/32 : Loss = 0.0019887294620275497\n",
      "Epoch 72, Batch 25/32 : Loss = 0.0023529729805886745\n",
      "Epoch 72, Batch 26/32 : Loss = 0.0011086973827332258\n",
      "Epoch 72, Batch 27/32 : Loss = 0.0020171061623841524\n",
      "Epoch 72, Batch 28/32 : Loss = 0.004763433709740639\n",
      "Epoch 72, Batch 29/32 : Loss = 0.0010182107798755169\n",
      "Epoch 72, Batch 30/32 : Loss = 0.005731205455958843\n",
      "Epoch 72, Batch 31/32 : Loss = 0.003399544395506382\n",
      "Epoch 72 finished in 0.0390803058942159 minutes\n",
      "Epoch 72 training_loss = 0.007374685723334551\n",
      ".--------G-----77---__------Z----W------n----W-------B----- => .-G7_-ZWnWB, Ground Truth is .-G7_-ZWnWB\n",
      "t------!--((----|----77----TT----q------22---((---$----**-- => t!(|7Tq2($*, Ground Truth is t!(|7Tq2($*\n",
      "Epoch 72 val_loss = 0.00942516140639782, word_accuracy = 0.9799196787148594\n",
      "Epoch 73, Batch 0/32 : Loss = 0.001448524184525013\n",
      "Epoch 73, Batch 1/32 : Loss = 0.002466766629368067\n",
      "Epoch 73, Batch 2/32 : Loss = 0.0034842262975871563\n",
      "Epoch 73, Batch 3/32 : Loss = 0.002211053390055895\n",
      "Epoch 73, Batch 4/32 : Loss = 0.002391281072050333\n",
      "Epoch 73, Batch 5/32 : Loss = 0.002341747749596834\n",
      "Epoch 73, Batch 6/32 : Loss = 0.0018684098031371832\n",
      "Epoch 73, Batch 7/32 : Loss = 0.013208855874836445\n",
      "Epoch 73, Batch 8/32 : Loss = 0.00669025257229805\n",
      "Epoch 73, Batch 9/32 : Loss = 0.0024192375130951405\n",
      "Epoch 73, Batch 10/32 : Loss = 0.006353674456477165\n",
      "Epoch 73, Batch 11/32 : Loss = 0.007904231548309326\n",
      "Epoch 73, Batch 12/32 : Loss = 0.042144134640693665\n",
      "Epoch 73, Batch 13/32 : Loss = 0.1772288680076599\n",
      "Epoch 73, Batch 14/32 : Loss = 0.007153833284974098\n",
      "Epoch 73, Batch 15/32 : Loss = 0.0012647387338802218\n",
      "Epoch 73, Batch 16/32 : Loss = 0.008768873289227486\n",
      "Epoch 73, Batch 17/32 : Loss = 0.001625917269848287\n",
      "Epoch 73, Batch 18/32 : Loss = 0.0026095344219356775\n",
      "Epoch 73, Batch 19/32 : Loss = 0.10915564745664597\n",
      "Epoch 73, Batch 20/32 : Loss = 0.010725153610110283\n",
      "Epoch 73, Batch 21/32 : Loss = 0.002180127426981926\n",
      "Epoch 73, Batch 22/32 : Loss = 0.009088744409382343\n",
      "Epoch 73, Batch 23/32 : Loss = 0.00839285645633936\n",
      "Epoch 73, Batch 24/32 : Loss = 0.0038961467798799276\n",
      "Epoch 73, Batch 25/32 : Loss = 0.010767693631350994\n",
      "Epoch 73, Batch 26/32 : Loss = 0.004724630154669285\n",
      "Epoch 73, Batch 27/32 : Loss = 0.05310427024960518\n",
      "Epoch 73, Batch 28/32 : Loss = 0.00907121691852808\n",
      "Epoch 73, Batch 29/32 : Loss = 0.002262863563373685\n",
      "Epoch 73, Batch 30/32 : Loss = 0.005711373873054981\n",
      "Epoch 73, Batch 31/32 : Loss = 0.1468278467655182\n",
      "Epoch 73 finished in 0.03893497387568156 minutes\n",
      "Epoch 73 training_loss = 0.017382116988301277\n",
      "(------*---#----N-----E----=---;--1--((--u---33---#----<------ => (*#NE=;1(u3#<, Ground Truth is (*#NE=;1(u3#<\n",
      "V-------------r--55---PP---t-----m-------@-----ss--**---v----- => V-r5Pt-m@s*v, Ground Truth is V-r5Pt-m@s*v\n",
      "Epoch 73 val_loss = 0.01596800424158573, word_accuracy = 0.9839357429718876\n",
      "Epoch 74, Batch 0/32 : Loss = 0.009840108454227448\n",
      "Epoch 74, Batch 1/32 : Loss = 0.007960847578942776\n",
      "Epoch 74, Batch 2/32 : Loss = 0.0018610151018947363\n",
      "Epoch 74, Batch 3/32 : Loss = 0.03677351027727127\n",
      "Epoch 74, Batch 4/32 : Loss = 0.005981644615530968\n",
      "Epoch 74, Batch 5/32 : Loss = 0.0026157391257584095\n",
      "Epoch 74, Batch 6/32 : Loss = 0.006836009677499533\n",
      "Epoch 74, Batch 7/32 : Loss = 0.00380684738047421\n",
      "Epoch 74, Batch 8/32 : Loss = 0.0020961062982678413\n",
      "Epoch 74, Batch 9/32 : Loss = 0.001803450402803719\n",
      "Epoch 74, Batch 10/32 : Loss = 0.07557068765163422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Batch 11/32 : Loss = 0.011755280196666718\n",
      "Epoch 74, Batch 12/32 : Loss = 0.002865167800337076\n",
      "Epoch 74, Batch 13/32 : Loss = 0.017323855310678482\n",
      "Epoch 74, Batch 14/32 : Loss = 0.0043331654742360115\n",
      "Epoch 74, Batch 15/32 : Loss = 0.009154846891760826\n",
      "Epoch 74, Batch 16/32 : Loss = 0.0035657105036079884\n",
      "Epoch 74, Batch 17/32 : Loss = 0.020598432049155235\n",
      "Epoch 74, Batch 18/32 : Loss = 0.0033559505827724934\n",
      "Epoch 74, Batch 19/32 : Loss = 0.0033405376598238945\n",
      "Epoch 74, Batch 20/32 : Loss = 0.005704324692487717\n",
      "Epoch 74, Batch 21/32 : Loss = 0.002038865815848112\n",
      "Epoch 74, Batch 22/32 : Loss = 0.005056002642959356\n",
      "Epoch 74, Batch 23/32 : Loss = 0.004958304110914469\n",
      "Epoch 74, Batch 24/32 : Loss = 0.0021769360173493624\n",
      "Epoch 74, Batch 25/32 : Loss = 0.004855466075241566\n",
      "Epoch 74, Batch 26/32 : Loss = 0.003288191743195057\n",
      "Epoch 74, Batch 27/32 : Loss = 0.0027157682925462723\n",
      "Epoch 74, Batch 28/32 : Loss = 0.007031608838587999\n",
      "Epoch 74, Batch 29/32 : Loss = 0.0071219466626644135\n",
      "Epoch 74, Batch 30/32 : Loss = 0.010950764641165733\n",
      "Epoch 74, Batch 31/32 : Loss = 0.005600611679255962\n",
      "Epoch 74 finished in 0.039418514569600424 minutes\n",
      "Epoch 74 training_loss = 0.009254205971956253\n",
      "t-----X--::-BB----a----J--/---[--Y----4---kk---W-----|---- => tX:BaJ/[Y4kW|, Ground Truth is tX:BaJ/[Y4kW|\n",
      "A--------x----0----0----f---$-----7--,,--O------(---7----- => Ax00f$7,O(7, Ground Truth is Ax00f$7,O(7\n",
      "Epoch 74 val_loss = 0.007064862176775932, word_accuracy = 0.9919678714859438\n",
      "Epoch 75, Batch 0/32 : Loss = 0.0023952224291861057\n",
      "Epoch 75, Batch 1/32 : Loss = 0.0018508897628635168\n",
      "Epoch 75, Batch 2/32 : Loss = 0.0023799852933734655\n",
      "Epoch 75, Batch 3/32 : Loss = 0.004870705772191286\n",
      "Epoch 75, Batch 4/32 : Loss = 0.06046213582158089\n",
      "Epoch 75, Batch 5/32 : Loss = 0.009776576422154903\n",
      "Epoch 75, Batch 6/32 : Loss = 0.0037585943937301636\n",
      "Epoch 75, Batch 7/32 : Loss = 0.0026399726048111916\n",
      "Epoch 75, Batch 8/32 : Loss = 0.001479073311202228\n",
      "Epoch 75, Batch 9/32 : Loss = 0.04356684163212776\n",
      "Epoch 75, Batch 10/32 : Loss = 0.021221240982413292\n",
      "Epoch 75, Batch 11/32 : Loss = 0.0015339571982622147\n",
      "Epoch 75, Batch 12/32 : Loss = 0.00757221132516861\n",
      "Epoch 75, Batch 13/32 : Loss = 0.0014514231588691473\n",
      "Epoch 75, Batch 14/32 : Loss = 0.01108241081237793\n",
      "Epoch 75, Batch 15/32 : Loss = 0.001409442164003849\n",
      "Epoch 75, Batch 16/32 : Loss = 0.0034501953050494194\n",
      "Epoch 75, Batch 17/32 : Loss = 0.0024623945355415344\n",
      "Epoch 75, Batch 18/32 : Loss = 0.0017147555481642485\n",
      "Epoch 75, Batch 19/32 : Loss = 0.0017559377010911703\n",
      "Epoch 75, Batch 20/32 : Loss = 0.0034338561818003654\n",
      "Epoch 75, Batch 21/32 : Loss = 0.0024257756303995848\n",
      "Epoch 75, Batch 22/32 : Loss = 0.0014348396798595786\n",
      "Epoch 75, Batch 23/32 : Loss = 0.008266993798315525\n",
      "Epoch 75, Batch 24/32 : Loss = 0.00289055984467268\n",
      "Epoch 75, Batch 25/32 : Loss = 0.0017305659130215645\n",
      "Epoch 75, Batch 26/32 : Loss = 0.05804562568664551\n",
      "Epoch 75, Batch 27/32 : Loss = 0.032410603016614914\n",
      "Epoch 75, Batch 28/32 : Loss = 0.021799124777317047\n",
      "Epoch 75, Batch 29/32 : Loss = 0.00304089835844934\n",
      "Epoch 75, Batch 30/32 : Loss = 0.002037846017628908\n",
      "Epoch 75, Batch 31/32 : Loss = 0.002473542932420969\n",
      "Epoch 75 finished in 0.04375673135121663 minutes\n",
      "Epoch 75 training_loss = 0.0104308370500803\n",
      "n-----::-{-^--ff-__--?---F--GG---$---$---11--D---- => n:{^f_?FG$$1D, Ground Truth is n:{^f_?FG$$1D\n",
      "k-----}--0---TT--)--0---MM---~--->---H---2---w---- => k}0T)0M~>H2w, Ground Truth is k}OT)OM~>H2w\n",
      "Epoch 75 val_loss = 0.008335381746292114, word_accuracy = 0.9859437751004017\n",
      "Epoch 76, Batch 0/32 : Loss = 0.002027854323387146\n",
      "Epoch 76, Batch 1/32 : Loss = 0.001853176741860807\n",
      "Epoch 76, Batch 2/32 : Loss = 0.0023336377926170826\n",
      "Epoch 76, Batch 3/32 : Loss = 0.002125319093465805\n",
      "Epoch 76, Batch 4/32 : Loss = 0.0015161653282120824\n",
      "Epoch 76, Batch 5/32 : Loss = 0.008958466351032257\n",
      "Epoch 76, Batch 6/32 : Loss = 0.014518136158585548\n",
      "Epoch 76, Batch 7/32 : Loss = 0.001991016324609518\n",
      "Epoch 76, Batch 8/32 : Loss = 0.0072020478546619415\n",
      "Epoch 76, Batch 9/32 : Loss = 0.0015735386405140162\n",
      "Epoch 76, Batch 10/32 : Loss = 0.06035640463232994\n",
      "Epoch 76, Batch 11/32 : Loss = 0.0017536217346787453\n",
      "Epoch 76, Batch 12/32 : Loss = 0.004510511178523302\n",
      "Epoch 76, Batch 13/32 : Loss = 0.002674791496247053\n",
      "Epoch 76, Batch 14/32 : Loss = 0.004168566316366196\n",
      "Epoch 76, Batch 15/32 : Loss = 0.003501452039927244\n",
      "Epoch 76, Batch 16/32 : Loss = 0.0016415547579526901\n",
      "Epoch 76, Batch 17/32 : Loss = 0.006836769171059132\n",
      "Epoch 76, Batch 18/32 : Loss = 0.006119299679994583\n",
      "Epoch 76, Batch 19/32 : Loss = 0.0014435747871175408\n",
      "Epoch 76, Batch 20/32 : Loss = 0.0014421415980905294\n",
      "Epoch 76, Batch 21/32 : Loss = 0.01625753752887249\n",
      "Epoch 76, Batch 22/32 : Loss = 0.006561822257936001\n",
      "Epoch 76, Batch 23/32 : Loss = 0.005043757148087025\n",
      "Epoch 76, Batch 24/32 : Loss = 0.004375252407044172\n",
      "Epoch 76, Batch 25/32 : Loss = 0.00205047894269228\n",
      "Epoch 76, Batch 26/32 : Loss = 0.006634663324803114\n",
      "Epoch 76, Batch 27/32 : Loss = 0.0026694941334426403\n",
      "Epoch 76, Batch 28/32 : Loss = 0.003529889974743128\n",
      "Epoch 76, Batch 29/32 : Loss = 0.0016570267034694552\n",
      "Epoch 76, Batch 30/32 : Loss = 0.002640678081661463\n",
      "Epoch 76, Batch 31/32 : Loss = 0.027080025523900986\n",
      "Epoch 76 finished in 0.04038296540578206 minutes\n",
      "Epoch 76 training_loss = 0.006212165113538504\n",
      "X-------+---\"--8---ww----MM----R----#---p----C----L--i-o---- => X+\"8wMR#pCLio, Ground Truth is X+\"8wMR#pCLio\n",
      "^-------TT---aa----5----%------)--//---V----X----R----#----- => ^Ta5%)/VXR#, Ground Truth is ^Ta5%)/VXR#\n",
      "Epoch 76 val_loss = 0.0039217364974319935, word_accuracy = 0.9939759036144579\n",
      "Epoch 77, Batch 0/32 : Loss = 0.0035874368622899055\n",
      "Epoch 77, Batch 1/32 : Loss = 0.002353460295125842\n",
      "Epoch 77, Batch 2/32 : Loss = 0.002820959547534585\n",
      "Epoch 77, Batch 3/32 : Loss = 0.002738972194492817\n",
      "Epoch 77, Batch 4/32 : Loss = 0.0018638898618519306\n",
      "Epoch 77, Batch 5/32 : Loss = 0.04857205972075462\n",
      "Epoch 77, Batch 6/32 : Loss = 0.011250566691160202\n",
      "Epoch 77, Batch 7/32 : Loss = 0.0022895217407494783\n",
      "Epoch 77, Batch 8/32 : Loss = 0.0012447303161025047\n",
      "Epoch 77, Batch 9/32 : Loss = 0.003543175058439374\n",
      "Epoch 77, Batch 10/32 : Loss = 0.0013193849008530378\n",
      "Epoch 77, Batch 11/32 : Loss = 0.0028439010493457317\n",
      "Epoch 77, Batch 12/32 : Loss = 0.001207547727972269\n",
      "Epoch 77, Batch 13/32 : Loss = 0.03053678758442402\n",
      "Epoch 77, Batch 14/32 : Loss = 0.0014782065991312265\n",
      "Epoch 77, Batch 15/32 : Loss = 0.002873180666938424\n",
      "Epoch 77, Batch 16/32 : Loss = 0.003707713447511196\n",
      "Epoch 77, Batch 17/32 : Loss = 0.04345405101776123\n",
      "Epoch 77, Batch 18/32 : Loss = 0.0016940219793468714\n",
      "Epoch 77, Batch 19/32 : Loss = 0.0014792794827371836\n",
      "Epoch 77, Batch 20/32 : Loss = 0.00588896544650197\n",
      "Epoch 77, Batch 21/32 : Loss = 0.009101921692490578\n",
      "Epoch 77, Batch 22/32 : Loss = 0.0032529791351407766\n",
      "Epoch 77, Batch 23/32 : Loss = 0.0035563590936362743\n",
      "Epoch 77, Batch 24/32 : Loss = 0.023768329992890358\n",
      "Epoch 77, Batch 25/32 : Loss = 0.0019231310579925776\n",
      "Epoch 77, Batch 26/32 : Loss = 0.0012332635233178735\n",
      "Epoch 77, Batch 27/32 : Loss = 0.007001981604844332\n",
      "Epoch 77, Batch 28/32 : Loss = 0.0012907588388770819\n",
      "Epoch 77, Batch 29/32 : Loss = 0.01176086813211441\n",
      "Epoch 77, Batch 30/32 : Loss = 0.0026244826149195433\n",
      "Epoch 77, Batch 31/32 : Loss = 0.006531745661050081\n",
      "Epoch 77 finished in 0.04044461647669474 minutes\n",
      "Epoch 77 training_loss = 0.007809747010469437\n",
      "$------mm----5---x--pp--8----m-----S---F---Z---C---'-}--- => $m5xp8mSFZC'}, Ground Truth is $m5xp8mSFZC'}\n",
      "p--------R---I-NN----&&---HH---..mm-----~----2---e---I--- => pRIN&H.m~2eI, Ground Truth is pRIN&H.m~2eI\n",
      "Epoch 77 val_loss = 0.004011352080851793, word_accuracy = 0.9879518072289156\n",
      "Epoch 78, Batch 0/32 : Loss = 0.0024422446731477976\n",
      "Epoch 78, Batch 1/32 : Loss = 0.0020915381610393524\n",
      "Epoch 78, Batch 2/32 : Loss = 0.003859775373712182\n",
      "Epoch 78, Batch 3/32 : Loss = 0.001914840773679316\n",
      "Epoch 78, Batch 4/32 : Loss = 0.00198897416703403\n",
      "Epoch 78, Batch 5/32 : Loss = 0.002397682052105665\n",
      "Epoch 78, Batch 6/32 : Loss = 0.03301580250263214\n",
      "Epoch 78, Batch 7/32 : Loss = 0.01035193633288145\n",
      "Epoch 78, Batch 8/32 : Loss = 0.051267605274915695\n",
      "Epoch 78, Batch 9/32 : Loss = 0.0009873933158814907\n",
      "Epoch 78, Batch 10/32 : Loss = 0.0019012743141502142\n",
      "Epoch 78, Batch 11/32 : Loss = 0.0031214405316859484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Batch 12/32 : Loss = 0.014396793209016323\n",
      "Epoch 78, Batch 13/32 : Loss = 0.0016443512868136168\n",
      "Epoch 78, Batch 14/32 : Loss = 0.0013619376113638282\n",
      "Epoch 78, Batch 15/32 : Loss = 0.014696544036269188\n",
      "Epoch 78, Batch 16/32 : Loss = 0.002940407022833824\n",
      "Epoch 78, Batch 17/32 : Loss = 0.002202215837314725\n",
      "Epoch 78, Batch 18/32 : Loss = 0.001595519483089447\n",
      "Epoch 78, Batch 19/32 : Loss = 0.0032318588346242905\n",
      "Epoch 78, Batch 20/32 : Loss = 0.034923214465379715\n",
      "Epoch 78, Batch 21/32 : Loss = 0.004762801341712475\n",
      "Epoch 78, Batch 22/32 : Loss = 0.003064280841499567\n",
      "Epoch 78, Batch 23/32 : Loss = 0.002695642411708832\n",
      "Epoch 78, Batch 24/32 : Loss = 0.0028552929870784283\n",
      "Epoch 78, Batch 25/32 : Loss = 0.0017359342891722918\n",
      "Epoch 78, Batch 26/32 : Loss = 0.007064362056553364\n",
      "Epoch 78, Batch 27/32 : Loss = 0.0030883438885211945\n",
      "Epoch 78, Batch 28/32 : Loss = 0.0021021843422204256\n",
      "Epoch 78, Batch 29/32 : Loss = 0.005118148867040873\n",
      "Epoch 78, Batch 30/32 : Loss = 0.003508116351440549\n",
      "Epoch 78, Batch 31/32 : Loss = 0.004380197264254093\n",
      "Epoch 78 finished in 0.03880503972371419 minutes\n",
      "Epoch 78 training_loss = 0.007353443652391434\n",
      "M-------------g----\"\"---,-'--CC-----^---,--)--..--2----\"----- => Mg\",'C^,).2\", Ground Truth is Mg\",'C^,).2\"\n",
      "M-----------5---\"\"--#----t--\"--ll-F----5----G-----E---22----- => M5\"#t\"lF5GE2, Ground Truth is M5\"#t\"lF5GE2\n",
      "Epoch 78 val_loss = 0.004061434417963028, word_accuracy = 0.9959839357429718\n",
      "Epoch 79, Batch 0/32 : Loss = 0.03064136393368244\n",
      "Epoch 79, Batch 1/32 : Loss = 0.0015857075341045856\n",
      "Epoch 79, Batch 2/32 : Loss = 0.011210378259420395\n",
      "Epoch 79, Batch 3/32 : Loss = 0.004056407138705254\n",
      "Epoch 79, Batch 4/32 : Loss = 0.0017745096702128649\n",
      "Epoch 79, Batch 5/32 : Loss = 0.0021875714883208275\n",
      "Epoch 79, Batch 6/32 : Loss = 0.001598257222212851\n",
      "Epoch 79, Batch 7/32 : Loss = 0.0020786651875823736\n",
      "Epoch 79, Batch 8/32 : Loss = 0.002397920936346054\n",
      "Epoch 79, Batch 9/32 : Loss = 0.07841535657644272\n",
      "Epoch 79, Batch 10/32 : Loss = 0.001943072653375566\n",
      "Epoch 79, Batch 11/32 : Loss = 0.002204963704571128\n",
      "Epoch 79, Batch 12/32 : Loss = 0.002012674929574132\n",
      "Epoch 79, Batch 13/32 : Loss = 0.0023213140666484833\n",
      "Epoch 79, Batch 14/32 : Loss = 0.002566043520346284\n",
      "Epoch 79, Batch 15/32 : Loss = 0.015003307722508907\n",
      "Epoch 79, Batch 16/32 : Loss = 0.00467543862760067\n",
      "Epoch 79, Batch 17/32 : Loss = 0.06746428459882736\n",
      "Epoch 79, Batch 18/32 : Loss = 0.004607291426509619\n",
      "Epoch 79, Batch 19/32 : Loss = 0.0016698073595762253\n",
      "Epoch 79, Batch 20/32 : Loss = 0.002389986999332905\n",
      "Epoch 79, Batch 21/32 : Loss = 0.001984363654628396\n",
      "Epoch 79, Batch 22/32 : Loss = 0.003412187099456787\n",
      "Epoch 79, Batch 23/32 : Loss = 0.0032762624323368073\n",
      "Epoch 79, Batch 24/32 : Loss = 0.004310557618737221\n",
      "Epoch 79, Batch 25/32 : Loss = 0.0015812362544238567\n",
      "Epoch 79, Batch 26/32 : Loss = 0.0026131724007427692\n",
      "Epoch 79, Batch 27/32 : Loss = 0.0013341816375032067\n",
      "Epoch 79, Batch 28/32 : Loss = 0.001806644257158041\n",
      "Epoch 79, Batch 29/32 : Loss = 0.0019089763518422842\n",
      "Epoch 79, Batch 30/32 : Loss = 0.007724338211119175\n",
      "Epoch 79, Batch 31/32 : Loss = 0.003093868959695101\n",
      "Epoch 79 finished in 0.04003320535024007 minutes\n",
      "Epoch 79 training_loss = 0.008775678463280201\n",
      "h--------Y---[--oo----E---]]--0---}---g----bb---(--- => hY[oE]0}gb(, Ground Truth is hY[oE]0}gb(\n",
      "7-------T----C-----S---f--}---?---L----5--j--K------ => 7TCSf}?L5jK, Ground Truth is 7TCSf}?L5jK\n",
      "Epoch 79 val_loss = 0.004414140246808529, word_accuracy = 0.9939759036144579\n",
      "Epoch 80, Batch 0/32 : Loss = 0.012876225635409355\n",
      "Epoch 80, Batch 1/32 : Loss = 0.0018176842713728547\n",
      "Epoch 80, Batch 2/32 : Loss = 0.001865222118794918\n",
      "Epoch 80, Batch 3/32 : Loss = 0.002978516975417733\n",
      "Epoch 80, Batch 4/32 : Loss = 0.0024302001111209393\n",
      "Epoch 80, Batch 5/32 : Loss = 0.0017077433876693249\n",
      "Epoch 80, Batch 6/32 : Loss = 0.001217799261212349\n",
      "Epoch 80, Batch 7/32 : Loss = 0.007201953325420618\n",
      "Epoch 80, Batch 8/32 : Loss = 0.008370906114578247\n",
      "Epoch 80, Batch 9/32 : Loss = 0.0013370339293032885\n",
      "Epoch 80, Batch 10/32 : Loss = 0.047844160348176956\n",
      "Epoch 80, Batch 11/32 : Loss = 0.002642149804159999\n",
      "Epoch 80, Batch 12/32 : Loss = 0.008334534242749214\n",
      "Epoch 80, Batch 13/32 : Loss = 0.005603361409157515\n",
      "Epoch 80, Batch 14/32 : Loss = 0.001565655693411827\n",
      "Epoch 80, Batch 15/32 : Loss = 0.006459331139922142\n",
      "Epoch 80, Batch 16/32 : Loss = 0.004184480290859938\n",
      "Epoch 80, Batch 17/32 : Loss = 0.0019834768027067184\n",
      "Epoch 80, Batch 18/32 : Loss = 0.002111174864694476\n",
      "Epoch 80, Batch 19/32 : Loss = 0.0023680594749748707\n",
      "Epoch 80, Batch 20/32 : Loss = 0.004077262710779905\n",
      "Epoch 80, Batch 21/32 : Loss = 0.002996829804033041\n",
      "Epoch 80, Batch 22/32 : Loss = 0.0017261473694816232\n",
      "Epoch 80, Batch 23/32 : Loss = 0.0022855044808238745\n",
      "Epoch 80, Batch 24/32 : Loss = 0.0037176120094954967\n",
      "Epoch 80, Batch 25/32 : Loss = 0.002239435911178589\n",
      "Epoch 80, Batch 26/32 : Loss = 0.0012035509571433067\n",
      "Epoch 80, Batch 27/32 : Loss = 0.019734935835003853\n",
      "Epoch 80, Batch 28/32 : Loss = 0.0026965830475091934\n",
      "Epoch 80, Batch 29/32 : Loss = 0.0015163240022957325\n",
      "Epoch 80, Batch 30/32 : Loss = 0.006560956593602896\n",
      "Epoch 80, Batch 31/32 : Loss = 0.001097708591260016\n",
      "Epoch 80 finished in 0.04061268568038941 minutes\n",
      "Epoch 80 training_loss = 0.005583679769188166\n",
      "1------4----C----o---b----O----n---}---E---X---&&---- => 14CobOn}EX&, Ground Truth is 14CobOn}EX&\n",
      ")----m-----P---bb---W-----y---}--&---kk--0---__-)---- => )mPbWy}&k0_), Ground Truth is )mPbWy}&k0_)\n",
      "Epoch 80 val_loss = 0.016177354380488396, word_accuracy = 0.9819277108433735\n",
      "Epoch 81, Batch 0/32 : Loss = 0.01005016639828682\n",
      "Epoch 81, Batch 1/32 : Loss = 0.002259595785290003\n",
      "Epoch 81, Batch 2/32 : Loss = 0.0016110797878354788\n",
      "Epoch 81, Batch 3/32 : Loss = 0.011443870142102242\n",
      "Epoch 81, Batch 4/32 : Loss = 0.0038002212531864643\n",
      "Epoch 81, Batch 5/32 : Loss = 0.0028960807248950005\n",
      "Epoch 81, Batch 6/32 : Loss = 0.0024958564899861813\n",
      "Epoch 81, Batch 7/32 : Loss = 0.003054495668038726\n",
      "Epoch 81, Batch 8/32 : Loss = 0.005622074007987976\n",
      "Epoch 81, Batch 9/32 : Loss = 0.001845355611294508\n",
      "Epoch 81, Batch 10/32 : Loss = 0.0026663830503821373\n",
      "Epoch 81, Batch 11/32 : Loss = 0.005676007829606533\n",
      "Epoch 81, Batch 12/32 : Loss = 0.0013098460622131824\n",
      "Epoch 81, Batch 13/32 : Loss = 0.0017442712560296059\n",
      "Epoch 81, Batch 14/32 : Loss = 0.001356320921331644\n",
      "Epoch 81, Batch 15/32 : Loss = 0.0011830423027276993\n",
      "Epoch 81, Batch 16/32 : Loss = 0.0013208973687142134\n",
      "Epoch 81, Batch 17/32 : Loss = 0.004242070950567722\n",
      "Epoch 81, Batch 18/32 : Loss = 0.010388578288257122\n",
      "Epoch 81, Batch 19/32 : Loss = 0.03136233985424042\n",
      "Epoch 81, Batch 20/32 : Loss = 0.001198874437250197\n",
      "Epoch 81, Batch 21/32 : Loss = 0.0010741563746705651\n",
      "Epoch 81, Batch 22/32 : Loss = 0.0034262612462043762\n",
      "Epoch 81, Batch 23/32 : Loss = 0.0011615438852459192\n",
      "Epoch 81, Batch 24/32 : Loss = 0.020747110247612\n",
      "Epoch 81, Batch 25/32 : Loss = 0.0009617898031137884\n",
      "Epoch 81, Batch 26/32 : Loss = 0.001796175492927432\n",
      "Epoch 81, Batch 27/32 : Loss = 0.0034014349803328514\n",
      "Epoch 81, Batch 28/32 : Loss = 0.001164136454463005\n",
      "Epoch 81, Batch 29/32 : Loss = 0.002174415858462453\n",
      "Epoch 81, Batch 30/32 : Loss = 0.016632210463285446\n",
      "Epoch 81, Batch 31/32 : Loss = 0.0012359815882518888\n",
      "Epoch 81 finished in 0.04200986623764038 minutes\n",
      "Epoch 81 training_loss = 0.0051476676017045975\n",
      "T--------i--oo---;--{--'--v-------=-------0----4--- => Tio;{'v-=-04, Ground Truth is Tio;{'v-=-04\n",
      "$-----mm----5--x--p---8---m----SS--F---Z--C---'-}-- => $m5xp8mSFZC'}, Ground Truth is $m5xp8mSFZC'}\n",
      "Epoch 81 val_loss = 0.004477764014154673, word_accuracy = 0.9919678714859438\n",
      "Epoch 82, Batch 0/32 : Loss = 0.0013474073493853211\n",
      "Epoch 82, Batch 1/32 : Loss = 0.004078330472111702\n",
      "Epoch 82, Batch 2/32 : Loss = 0.001763132051564753\n",
      "Epoch 82, Batch 3/32 : Loss = 0.0023749063257128\n",
      "Epoch 82, Batch 4/32 : Loss = 0.0020689149387180805\n",
      "Epoch 82, Batch 5/32 : Loss = 0.003911444451659918\n",
      "Epoch 82, Batch 6/32 : Loss = 0.0026600195560604334\n",
      "Epoch 82, Batch 7/32 : Loss = 0.0013227416202425957\n",
      "Epoch 82, Batch 8/32 : Loss = 0.020224036648869514\n",
      "Epoch 82, Batch 9/32 : Loss = 0.0023739063180983067\n",
      "Epoch 82, Batch 10/32 : Loss = 0.009237432852387428\n",
      "Epoch 82, Batch 11/32 : Loss = 0.0020500081591308117\n",
      "Epoch 82, Batch 12/32 : Loss = 0.034845758229494095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Batch 13/32 : Loss = 0.058514900505542755\n",
      "Epoch 82, Batch 14/32 : Loss = 0.003024251665920019\n",
      "Epoch 82, Batch 15/32 : Loss = 0.0019628209993243217\n",
      "Epoch 82, Batch 16/32 : Loss = 0.013194598257541656\n",
      "Epoch 82, Batch 17/32 : Loss = 0.005365779157727957\n",
      "Epoch 82, Batch 18/32 : Loss = 0.006171423941850662\n",
      "Epoch 82, Batch 19/32 : Loss = 0.001340122427791357\n",
      "Epoch 82, Batch 20/32 : Loss = 0.0011918691452592611\n",
      "Epoch 82, Batch 21/32 : Loss = 0.002114924369379878\n",
      "Epoch 82, Batch 22/32 : Loss = 0.007142174057662487\n",
      "Epoch 82, Batch 23/32 : Loss = 0.0036577265709638596\n",
      "Epoch 82, Batch 24/32 : Loss = 0.02032296732068062\n",
      "Epoch 82, Batch 25/32 : Loss = 0.005278243217617273\n",
      "Epoch 82, Batch 26/32 : Loss = 0.0033264828380197287\n",
      "Epoch 82, Batch 27/32 : Loss = 0.010146151296794415\n",
      "Epoch 82, Batch 28/32 : Loss = 0.0023174514062702656\n",
      "Epoch 82, Batch 29/32 : Loss = 0.0019383116159588099\n",
      "Epoch 82, Batch 30/32 : Loss = 0.03347142040729523\n",
      "Epoch 82, Batch 31/32 : Loss = 0.0033129784278571606\n",
      "Epoch 82 finished in 0.0430193821589152 minutes\n",
      "Epoch 82 training_loss = 0.008647511713206768\n",
      "M---------55--\"---#---t--\"--l--F---5---GG----E---22---- => M5\"#t\"lF5GE2, Ground Truth is M5\"#t\"lF5GE2\n",
      "8--------|---_---^--dd---aa---m-----!-xx--ee--i--C----- => 8|_^dam!xeiC, Ground Truth is 8|_^dam!xeiC\n",
      "Epoch 82 val_loss = 0.01240625698119402, word_accuracy = 0.9759036144578314\n",
      "Epoch 83, Batch 0/32 : Loss = 0.0024603004567325115\n",
      "Epoch 83, Batch 1/32 : Loss = 0.001596258021891117\n",
      "Epoch 83, Batch 2/32 : Loss = 0.031234845519065857\n",
      "Epoch 83, Batch 3/32 : Loss = 0.0036282737273722887\n",
      "Epoch 83, Batch 4/32 : Loss = 0.007180634420365095\n",
      "Epoch 83, Batch 5/32 : Loss = 0.0030019867699593306\n",
      "Epoch 83, Batch 6/32 : Loss = 0.009824695996940136\n",
      "Epoch 83, Batch 7/32 : Loss = 0.00782527681440115\n",
      "Epoch 83, Batch 8/32 : Loss = 0.003884843084961176\n",
      "Epoch 83, Batch 9/32 : Loss = 0.003123918082565069\n",
      "Epoch 83, Batch 10/32 : Loss = 0.00723712844774127\n",
      "Epoch 83, Batch 11/32 : Loss = 0.0021613072603940964\n",
      "Epoch 83, Batch 12/32 : Loss = 0.0018272758461534977\n",
      "Epoch 83, Batch 13/32 : Loss = 0.0034048284869641066\n",
      "Epoch 83, Batch 14/32 : Loss = 0.003214001189917326\n",
      "Epoch 83, Batch 15/32 : Loss = 0.0035647209733724594\n",
      "Epoch 83, Batch 16/32 : Loss = 0.02915716916322708\n",
      "Epoch 83, Batch 17/32 : Loss = 0.011431812308728695\n",
      "Epoch 83, Batch 18/32 : Loss = 0.003855702467262745\n",
      "Epoch 83, Batch 19/32 : Loss = 0.0032862790394574404\n",
      "Epoch 83, Batch 20/32 : Loss = 0.012606826610863209\n",
      "Epoch 83, Batch 21/32 : Loss = 0.0023604929447174072\n",
      "Epoch 83, Batch 22/32 : Loss = 0.0023360014893114567\n",
      "Epoch 83, Batch 23/32 : Loss = 0.0019728983752429485\n",
      "Epoch 83, Batch 24/32 : Loss = 0.0034879015292972326\n",
      "Epoch 83, Batch 25/32 : Loss = 0.012026369571685791\n",
      "Epoch 83, Batch 26/32 : Loss = 0.020545676350593567\n",
      "Epoch 83, Batch 27/32 : Loss = 0.007938784547150135\n",
      "Epoch 83, Batch 28/32 : Loss = 0.0014741106424480677\n",
      "Epoch 83, Batch 29/32 : Loss = 0.004322200082242489\n",
      "Epoch 83, Batch 30/32 : Loss = 0.009805646724998951\n",
      "Epoch 83, Batch 31/32 : Loss = 0.0016731098294258118\n",
      "Epoch 83 finished in 0.04084402322769165 minutes\n",
      "Epoch 83 training_loss = 0.007132121827453375\n",
      "0--------55---66---hh---__---u----~---<<----OO-----U-----J--..-0---- => 056h_u~<OUJ.0, Ground Truth is 056h_u~<OUJ.0\n",
      "$--------mm-----55---x---p----8----m------S----F----Z----C---'-}---- => $m5xp8mSFZC'}, Ground Truth is $m5xp8mSFZC'}\n",
      "Epoch 83 val_loss = 0.01532332319766283, word_accuracy = 0.9738955823293173\n",
      "Epoch 84, Batch 0/32 : Loss = 0.0023860321380198\n",
      "Epoch 84, Batch 1/32 : Loss = 0.016857333481311798\n",
      "Epoch 84, Batch 2/32 : Loss = 0.002008648356422782\n",
      "Epoch 84, Batch 3/32 : Loss = 0.0013287565670907497\n",
      "Epoch 84, Batch 4/32 : Loss = 0.0018152669072151184\n",
      "Epoch 84, Batch 5/32 : Loss = 0.031092649325728416\n",
      "Epoch 84, Batch 6/32 : Loss = 0.003454112447798252\n",
      "Epoch 84, Batch 7/32 : Loss = 0.03686327487230301\n",
      "Epoch 84, Batch 8/32 : Loss = 0.013594117015600204\n",
      "Epoch 84, Batch 9/32 : Loss = 0.002945499960333109\n",
      "Epoch 84, Batch 10/32 : Loss = 0.01815190352499485\n",
      "Epoch 84, Batch 11/32 : Loss = 0.005463066045194864\n",
      "Epoch 84, Batch 12/32 : Loss = 0.002924809930846095\n",
      "Epoch 84, Batch 13/32 : Loss = 0.0022093174047768116\n",
      "Epoch 84, Batch 14/32 : Loss = 0.0038210181519389153\n",
      "Epoch 84, Batch 15/32 : Loss = 0.0017249224474653602\n",
      "Epoch 84, Batch 16/32 : Loss = 0.0025734701193869114\n",
      "Epoch 84, Batch 17/32 : Loss = 0.006157758645713329\n",
      "Epoch 84, Batch 18/32 : Loss = 0.022244855761528015\n",
      "Epoch 84, Batch 19/32 : Loss = 0.016158323734998703\n",
      "Epoch 84, Batch 20/32 : Loss = 0.06214875355362892\n",
      "Epoch 84, Batch 21/32 : Loss = 0.015266532078385353\n",
      "Epoch 84, Batch 22/32 : Loss = 0.0027655656449496746\n",
      "Epoch 84, Batch 23/32 : Loss = 0.016254888847470284\n",
      "Epoch 84, Batch 24/32 : Loss = 0.006183694116771221\n",
      "Epoch 84, Batch 25/32 : Loss = 0.005531860515475273\n",
      "Epoch 84, Batch 26/32 : Loss = 0.004174601286649704\n",
      "Epoch 84, Batch 27/32 : Loss = 0.004112197086215019\n",
      "Epoch 84, Batch 28/32 : Loss = 0.0061668637208640575\n",
      "Epoch 84, Batch 29/32 : Loss = 0.05373541638255119\n",
      "Epoch 84, Batch 30/32 : Loss = 0.002508751815184951\n",
      "Epoch 84, Batch 31/32 : Loss = 0.0010977286146953702\n",
      "Epoch 84 finished in 0.043075275421142575 minutes\n",
      "Epoch 84 training_loss = 0.011976271867752075\n",
      "*-------oo---55---s---L---[--a----%-----**--aa---C-----D---`-- => *o5sL[a%*aCD`, Ground Truth is *o5sL[a%*aCD`\n",
      "n------:--{---^---f--__--?----F----G----$$---$$----1----D----- => n:{^f_?FG$$1D, Ground Truth is n:{^f_?FG$$1D\n",
      "Epoch 84 val_loss = 0.021503327414393425, word_accuracy = 0.9477911646586346\n",
      "Epoch 85, Batch 0/32 : Loss = 0.07394857704639435\n",
      "Epoch 85, Batch 1/32 : Loss = 0.005092693492770195\n",
      "Epoch 85, Batch 2/32 : Loss = 0.00911794975399971\n",
      "Epoch 85, Batch 3/32 : Loss = 0.00805792398750782\n",
      "Epoch 85, Batch 4/32 : Loss = 0.004443766549229622\n",
      "Epoch 85, Batch 5/32 : Loss = 0.006121301092207432\n",
      "Epoch 85, Batch 6/32 : Loss = 0.023540854454040527\n",
      "Epoch 85, Batch 7/32 : Loss = 0.0031056259758770466\n",
      "Epoch 85, Batch 8/32 : Loss = 0.015397357754409313\n",
      "Epoch 85, Batch 9/32 : Loss = 0.0031801280565559864\n",
      "Epoch 85, Batch 10/32 : Loss = 0.05716899037361145\n",
      "Epoch 85, Batch 11/32 : Loss = 0.002808281686156988\n",
      "Epoch 85, Batch 12/32 : Loss = 0.2843100428581238\n",
      "Epoch 85, Batch 13/32 : Loss = 0.004277941305190325\n",
      "Epoch 85, Batch 14/32 : Loss = 0.0048698605969548225\n",
      "Epoch 85, Batch 15/32 : Loss = 0.016153918579220772\n",
      "Epoch 85, Batch 16/32 : Loss = 0.011127435602247715\n",
      "Epoch 85, Batch 17/32 : Loss = 0.06558000296354294\n",
      "Epoch 85, Batch 18/32 : Loss = 0.010373326949775219\n",
      "Epoch 85, Batch 19/32 : Loss = 0.010541098192334175\n",
      "Epoch 85, Batch 20/32 : Loss = 0.025444023311138153\n",
      "Epoch 85, Batch 21/32 : Loss = 0.019119467586278915\n",
      "Epoch 85, Batch 22/32 : Loss = 0.007660670205950737\n",
      "Epoch 85, Batch 23/32 : Loss = 0.005404025316238403\n",
      "Epoch 85, Batch 24/32 : Loss = 0.009524203836917877\n",
      "Epoch 85, Batch 25/32 : Loss = 0.057256173342466354\n",
      "Epoch 85, Batch 26/32 : Loss = 0.008289812132716179\n",
      "Epoch 85, Batch 27/32 : Loss = 0.01921055279672146\n",
      "Epoch 85, Batch 28/32 : Loss = 0.004047321155667305\n",
      "Epoch 85, Batch 29/32 : Loss = 0.004129217471927404\n",
      "Epoch 85, Batch 30/32 : Loss = 0.011661806143820286\n",
      "Epoch 85, Batch 31/32 : Loss = 0.0067973993718624115\n",
      "Epoch 85 finished in 0.04055509567260742 minutes\n",
      "Epoch 85 training_loss = 0.025439811870455742\n",
      "4--------w-----`--N-----(---\\---f--2----@@-----AA----s---- => 4w`N(\\f2@As, Ground Truth is 4w`N(\\f2@As\n",
      "1-------4----C----o----b----O------n---}---E----X---&&---- => 14CobOn}EX&, Ground Truth is 14CobOn}EX&\n",
      "Epoch 85 val_loss = 0.013357727788388729, word_accuracy = 0.9779116465863453\n",
      "Epoch 86, Batch 0/32 : Loss = 0.01227976568043232\n",
      "Epoch 86, Batch 1/32 : Loss = 0.009268094785511494\n",
      "Epoch 86, Batch 2/32 : Loss = 0.0021371422335505486\n",
      "Epoch 86, Batch 3/32 : Loss = 0.0036657541058957577\n",
      "Epoch 86, Batch 4/32 : Loss = 0.0620749369263649\n",
      "Epoch 86, Batch 5/32 : Loss = 0.004939873702824116\n",
      "Epoch 86, Batch 6/32 : Loss = 0.0045038145035505295\n",
      "Epoch 86, Batch 7/32 : Loss = 0.003512693103402853\n",
      "Epoch 86, Batch 8/32 : Loss = 0.0040302760899066925\n",
      "Epoch 86, Batch 9/32 : Loss = 0.0026628589257597923\n",
      "Epoch 86, Batch 10/32 : Loss = 0.002601923421025276\n",
      "Epoch 86, Batch 11/32 : Loss = 0.004583172500133514\n",
      "Epoch 86, Batch 12/32 : Loss = 0.00901365838944912\n",
      "Epoch 86, Batch 13/32 : Loss = 0.0037321532145142555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Batch 14/32 : Loss = 0.0024920161813497543\n",
      "Epoch 86, Batch 15/32 : Loss = 0.007623789831995964\n",
      "Epoch 86, Batch 16/32 : Loss = 0.006166525650769472\n",
      "Epoch 86, Batch 17/32 : Loss = 0.0034596133045852184\n",
      "Epoch 86, Batch 18/32 : Loss = 0.004696926102042198\n",
      "Epoch 86, Batch 19/32 : Loss = 0.003951398655772209\n",
      "Epoch 86, Batch 20/32 : Loss = 0.0030391961336135864\n",
      "Epoch 86, Batch 21/32 : Loss = 0.002769127022475004\n",
      "Epoch 86, Batch 22/32 : Loss = 0.10575240850448608\n",
      "Epoch 86, Batch 23/32 : Loss = 0.009369251318275928\n",
      "Epoch 86, Batch 24/32 : Loss = 0.004473352804780006\n",
      "Epoch 86, Batch 25/32 : Loss = 0.0032209944911301136\n",
      "Epoch 86, Batch 26/32 : Loss = 0.01994648203253746\n",
      "Epoch 86, Batch 27/32 : Loss = 0.0030088939238339663\n",
      "Epoch 86, Batch 28/32 : Loss = 0.014460564590990543\n",
      "Epoch 86, Batch 29/32 : Loss = 0.004227965138852596\n",
      "Epoch 86, Batch 30/32 : Loss = 0.0021891342476010323\n",
      "Epoch 86, Batch 31/32 : Loss = 0.20595327019691467\n",
      "Epoch 86 finished in 0.03904975652694702 minutes\n",
      "Epoch 86 training_loss = 0.011424833908677101\n",
      "c------DD---j-44--}--i-C---->---**--e---6---c--- => cDj4}iC>*e6c, Ground Truth is cDj4}iC>*e6c\n",
      "6------t-qq---*--->----p---K----m------S---`jj-- => 6tq*>pKmS`j, Ground Truth is 6tq*>pKmS`j\n",
      "Epoch 86 val_loss = 0.013972609303891659, word_accuracy = 0.9879518072289156\n",
      "Epoch 87, Batch 0/32 : Loss = 0.0020985077135264874\n",
      "Epoch 87, Batch 1/32 : Loss = 0.004043614491820335\n",
      "Epoch 87, Batch 2/32 : Loss = 0.002614665310829878\n",
      "Epoch 87, Batch 3/32 : Loss = 0.0025478319730609655\n",
      "Epoch 87, Batch 4/32 : Loss = 0.003631256055086851\n",
      "Epoch 87, Batch 5/32 : Loss = 0.08707790076732635\n",
      "Epoch 87, Batch 6/32 : Loss = 0.0044583529233932495\n",
      "Epoch 87, Batch 7/32 : Loss = 0.002475917572155595\n",
      "Epoch 87, Batch 8/32 : Loss = 0.029706085100769997\n",
      "Epoch 87, Batch 9/32 : Loss = 0.0038756595458835363\n",
      "Epoch 87, Batch 10/32 : Loss = 0.0027525881305336952\n",
      "Epoch 87, Batch 11/32 : Loss = 0.0018106953939422965\n",
      "Epoch 87, Batch 12/32 : Loss = 0.004687352105975151\n",
      "Epoch 87, Batch 13/32 : Loss = 0.0037361527793109417\n",
      "Epoch 87, Batch 14/32 : Loss = 0.07536806166172028\n",
      "Epoch 87, Batch 15/32 : Loss = 0.009093746542930603\n",
      "Epoch 87, Batch 16/32 : Loss = 0.015957076102495193\n",
      "Epoch 87, Batch 17/32 : Loss = 0.0051370346918702126\n",
      "Epoch 87, Batch 18/32 : Loss = 0.0031265905126929283\n",
      "Epoch 87, Batch 19/32 : Loss = 0.005232681520283222\n",
      "Epoch 87, Batch 20/32 : Loss = 0.0022315573878586292\n",
      "Epoch 87, Batch 21/32 : Loss = 0.002279566368088126\n",
      "Epoch 87, Batch 22/32 : Loss = 0.0024057519622147083\n",
      "Epoch 87, Batch 23/32 : Loss = 0.004212606232613325\n",
      "Epoch 87, Batch 24/32 : Loss = 0.010211393237113953\n",
      "Epoch 87, Batch 25/32 : Loss = 0.005992090329527855\n",
      "Epoch 87, Batch 26/32 : Loss = 0.0022316575050354004\n",
      "Epoch 87, Batch 27/32 : Loss = 0.020506037399172783\n",
      "Epoch 87, Batch 28/32 : Loss = 0.0037568239495158195\n",
      "Epoch 87, Batch 29/32 : Loss = 0.004847180098295212\n",
      "Epoch 87, Batch 30/32 : Loss = 0.004087948240339756\n",
      "Epoch 87, Batch 31/32 : Loss = 0.0046737948432564735\n",
      "Epoch 87 finished in 0.03930240472157796 minutes\n",
      "Epoch 87 training_loss = 0.010691681876778603\n",
      "^------}--`--]--2-----F---9----DD----e---h----)--f---- => ^}`]2F9Deh)f, Ground Truth is ^}`]2F9Deh)f\n",
      "h------W-----@-----A---L---D----Y--o---\\--a---\\--8--.. => hW@ALDYo\\a\\8., Ground Truth is hW@ALDYo\\a\\8.\n",
      "Epoch 87 val_loss = 0.008186249993741512, word_accuracy = 0.9779116465863453\n",
      "Epoch 88, Batch 0/32 : Loss = 0.007736303843557835\n",
      "Epoch 88, Batch 1/32 : Loss = 0.0019920107442885637\n",
      "Epoch 88, Batch 2/32 : Loss = 0.0068001155741512775\n",
      "Epoch 88, Batch 3/32 : Loss = 0.012241791933774948\n",
      "Epoch 88, Batch 4/32 : Loss = 0.010451621375977993\n",
      "Epoch 88, Batch 5/32 : Loss = 0.0031402958557009697\n",
      "Epoch 88, Batch 6/32 : Loss = 0.00195615878328681\n",
      "Epoch 88, Batch 7/32 : Loss = 0.017453715205192566\n",
      "Epoch 88, Batch 8/32 : Loss = 0.013803775422275066\n",
      "Epoch 88, Batch 9/32 : Loss = 0.005392473191022873\n",
      "Epoch 88, Batch 10/32 : Loss = 0.003792770206928253\n",
      "Epoch 88, Batch 11/32 : Loss = 0.002742636250331998\n",
      "Epoch 88, Batch 12/32 : Loss = 0.005196013487875462\n",
      "Epoch 88, Batch 13/32 : Loss = 0.0033857848029583693\n",
      "Epoch 88, Batch 14/32 : Loss = 0.02162269875407219\n",
      "Epoch 88, Batch 15/32 : Loss = 0.0025708023458719254\n",
      "Epoch 88, Batch 16/32 : Loss = 0.004333664663136005\n",
      "Epoch 88, Batch 17/32 : Loss = 0.002892834600061178\n",
      "Epoch 88, Batch 18/32 : Loss = 0.00243582297116518\n",
      "Epoch 88, Batch 19/32 : Loss = 0.0018275558250024915\n",
      "Epoch 88, Batch 20/32 : Loss = 0.0038382909260690212\n",
      "Epoch 88, Batch 21/32 : Loss = 0.027131328359246254\n",
      "Epoch 88, Batch 22/32 : Loss = 0.004218457266688347\n",
      "Epoch 88, Batch 23/32 : Loss = 0.003136600600555539\n",
      "Epoch 88, Batch 24/32 : Loss = 0.002660531084984541\n",
      "Epoch 88, Batch 25/32 : Loss = 0.001464079599827528\n",
      "Epoch 88, Batch 26/32 : Loss = 0.0021599396131932735\n",
      "Epoch 88, Batch 27/32 : Loss = 0.007856571115553379\n",
      "Epoch 88, Batch 28/32 : Loss = 0.0022243696730583906\n",
      "Epoch 88, Batch 29/32 : Loss = 0.006261469330638647\n",
      "Epoch 88, Batch 30/32 : Loss = 0.0021852580830454826\n",
      "Epoch 88, Batch 31/32 : Loss = 0.0016781846061348915\n",
      "Epoch 88 finished in 0.04085583686828613 minutes\n",
      "Epoch 88 training_loss = 0.00626877136528492\n",
      "G-------NN----4---{---U---kk--d---2----E----y--9---u---)- => GN4{Ukd2Ey9u), Ground Truth is GN4{Ukd2Ey9u)\n",
      "u-------'--=----x---EE----@------+----00---0----L----f--- => u'=xE@+00Lf, Ground Truth is u'=xE@+00Lf\n",
      "Epoch 88 val_loss = 0.010222275741398335, word_accuracy = 0.9859437751004017\n",
      "Epoch 89, Batch 0/32 : Loss = 0.013400244526565075\n",
      "Epoch 89, Batch 1/32 : Loss = 0.01952657662332058\n",
      "Epoch 89, Batch 2/32 : Loss = 0.0016831527464091778\n",
      "Epoch 89, Batch 3/32 : Loss = 0.017624422907829285\n",
      "Epoch 89, Batch 4/32 : Loss = 0.003951376304030418\n",
      "Epoch 89, Batch 5/32 : Loss = 0.004067692905664444\n",
      "Epoch 89, Batch 6/32 : Loss = 0.0016504430677741766\n",
      "Epoch 89, Batch 7/32 : Loss = 0.002573581412434578\n",
      "Epoch 89, Batch 8/32 : Loss = 0.004464066121727228\n",
      "Epoch 89, Batch 9/32 : Loss = 0.005471750628203154\n",
      "Epoch 89, Batch 10/32 : Loss = 0.007591142784804106\n",
      "Epoch 89, Batch 11/32 : Loss = 0.006502891890704632\n",
      "Epoch 89, Batch 12/32 : Loss = 0.006423556711524725\n",
      "Epoch 89, Batch 13/32 : Loss = 0.0021386449225246906\n",
      "Epoch 89, Batch 14/32 : Loss = 0.010794629342854023\n",
      "Epoch 89, Batch 15/32 : Loss = 0.003147959476336837\n",
      "Epoch 89, Batch 16/32 : Loss = 0.00790129043161869\n",
      "Epoch 89, Batch 17/32 : Loss = 0.003053997177630663\n",
      "Epoch 89, Batch 18/32 : Loss = 0.00802716612815857\n",
      "Epoch 89, Batch 19/32 : Loss = 0.02426391839981079\n",
      "Epoch 89, Batch 20/32 : Loss = 0.013103486970067024\n",
      "Epoch 89, Batch 21/32 : Loss = 0.0012725635897368193\n",
      "Epoch 89, Batch 22/32 : Loss = 0.0013579737860709429\n",
      "Epoch 89, Batch 23/32 : Loss = 0.02782844379544258\n",
      "Epoch 89, Batch 24/32 : Loss = 0.004551955498754978\n",
      "Epoch 89, Batch 25/32 : Loss = 0.004791774787008762\n",
      "Epoch 89, Batch 26/32 : Loss = 0.0017338497564196587\n",
      "Epoch 89, Batch 27/32 : Loss = 0.0023675160482525826\n",
      "Epoch 89, Batch 28/32 : Loss = 0.0038832202553749084\n",
      "Epoch 89, Batch 29/32 : Loss = 0.002477051457390189\n",
      "Epoch 89, Batch 30/32 : Loss = 0.001486773369833827\n",
      "Epoch 89, Batch 31/32 : Loss = 0.04445677623152733\n",
      "Epoch 89 finished in 0.04043566385904948 minutes\n",
      "Epoch 89 training_loss = 0.007218319457024336\n",
      "k------}---O-----T----)--O-----MM-----~---->----H----22---w----- => k}OT)OM~>H2w, Ground Truth is k}OT)OM~>H2w\n",
      "V-------4---mm-----HH-------OO----t--o--->>----W-----;--1--3---- => V4mH-Oto>W;13, Ground Truth is V4mH-Oto>W;13\n",
      "Epoch 89 val_loss = 0.00853950809687376, word_accuracy = 0.9899598393574297\n",
      "Epoch 90, Batch 0/32 : Loss = 0.023434046655893326\n",
      "Epoch 90, Batch 1/32 : Loss = 0.0027722856029868126\n",
      "Epoch 90, Batch 2/32 : Loss = 0.0020243674516677856\n",
      "Epoch 90, Batch 3/32 : Loss = 0.005868617910891771\n",
      "Epoch 90, Batch 4/32 : Loss = 0.003298512427136302\n",
      "Epoch 90, Batch 5/32 : Loss = 0.004560185596346855\n",
      "Epoch 90, Batch 6/32 : Loss = 0.01084190234541893\n",
      "Epoch 90, Batch 7/32 : Loss = 0.002651832066476345\n",
      "Epoch 90, Batch 8/32 : Loss = 0.002513444982469082\n",
      "Epoch 90, Batch 9/32 : Loss = 0.005159489810466766\n",
      "Epoch 90, Batch 10/32 : Loss = 0.0036099585704505444\n",
      "Epoch 90, Batch 11/32 : Loss = 0.0022945194505155087\n",
      "Epoch 90, Batch 12/32 : Loss = 0.0038086646236479282\n",
      "Epoch 90, Batch 13/32 : Loss = 0.0032512175384908915\n",
      "Epoch 90, Batch 14/32 : Loss = 0.0013895484153181314\n",
      "Epoch 90, Batch 15/32 : Loss = 0.002462848089635372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Batch 16/32 : Loss = 0.0021991939283907413\n",
      "Epoch 90, Batch 17/32 : Loss = 0.0013708279002457857\n",
      "Epoch 90, Batch 18/32 : Loss = 0.004601950757205486\n",
      "Epoch 90, Batch 19/32 : Loss = 0.0011970504419878125\n",
      "Epoch 90, Batch 20/32 : Loss = 0.0018956842832267284\n",
      "Epoch 90, Batch 21/32 : Loss = 0.0013762324815616012\n",
      "Epoch 90, Batch 22/32 : Loss = 0.0028745015151798725\n",
      "Epoch 90, Batch 23/32 : Loss = 0.009472106583416462\n",
      "Epoch 90, Batch 24/32 : Loss = 0.0019632698968052864\n",
      "Epoch 90, Batch 25/32 : Loss = 0.0031178228091448545\n",
      "Epoch 90, Batch 26/32 : Loss = 0.003635820932686329\n",
      "Epoch 90, Batch 27/32 : Loss = 0.003438242245465517\n",
      "Epoch 90, Batch 28/32 : Loss = 0.0015486131887882948\n",
      "Epoch 90, Batch 29/32 : Loss = 0.001798903220333159\n",
      "Epoch 90, Batch 30/32 : Loss = 0.0016526805702596903\n",
      "Epoch 90, Batch 31/32 : Loss = 0.0007739041466265917\n",
      "Epoch 90 finished in 0.041100509961446124 minutes\n",
      "Epoch 90 training_loss = 0.003925496246665716\n",
      ";-------Z---`--E---Pbb---9---b--M---!!---Y---u---C-------- => ;Z`EPb9bM!YuC, Ground Truth is ;Z`EPb9bM!5uC\n",
      "t----XX---:-BB----a---JJ--//--[--Y----4---kk---W-----|---- => tX:BaJ/[Y4kW|, Ground Truth is tX:BaJ/[Y4kW|\n",
      "Epoch 90 val_loss = 0.004174493718892336, word_accuracy = 0.9919678714859438\n",
      "Epoch 91, Batch 0/32 : Loss = 0.001187906600534916\n",
      "Epoch 91, Batch 1/32 : Loss = 0.002183968899771571\n",
      "Epoch 91, Batch 2/32 : Loss = 0.0013020762708038092\n",
      "Epoch 91, Batch 3/32 : Loss = 0.0016618851805105805\n",
      "Epoch 91, Batch 4/32 : Loss = 0.00170198455452919\n",
      "Epoch 91, Batch 5/32 : Loss = 0.008770814165472984\n",
      "Epoch 91, Batch 6/32 : Loss = 0.0015047835186123848\n",
      "Epoch 91, Batch 7/32 : Loss = 0.005132969468832016\n",
      "Epoch 91, Batch 8/32 : Loss = 0.0013288052286952734\n",
      "Epoch 91, Batch 9/32 : Loss = 0.006298841908574104\n",
      "Epoch 91, Batch 10/32 : Loss = 0.008237800560891628\n",
      "Epoch 91, Batch 11/32 : Loss = 0.0031149922870099545\n",
      "Epoch 91, Batch 12/32 : Loss = 0.0012200655182823539\n",
      "Epoch 91, Batch 13/32 : Loss = 0.0013996867928653955\n",
      "Epoch 91, Batch 14/32 : Loss = 0.006212132517248392\n",
      "Epoch 91, Batch 15/32 : Loss = 0.0015015013050287962\n",
      "Epoch 91, Batch 16/32 : Loss = 0.0018948352662846446\n",
      "Epoch 91, Batch 17/32 : Loss = 0.0009749972959980369\n",
      "Epoch 91, Batch 18/32 : Loss = 0.0011955711524933577\n",
      "Epoch 91, Batch 19/32 : Loss = 0.08202477544546127\n",
      "Epoch 91, Batch 20/32 : Loss = 0.0012878788402304053\n",
      "Epoch 91, Batch 21/32 : Loss = 0.001298497780226171\n",
      "Epoch 91, Batch 22/32 : Loss = 0.02454799972474575\n",
      "Epoch 91, Batch 23/32 : Loss = 0.001277424395084381\n",
      "Epoch 91, Batch 24/32 : Loss = 0.0010789301013574004\n",
      "Epoch 91, Batch 25/32 : Loss = 0.001477892859838903\n",
      "Epoch 91, Batch 26/32 : Loss = 0.0014639691216871142\n",
      "Epoch 91, Batch 27/32 : Loss = 0.036145053803920746\n",
      "Epoch 91, Batch 28/32 : Loss = 0.0016332166269421577\n",
      "Epoch 91, Batch 29/32 : Loss = 0.004975213669240475\n",
      "Epoch 91, Batch 30/32 : Loss = 0.013691925443708897\n",
      "Epoch 91, Batch 31/32 : Loss = 0.009329850785434246\n",
      "Epoch 91 finished in 0.040373341242472334 minutes\n",
      "Epoch 91 training_loss = 0.007354043889790773\n",
      "%---------ee---=----9---\"---#----G----'-l--a---d-----@----- => %e=9\"#G'lad@, Ground Truth is %e=9\"#G'lad@\n",
      "K----------44---__---o----]-----z---II-W-------9----ss----- => K4_o]-zIW9s, Ground Truth is K4_o]-zIW9s\n",
      "Epoch 91 val_loss = 0.03193027526140213, word_accuracy = 0.9779116465863453\n",
      "Epoch 92, Batch 0/32 : Loss = 0.002422230551019311\n",
      "Epoch 92, Batch 1/32 : Loss = 0.004095081239938736\n",
      "Epoch 92, Batch 2/32 : Loss = 0.010784324258565903\n",
      "Epoch 92, Batch 3/32 : Loss = 0.004203725606203079\n",
      "Epoch 92, Batch 4/32 : Loss = 0.0022094089072197676\n",
      "Epoch 92, Batch 5/32 : Loss = 0.01369986217468977\n",
      "Epoch 92, Batch 6/32 : Loss = 0.019558629021048546\n",
      "Epoch 92, Batch 7/32 : Loss = 0.00482951570302248\n",
      "Epoch 92, Batch 8/32 : Loss = 0.0013634196948260069\n",
      "Epoch 92, Batch 9/32 : Loss = 0.003873443230986595\n",
      "Epoch 92, Batch 10/32 : Loss = 0.022917773574590683\n",
      "Epoch 92, Batch 11/32 : Loss = 0.002314303070306778\n",
      "Epoch 92, Batch 12/32 : Loss = 0.16829684376716614\n",
      "Epoch 92, Batch 13/32 : Loss = 0.008242390118539333\n",
      "Epoch 92, Batch 14/32 : Loss = 0.0039368607103824615\n",
      "Epoch 92, Batch 15/32 : Loss = 0.002093867864459753\n",
      "Epoch 92, Batch 16/32 : Loss = 0.0033010393381118774\n",
      "Epoch 92, Batch 17/32 : Loss = 0.01688200607895851\n",
      "Epoch 92, Batch 18/32 : Loss = 0.009626412764191628\n",
      "Epoch 92, Batch 19/32 : Loss = 0.00921183917671442\n",
      "Epoch 92, Batch 20/32 : Loss = 0.006052636541426182\n",
      "Epoch 92, Batch 21/32 : Loss = 0.0029229940846562386\n",
      "Epoch 92, Batch 22/32 : Loss = 0.0018682938534766436\n",
      "Epoch 92, Batch 23/32 : Loss = 0.006719444878399372\n",
      "Epoch 92, Batch 24/32 : Loss = 0.0200751144438982\n",
      "Epoch 92, Batch 25/32 : Loss = 0.003290998749434948\n",
      "Epoch 92, Batch 26/32 : Loss = 0.015950871631503105\n",
      "Epoch 92, Batch 27/32 : Loss = 0.0035242619924247265\n",
      "Epoch 92, Batch 28/32 : Loss = 0.002517496468499303\n",
      "Epoch 92, Batch 29/32 : Loss = 0.004159736447036266\n",
      "Epoch 92, Batch 30/32 : Loss = 0.004083240404725075\n",
      "Epoch 92, Batch 31/32 : Loss = 0.0044318861328065395\n",
      "Epoch 92 finished in 0.040411758422851565 minutes\n",
      "Epoch 92 training_loss = 0.012388180010020733\n",
      "U------$$---1---\"--f--#---r-``-&&---11---W-----6----- => U$1\"f#r`&1W6, Ground Truth is U$1\"f#r`&1W6\n",
      "n------55---9---<<----a---HH----?---BB---#----3---~~- => n59<aH?B#3~, Ground Truth is n59<aH?B#3~\n",
      "Epoch 92 val_loss = 0.004942155908793211, word_accuracy = 0.9939759036144579\n",
      "Epoch 93, Batch 0/32 : Loss = 0.02134675160050392\n",
      "Epoch 93, Batch 1/32 : Loss = 0.009328553453087807\n",
      "Epoch 93, Batch 2/32 : Loss = 0.007360184099525213\n",
      "Epoch 93, Batch 3/32 : Loss = 0.0033301590010523796\n",
      "Epoch 93, Batch 4/32 : Loss = 0.006814322434365749\n",
      "Epoch 93, Batch 5/32 : Loss = 0.0035315239802002907\n",
      "Epoch 93, Batch 6/32 : Loss = 0.002592931967228651\n",
      "Epoch 93, Batch 7/32 : Loss = 0.004317809361964464\n",
      "Epoch 93, Batch 8/32 : Loss = 0.0063176644034683704\n",
      "Epoch 93, Batch 9/32 : Loss = 0.009569915011525154\n",
      "Epoch 93, Batch 10/32 : Loss = 0.0030739931389689445\n",
      "Epoch 93, Batch 11/32 : Loss = 0.0035621561110019684\n",
      "Epoch 93, Batch 12/32 : Loss = 0.0019353446550667286\n",
      "Epoch 93, Batch 13/32 : Loss = 0.002737241331487894\n",
      "Epoch 93, Batch 14/32 : Loss = 0.023449087515473366\n",
      "Epoch 93, Batch 15/32 : Loss = 0.02725275233387947\n",
      "Epoch 93, Batch 16/32 : Loss = 0.005283602979034185\n",
      "Epoch 93, Batch 17/32 : Loss = 0.0026245038025081158\n",
      "Epoch 93, Batch 18/32 : Loss = 0.022113002836704254\n",
      "Epoch 93, Batch 19/32 : Loss = 0.0021476519759744406\n",
      "Epoch 93, Batch 20/32 : Loss = 0.0013726605102419853\n",
      "Epoch 93, Batch 21/32 : Loss = 0.0047830333933234215\n",
      "Epoch 93, Batch 22/32 : Loss = 0.0031210517045110464\n",
      "Epoch 93, Batch 23/32 : Loss = 0.07930408418178558\n",
      "Epoch 93, Batch 24/32 : Loss = 0.004481224808841944\n",
      "Epoch 93, Batch 25/32 : Loss = 0.002661667997017503\n",
      "Epoch 93, Batch 26/32 : Loss = 0.0018317849608138204\n",
      "Epoch 93, Batch 27/32 : Loss = 0.003201715648174286\n",
      "Epoch 93, Batch 28/32 : Loss = 0.0027948657516390085\n",
      "Epoch 93, Batch 29/32 : Loss = 0.006367071531713009\n",
      "Epoch 93, Batch 30/32 : Loss = 0.02475794404745102\n",
      "Epoch 93, Batch 31/32 : Loss = 0.012451975606381893\n",
      "Epoch 93 finished in 0.04005176226298014 minutes\n",
      "Epoch 93 training_loss = 0.009796714410185814\n",
      "h---------Y----[[--oo-----E-----]---0----}----g----bb-----(--- => hY[oE]0}gb(, Ground Truth is hY[oE]0}gb(\n",
      "V-----------r---55---PP---t------m-------@@-----s---**---v---- => V-r5Pt-m@s*v, Ground Truth is V-r5Pt-m@s*v\n",
      "Epoch 93 val_loss = 0.012718835845589638, word_accuracy = 0.9618473895582329\n",
      "Epoch 94, Batch 0/32 : Loss = 0.008010665886104107\n",
      "Epoch 94, Batch 1/32 : Loss = 0.0021837097592651844\n",
      "Epoch 94, Batch 2/32 : Loss = 0.004321014974266291\n",
      "Epoch 94, Batch 3/32 : Loss = 0.0054619451984763145\n",
      "Epoch 94, Batch 4/32 : Loss = 0.0025477721355855465\n",
      "Epoch 94, Batch 5/32 : Loss = 0.015226989984512329\n",
      "Epoch 94, Batch 6/32 : Loss = 0.0385105162858963\n",
      "Epoch 94, Batch 7/32 : Loss = 0.00934000313282013\n",
      "Epoch 94, Batch 8/32 : Loss = 0.00418469775468111\n",
      "Epoch 94, Batch 9/32 : Loss = 0.0019414788112044334\n",
      "Epoch 94, Batch 10/32 : Loss = 0.002494530286639929\n",
      "Epoch 94, Batch 11/32 : Loss = 0.0014759855112060905\n",
      "Epoch 94, Batch 12/32 : Loss = 0.0023471973836421967\n",
      "Epoch 94, Batch 13/32 : Loss = 0.009236373007297516\n",
      "Epoch 94, Batch 14/32 : Loss = 0.0033979215659201145\n",
      "Epoch 94, Batch 15/32 : Loss = 0.002550046890974045\n",
      "Epoch 94, Batch 16/32 : Loss = 0.009648095816373825\n",
      "Epoch 94, Batch 17/32 : Loss = 0.0016719511477276683\n",
      "Epoch 94, Batch 18/32 : Loss = 0.004652706906199455\n",
      "Epoch 94, Batch 19/32 : Loss = 0.0021751862950623035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Batch 20/32 : Loss = 0.12463801354169846\n",
      "Epoch 94, Batch 21/32 : Loss = 0.03479408100247383\n",
      "Epoch 94, Batch 22/32 : Loss = 0.007624371442943811\n",
      "Epoch 94, Batch 23/32 : Loss = 0.0016973952297121286\n",
      "Epoch 94, Batch 24/32 : Loss = 0.023696113377809525\n",
      "Epoch 94, Batch 25/32 : Loss = 0.0023972527123987675\n",
      "Epoch 94, Batch 26/32 : Loss = 0.0026180697605013847\n",
      "Epoch 94, Batch 27/32 : Loss = 0.0013958632480353117\n",
      "Epoch 94, Batch 28/32 : Loss = 0.0015509335789829493\n",
      "Epoch 94, Batch 29/32 : Loss = 0.008329613134264946\n",
      "Epoch 94, Batch 30/32 : Loss = 0.01590997353196144\n",
      "Epoch 94, Batch 31/32 : Loss = 0.02393481321632862\n",
      "Epoch 94 finished in 0.04018682241439819 minutes\n",
      "Epoch 94 training_loss = 0.011534851975739002\n",
      "x----------0-----TT-----CC-----^---:---0-----<------a-----qq----**- => x0TC^:0<aq*, Ground Truth is x0TC^:0<aq*\n",
      "v---------v---!--D-----hh----EE-----c----G------%-------2-----4---- => vv!DhEcG%24, Ground Truth is vv!DhEcG%24\n",
      "Epoch 94 val_loss = 0.009252787567675114, word_accuracy = 0.9839357429718876\n",
      "Epoch 95, Batch 0/32 : Loss = 0.07693939656019211\n",
      "Epoch 95, Batch 1/32 : Loss = 0.0018414665246382356\n",
      "Epoch 95, Batch 2/32 : Loss = 0.004147302359342575\n",
      "Epoch 95, Batch 3/32 : Loss = 0.0049192472361028194\n",
      "Epoch 95, Batch 4/32 : Loss = 0.0017292031552642584\n",
      "Epoch 95, Batch 5/32 : Loss = 0.003185013309121132\n",
      "Epoch 95, Batch 6/32 : Loss = 0.0012040804140269756\n",
      "Epoch 95, Batch 7/32 : Loss = 0.010792601853609085\n",
      "Epoch 95, Batch 8/32 : Loss = 0.0036767092533409595\n",
      "Epoch 95, Batch 9/32 : Loss = 0.0036428654566407204\n",
      "Epoch 95, Batch 10/32 : Loss = 0.0015972761902958155\n",
      "Epoch 95, Batch 11/32 : Loss = 0.0020265611819922924\n",
      "Epoch 95, Batch 12/32 : Loss = 0.001939677633345127\n",
      "Epoch 95, Batch 13/32 : Loss = 0.0018417700193822384\n",
      "Epoch 95, Batch 14/32 : Loss = 0.002677877899259329\n",
      "Epoch 95, Batch 15/32 : Loss = 0.004322471097111702\n",
      "Epoch 95, Batch 16/32 : Loss = 0.006660374812781811\n",
      "Epoch 95, Batch 17/32 : Loss = 0.002959379693493247\n",
      "Epoch 95, Batch 18/32 : Loss = 0.0023415822070091963\n",
      "Epoch 95, Batch 19/32 : Loss = 0.001349325873889029\n",
      "Epoch 95, Batch 20/32 : Loss = 0.001485178479924798\n",
      "Epoch 95, Batch 21/32 : Loss = 0.0018257132032886147\n",
      "Epoch 95, Batch 22/32 : Loss = 0.002457312773913145\n",
      "Epoch 95, Batch 23/32 : Loss = 0.00505817262455821\n",
      "Epoch 95, Batch 24/32 : Loss = 0.002034991281107068\n",
      "Epoch 95, Batch 25/32 : Loss = 0.0033794804476201534\n",
      "Epoch 95, Batch 26/32 : Loss = 0.027033090591430664\n",
      "Epoch 95, Batch 27/32 : Loss = 0.0019096147734671831\n",
      "Epoch 95, Batch 28/32 : Loss = 0.001794338459149003\n",
      "Epoch 95, Batch 29/32 : Loss = 0.0065638478845357895\n",
      "Epoch 95, Batch 30/32 : Loss = 0.0011586470063775778\n",
      "Epoch 95, Batch 31/32 : Loss = 0.0028060940094292164\n",
      "Epoch 95 finished in 0.04081459045410156 minutes\n",
      "Epoch 95 training_loss = 0.006260090507566929\n",
      "S--------h-----v--6---3---b---X---qq---&--__--WW---jj--- => Sh-v63bXq&_Wj, Ground Truth is Sh-v63bXq&_Wj\n",
      "4-------ww---``--N-----(--\\---f--22----@-----AA---ss---- => 4w`N(\\f2@As, Ground Truth is 4w`N(\\f2@As\n",
      "Epoch 95 val_loss = 0.0027685228269547224, word_accuracy = 0.9979919678714859\n",
      "Epoch 96, Batch 0/32 : Loss = 0.010975593701004982\n",
      "Epoch 96, Batch 1/32 : Loss = 0.0033237156458199024\n",
      "Epoch 96, Batch 2/32 : Loss = 0.012751689180731773\n",
      "Epoch 96, Batch 3/32 : Loss = 0.002987818792462349\n",
      "Epoch 96, Batch 4/32 : Loss = 0.00112893246114254\n",
      "Epoch 96, Batch 5/32 : Loss = 0.00420963391661644\n",
      "Epoch 96, Batch 6/32 : Loss = 0.0010563121177256107\n",
      "Epoch 96, Batch 7/32 : Loss = 0.0012444127351045609\n",
      "Epoch 96, Batch 8/32 : Loss = 0.00823180004954338\n",
      "Epoch 96, Batch 9/32 : Loss = 0.0033246141392737627\n",
      "Epoch 96, Batch 10/32 : Loss = 0.0014842605451121926\n",
      "Epoch 96, Batch 11/32 : Loss = 0.002381648402661085\n",
      "Epoch 96, Batch 12/32 : Loss = 0.0011555003002285957\n",
      "Epoch 96, Batch 13/32 : Loss = 0.0020215841941535473\n",
      "Epoch 96, Batch 14/32 : Loss = 0.003983731381595135\n",
      "Epoch 96, Batch 15/32 : Loss = 0.0023075370118021965\n",
      "Epoch 96, Batch 16/32 : Loss = 0.006667378358542919\n",
      "Epoch 96, Batch 17/32 : Loss = 0.0018602903001010418\n",
      "Epoch 96, Batch 18/32 : Loss = 0.0024144561029970646\n",
      "Epoch 96, Batch 19/32 : Loss = 0.17480342090129852\n",
      "Epoch 96, Batch 20/32 : Loss = 0.0031329935882240534\n",
      "Epoch 96, Batch 21/32 : Loss = 0.004795089364051819\n",
      "Epoch 96, Batch 22/32 : Loss = 0.0034234686754643917\n",
      "Epoch 96, Batch 23/32 : Loss = 0.001360075082629919\n",
      "Epoch 96, Batch 24/32 : Loss = 0.0016506362007930875\n",
      "Epoch 96, Batch 25/32 : Loss = 0.0012088390067219734\n",
      "Epoch 96, Batch 26/32 : Loss = 0.0024360031820833683\n",
      "Epoch 96, Batch 27/32 : Loss = 0.014883475378155708\n",
      "Epoch 96, Batch 28/32 : Loss = 0.003116236301138997\n",
      "Epoch 96, Batch 29/32 : Loss = 0.0020519609097391367\n",
      "Epoch 96, Batch 30/32 : Loss = 0.007185290567576885\n",
      "Epoch 96, Batch 31/32 : Loss = 0.0013853565324097872\n",
      "Epoch 96 finished in 0.03962174654006958 minutes\n",
      "Epoch 96 training_loss = 0.00943715963512659\n",
      "#------#---/--||--D---I-66--HH---5---h---S---k---- => ##/|DI6H5hSk, Ground Truth is ##/|DI6H5hSk\n",
      "&----.--Z---NN----V---t--1---Z---__--L---Q----p--- => &.ZNVt1Z_LQp, Ground Truth is &.ZNVt1Z_LQp\n",
      "Epoch 96 val_loss = 0.029162289574742317, word_accuracy = 0.9839357429718876\n",
      "Epoch 97, Batch 0/32 : Loss = 0.030688444152474403\n",
      "Epoch 97, Batch 1/32 : Loss = 0.016098080202937126\n",
      "Epoch 97, Batch 2/32 : Loss = 0.3045530617237091\n",
      "Epoch 97, Batch 3/32 : Loss = 0.005323976743966341\n",
      "Epoch 97, Batch 4/32 : Loss = 0.0037197961937636137\n",
      "Epoch 97, Batch 5/32 : Loss = 0.009052403271198273\n",
      "Epoch 97, Batch 6/32 : Loss = 0.0027400008402764797\n",
      "Epoch 97, Batch 7/32 : Loss = 0.0058771949261426926\n",
      "Epoch 97, Batch 8/32 : Loss = 0.0013027922250330448\n",
      "Epoch 97, Batch 9/32 : Loss = 0.02523833140730858\n",
      "Epoch 97, Batch 10/32 : Loss = 0.003411464160308242\n",
      "Epoch 97, Batch 11/32 : Loss = 0.023330528289079666\n",
      "Epoch 97, Batch 12/32 : Loss = 0.0036546867340803146\n",
      "Epoch 97, Batch 13/32 : Loss = 0.003277361858636141\n",
      "Epoch 97, Batch 14/32 : Loss = 0.0027682019863277674\n",
      "Epoch 97, Batch 15/32 : Loss = 0.0057499101385474205\n",
      "Epoch 97, Batch 16/32 : Loss = 0.0041069164872169495\n",
      "Epoch 97, Batch 17/32 : Loss = 0.004389875568449497\n",
      "Epoch 97, Batch 18/32 : Loss = 0.016604626551270485\n",
      "Epoch 97, Batch 19/32 : Loss = 0.004185637459158897\n",
      "Epoch 97, Batch 20/32 : Loss = 0.003797747427597642\n",
      "Epoch 97, Batch 21/32 : Loss = 0.002723047975450754\n",
      "Epoch 97, Batch 22/32 : Loss = 0.014149563387036324\n",
      "Epoch 97, Batch 23/32 : Loss = 0.0035237122792750597\n",
      "Epoch 97, Batch 24/32 : Loss = 0.023921076208353043\n",
      "Epoch 97, Batch 25/32 : Loss = 0.0025534466840326786\n",
      "Epoch 97, Batch 26/32 : Loss = 0.002881967928260565\n",
      "Epoch 97, Batch 27/32 : Loss = 0.005119129084050655\n",
      "Epoch 97, Batch 28/32 : Loss = 0.00456238491460681\n",
      "Epoch 97, Batch 29/32 : Loss = 0.0023930957540869713\n",
      "Epoch 97, Batch 30/32 : Loss = 0.003978126682341099\n",
      "Epoch 97, Batch 31/32 : Loss = 0.14494603872299194\n",
      "Epoch 97 finished in 0.03915595610936483 minutes\n",
      "Epoch 97 training_loss = 0.018113890662789345\n",
      "G---------e---4---`--z---}--AA---f--l-I-AA---pp---- => Ge4`z}AflIAp, Ground Truth is Ge4`z}AflIAp\n",
      "E------]--O---C---MM----n--8---5---q--dd-''/--x---- => E]OCMn85qd'/x, Ground Truth is E]OCMn85qd'/x\n",
      "Epoch 97 val_loss = 0.010275999084115028, word_accuracy = 0.9879518072289156\n",
      "Epoch 98, Batch 0/32 : Loss = 0.0038643477018922567\n",
      "Epoch 98, Batch 1/32 : Loss = 0.008554263040423393\n",
      "Epoch 98, Batch 2/32 : Loss = 0.0026438352651894093\n",
      "Epoch 98, Batch 3/32 : Loss = 0.004346459172666073\n",
      "Epoch 98, Batch 4/32 : Loss = 0.012272464111447334\n",
      "Epoch 98, Batch 5/32 : Loss = 0.002441355027258396\n",
      "Epoch 98, Batch 6/32 : Loss = 0.002222823677584529\n",
      "Epoch 98, Batch 7/32 : Loss = 0.0026944689452648163\n",
      "Epoch 98, Batch 8/32 : Loss = 0.0020137103274464607\n",
      "Epoch 98, Batch 9/32 : Loss = 0.004214676097035408\n",
      "Epoch 98, Batch 10/32 : Loss = 0.0022229463793337345\n",
      "Epoch 98, Batch 11/32 : Loss = 0.030083157122135162\n",
      "Epoch 98, Batch 12/32 : Loss = 0.003000374650582671\n",
      "Epoch 98, Batch 13/32 : Loss = 0.0033028707839548588\n",
      "Epoch 98, Batch 14/32 : Loss = 0.0021048143971711397\n",
      "Epoch 98, Batch 15/32 : Loss = 0.011883104220032692\n",
      "Epoch 98, Batch 16/32 : Loss = 0.024417055770754814\n",
      "Epoch 98, Batch 17/32 : Loss = 0.0025065625086426735\n",
      "Epoch 98, Batch 18/32 : Loss = 0.0113194165751338\n",
      "Epoch 98, Batch 19/32 : Loss = 0.009361082687973976\n",
      "Epoch 98, Batch 20/32 : Loss = 0.01058658491820097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Batch 21/32 : Loss = 0.005177028942853212\n",
      "Epoch 98, Batch 22/32 : Loss = 0.009977824985980988\n",
      "Epoch 98, Batch 23/32 : Loss = 0.006810920313000679\n",
      "Epoch 98, Batch 24/32 : Loss = 0.003858048003166914\n",
      "Epoch 98, Batch 25/32 : Loss = 0.002140437951311469\n",
      "Epoch 98, Batch 26/32 : Loss = 0.002812227001413703\n",
      "Epoch 98, Batch 27/32 : Loss = 0.12196386605501175\n",
      "Epoch 98, Batch 28/32 : Loss = 0.002620680257678032\n",
      "Epoch 98, Batch 29/32 : Loss = 0.05760505050420761\n",
      "Epoch 98, Batch 30/32 : Loss = 0.0011980485869571567\n",
      "Epoch 98, Batch 31/32 : Loss = 0.038093551993370056\n",
      "Epoch 98 finished in 0.03959552049636841 minutes\n",
      "Epoch 98 training_loss = 0.012047620490193367\n",
      "T--------P---uu---g----7---\\\\--n--''-?---k---^- => TPug7\\n'?k^, Ground Truth is TPug7\\n'?k^\n",
      "f----66----U----(---K-----S---qq------!--5---;- => f6U(KSq-!5;, Ground Truth is f6U(KSq-!5;\n",
      "Epoch 98 val_loss = 0.004511297680437565, word_accuracy = 0.9959839357429718\n",
      "Epoch 99, Batch 0/32 : Loss = 0.00553300604224205\n",
      "Epoch 99, Batch 1/32 : Loss = 0.0015491421800106764\n",
      "Epoch 99, Batch 2/32 : Loss = 0.002863253466784954\n",
      "Epoch 99, Batch 3/32 : Loss = 0.0028913263231515884\n",
      "Epoch 99, Batch 4/32 : Loss = 0.009991477243602276\n",
      "Epoch 99, Batch 5/32 : Loss = 0.0057541076093912125\n",
      "Epoch 99, Batch 6/32 : Loss = 0.002323172055184841\n",
      "Epoch 99, Batch 7/32 : Loss = 0.054063912481069565\n",
      "Epoch 99, Batch 8/32 : Loss = 0.014308679848909378\n",
      "Epoch 99, Batch 9/32 : Loss = 0.012376027181744576\n",
      "Epoch 99, Batch 10/32 : Loss = 0.015289027243852615\n",
      "Epoch 99, Batch 11/32 : Loss = 0.0032895072363317013\n",
      "Epoch 99, Batch 12/32 : Loss = 0.0018805223517119884\n",
      "Epoch 99, Batch 13/32 : Loss = 0.006220684852451086\n",
      "Epoch 99, Batch 14/32 : Loss = 0.005444503389298916\n",
      "Epoch 99, Batch 15/32 : Loss = 0.002704596146941185\n",
      "Epoch 99, Batch 16/32 : Loss = 0.0022103693336248398\n",
      "Epoch 99, Batch 17/32 : Loss = 0.004004763439297676\n",
      "Epoch 99, Batch 18/32 : Loss = 0.01424581091850996\n",
      "Epoch 99, Batch 19/32 : Loss = 0.0035442207008600235\n",
      "Epoch 99, Batch 20/32 : Loss = 0.002661679871380329\n",
      "Epoch 99, Batch 21/32 : Loss = 0.03814661130309105\n",
      "Epoch 99, Batch 22/32 : Loss = 0.0028240324463695288\n",
      "Epoch 99, Batch 23/32 : Loss = 0.01819746568799019\n",
      "Epoch 99, Batch 24/32 : Loss = 0.0020496160723268986\n",
      "Epoch 99, Batch 25/32 : Loss = 0.002061852253973484\n",
      "Epoch 99, Batch 26/32 : Loss = 0.0027310261502861977\n",
      "Epoch 99, Batch 27/32 : Loss = 0.0017243846086785197\n",
      "Epoch 99, Batch 28/32 : Loss = 0.006189391482621431\n",
      "Epoch 99, Batch 29/32 : Loss = 0.002512603998184204\n",
      "Epoch 99, Batch 30/32 : Loss = 0.001771656097844243\n",
      "Epoch 99, Batch 31/32 : Loss = 0.011839602142572403\n",
      "Epoch 99 finished in 0.03891458511352539 minutes\n",
      "Epoch 99 training_loss = 0.00812331959605217\n",
      "N----------A-----X----77----E---##--------4----q----n----8----w-----22------ => NAX7E#-4qn8w2, Ground Truth is NAX7E#-4qn8w2\n",
      "&------------C------S-----4----FF---J----J----FF----n----%%-------m--------- => &CS4FJJFn%m, Ground Truth is &CS4FJJFn%m\n",
      "Epoch 99 val_loss = 0.007590023335069418, word_accuracy = 0.9819277108433735\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(epochs_num):\n",
    "    \n",
    "    tick = time.time()\n",
    "    train_loss = train(crnn, criterion, optimizer, logger, \n",
    "                       train_dataloder, batch_size, epoch)\n",
    "    tock = time.time()\n",
    "    \n",
    "    print(f'Epoch {epoch} finished in {(tock - tick) / 60} minutes')\n",
    "    print(f'Epoch {epoch} training_loss = {train_loss}')\n",
    "    \n",
    "    if epoch % val_each == 0:\n",
    "        val_loss, val_accurcy = val(crnn, criterion, logger, val_dataloder,\n",
    "                                    epoch, batch_size)\n",
    "        print(f'Epoch {epoch} val_loss = {val_loss}, word_accuracy = {val_accurcy}')\n",
    "            \n",
    "        # save best checkpoint\n",
    "        if best_acc <= val_accurcy:\n",
    "            best_acc = val_accurcy\n",
    "            checkpoint = {'input_hight':32,\n",
    "                          'output_size':len(alphapet)+1,\n",
    "                          'alphapet':alphapet,\n",
    "                          'train_transforms':train_transforms,\n",
    "                          'optim_dic':optimizer.state_dict(),\n",
    "                          'state_dic':crnn.state_dict(),\n",
    "                          'epoch':epoch\n",
    "                         }\n",
    "            torch.save(checkpoint,'checkpoints/best_checkpoint.pth')\n",
    "    \n",
    "    # save last epoch\n",
    "    checkpoint = {'input_hight':32,\n",
    "                  'output_size':len(alphapet)+1,\n",
    "                  'alphapet':alphapet,\n",
    "                  'train_transforms':train_transforms,\n",
    "                  'optim_dic':optimizer.state_dict(),\n",
    "                  'state_dic':crnn.state_dict(),\n",
    "                  'epoch':epoch\n",
    "                 }\n",
    "    torch.save(checkpoint,'checkpoints/last_checkpoint.pth')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
