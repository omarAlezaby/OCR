{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"password_trainv3.csv\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    char_ststistics = {}\n",
    "    for i, row in enumerate(reader):\n",
    "        if(i == 0): continue\n",
    "        for c in row[1]:\n",
    "            if c in char_ststistics: char_ststistics[c] += 1\n",
    "            else: char_ststistics[c] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 66\n",
      "\" 72\n",
      "# 114\n",
      "$ 99\n",
      "% 66\n",
      "& 63\n",
      "' 84\n",
      "( 54\n",
      ") 66\n",
      "* 96\n",
      "+ 54\n",
      ", 72\n",
      "- 84\n",
      ". 81\n",
      "/ 57\n",
      "0 114\n",
      "1 120\n",
      "2 114\n",
      "3 105\n",
      "4 138\n",
      "5 111\n",
      "6 114\n",
      "7 99\n",
      "8 90\n",
      "9 105\n",
      ": 57\n",
      "; 69\n",
      "< 75\n",
      "= 84\n",
      "> 72\n",
      "? 66\n",
      "@ 60\n",
      "A 54\n",
      "B 78\n",
      "C 84\n",
      "D 75\n",
      "E 84\n",
      "F 69\n",
      "G 84\n",
      "H 54\n",
      "I 87\n",
      "J 78\n",
      "K 39\n",
      "L 72\n",
      "M 69\n",
      "N 78\n",
      "O 75\n",
      "P 75\n",
      "Q 66\n",
      "R 81\n",
      "S 81\n",
      "T 99\n",
      "U 42\n",
      "V 69\n",
      "W 96\n",
      "X 75\n",
      "Y 69\n",
      "Z 72\n",
      "[ 60\n",
      "\\ 63\n",
      "] 87\n",
      "^ 84\n",
      "_ 54\n",
      "` 81\n",
      "a 93\n",
      "b 69\n",
      "c 63\n",
      "d 96\n",
      "e 60\n",
      "f 72\n",
      "g 87\n",
      "h 87\n",
      "i 81\n",
      "j 66\n",
      "k 66\n",
      "l 45\n",
      "m 111\n",
      "n 114\n",
      "o 63\n",
      "p 72\n",
      "q 69\n",
      "r 69\n",
      "s 48\n",
      "t 69\n",
      "u 60\n",
      "v 63\n",
      "w 81\n",
      "x 60\n",
      "y 66\n",
      "z 63\n",
      "{ 57\n",
      "| 63\n",
      "} 81\n",
      "~ 60\n",
      "min value 39\n",
      "max value 138\n",
      "94\n",
      "!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "alphapet = ''\n",
    "for c in sorted(char_ststistics):\n",
    "    print(c, char_ststistics[c])\n",
    "    values.append(char_ststistics[c])\n",
    "    alphapet += c\n",
    "\n",
    "print(f'min value {min(values)}\\nmax value {max(values)}')\n",
    "print(len(alphapet))\n",
    "print (alphapet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, sampler, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Passwords_data(Dataset):\n",
    "    def __init__ (self, csv_Path, imgs_path, transformers = None):\n",
    "        with open(csv_Path, 'r') as csv_file:\n",
    "            # images names and labels in matching indices\n",
    "            reader = csv.reader(csv_file)\n",
    "            self.imgs = []\n",
    "            self.lables = []\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0: continue\n",
    "                self.imgs.append(row[0])\n",
    "                self.lables.append(row[1])\n",
    "                \n",
    "            self.imgs_file = imgs_path\n",
    "            self.transformers = transformers\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # image\n",
    "        img_path = self.imgs_file + '/' + self.imgs[index]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        #print(img.size)\n",
    "        \n",
    "        # image augmentation\n",
    "        if self.transformers != None:\n",
    "            img = self.transformers(img)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # lable\n",
    "        lable = self.lables[index]\n",
    "        \n",
    "        return (img_path, img, lable)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeNormalize (object):\n",
    "    def __init__(self, img_size):\n",
    "        self.img_size = img_size # imgH, imgW\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img = transforms.Resize(size=self.img_size)(transforms.ToPILImage()(img))\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img.sub_(0.5).div_(0.5) # normalize gray scale\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignBatch(object):\n",
    "    def __init__(self, imgH = 32, imgW = 100, keep_ratio = True, min_ratio = 1, padding = False):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio = keep_ratio\n",
    "        self.min_ratio = min_ratio\n",
    "        self.padding = padding\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        img_paths, imgs, lables = zip(*batch)\n",
    "        imgH = self.imgH\n",
    "        imgW = self.imgW\n",
    "        \n",
    "        if(self.keep_ratio):\n",
    "            max_ratio = 0\n",
    "            for img in imgs:\n",
    "                #print(img.shape)\n",
    "                _, h, w = img.shape\n",
    "                if max_ratio < (w/h): max_ratio = (w/h)\n",
    "            if self.padding:\n",
    "                imgs_padded = []\n",
    "                for i, img in enumerate(imgs):\n",
    "                    _, h, w = img.shape\n",
    "                    w_new = h * max_ratio\n",
    "                    pad = int((w_new - w) / 2)\n",
    "                    #print(img.shape)\n",
    "                    imgs_padded.append(F.pad(img, (pad, pad), \"constant\"))\n",
    "                imgs = imgs_padded\n",
    "            \n",
    "            imgW = int(max_ratio * imgH)\n",
    "            \n",
    "        resizer = ResizeNormalize((imgH, imgW))\n",
    "        imgs = [resizer(img).unsqueeze(0) for img in imgs]\n",
    "        imgs = torch.cat(imgs, 0)\n",
    "        return img_paths, imgs, lables\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingSampler(sampler.Sampler):\n",
    "    \n",
    "    def __init__ (self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.number_sampels = len(data_source)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__ (self):\n",
    "        n_batches = len(self)//self.batch_size\n",
    "        tail = len(self) % self.batch_size\n",
    "        index = torch.LongTensor(len(self)).fill_(0)\n",
    "        for i in range(n_batches):\n",
    "            random_start = random.randint(0, len(self)-self.batch_size)\n",
    "            b_indx = random_start + torch.arange(0, self.batch_size)\n",
    "            index[i*self.batch_size : (i+1) * self.batch_size] = b_indx\n",
    "        if tail:\n",
    "            random_start = random.randint(0, len(self)-tail)\n",
    "            b_indx = random_start + torch.arange(0, tail-1)\n",
    "            index[n_batches * self.batch_size:] = b_indx\n",
    "            \n",
    "        return iter(index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.number_sampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Passwords_data('password_trainv3.csv', 'train_passwordv3')\n",
    "\n",
    "sampler = MatchingSampler(dataset, 4)\n",
    "align = AlignBatch()\n",
    "dataloder = DataLoader(dataset, batch_size=4, collate_fn=align, shuffle= False)\n",
    "\n",
    "img_pathes, imgs, lables = next(iter(dataloder))\n",
    "\n",
    "for sample in zip(img_pathes, imgs, lables):\n",
    "    #print(sample)\n",
    "    img_path, img, lable = sample\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.squeeze(0).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDrictionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BiDrictionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        t_steps, b_size, h_num = recurrent.shape\n",
    "        recurrent = recurrent.view(t_steps*b_size, h_num) # prepare the linear layer input\n",
    "        \n",
    "        output = self.embedding(recurrent)\n",
    "        output = output.view(t_steps, b_size, -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, inFilt, outFilt, kerSiz, padSiz, strideSiz, b_n = False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(inFilt, outFilt, kerSiz, padSiz, strideSiz)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, inChannal, nClasses, nHidden, nLSTMs = 2):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH == 32 , 'the image input hight must be 32'\n",
    "        \n",
    "        ks = [3, 3, 3, 3, 3, 3, 2] # kernal Size\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0] # padding\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1] # stride\n",
    "        fn = [64, 128, 256, 256, 512, 512, 512] # filters number\n",
    "        \n",
    "        cnn = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        def conv_layer(layNum, b_n = False):\n",
    "            nIn = inChannal if layNum == 0 else fn[layNum-1]\n",
    "            nOut = fn[layNum]\n",
    "            # Conv Layer\n",
    "            cnn.add_module(f'conv{layNum}', nn.Conv2d(nIn, nOut, ks[layNum], ss[layNum], ps[layNum]))\n",
    "            # btach normalization\n",
    "            if b_n:\n",
    "                cnn.add_module(f'batchnorm{layNum}', nn.BatchNorm2d(nOut))\n",
    "            # non Linearity (ReLU)\n",
    "            cnn.add_module(f'relu{layNum}', nn.ReLU(inplace=True))\n",
    "            \n",
    "        # Cnn Arch\n",
    "        conv_layer(0)\n",
    "        cnn.add_module(f'pooling{0}', nn.MaxPool2d(2, 2))  # 64 x 16\n",
    "        conv_layer(1)\n",
    "        cnn.add_module(f'pooling{1}', nn.MaxPool2d(2, 2))  # 128 x 8\n",
    "        conv_layer(2, b_n=True)\n",
    "        conv_layer(3)\n",
    "        # the irregular shape of stride and padding beacause of the shape of some char like (i, ..)\n",
    "        cnn.add_module(f'pooling{2}', nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 256 x 4\n",
    "        conv_layer(4, b_n=True)\n",
    "        conv_layer(5)\n",
    "        cnn.add_module(f'pooling{3}', nn.MaxPool2d((2, 2), (2, 1), (0, 1))) # 512 x 2\n",
    "        conv_layer(6, b_n=True) #  512 x 1\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        # Rnn Arch\n",
    "        rnn = nn.Sequential(\n",
    "            BiDrictionalLSTM(512, nHidden, nHidden),\n",
    "            BiDrictionalLSTM(nHidden, nHidden, nClasses)\n",
    "        )\n",
    "\n",
    "        self.rnn = rnn\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # cnn pass\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.shape\n",
    "        \n",
    "        assert h == 1, 'the hight after cnn must equal 1'\n",
    "        \n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1) # sequance, batch, features\n",
    "        \n",
    "        # rnn pass \n",
    "        rnn = self.rnn(conv)\n",
    "        \n",
    "        output = self.softmax(rnn)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN(32, 1, 37, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.load_state_dict(torch.load('crnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model state dict\n",
    "st = crnn.state_dict()\n",
    "for name in st:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint state dic\n",
    "st = torch.load('crnn.pth')\n",
    "for name,k in st.items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "        \"\"\"Log scalar variables.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value) for tag, value in tag_value_pairs])\n",
    "        self.writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label String Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class strLabelConverter(object):\n",
    "    \"\"\"Convert between str and label.\n",
    "\n",
    "    NOTE:\n",
    "        Insert `blank` to the alphabet for CTC.\n",
    "\n",
    "    Args:\n",
    "        alphabet (str): set of the possible characters.\n",
    "        ignore_case (bool, default=True): whether or not to ignore all of the case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, ignore_case=True):\n",
    "        self._ignore_case = ignore_case\n",
    "        if self._ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        self.alphabet = alphabet + '-'  # for `-1` index\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(alphabet):\n",
    "            # NOTE: 0 is reserved for 'blank' required by wrap_ctc\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Support batch or single str.\n",
    "\n",
    "        Args:\n",
    "            text (str or list of str): texts to convert.\n",
    "\n",
    "        Returns:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [\n",
    "                self.dict[char.lower() if self._ignore_case else char]\n",
    "                for char in text\n",
    "            ]\n",
    "            length = [len(text)]\n",
    "        elif isinstance(text, collections.abc.Iterable):\n",
    "            length = [len(s) for s in text]\n",
    "            text = ''.join(text)\n",
    "            text, _ = self.encode(text)\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode(self, t, length, raw=False):\n",
    "        \"\"\"Decode encoded texts back into strs.\n",
    "\n",
    "        Args:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: when the texts and its length does not match.\n",
    "\n",
    "        Returns:\n",
    "            text (str or list of str): texts to convert.\n",
    "        \"\"\"\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(), length)\n",
    "            if raw:\n",
    "                return ''.join([self.alphabet[i - 1] for i in t]), [i - 1 for i in t]\n",
    "            else:\n",
    "                char_list = []\n",
    "                lables_list = []\n",
    "                for i in range(length):\n",
    "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
    "                        char_list.append(self.alphabet[t[i] - 1])\n",
    "                        lables_list.append(t[i] - 1)\n",
    "                return ''.join(char_list), lables_list\n",
    "        else:\n",
    "            # batch mode\n",
    "            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(t.numel(), length.sum())\n",
    "            texts = []\n",
    "            lables = []\n",
    "            index = 0\n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                text, lable = self.decode(\n",
    "                        t[index:index + l], torch.IntTensor([l]), raw=raw)\n",
    "                texts.append(text)\n",
    "                lables.append(lable)\n",
    "                index += l\n",
    "            return texts, lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import PIL\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, translate=(.03,.03))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, scale=(.95,1.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomAffine(degrees=0, shear=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(degrees=3, expand=True), \n",
    "                                       transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomPerspective()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Passwords_data('password_trainv3.csv', 'train_passwordv3', transformers=train_transforms)\n",
    "\n",
    "sampler = MatchingSampler(dataset, 4)\n",
    "align = AlignBatch()\n",
    "dataloder = DataLoader(dataset, batch_size=4, collate_fn=align, shuffle= True)\n",
    "\n",
    "img_pathes, imgs, lables = next(iter(dataloder))\n",
    "#print(converter.encode(lables))\n",
    "\n",
    "for sample in zip(img_pathes, imgs, lables):\n",
    "    #print(sample)\n",
    "    img_path, img, lable = sample\n",
    "    print(img.shape)\n",
    "    plt.imshow(img.squeeze(0).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect cuda device? True\n",
      "number of classes id 94 + blank\n"
     ]
    }
   ],
   "source": [
    "# training variables \n",
    "epochs_num = 200\n",
    "batch_size = 16\n",
    "cuda = torch.cuda.is_available()\n",
    "n_workers = 4\n",
    "nClasses = len(char_ststistics) + 1\n",
    "inChannels = 1\n",
    "imgH = 32\n",
    "nHidden = 256\n",
    "lr = .001\n",
    "test_display = 4\n",
    "val_each = 1\n",
    "use_pretrained = True\n",
    "pre_trained = 'crnn.pth'\n",
    "fixed_seed = 12\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'detect cuda device? {cuda}')\n",
    "print(f'number of classes id {nClasses-1} + blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seeds\n",
    "random.seed(fixed_seed)\n",
    "np.random.seed(fixed_seed)\n",
    "torch.manual_seed(fixed_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=0, translate=(.03,.03)),\n",
    "            transforms.RandomAffine(degrees=0, scale=(.95,1.05)),\n",
    "            transforms.RandomAffine(degrees=0, shear=20),\n",
    "            transforms.RandomRotation(degrees=3, expand=True)]),\n",
    "        transforms.ColorJitter(brightness=.3, contrast=.3, saturation=.3)],  p=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Passwords_data('password_train.csv', 'train_passwordv3', transformers=train_transforms)\n",
    "\n",
    "#sampler = MatchingSampler(dataset, batch_size)\n",
    "align = AlignBatch()\n",
    "train_dataloder = DataLoader(train_dataset, batch_size=batch_size, collate_fn=align, \n",
    "                       shuffle= True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Passwords_data('password_val.csv', 'train_passwordv3')\n",
    "\n",
    "#sampler = MatchingSampler(dataset, batch_size)\n",
    "align = AlignBatch()\n",
    "val_dataloder = DataLoader(val_dataset, batch_size=batch_size, collate_fn=align, \n",
    "                       shuffle= True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = strLabelConverter(alphapet, ignore_case=False)\n",
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu0): ReLU(inplace)\n",
      "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): ReLU(inplace)\n",
      "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace)\n",
      "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu3): ReLU(inplace)\n",
      "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu4): ReLU(inplace)\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu5): ReLU(inplace)\n",
      "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu6): ReLU(inplace)\n",
      "  )\n",
      "  (rnn): Sequential(\n",
      "    (0): BiDrictionalLSTM(\n",
      "      (rnn): LSTM(512, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): BiDrictionalLSTM(\n",
      "      (rnn): LSTM(256, 256, bidirectional=True)\n",
      "      (embedding): Linear(in_features=512, out_features=95, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "crnn = CRNN(imgH, inChannels, nClasses, nHidden)\n",
    "crnn.apply(weights_init)\n",
    "if use_pretrained :\n",
    "    model_dict = crnn.state_dict() # state of the current model\n",
    "    pretrained_dict = torch.load(pre_trained) # state of the pretrained model\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k != 'rnn.1.embedding.weight' and k != 'rnn.1.embedding.bias'} # remove the classifier from the state\n",
    "    classifier_dict = {k: v for k, v in model_dict.items() if k == 'rnn.1.embedding.weight' or k == 'rnn.1.embedding.bias'} # get the classifier weight from new model\n",
    "    pretrained_dict.update(classifier_dict) # update without classifier\n",
    "    crnn.load_state_dict(pretrained_dict)\n",
    "    \n",
    "print(crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda :\n",
    "    crnn = crnn.to(device)\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(crnn.parameters(), lr=lr)\n",
    "lr_sheduler = optim.lr_scheduler.StepLR(optimizer, 100, gamma=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logger file\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logger = Logger('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, logger, train_dataloder, batch_size, epoch_num):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    samples_num = 0\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(train_dataloder):\n",
    "        batches_done = len(train_dataloder) * epoch_num + batch_i\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device))\n",
    "        targets, lenghts = converter.encode(targets)\n",
    "        targets = Variable(targets.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        # pass to the network\n",
    "        preds = model(imgs)\n",
    "        preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "        \n",
    "        # loss\n",
    "        #print(preds_size.shape)\n",
    "        loss = criterion(preds, targets.cpu(), preds_size, t_lens)\n",
    "        epoch_loss += loss * imgs.shape[0]\n",
    "        logger.scalar_summary('loss_batches', loss, batches_done)\n",
    "        print(f'Epoch {epoch_num}, Batch {batch_i}/{len(train_dataloder)} : Loss = {loss}')\n",
    "        \n",
    "        # optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    epoch_loss /= samples_num\n",
    "    logger.scalar_summary('loss_epochs', epoch_loss, epoch_num)\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, criterion, logger, val_dataloder, epoch_num, batch_size=16):\n",
    "    model.eval()\n",
    "    \n",
    "    nCorrect_words = 0\n",
    "    #nCorrect_chars = 0\n",
    "    val_loss = 0\n",
    "    samples_num = 0\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(val_dataloder):\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device), requires_grad=False)\n",
    "        targets_encoded, lenghts = converter.encode(targets)\n",
    "        targets_encoded = Variable(targets_encoded.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # pass to the network\n",
    "            preds = model(imgs)\n",
    "            preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(preds, targets_encoded.cpu(), preds_size, t_lens)\n",
    "            val_loss += loss * imgs.shape[0]\n",
    "            \n",
    "            # get the nework prediction\n",
    "            _, preds = preds.max(2)\n",
    "            preds = preds.transpose(1,0).contiguous().view(-1)\n",
    "            words_preds, lables_preds = converter.decode(preds, preds_size)\n",
    "            \n",
    "            for word_pred, target in zip(words_preds, targets):\n",
    "                if word_pred == target:\n",
    "                    nCorrect_words += 1\n",
    "    \n",
    "    # display some of the network prediction\n",
    "    row_preds, _ = converter.decode(preds, preds_size, raw=True)[:test_display]\n",
    "\n",
    "    for row_pred, word_pred, gt in zip(row_preds, words_preds, targets):\n",
    "        print(f'{row_pred} => {word_pred}, Ground Truth is {gt}')\n",
    "    \n",
    "    #compute loss and accurcy\n",
    "    word_accurcy = nCorrect_words / samples_num\n",
    "    val_loss /= samples_num\n",
    "    logger.scalar_summary('val_loss', val_loss, epoch_num)\n",
    "    logger.scalar_summary('val_WordAccurcy', word_accurcy, epoch_num)\n",
    "    \n",
    "    return val_loss, word_accurcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0/32 : Loss = 21.643980026245117\n",
      "Epoch 0, Batch 1/32 : Loss = 19.478717803955078\n",
      "Epoch 0, Batch 2/32 : Loss = 23.58216094970703\n",
      "Epoch 0, Batch 3/32 : Loss = 21.24152946472168\n",
      "Epoch 0, Batch 4/32 : Loss = 19.950885772705078\n",
      "Epoch 0, Batch 5/32 : Loss = 18.413835525512695\n",
      "Epoch 0, Batch 6/32 : Loss = 16.70687484741211\n",
      "Epoch 0, Batch 7/32 : Loss = 14.276725769042969\n",
      "Epoch 0, Batch 8/32 : Loss = 12.767298698425293\n",
      "Epoch 0, Batch 9/32 : Loss = 10.44369888305664\n",
      "Epoch 0, Batch 10/32 : Loss = 8.966351509094238\n",
      "Epoch 0, Batch 11/32 : Loss = 6.584331035614014\n",
      "Epoch 0, Batch 12/32 : Loss = 5.221884727478027\n",
      "Epoch 0, Batch 13/32 : Loss = 4.7490434646606445\n",
      "Epoch 0, Batch 14/32 : Loss = 4.781960487365723\n",
      "Epoch 0, Batch 15/32 : Loss = 5.086452484130859\n",
      "Epoch 0, Batch 16/32 : Loss = 5.437365531921387\n",
      "Epoch 0, Batch 17/32 : Loss = 5.5425124168396\n",
      "Epoch 0, Batch 18/32 : Loss = 5.738100528717041\n",
      "Epoch 0, Batch 19/32 : Loss = 5.7718353271484375\n",
      "Epoch 0, Batch 20/32 : Loss = 5.76609992980957\n",
      "Epoch 0, Batch 21/32 : Loss = 5.416781902313232\n",
      "Epoch 0, Batch 22/32 : Loss = 5.352563858032227\n",
      "Epoch 0, Batch 23/32 : Loss = 4.9564008712768555\n",
      "Epoch 0, Batch 24/32 : Loss = 4.813754558563232\n",
      "Epoch 0, Batch 25/32 : Loss = 4.795534610748291\n",
      "Epoch 0, Batch 26/32 : Loss = 4.81540060043335\n",
      "Epoch 0, Batch 27/32 : Loss = 4.876001358032227\n",
      "Epoch 0, Batch 28/32 : Loss = 4.764046669006348\n",
      "Epoch 0, Batch 29/32 : Loss = 4.973443508148193\n",
      "Epoch 0, Batch 30/32 : Loss = 4.807186126708984\n",
      "Epoch 0, Batch 31/32 : Loss = 4.752581596374512\n",
      "Epoch 0 finished in 0.051804113388061526 minutes\n",
      "Epoch 0 training_loss = 9.391705513000488\n",
      "---------------------------------------------------------------- => , Ground Truth is CDEgm\"mF<Q82\n",
      "---------------------------------------------------------------- => , Ground Truth is k$|',9YNWmT8\n",
      "---------------------------------------------------------------- => , Ground Truth is \"]t4e^WQ4>g\n",
      "---------------------------------------------------------------- => , Ground Truth is cR;9y?2diO{!\n",
      "Epoch 0 val_loss = 4.787904739379883, word_accuracy = 0.0\n",
      "Epoch 1, Batch 0/32 : Loss = 4.689157485961914\n",
      "Epoch 1, Batch 1/32 : Loss = 4.700592041015625\n",
      "Epoch 1, Batch 2/32 : Loss = 4.620172500610352\n",
      "Epoch 1, Batch 3/32 : Loss = 4.621940612792969\n",
      "Epoch 1, Batch 4/32 : Loss = 4.68099308013916\n",
      "Epoch 1, Batch 5/32 : Loss = 4.637124538421631\n",
      "Epoch 1, Batch 6/32 : Loss = 4.629345417022705\n",
      "Epoch 1, Batch 7/32 : Loss = 4.719394207000732\n",
      "Epoch 1, Batch 8/32 : Loss = 4.653402328491211\n",
      "Epoch 1, Batch 9/32 : Loss = 4.6166815757751465\n",
      "Epoch 1, Batch 10/32 : Loss = 4.645992755889893\n",
      "Epoch 1, Batch 11/32 : Loss = 4.605813503265381\n",
      "Epoch 1, Batch 12/32 : Loss = 4.596208572387695\n",
      "Epoch 1, Batch 13/32 : Loss = 4.608946800231934\n",
      "Epoch 1, Batch 14/32 : Loss = 4.596246719360352\n",
      "Epoch 1, Batch 15/32 : Loss = 4.6815266609191895\n",
      "Epoch 1, Batch 16/32 : Loss = 4.607791423797607\n",
      "Epoch 1, Batch 17/32 : Loss = 4.613523006439209\n",
      "Epoch 1, Batch 18/32 : Loss = 4.6402082443237305\n",
      "Epoch 1, Batch 19/32 : Loss = 4.621063709259033\n",
      "Epoch 1, Batch 20/32 : Loss = 4.557777404785156\n",
      "Epoch 1, Batch 21/32 : Loss = 4.56578254699707\n",
      "Epoch 1, Batch 22/32 : Loss = 4.594797134399414\n",
      "Epoch 1, Batch 23/32 : Loss = 4.569991588592529\n",
      "Epoch 1, Batch 24/32 : Loss = 4.587987899780273\n",
      "Epoch 1, Batch 25/32 : Loss = 4.604214668273926\n",
      "Epoch 1, Batch 26/32 : Loss = 4.518792152404785\n",
      "Epoch 1, Batch 27/32 : Loss = 4.526838779449463\n",
      "Epoch 1, Batch 28/32 : Loss = 4.557618141174316\n",
      "Epoch 1, Batch 29/32 : Loss = 4.524069786071777\n",
      "Epoch 1, Batch 30/32 : Loss = 4.515413284301758\n",
      "Epoch 1, Batch 31/32 : Loss = 4.468944549560547\n",
      "Epoch 1 finished in 0.05221314430236816 minutes\n",
      "Epoch 1 training_loss = 4.609414100646973\n",
      "-------------------------------------------------------------------------- => , Ground Truth is 59gmJuxcx.d\\\n",
      "-------------------------------------------------------------------------- => , Ground Truth is W=2+E1nTXrCan\n",
      "-------------------------------------------------------------------------- => , Ground Truth is d:X9eaF,8-VR\n",
      "-------------------------------------------------------------------------- => , Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 1 val_loss = 4.590762138366699, word_accuracy = 0.0\n",
      "Epoch 2, Batch 0/32 : Loss = 4.424741268157959\n",
      "Epoch 2, Batch 1/32 : Loss = 4.412521839141846\n",
      "Epoch 2, Batch 2/32 : Loss = 4.489790439605713\n",
      "Epoch 2, Batch 3/32 : Loss = 4.467717170715332\n",
      "Epoch 2, Batch 4/32 : Loss = 4.440957069396973\n",
      "Epoch 2, Batch 5/32 : Loss = 4.422163009643555\n",
      "Epoch 2, Batch 6/32 : Loss = 4.456414222717285\n",
      "Epoch 2, Batch 7/32 : Loss = 4.319553375244141\n",
      "Epoch 2, Batch 8/32 : Loss = 4.422583103179932\n",
      "Epoch 2, Batch 9/32 : Loss = 4.424187660217285\n",
      "Epoch 2, Batch 10/32 : Loss = 4.399361610412598\n",
      "Epoch 2, Batch 11/32 : Loss = 4.326019287109375\n",
      "Epoch 2, Batch 12/32 : Loss = 4.330455780029297\n",
      "Epoch 2, Batch 13/32 : Loss = 4.270493984222412\n",
      "Epoch 2, Batch 14/32 : Loss = 4.308164596557617\n",
      "Epoch 2, Batch 15/32 : Loss = 4.371084213256836\n",
      "Epoch 2, Batch 16/32 : Loss = 4.304071426391602\n",
      "Epoch 2, Batch 17/32 : Loss = 4.308238983154297\n",
      "Epoch 2, Batch 18/32 : Loss = 4.273022651672363\n",
      "Epoch 2, Batch 19/32 : Loss = 4.285940170288086\n",
      "Epoch 2, Batch 20/32 : Loss = 4.256645202636719\n",
      "Epoch 2, Batch 21/32 : Loss = 4.262175559997559\n",
      "Epoch 2, Batch 22/32 : Loss = 4.12625789642334\n",
      "Epoch 2, Batch 23/32 : Loss = 4.162040710449219\n",
      "Epoch 2, Batch 24/32 : Loss = 4.140751838684082\n",
      "Epoch 2, Batch 25/32 : Loss = 4.151789665222168\n",
      "Epoch 2, Batch 26/32 : Loss = 4.0890727043151855\n",
      "Epoch 2, Batch 27/32 : Loss = 4.048218727111816\n",
      "Epoch 2, Batch 28/32 : Loss = 4.007646560668945\n",
      "Epoch 2, Batch 29/32 : Loss = 3.925060510635376\n",
      "Epoch 2, Batch 30/32 : Loss = 3.9075310230255127\n",
      "Epoch 2, Batch 31/32 : Loss = 4.0782470703125\n",
      "Epoch 2 finished in 0.05294296344121297 minutes\n",
      "Epoch 2 training_loss = 4.274520397186279\n",
      "---------------------------------------------------------------- => , Ground Truth is YW]i\\|M<M88\n",
      "---------------------------------------------------------------- => , Ground Truth is {BYRayh#2>E4\n",
      "---------------------------------------------------------------- => , Ground Truth is 7nDPntwd\\QaR\n",
      "---------------------------------------------------------------- => , Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 2 val_loss = 4.092461109161377, word_accuracy = 0.0\n",
      "Epoch 3, Batch 0/32 : Loss = 3.8225886821746826\n",
      "Epoch 3, Batch 1/32 : Loss = 3.8177032470703125\n",
      "Epoch 3, Batch 2/32 : Loss = 3.7987136840820312\n",
      "Epoch 3, Batch 3/32 : Loss = 3.72946834564209\n",
      "Epoch 3, Batch 4/32 : Loss = 3.7696826457977295\n",
      "Epoch 3, Batch 5/32 : Loss = 3.718949317932129\n",
      "Epoch 3, Batch 6/32 : Loss = 3.5776166915893555\n",
      "Epoch 3, Batch 7/32 : Loss = 3.5279316902160645\n",
      "Epoch 3, Batch 8/32 : Loss = 3.570005416870117\n",
      "Epoch 3, Batch 9/32 : Loss = 3.7340893745422363\n",
      "Epoch 3, Batch 10/32 : Loss = 3.4220452308654785\n",
      "Epoch 3, Batch 11/32 : Loss = 3.647202968597412\n",
      "Epoch 3, Batch 12/32 : Loss = 3.369819164276123\n",
      "Epoch 3, Batch 13/32 : Loss = 3.3511271476745605\n",
      "Epoch 3, Batch 14/32 : Loss = 3.3177390098571777\n",
      "Epoch 3, Batch 15/32 : Loss = 3.3325281143188477\n",
      "Epoch 3, Batch 16/32 : Loss = 3.2968804836273193\n",
      "Epoch 3, Batch 17/32 : Loss = 3.120387077331543\n",
      "Epoch 3, Batch 18/32 : Loss = 3.114417552947998\n",
      "Epoch 3, Batch 19/32 : Loss = 3.0148305892944336\n",
      "Epoch 3, Batch 20/32 : Loss = 2.973395586013794\n",
      "Epoch 3, Batch 21/32 : Loss = 3.062628746032715\n",
      "Epoch 3, Batch 22/32 : Loss = 2.9628562927246094\n",
      "Epoch 3, Batch 23/32 : Loss = 2.7512497901916504\n",
      "Epoch 3, Batch 24/32 : Loss = 2.587456464767456\n",
      "Epoch 3, Batch 25/32 : Loss = 2.6006388664245605\n",
      "Epoch 3, Batch 26/32 : Loss = 2.753655433654785\n",
      "Epoch 3, Batch 27/32 : Loss = 2.596498727798462\n",
      "Epoch 3, Batch 28/32 : Loss = 2.5949795246124268\n",
      "Epoch 3, Batch 29/32 : Loss = 2.4498701095581055\n",
      "Epoch 3, Batch 30/32 : Loss = 2.39908504486084\n",
      "Epoch 3, Batch 31/32 : Loss = 2.665163278579712\n",
      "Epoch 3 finished in 0.05467157363891602 minutes\n",
      "Epoch 3 training_loss = 3.2166800498962402\n",
      "----------XX------------a---------8------------R----- => Xa8R, Ground Truth is d:X9eaF,8-VR\n",
      "----------------------------3------------------------ => 3, Ground Truth is 0J!(;A3,')rr7\n",
      "-------------------------33---------------66----X---- => 36X, Ground Truth is /MoE^3x/&6X\n",
      "----------------------Z------------------------------ => Z, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 3 val_loss = 2.5412707328796387, word_accuracy = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 0/32 : Loss = 2.3384246826171875\n",
      "Epoch 4, Batch 1/32 : Loss = 2.160064935684204\n",
      "Epoch 4, Batch 2/32 : Loss = 1.947206974029541\n",
      "Epoch 4, Batch 3/32 : Loss = 2.078225612640381\n",
      "Epoch 4, Batch 4/32 : Loss = 2.355708599090576\n",
      "Epoch 4, Batch 5/32 : Loss = 1.9781115055084229\n",
      "Epoch 4, Batch 6/32 : Loss = 2.270360231399536\n",
      "Epoch 4, Batch 7/32 : Loss = 1.926094651222229\n",
      "Epoch 4, Batch 8/32 : Loss = 1.7926443815231323\n",
      "Epoch 4, Batch 9/32 : Loss = 1.9302606582641602\n",
      "Epoch 4, Batch 10/32 : Loss = 1.7675186395645142\n",
      "Epoch 4, Batch 11/32 : Loss = 1.7046866416931152\n",
      "Epoch 4, Batch 12/32 : Loss = 2.185209274291992\n",
      "Epoch 4, Batch 13/32 : Loss = 1.6642831563949585\n",
      "Epoch 4, Batch 14/32 : Loss = 1.577927589416504\n",
      "Epoch 4, Batch 15/32 : Loss = 1.8931968212127686\n",
      "Epoch 4, Batch 16/32 : Loss = 1.7782456874847412\n",
      "Epoch 4, Batch 17/32 : Loss = 1.4123470783233643\n",
      "Epoch 4, Batch 18/32 : Loss = 1.352522611618042\n",
      "Epoch 4, Batch 19/32 : Loss = 1.9937174320220947\n",
      "Epoch 4, Batch 20/32 : Loss = 1.3488634824752808\n",
      "Epoch 4, Batch 21/32 : Loss = 1.3389203548431396\n",
      "Epoch 4, Batch 22/32 : Loss = 1.463196873664856\n",
      "Epoch 4, Batch 23/32 : Loss = 1.5688637495040894\n",
      "Epoch 4, Batch 24/32 : Loss = 1.1642520427703857\n",
      "Epoch 4, Batch 25/32 : Loss = 1.4091238975524902\n",
      "Epoch 4, Batch 26/32 : Loss = 1.1324841976165771\n",
      "Epoch 4, Batch 27/32 : Loss = 1.4075387716293335\n",
      "Epoch 4, Batch 28/32 : Loss = 1.1105256080627441\n",
      "Epoch 4, Batch 29/32 : Loss = 1.0830919742584229\n",
      "Epoch 4, Batch 30/32 : Loss = 1.123276948928833\n",
      "Epoch 4, Batch 31/32 : Loss = 1.0136923789978027\n",
      "Epoch 4 finished in 0.051447272300720215 minutes\n",
      "Epoch 4 training_loss = 1.6830073595046997\n",
      "---o------------BB----r--&-----9--------0-----w--------- => oBr&90w, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---55---99---g----m------J---uu---x-------X----dd------- => 59gmJuxXd, Ground Truth is 59gmJuxcx.d\\\n",
      "----------3----g-------z---#-----Y------]--q-----t---*-- => 3gz#Y]qt*, Ground Truth is Q3glz#Y:]q+*\n",
      "----------0--------X------RR----8----ii-P-----w-----)--- => 0XR8iPw), Ground Truth is Err:509\n",
      "Epoch 4 val_loss = 1.2961076498031616, word_accuracy = 0.03\n",
      "Epoch 5, Batch 0/32 : Loss = 0.920452356338501\n",
      "Epoch 5, Batch 1/32 : Loss = 1.2244060039520264\n",
      "Epoch 5, Batch 2/32 : Loss = 1.2417707443237305\n",
      "Epoch 5, Batch 3/32 : Loss = 0.8518489599227905\n",
      "Epoch 5, Batch 4/32 : Loss = 0.9489600658416748\n",
      "Epoch 5, Batch 5/32 : Loss = 0.8090738654136658\n",
      "Epoch 5, Batch 6/32 : Loss = 0.7331298589706421\n",
      "Epoch 5, Batch 7/32 : Loss = 0.6895642280578613\n",
      "Epoch 5, Batch 8/32 : Loss = 0.8889551162719727\n",
      "Epoch 5, Batch 9/32 : Loss = 1.1479949951171875\n",
      "Epoch 5, Batch 10/32 : Loss = 1.053628921508789\n",
      "Epoch 5, Batch 11/32 : Loss = 1.1090190410614014\n",
      "Epoch 5, Batch 12/32 : Loss = 0.7502605319023132\n",
      "Epoch 5, Batch 13/32 : Loss = 1.016627311706543\n",
      "Epoch 5, Batch 14/32 : Loss = 0.6825166940689087\n",
      "Epoch 5, Batch 15/32 : Loss = 0.7045973539352417\n",
      "Epoch 5, Batch 16/32 : Loss = 0.9514696598052979\n",
      "Epoch 5, Batch 17/32 : Loss = 0.6052957773208618\n",
      "Epoch 5, Batch 18/32 : Loss = 0.5502312779426575\n",
      "Epoch 5, Batch 19/32 : Loss = 0.6312465071678162\n",
      "Epoch 5, Batch 20/32 : Loss = 0.8629370331764221\n",
      "Epoch 5, Batch 21/32 : Loss = 0.5892020463943481\n",
      "Epoch 5, Batch 22/32 : Loss = 0.865878701210022\n",
      "Epoch 5, Batch 23/32 : Loss = 0.5027734041213989\n",
      "Epoch 5, Batch 24/32 : Loss = 0.6050410270690918\n",
      "Epoch 5, Batch 25/32 : Loss = 0.598521888256073\n",
      "Epoch 5, Batch 26/32 : Loss = 0.53706955909729\n",
      "Epoch 5, Batch 27/32 : Loss = 0.5326060652732849\n",
      "Epoch 5, Batch 28/32 : Loss = 0.7689354419708252\n",
      "Epoch 5, Batch 29/32 : Loss = 0.7495341300964355\n",
      "Epoch 5, Batch 30/32 : Loss = 0.48051851987838745\n",
      "Epoch 5, Batch 31/32 : Loss = 4.0850396156311035\n",
      "Epoch 5 finished in 0.051445960998535156 minutes\n",
      "Epoch 5 training_loss = 0.8068979978561401\n",
      "------33----\\---$---->>----S----------M----ii--B----- => 3\\$>SMiB, Ground Truth is :-3\\$>S\\MiB\n",
      "---------0----[--xx---)--RR---8----i--P----w-----)--- => -0[x)R8iPw), Ground Truth is Err:509\n",
      "--------M----o----E-------3---xx--/--&&---6----X----- => MoE3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "-------\"--}-----B----r--&----9---,---0-----w----}---- => \"}Br&9,0w}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 5 val_loss = 0.7876121997833252, word_accuracy = 0.23\n",
      "Epoch 6, Batch 0/32 : Loss = 0.6981513500213623\n",
      "Epoch 6, Batch 1/32 : Loss = 0.3478569984436035\n",
      "Epoch 6, Batch 2/32 : Loss = 0.7054541707038879\n",
      "Epoch 6, Batch 3/32 : Loss = 0.9339599609375\n",
      "Epoch 6, Batch 4/32 : Loss = 0.39966174960136414\n",
      "Epoch 6, Batch 5/32 : Loss = 0.6504957675933838\n",
      "Epoch 6, Batch 6/32 : Loss = 0.3767648935317993\n",
      "Epoch 6, Batch 7/32 : Loss = 0.3289087414741516\n",
      "Epoch 6, Batch 8/32 : Loss = 0.371661901473999\n",
      "Epoch 6, Batch 9/32 : Loss = 0.3923353850841522\n",
      "Epoch 6, Batch 10/32 : Loss = 0.3089748024940491\n",
      "Epoch 6, Batch 11/32 : Loss = 0.6054575443267822\n",
      "Epoch 6, Batch 12/32 : Loss = 0.36243537068367004\n",
      "Epoch 6, Batch 13/32 : Loss = 0.3221701383590698\n",
      "Epoch 6, Batch 14/32 : Loss = 0.3126618266105652\n",
      "Epoch 6, Batch 15/32 : Loss = 0.3374418020248413\n",
      "Epoch 6, Batch 16/32 : Loss = 0.48365649580955505\n",
      "Epoch 6, Batch 17/32 : Loss = 0.7677146196365356\n",
      "Epoch 6, Batch 18/32 : Loss = 0.3899379372596741\n",
      "Epoch 6, Batch 19/32 : Loss = 0.29864442348480225\n",
      "Epoch 6, Batch 20/32 : Loss = 0.4057078957557678\n",
      "Epoch 6, Batch 21/32 : Loss = 0.23869433999061584\n",
      "Epoch 6, Batch 22/32 : Loss = 0.5348014831542969\n",
      "Epoch 6, Batch 23/32 : Loss = 0.2800161838531494\n",
      "Epoch 6, Batch 24/32 : Loss = 0.2189217507839203\n",
      "Epoch 6, Batch 25/32 : Loss = 0.3270740211009979\n",
      "Epoch 6, Batch 26/32 : Loss = 0.2759547829627991\n",
      "Epoch 6, Batch 27/32 : Loss = 0.9927507638931274\n",
      "Epoch 6, Batch 28/32 : Loss = 0.2544909417629242\n",
      "Epoch 6, Batch 29/32 : Loss = 0.3045721650123596\n",
      "Epoch 6, Batch 30/32 : Loss = 0.2382696121931076\n",
      "Epoch 6, Batch 31/32 : Loss = 3.1698403358459473\n",
      "Epoch 6 finished in 0.19480498631795248 minutes\n",
      "Epoch 6 training_loss = 0.445360004901886\n",
      "---k----O-----/--,--y---c---*----1---PP---}--#-----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----=-----0----[---x---)---RR----8----i--P-----w-----)--- => =0[x)R8iPw), Ground Truth is Err:509\n",
      "--B----.---Y---I--W------6----FF---h----X------Y----2---- => B.YIW6FhXY2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----d----'--X----9---e----a----FF----8-------V----R----- => d'X9eaF8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 6 val_loss = 0.5860494375228882, word_accuracy = 0.48\n",
      "Epoch 7, Batch 0/32 : Loss = 0.4463951289653778\n",
      "Epoch 7, Batch 1/32 : Loss = 0.45043325424194336\n",
      "Epoch 7, Batch 2/32 : Loss = 0.21121567487716675\n",
      "Epoch 7, Batch 3/32 : Loss = 0.16559666395187378\n",
      "Epoch 7, Batch 4/32 : Loss = 0.515922486782074\n",
      "Epoch 7, Batch 5/32 : Loss = 0.195570170879364\n",
      "Epoch 7, Batch 6/32 : Loss = 0.21955107152462006\n",
      "Epoch 7, Batch 7/32 : Loss = 0.375349760055542\n",
      "Epoch 7, Batch 8/32 : Loss = 0.6151633262634277\n",
      "Epoch 7, Batch 9/32 : Loss = 0.23406851291656494\n",
      "Epoch 7, Batch 10/32 : Loss = 0.2884078919887543\n",
      "Epoch 7, Batch 11/32 : Loss = 0.1867031753063202\n",
      "Epoch 7, Batch 12/32 : Loss = 0.16689971089363098\n",
      "Epoch 7, Batch 13/32 : Loss = 0.1446586549282074\n",
      "Epoch 7, Batch 14/32 : Loss = 0.22442752122879028\n",
      "Epoch 7, Batch 15/32 : Loss = 0.16411414742469788\n",
      "Epoch 7, Batch 16/32 : Loss = 0.18032129108905792\n",
      "Epoch 7, Batch 17/32 : Loss = 0.507253110408783\n",
      "Epoch 7, Batch 18/32 : Loss = 0.4076690077781677\n",
      "Epoch 7, Batch 19/32 : Loss = 0.1642545759677887\n",
      "Epoch 7, Batch 20/32 : Loss = 0.20145325362682343\n",
      "Epoch 7, Batch 21/32 : Loss = 0.4603278636932373\n",
      "Epoch 7, Batch 22/32 : Loss = 0.4041094183921814\n",
      "Epoch 7, Batch 23/32 : Loss = 0.2220214456319809\n",
      "Epoch 7, Batch 24/32 : Loss = 0.13884121179580688\n",
      "Epoch 7, Batch 25/32 : Loss = 0.1976097822189331\n",
      "Epoch 7, Batch 26/32 : Loss = 0.42980659008026123\n",
      "Epoch 7, Batch 27/32 : Loss = 0.21540388464927673\n",
      "Epoch 7, Batch 28/32 : Loss = 0.38086646795272827\n",
      "Epoch 7, Batch 29/32 : Loss = 0.1246933788061142\n",
      "Epoch 7, Batch 30/32 : Loss = 0.4632608890533447\n",
      "Epoch 7, Batch 31/32 : Loss = 0.12652140855789185\n",
      "Epoch 7 finished in 0.053542677561442056 minutes\n",
      "Epoch 7 training_loss = 0.2929536998271942\n",
      "---d----:--XX---9----e----aa----F-----8--------V---RR------ => d:X9eaF8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----d----!--NN----r---A---jj--*---$---3----hh---55---n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "--4----r--{-----%%--//--'-)---w-----&----NN-----+---P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---#----G----9----E-----=----h---55---#---22---J---}--k---- => #G9E=h5#2J}k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 7 val_loss = 0.521806538105011, word_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 0/32 : Loss = 0.11571566015481949\n",
      "Epoch 8, Batch 1/32 : Loss = 0.3791310787200928\n",
      "Epoch 8, Batch 2/32 : Loss = 0.13696901500225067\n",
      "Epoch 8, Batch 3/32 : Loss = 0.14420795440673828\n",
      "Epoch 8, Batch 4/32 : Loss = 0.10432037711143494\n",
      "Epoch 8, Batch 5/32 : Loss = 0.24922938644886017\n",
      "Epoch 8, Batch 6/32 : Loss = 0.09343170374631882\n",
      "Epoch 8, Batch 7/32 : Loss = 0.3146168887615204\n",
      "Epoch 8, Batch 8/32 : Loss = 0.365764319896698\n",
      "Epoch 8, Batch 9/32 : Loss = 0.10468517243862152\n",
      "Epoch 8, Batch 10/32 : Loss = 0.26260876655578613\n",
      "Epoch 8, Batch 11/32 : Loss = 0.1904647946357727\n",
      "Epoch 8, Batch 12/32 : Loss = 0.12089686095714569\n",
      "Epoch 8, Batch 13/32 : Loss = 0.11706064641475677\n",
      "Epoch 8, Batch 14/32 : Loss = 0.3630850613117218\n",
      "Epoch 8, Batch 15/32 : Loss = 0.27358824014663696\n",
      "Epoch 8, Batch 16/32 : Loss = 0.16415202617645264\n",
      "Epoch 8, Batch 17/32 : Loss = 0.10589954257011414\n",
      "Epoch 8, Batch 18/32 : Loss = 0.11847132444381714\n",
      "Epoch 8, Batch 19/32 : Loss = 0.1368398368358612\n",
      "Epoch 8, Batch 20/32 : Loss = 0.15203258395195007\n",
      "Epoch 8, Batch 21/32 : Loss = 0.31126493215560913\n",
      "Epoch 8, Batch 22/32 : Loss = 0.232404887676239\n",
      "Epoch 8, Batch 23/32 : Loss = 0.17957650125026703\n",
      "Epoch 8, Batch 24/32 : Loss = 0.10456352680921555\n",
      "Epoch 8, Batch 25/32 : Loss = 0.09062386304140091\n",
      "Epoch 8, Batch 26/32 : Loss = 0.27766504883766174\n",
      "Epoch 8, Batch 27/32 : Loss = 0.10509428381919861\n",
      "Epoch 8, Batch 28/32 : Loss = 0.21083196997642517\n",
      "Epoch 8, Batch 29/32 : Loss = 0.36707064509391785\n",
      "Epoch 8, Batch 30/32 : Loss = 0.10892854630947113\n",
      "Epoch 8, Batch 31/32 : Loss = 0.2915135324001312\n",
      "Epoch 8 finished in 0.05212748050689697 minutes\n",
      "Epoch 8 training_loss = 0.19398023188114166\n",
      "--.--C-----O------w------uu---`---u----.--RR-----{--3-----\"--##------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "------Q------3------g----I---z----#------Y----:--]---q------+----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "--.---c----OO------w------u----`--uu----.--RR----{---3----\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----++---:--z-----7----8----dd-----S-----v---5-----S----JJ----B----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 8 val_loss = 0.5003883838653564, word_accuracy = 0.51\n",
      "Epoch 9, Batch 0/32 : Loss = 0.3404184579849243\n",
      "Epoch 9, Batch 1/32 : Loss = 0.06235137954354286\n",
      "Epoch 9, Batch 2/32 : Loss = 0.11626942455768585\n",
      "Epoch 9, Batch 3/32 : Loss = 0.13298740983009338\n",
      "Epoch 9, Batch 4/32 : Loss = 0.06436075270175934\n",
      "Epoch 9, Batch 5/32 : Loss = 0.10188428312540054\n",
      "Epoch 9, Batch 6/32 : Loss = 0.12124943733215332\n",
      "Epoch 9, Batch 7/32 : Loss = 0.33203965425491333\n",
      "Epoch 9, Batch 8/32 : Loss = 0.08576136827468872\n",
      "Epoch 9, Batch 9/32 : Loss = 0.10122928768396378\n",
      "Epoch 9, Batch 10/32 : Loss = 0.25931382179260254\n",
      "Epoch 9, Batch 11/32 : Loss = 0.10379786789417267\n",
      "Epoch 9, Batch 12/32 : Loss = 0.10586605966091156\n",
      "Epoch 9, Batch 13/32 : Loss = 0.21196061372756958\n",
      "Epoch 9, Batch 14/32 : Loss = 0.10389196872711182\n",
      "Epoch 9, Batch 15/32 : Loss = 0.2314830869436264\n",
      "Epoch 9, Batch 16/32 : Loss = 0.09509841352701187\n",
      "Epoch 9, Batch 17/32 : Loss = 0.11501497030258179\n",
      "Epoch 9, Batch 18/32 : Loss = 0.07740181684494019\n",
      "Epoch 9, Batch 19/32 : Loss = 0.15739090740680695\n",
      "Epoch 9, Batch 20/32 : Loss = 0.3498224914073944\n",
      "Epoch 9, Batch 21/32 : Loss = 0.18942008912563324\n",
      "Epoch 9, Batch 22/32 : Loss = 0.07421760261058807\n",
      "Epoch 9, Batch 23/32 : Loss = 0.0724811777472496\n",
      "Epoch 9, Batch 24/32 : Loss = 0.1820077747106552\n",
      "Epoch 9, Batch 25/32 : Loss = 0.13432465493679047\n",
      "Epoch 9, Batch 26/32 : Loss = 0.07423841953277588\n",
      "Epoch 9, Batch 27/32 : Loss = 0.15715435147285461\n",
      "Epoch 9, Batch 28/32 : Loss = 0.12849847972393036\n",
      "Epoch 9, Batch 29/32 : Loss = 0.07698631286621094\n",
      "Epoch 9, Batch 30/32 : Loss = 0.16098257899284363\n",
      "Epoch 9, Batch 31/32 : Loss = 0.2944302260875702\n",
      "Epoch 9 finished in 0.05421239137649536 minutes\n",
      "Epoch 9 training_loss = 0.14640027284622192\n",
      "----0----Q-----6----<<----<----(---TT----N----5----=----P-------m------- => 0Q6<<(TN5=Pm, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----XX---7----0---j---@------S----Z----L---44----C----m-------MM-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----W------=----22----+----E----11---n----TT----X---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "--.---C-----O------W-------u----`---u----.--RR-----{---3----\"---#------- => .COWu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 9 val_loss = 0.4989645779132843, word_accuracy = 0.56\n",
      "Epoch 10, Batch 0/32 : Loss = 0.074806347489357\n",
      "Epoch 10, Batch 1/32 : Loss = 0.1446678787469864\n",
      "Epoch 10, Batch 2/32 : Loss = 0.09324240684509277\n",
      "Epoch 10, Batch 3/32 : Loss = 0.0698215439915657\n",
      "Epoch 10, Batch 4/32 : Loss = 0.07053101807832718\n",
      "Epoch 10, Batch 5/32 : Loss = 0.07903178036212921\n",
      "Epoch 10, Batch 6/32 : Loss = 0.15027287602424622\n",
      "Epoch 10, Batch 7/32 : Loss = 0.2013205587863922\n",
      "Epoch 10, Batch 8/32 : Loss = 0.11018984764814377\n",
      "Epoch 10, Batch 9/32 : Loss = 0.09025000780820847\n",
      "Epoch 10, Batch 10/32 : Loss = 0.06272335350513458\n",
      "Epoch 10, Batch 11/32 : Loss = 0.047835737466812134\n",
      "Epoch 10, Batch 12/32 : Loss = 0.1951182633638382\n",
      "Epoch 10, Batch 13/32 : Loss = 0.1589495837688446\n",
      "Epoch 10, Batch 14/32 : Loss = 0.13300856947898865\n",
      "Epoch 10, Batch 15/32 : Loss = 0.076093390583992\n",
      "Epoch 10, Batch 16/32 : Loss = 0.0760502815246582\n",
      "Epoch 10, Batch 17/32 : Loss = 0.09447641670703888\n",
      "Epoch 10, Batch 18/32 : Loss = 0.23133257031440735\n",
      "Epoch 10, Batch 19/32 : Loss = 0.08326377719640732\n",
      "Epoch 10, Batch 20/32 : Loss = 0.24737444519996643\n",
      "Epoch 10, Batch 21/32 : Loss = 0.18467660248279572\n",
      "Epoch 10, Batch 22/32 : Loss = 0.052656516432762146\n",
      "Epoch 10, Batch 23/32 : Loss = 0.1358143538236618\n",
      "Epoch 10, Batch 24/32 : Loss = 0.059216197580099106\n",
      "Epoch 10, Batch 25/32 : Loss = 0.32374629378318787\n",
      "Epoch 10, Batch 26/32 : Loss = 0.0771857351064682\n",
      "Epoch 10, Batch 27/32 : Loss = 0.07944543659687042\n",
      "Epoch 10, Batch 28/32 : Loss = 0.2578714191913605\n",
      "Epoch 10, Batch 29/32 : Loss = 0.03878497704863548\n",
      "Epoch 10, Batch 30/32 : Loss = 0.24639423191547394\n",
      "Epoch 10, Batch 31/32 : Loss = 0.0627216175198555\n",
      "Epoch 10 finished in 0.054898007710774736 minutes\n",
      "Epoch 10 training_loss = 0.12703590095043182\n",
      "-----8-----K----ll-ZZ-----5-----p----$----aa----}---w-----,--- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---d----:---X----9----e-----a----F---,--8--------V----RR------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----E-----00----[---x----)---RR-----8----i--PP-----w------)--- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "----J--;---q----++--|---z---yy---U-------%----U-----1---x----- => J;q+|zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 10 val_loss = 0.4835295081138611, word_accuracy = 0.64\n",
      "Epoch 11, Batch 0/32 : Loss = 0.07367823272943497\n",
      "Epoch 11, Batch 1/32 : Loss = 0.04236605018377304\n",
      "Epoch 11, Batch 2/32 : Loss = 0.05119241774082184\n",
      "Epoch 11, Batch 3/32 : Loss = 0.15725873410701752\n",
      "Epoch 11, Batch 4/32 : Loss = 0.06315171718597412\n",
      "Epoch 11, Batch 5/32 : Loss = 0.07658573240041733\n",
      "Epoch 11, Batch 6/32 : Loss = 0.1547146737575531\n",
      "Epoch 11, Batch 7/32 : Loss = 0.12546738982200623\n",
      "Epoch 11, Batch 8/32 : Loss = 0.0574549175798893\n",
      "Epoch 11, Batch 9/32 : Loss = 0.04704564809799194\n",
      "Epoch 11, Batch 10/32 : Loss = 0.1410369873046875\n",
      "Epoch 11, Batch 11/32 : Loss = 0.07270821928977966\n",
      "Epoch 11, Batch 12/32 : Loss = 0.07384058833122253\n",
      "Epoch 11, Batch 13/32 : Loss = 0.1569928377866745\n",
      "Epoch 11, Batch 14/32 : Loss = 0.06033802032470703\n",
      "Epoch 11, Batch 15/32 : Loss = 0.05400816351175308\n",
      "Epoch 11, Batch 16/32 : Loss = 0.11672602593898773\n",
      "Epoch 11, Batch 17/32 : Loss = 0.12120874971151352\n",
      "Epoch 11, Batch 18/32 : Loss = 0.043945182114839554\n",
      "Epoch 11, Batch 19/32 : Loss = 0.26672759652137756\n",
      "Epoch 11, Batch 20/32 : Loss = 0.08050574362277985\n",
      "Epoch 11, Batch 21/32 : Loss = 0.0642993152141571\n",
      "Epoch 11, Batch 22/32 : Loss = 0.041487812995910645\n",
      "Epoch 11, Batch 23/32 : Loss = 0.08687605708837509\n",
      "Epoch 11, Batch 24/32 : Loss = 0.0990959033370018\n",
      "Epoch 11, Batch 25/32 : Loss = 0.08879483491182327\n",
      "Epoch 11, Batch 26/32 : Loss = 0.1443086862564087\n",
      "Epoch 11, Batch 27/32 : Loss = 0.14314113557338715\n",
      "Epoch 11, Batch 28/32 : Loss = 0.12632104754447937\n",
      "Epoch 11, Batch 29/32 : Loss = 0.15328510105609894\n",
      "Epoch 11, Batch 30/32 : Loss = 0.23114773631095886\n",
      "Epoch 11, Batch 31/32 : Loss = 0.15090110898017883\n",
      "Epoch 11 finished in 0.17277147769927978 minutes\n",
      "Epoch 11 training_loss = 0.1039220467209816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------k----$-----l--F-----DD-----e----hh----]---k---0-----\\----X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----=------0------[---X----))---RR-----88----ii--PP-----w-------)---- => =0[X)R8iPw), Ground Truth is Err:509\n",
      "----X----7---00---j---@-----S----Z----L---44----C----m-------MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "-;-------3------\\\\---$$------>------SS-----\\\\-----M-------ii---B------ => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 11 val_loss = 0.5100294947624207, word_accuracy = 0.56\n",
      "Epoch 12, Batch 0/32 : Loss = 0.09622953832149506\n",
      "Epoch 12, Batch 1/32 : Loss = 0.17512357234954834\n",
      "Epoch 12, Batch 2/32 : Loss = 0.03868137300014496\n",
      "Epoch 12, Batch 3/32 : Loss = 0.07819114625453949\n",
      "Epoch 12, Batch 4/32 : Loss = 0.04810584336519241\n",
      "Epoch 12, Batch 5/32 : Loss = 0.07015082985162735\n",
      "Epoch 12, Batch 6/32 : Loss = 0.03713603317737579\n",
      "Epoch 12, Batch 7/32 : Loss = 0.1520155519247055\n",
      "Epoch 12, Batch 8/32 : Loss = 0.0985426977276802\n",
      "Epoch 12, Batch 9/32 : Loss = 0.03171667829155922\n",
      "Epoch 12, Batch 10/32 : Loss = 0.12507328391075134\n",
      "Epoch 12, Batch 11/32 : Loss = 0.03676813840866089\n",
      "Epoch 12, Batch 12/32 : Loss = 0.02916279435157776\n",
      "Epoch 12, Batch 13/32 : Loss = 0.040031734853982925\n",
      "Epoch 12, Batch 14/32 : Loss = 0.04713054001331329\n",
      "Epoch 12, Batch 15/32 : Loss = 0.05285334214568138\n",
      "Epoch 12, Batch 16/32 : Loss = 0.13358955085277557\n",
      "Epoch 12, Batch 17/32 : Loss = 0.04553654417395592\n",
      "Epoch 12, Batch 18/32 : Loss = 0.08679056167602539\n",
      "Epoch 12, Batch 19/32 : Loss = 0.05583421513438225\n",
      "Epoch 12, Batch 20/32 : Loss = 0.03057507425546646\n",
      "Epoch 12, Batch 21/32 : Loss = 0.04670140892267227\n",
      "Epoch 12, Batch 22/32 : Loss = 0.03739669919013977\n",
      "Epoch 12, Batch 23/32 : Loss = 0.03420279175043106\n",
      "Epoch 12, Batch 24/32 : Loss = 0.10544541478157043\n",
      "Epoch 12, Batch 25/32 : Loss = 0.07395529001951218\n",
      "Epoch 12, Batch 26/32 : Loss = 0.11884912848472595\n",
      "Epoch 12, Batch 27/32 : Loss = 0.03699421510100365\n",
      "Epoch 12, Batch 28/32 : Loss = 0.3612816333770752\n",
      "Epoch 12, Batch 29/32 : Loss = 0.09294285625219345\n",
      "Epoch 12, Batch 30/32 : Loss = 0.0928286463022232\n",
      "Epoch 12, Batch 31/32 : Loss = 0.05484766513109207\n",
      "Epoch 12 finished in 0.05216300884882609 minutes\n",
      "Epoch 12 training_loss = 0.08085761219263077\n",
      "--.--C-----O------w------uu---`---u----.---R-----{--33----\"---#------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-------Q------3-----g-----I--zz---#------Y----:--]---q------+----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---7----n-----D-----PP----n----t---w-----d-----\\---Q------a-----R---- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----k----$-----|--'-,---9----Y-----N------W------mm-------T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 12 val_loss = 0.5011205673217773, word_accuracy = 0.64\n",
      "Epoch 13, Batch 0/32 : Loss = 0.06318444013595581\n",
      "Epoch 13, Batch 1/32 : Loss = 0.11532469838857651\n",
      "Epoch 13, Batch 2/32 : Loss = 0.12148883938789368\n",
      "Epoch 13, Batch 3/32 : Loss = 0.044530536979436874\n",
      "Epoch 13, Batch 4/32 : Loss = 0.18758973479270935\n",
      "Epoch 13, Batch 5/32 : Loss = 0.0428786426782608\n",
      "Epoch 13, Batch 6/32 : Loss = 0.061991509050130844\n",
      "Epoch 13, Batch 7/32 : Loss = 0.04664048179984093\n",
      "Epoch 13, Batch 8/32 : Loss = 0.05615173280239105\n",
      "Epoch 13, Batch 9/32 : Loss = 0.04625745117664337\n",
      "Epoch 13, Batch 10/32 : Loss = 0.07240890711545944\n",
      "Epoch 13, Batch 11/32 : Loss = 0.03376441448926926\n",
      "Epoch 13, Batch 12/32 : Loss = 0.09152144193649292\n",
      "Epoch 13, Batch 13/32 : Loss = 0.22178252041339874\n",
      "Epoch 13, Batch 14/32 : Loss = 0.11442984640598297\n",
      "Epoch 13, Batch 15/32 : Loss = 0.06424426287412643\n",
      "Epoch 13, Batch 16/32 : Loss = 0.04464855045080185\n",
      "Epoch 13, Batch 17/32 : Loss = 0.05936729162931442\n",
      "Epoch 13, Batch 18/32 : Loss = 0.1814410239458084\n",
      "Epoch 13, Batch 19/32 : Loss = 0.03596124053001404\n",
      "Epoch 13, Batch 20/32 : Loss = 0.2519465982913971\n",
      "Epoch 13, Batch 21/32 : Loss = 0.03139369934797287\n",
      "Epoch 13, Batch 22/32 : Loss = 0.03898794576525688\n",
      "Epoch 13, Batch 23/32 : Loss = 0.20511087775230408\n",
      "Epoch 13, Batch 24/32 : Loss = 0.09904192388057709\n",
      "Epoch 13, Batch 25/32 : Loss = 0.10894577205181122\n",
      "Epoch 13, Batch 26/32 : Loss = 0.0371905118227005\n",
      "Epoch 13, Batch 27/32 : Loss = 0.02556282840669155\n",
      "Epoch 13, Batch 28/32 : Loss = 0.029805604368448257\n",
      "Epoch 13, Batch 29/32 : Loss = 0.02609437331557274\n",
      "Epoch 13, Batch 30/32 : Loss = 0.05323725938796997\n",
      "Epoch 13, Batch 31/32 : Loss = 0.19301339983940125\n",
      "Epoch 13 finished in 0.05236412684122722 minutes\n",
      "Epoch 13 training_loss = 0.0847245454788208\n",
      "----z----0----\"----G------/---~~----$$----c-----1---j--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----0-----cc----++----b-----I--\"---b-----6----.---Q--------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "---7---n----D-----P----n---t--w-----d----\\---Q----aa---RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----4----r--{{-----%---/---'-)---w-----&-----N-----+----P--- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 13 val_loss = 0.5216864943504333, word_accuracy = 0.63\n",
      "Epoch 14, Batch 0/32 : Loss = 0.03351160138845444\n",
      "Epoch 14, Batch 1/32 : Loss = 0.015976060181856155\n",
      "Epoch 14, Batch 2/32 : Loss = 0.045022547245025635\n",
      "Epoch 14, Batch 3/32 : Loss = 0.03467812389135361\n",
      "Epoch 14, Batch 4/32 : Loss = 0.11013965308666229\n",
      "Epoch 14, Batch 5/32 : Loss = 0.41156715154647827\n",
      "Epoch 14, Batch 6/32 : Loss = 0.034971922636032104\n",
      "Epoch 14, Batch 7/32 : Loss = 0.025980506092309952\n",
      "Epoch 14, Batch 8/32 : Loss = 0.02133779600262642\n",
      "Epoch 14, Batch 9/32 : Loss = 0.0710143968462944\n",
      "Epoch 14, Batch 10/32 : Loss = 0.06836260855197906\n",
      "Epoch 14, Batch 11/32 : Loss = 0.11557815968990326\n",
      "Epoch 14, Batch 12/32 : Loss = 0.026456624269485474\n",
      "Epoch 14, Batch 13/32 : Loss = 0.05318615213036537\n",
      "Epoch 14, Batch 14/32 : Loss = 0.10749027878046036\n",
      "Epoch 14, Batch 15/32 : Loss = 0.05218213051557541\n",
      "Epoch 14, Batch 16/32 : Loss = 0.026516582816839218\n",
      "Epoch 14, Batch 17/32 : Loss = 0.03233001008629799\n",
      "Epoch 14, Batch 18/32 : Loss = 0.2060098648071289\n",
      "Epoch 14, Batch 19/32 : Loss = 0.07775741815567017\n",
      "Epoch 14, Batch 20/32 : Loss = 0.23496051132678986\n",
      "Epoch 14, Batch 21/32 : Loss = 0.07355841994285583\n",
      "Epoch 14, Batch 22/32 : Loss = 0.046968672424554825\n",
      "Epoch 14, Batch 23/32 : Loss = 0.06424203515052795\n",
      "Epoch 14, Batch 24/32 : Loss = 0.09020491689443588\n",
      "Epoch 14, Batch 25/32 : Loss = 0.13358250260353088\n",
      "Epoch 14, Batch 26/32 : Loss = 0.1590633988380432\n",
      "Epoch 14, Batch 27/32 : Loss = 0.07020501792430878\n",
      "Epoch 14, Batch 28/32 : Loss = 0.023351047188043594\n",
      "Epoch 14, Batch 29/32 : Loss = 0.07483060657978058\n",
      "Epoch 14, Batch 30/32 : Loss = 0.03421847149729729\n",
      "Epoch 14, Batch 31/32 : Loss = 0.36329221725463867\n",
      "Epoch 14 finished in 0.05155264536539714 minutes\n",
      "Epoch 14 training_loss = 0.08419812470674515\n",
      "----k---$----l-FF----D----e----h---]---k---0----\\---X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "--{--BB----Y----R----a---y---h---#----2---->----E---44----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "--4----r--{-----%%--//--'-)---w-----&----NN----++---P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----d----!--NN-----r--AA---j--*---$$---3----hh---5----nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 14 val_loss = 0.5128530859947205, word_accuracy = 0.64\n",
      "Epoch 15, Batch 0/32 : Loss = 0.08017292618751526\n",
      "Epoch 15, Batch 1/32 : Loss = 0.034207332879304886\n",
      "Epoch 15, Batch 2/32 : Loss = 0.03230135887861252\n",
      "Epoch 15, Batch 3/32 : Loss = 0.022827982902526855\n",
      "Epoch 15, Batch 4/32 : Loss = 0.08862052857875824\n",
      "Epoch 15, Batch 5/32 : Loss = 0.025652490556240082\n",
      "Epoch 15, Batch 6/32 : Loss = 0.06204131990671158\n",
      "Epoch 15, Batch 7/32 : Loss = 0.10742523521184921\n",
      "Epoch 15, Batch 8/32 : Loss = 0.01843167282640934\n",
      "Epoch 15, Batch 9/32 : Loss = 0.01897643879055977\n",
      "Epoch 15, Batch 10/32 : Loss = 0.12018422782421112\n",
      "Epoch 15, Batch 11/32 : Loss = 0.08753150701522827\n",
      "Epoch 15, Batch 12/32 : Loss = 0.11297621577978134\n",
      "Epoch 15, Batch 13/32 : Loss = 0.11006951332092285\n",
      "Epoch 15, Batch 14/32 : Loss = 0.04019990935921669\n",
      "Epoch 15, Batch 15/32 : Loss = 0.03691110759973526\n",
      "Epoch 15, Batch 16/32 : Loss = 0.059230223298072815\n",
      "Epoch 15, Batch 17/32 : Loss = 0.021409539505839348\n",
      "Epoch 15, Batch 18/32 : Loss = 0.028994431719183922\n",
      "Epoch 15, Batch 19/32 : Loss = 0.06374254822731018\n",
      "Epoch 15, Batch 20/32 : Loss = 0.17655989527702332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 21/32 : Loss = 0.030972333624958992\n",
      "Epoch 15, Batch 22/32 : Loss = 0.04172181338071823\n",
      "Epoch 15, Batch 23/32 : Loss = 0.034759312868118286\n",
      "Epoch 15, Batch 24/32 : Loss = 0.08018998801708221\n",
      "Epoch 15, Batch 25/32 : Loss = 0.04066680371761322\n",
      "Epoch 15, Batch 26/32 : Loss = 0.03049156442284584\n",
      "Epoch 15, Batch 27/32 : Loss = 0.01913805678486824\n",
      "Epoch 15, Batch 28/32 : Loss = 0.05824131891131401\n",
      "Epoch 15, Batch 29/32 : Loss = 0.1870623081922531\n",
      "Epoch 15, Batch 30/32 : Loss = 0.01550145260989666\n",
      "Epoch 15, Batch 31/32 : Loss = 0.018260706216096878\n",
      "Epoch 15 finished in 0.05164132912953694 minutes\n",
      "Epoch 15 training_loss = 0.06070663407444954\n",
      "-----Y----W-----]--i--\\--|----MM----<-----MM----8---8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "---k----O-----/--,--yy--cc---*----1---PP---}---#----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "--BB---.---Y---I--W-------6----F----h----X---'--Y----22--- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----kk--$$---l--F----D----e---hh---]---k---0---\\---X----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 15 val_loss = 0.4792747497558594, word_accuracy = 0.69\n",
      "Epoch 16, Batch 0/32 : Loss = 0.030077099800109863\n",
      "Epoch 16, Batch 1/32 : Loss = 0.01663142256438732\n",
      "Epoch 16, Batch 2/32 : Loss = 0.04572258144617081\n",
      "Epoch 16, Batch 3/32 : Loss = 0.03108113445341587\n",
      "Epoch 16, Batch 4/32 : Loss = 0.022025946527719498\n",
      "Epoch 16, Batch 5/32 : Loss = 0.04698973149061203\n",
      "Epoch 16, Batch 6/32 : Loss = 0.08669257909059525\n",
      "Epoch 16, Batch 7/32 : Loss = 0.06883735954761505\n",
      "Epoch 16, Batch 8/32 : Loss = 0.025197897106409073\n",
      "Epoch 16, Batch 9/32 : Loss = 0.026267006993293762\n",
      "Epoch 16, Batch 10/32 : Loss = 0.03127504885196686\n",
      "Epoch 16, Batch 11/32 : Loss = 0.041310232132673264\n",
      "Epoch 16, Batch 12/32 : Loss = 0.05879973620176315\n",
      "Epoch 16, Batch 13/32 : Loss = 0.01372939720749855\n",
      "Epoch 16, Batch 14/32 : Loss = 0.04643537849187851\n",
      "Epoch 16, Batch 15/32 : Loss = 0.09424552321434021\n",
      "Epoch 16, Batch 16/32 : Loss = 0.034296758472919464\n",
      "Epoch 16, Batch 17/32 : Loss = 0.3151882290840149\n",
      "Epoch 16, Batch 18/32 : Loss = 0.050314582884311676\n",
      "Epoch 16, Batch 19/32 : Loss = 0.12210999429225922\n",
      "Epoch 16, Batch 20/32 : Loss = 0.06876453012228012\n",
      "Epoch 16, Batch 21/32 : Loss = 0.15269802510738373\n",
      "Epoch 16, Batch 22/32 : Loss = 0.06531541794538498\n",
      "Epoch 16, Batch 23/32 : Loss = 0.015518647618591785\n",
      "Epoch 16, Batch 24/32 : Loss = 0.0749160647392273\n",
      "Epoch 16, Batch 25/32 : Loss = 0.09425044059753418\n",
      "Epoch 16, Batch 26/32 : Loss = 0.05883490666747093\n",
      "Epoch 16, Batch 27/32 : Loss = 0.03632877767086029\n",
      "Epoch 16, Batch 28/32 : Loss = 0.021317489445209503\n",
      "Epoch 16, Batch 29/32 : Loss = 0.056668348610401154\n",
      "Epoch 16, Batch 30/32 : Loss = 0.03728223592042923\n",
      "Epoch 16, Batch 31/32 : Loss = 0.04812776297330856\n",
      "Epoch 16 finished in 0.05319952964782715 minutes\n",
      "Epoch 16 training_loss = 0.060887981206178665\n",
      "----55----9----g-----m-------J----u----X---c----X--.--d----\\----- => 59gmJuXcX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "-----+---:--z----7---8----dd----S-----v---5----S----JJ---B------- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "---o----\"---}---!--B-----r---&-----9----;-`---O------w-----}----- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----c-----RR----;--9-----y----??---22----d----i---O------{--!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "Epoch 16 val_loss = 0.4807768762111664, word_accuracy = 0.67\n",
      "Epoch 17, Batch 0/32 : Loss = 0.03874615579843521\n",
      "Epoch 17, Batch 1/32 : Loss = 0.01679462380707264\n",
      "Epoch 17, Batch 2/32 : Loss = 0.02199508249759674\n",
      "Epoch 17, Batch 3/32 : Loss = 0.052021853625774384\n",
      "Epoch 17, Batch 4/32 : Loss = 0.015139540657401085\n",
      "Epoch 17, Batch 5/32 : Loss = 0.1483478546142578\n",
      "Epoch 17, Batch 6/32 : Loss = 0.04594764858484268\n",
      "Epoch 17, Batch 7/32 : Loss = 0.02369178831577301\n",
      "Epoch 17, Batch 8/32 : Loss = 0.11781081557273865\n",
      "Epoch 17, Batch 9/32 : Loss = 0.0293388981372118\n",
      "Epoch 17, Batch 10/32 : Loss = 0.07784976065158844\n",
      "Epoch 17, Batch 11/32 : Loss = 0.025374548509716988\n",
      "Epoch 17, Batch 12/32 : Loss = 0.16125749051570892\n",
      "Epoch 17, Batch 13/32 : Loss = 0.02220946177840233\n",
      "Epoch 17, Batch 14/32 : Loss = 0.014620393514633179\n",
      "Epoch 17, Batch 15/32 : Loss = 0.04240058362483978\n",
      "Epoch 17, Batch 16/32 : Loss = 0.018801476806402206\n",
      "Epoch 17, Batch 17/32 : Loss = 0.026510238647460938\n",
      "Epoch 17, Batch 18/32 : Loss = 0.07869654893875122\n",
      "Epoch 17, Batch 19/32 : Loss = 0.04048173129558563\n",
      "Epoch 17, Batch 20/32 : Loss = 0.06330125778913498\n",
      "Epoch 17, Batch 21/32 : Loss = 0.09259431809186935\n",
      "Epoch 17, Batch 22/32 : Loss = 0.03071119263768196\n",
      "Epoch 17, Batch 23/32 : Loss = 0.06344801187515259\n",
      "Epoch 17, Batch 24/32 : Loss = 0.06092025339603424\n",
      "Epoch 17, Batch 25/32 : Loss = 0.01993517577648163\n",
      "Epoch 17, Batch 26/32 : Loss = 0.02646278589963913\n",
      "Epoch 17, Batch 27/32 : Loss = 0.06715230643749237\n",
      "Epoch 17, Batch 28/32 : Loss = 0.07487113028764725\n",
      "Epoch 17, Batch 29/32 : Loss = 0.03141072764992714\n",
      "Epoch 17, Batch 30/32 : Loss = 0.11521565914154053\n",
      "Epoch 17, Batch 31/32 : Loss = 0.07784155011177063\n",
      "Epoch 17 finished in 0.054748010635375974 minutes\n",
      "Epoch 17 training_loss = 0.05377636477351189\n",
      "----o----\"---}---!--BB-----r---&-----99---;--`---O-------w-----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----<<------v----O-------T----`---4-----VV---[[--0----------Q------ => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----{---BB----YY----RR---aa----y---h----#----22---->-----E----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "--BB----.----Y----I---W-------6-----FF----hh----XX---'---Y----22---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 17 val_loss = 0.5091777443885803, word_accuracy = 0.71\n",
      "Epoch 18, Batch 0/32 : Loss = 0.013573197647929192\n",
      "Epoch 18, Batch 1/32 : Loss = 0.01576491817831993\n",
      "Epoch 18, Batch 2/32 : Loss = 0.03851887583732605\n",
      "Epoch 18, Batch 3/32 : Loss = 0.11664421856403351\n",
      "Epoch 18, Batch 4/32 : Loss = 0.07265187054872513\n",
      "Epoch 18, Batch 5/32 : Loss = 0.024752290919423103\n",
      "Epoch 18, Batch 6/32 : Loss = 0.05275636166334152\n",
      "Epoch 18, Batch 7/32 : Loss = 0.044414814561605453\n",
      "Epoch 18, Batch 8/32 : Loss = 0.0199006088078022\n",
      "Epoch 18, Batch 9/32 : Loss = 0.07785613089799881\n",
      "Epoch 18, Batch 10/32 : Loss = 0.08607325702905655\n",
      "Epoch 18, Batch 11/32 : Loss = 0.050783321261405945\n",
      "Epoch 18, Batch 12/32 : Loss = 0.0431608147919178\n",
      "Epoch 18, Batch 13/32 : Loss = 0.02453121729195118\n",
      "Epoch 18, Batch 14/32 : Loss = 0.01918373815715313\n",
      "Epoch 18, Batch 15/32 : Loss = 0.10974954068660736\n",
      "Epoch 18, Batch 16/32 : Loss = 0.047334492206573486\n",
      "Epoch 18, Batch 17/32 : Loss = 0.01835440844297409\n",
      "Epoch 18, Batch 18/32 : Loss = 0.015681486576795578\n",
      "Epoch 18, Batch 19/32 : Loss = 0.018159741535782814\n",
      "Epoch 18, Batch 20/32 : Loss = 0.017681784927845\n",
      "Epoch 18, Batch 21/32 : Loss = 0.08634740114212036\n",
      "Epoch 18, Batch 22/32 : Loss = 0.014047977514564991\n",
      "Epoch 18, Batch 23/32 : Loss = 0.14647749066352844\n",
      "Epoch 18, Batch 24/32 : Loss = 0.07866344600915909\n",
      "Epoch 18, Batch 25/32 : Loss = 0.027209870517253876\n",
      "Epoch 18, Batch 26/32 : Loss = 0.1794123351573944\n",
      "Epoch 18, Batch 27/32 : Loss = 0.031404610723257065\n",
      "Epoch 18, Batch 28/32 : Loss = 0.05731631815433502\n",
      "Epoch 18, Batch 29/32 : Loss = 0.016523854807019234\n",
      "Epoch 18, Batch 30/32 : Loss = 0.02269158512353897\n",
      "Epoch 18, Batch 31/32 : Loss = 0.03783196210861206\n",
      "Epoch 18 finished in 0.05434614419937134 minutes\n",
      "Epoch 18 training_loss = 0.051159873604774475\n",
      "----d----!--NN----r---A---j--*----$---3----h---55----n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "--BB---.---Y---I--WW-----6----FF---h----X---'--Y----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----\"--]--t--44---e----^---WW------Q----4---->-----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "----5----9---gg---mm-----J---uu---x--c---x---.-d----\\---- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 18 val_loss = 0.5197573900222778, word_accuracy = 0.71\n",
      "Epoch 19, Batch 0/32 : Loss = 0.015372848138213158\n",
      "Epoch 19, Batch 1/32 : Loss = 0.12590575218200684\n",
      "Epoch 19, Batch 2/32 : Loss = 0.0664483979344368\n",
      "Epoch 19, Batch 3/32 : Loss = 0.01495409570634365\n",
      "Epoch 19, Batch 4/32 : Loss = 0.01105495821684599\n",
      "Epoch 19, Batch 5/32 : Loss = 0.02547692507505417\n",
      "Epoch 19, Batch 6/32 : Loss = 0.09670327603816986\n",
      "Epoch 19, Batch 7/32 : Loss = 0.017470896244049072\n",
      "Epoch 19, Batch 8/32 : Loss = 0.017384979873895645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 9/32 : Loss = 0.0651269406080246\n",
      "Epoch 19, Batch 10/32 : Loss = 0.03718618303537369\n",
      "Epoch 19, Batch 11/32 : Loss = 0.020434096455574036\n",
      "Epoch 19, Batch 12/32 : Loss = 0.01894071325659752\n",
      "Epoch 19, Batch 13/32 : Loss = 0.07461708039045334\n",
      "Epoch 19, Batch 14/32 : Loss = 0.02856498584151268\n",
      "Epoch 19, Batch 15/32 : Loss = 0.012343715876340866\n",
      "Epoch 19, Batch 16/32 : Loss = 0.08258219063282013\n",
      "Epoch 19, Batch 17/32 : Loss = 0.03148640692234039\n",
      "Epoch 19, Batch 18/32 : Loss = 0.020488504320383072\n",
      "Epoch 19, Batch 19/32 : Loss = 0.013414212502539158\n",
      "Epoch 19, Batch 20/32 : Loss = 0.04755299538373947\n",
      "Epoch 19, Batch 21/32 : Loss = 0.018986891955137253\n",
      "Epoch 19, Batch 22/32 : Loss = 0.0182785801589489\n",
      "Epoch 19, Batch 23/32 : Loss = 0.11294485628604889\n",
      "Epoch 19, Batch 24/32 : Loss = 0.052679818123579025\n",
      "Epoch 19, Batch 25/32 : Loss = 0.011384239420294762\n",
      "Epoch 19, Batch 26/32 : Loss = 0.1555735468864441\n",
      "Epoch 19, Batch 27/32 : Loss = 0.03450489789247513\n",
      "Epoch 19, Batch 28/32 : Loss = 0.048426367342472076\n",
      "Epoch 19, Batch 29/32 : Loss = 0.03164739906787872\n",
      "Epoch 19, Batch 30/32 : Loss = 0.019710347056388855\n",
      "Epoch 19, Batch 31/32 : Loss = 0.010750161483883858\n",
      "Epoch 19 finished in 0.053826793034871416 minutes\n",
      "Epoch 19 training_loss = 0.04334106296300888\n",
      "------W------=-----2----+----E----11---n----T----XX---r---C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "------YY-----W-------]--i--\\\\---|-----MM------<------MMM-----8----8------- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----+++---:--z-----77---88-----d-----SS-----v---5-----SS----JJ----BB----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----C-----DD-----E----g----mm------'---m------F----<-----Q-----8----2----- => CDEgm'mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 19 val_loss = 0.4994884133338928, word_accuracy = 0.71\n",
      "Epoch 20, Batch 0/32 : Loss = 0.035506244748830795\n",
      "Epoch 20, Batch 1/32 : Loss = 0.0153403514996171\n",
      "Epoch 20, Batch 2/32 : Loss = 0.03644324094057083\n",
      "Epoch 20, Batch 3/32 : Loss = 0.024585608392953873\n",
      "Epoch 20, Batch 4/32 : Loss = 0.009867394343018532\n",
      "Epoch 20, Batch 5/32 : Loss = 0.013789942488074303\n",
      "Epoch 20, Batch 6/32 : Loss = 0.048258278518915176\n",
      "Epoch 20, Batch 7/32 : Loss = 0.017196249216794968\n",
      "Epoch 20, Batch 8/32 : Loss = 0.012420177459716797\n",
      "Epoch 20, Batch 9/32 : Loss = 0.043074287474155426\n",
      "Epoch 20, Batch 10/32 : Loss = 0.017682235687971115\n",
      "Epoch 20, Batch 11/32 : Loss = 0.022777054458856583\n",
      "Epoch 20, Batch 12/32 : Loss = 0.013468327932059765\n",
      "Epoch 20, Batch 13/32 : Loss = 0.01412296760827303\n",
      "Epoch 20, Batch 14/32 : Loss = 0.041057877242565155\n",
      "Epoch 20, Batch 15/32 : Loss = 0.24656668305397034\n",
      "Epoch 20, Batch 16/32 : Loss = 0.23971694707870483\n",
      "Epoch 20, Batch 17/32 : Loss = 0.03950532153248787\n",
      "Epoch 20, Batch 18/32 : Loss = 0.015377532690763474\n",
      "Epoch 20, Batch 19/32 : Loss = 0.012851610779762268\n",
      "Epoch 20, Batch 20/32 : Loss = 0.011141248047351837\n",
      "Epoch 20, Batch 21/32 : Loss = 0.09750530868768692\n",
      "Epoch 20, Batch 22/32 : Loss = 0.04539967700839043\n",
      "Epoch 20, Batch 23/32 : Loss = 0.0126790227368474\n",
      "Epoch 20, Batch 24/32 : Loss = 0.014932906255126\n",
      "Epoch 20, Batch 25/32 : Loss = 0.15661895275115967\n",
      "Epoch 20, Batch 26/32 : Loss = 0.03777889162302017\n",
      "Epoch 20, Batch 27/32 : Loss = 0.030116109177470207\n",
      "Epoch 20, Batch 28/32 : Loss = 0.04106868803501129\n",
      "Epoch 20, Batch 29/32 : Loss = 0.027480095624923706\n",
      "Epoch 20, Batch 30/32 : Loss = 0.15952610969543457\n",
      "Epoch 20, Batch 31/32 : Loss = 0.35622039437294006\n",
      "Epoch 20 finished in 0.054403980573018394 minutes\n",
      "Epoch 20 training_loss = 0.05135365575551987\n",
      "----0----Q----6---<----<----(--T----N---5---=----P---(-m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "------k----$---l--F----DD----e----hh---]---k---0----\\---XX----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----YY----W------]--i--\\---|----MMM----<------MM----88---88---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "----5--->>---'--*---Y----A---'--O-----D----*--#----OO----g----- => 5>'*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 20 val_loss = 0.5386767983436584, word_accuracy = 0.54\n",
      "Epoch 21, Batch 0/32 : Loss = 0.022657766938209534\n",
      "Epoch 21, Batch 1/32 : Loss = 0.010076982900500298\n",
      "Epoch 21, Batch 2/32 : Loss = 0.09792938083410263\n",
      "Epoch 21, Batch 3/32 : Loss = 0.018369387835264206\n",
      "Epoch 21, Batch 4/32 : Loss = 0.13175055384635925\n",
      "Epoch 21, Batch 5/32 : Loss = 0.04087912291288376\n",
      "Epoch 21, Batch 6/32 : Loss = 0.023100443184375763\n",
      "Epoch 21, Batch 7/32 : Loss = 0.027112141251564026\n",
      "Epoch 21, Batch 8/32 : Loss = 0.06965561956167221\n",
      "Epoch 21, Batch 9/32 : Loss = 0.021197758615016937\n",
      "Epoch 21, Batch 10/32 : Loss = 0.03560202568769455\n",
      "Epoch 21, Batch 11/32 : Loss = 0.033179935067892075\n",
      "Epoch 21, Batch 12/32 : Loss = 0.02568860352039337\n",
      "Epoch 21, Batch 13/32 : Loss = 0.04946575313806534\n",
      "Epoch 21, Batch 14/32 : Loss = 0.026079580187797546\n",
      "Epoch 21, Batch 15/32 : Loss = 0.015496725216507912\n",
      "Epoch 21, Batch 16/32 : Loss = 0.03440871834754944\n",
      "Epoch 21, Batch 17/32 : Loss = 0.03135494142770767\n",
      "Epoch 21, Batch 18/32 : Loss = 0.012229464948177338\n",
      "Epoch 21, Batch 19/32 : Loss = 0.00886762049049139\n",
      "Epoch 21, Batch 20/32 : Loss = 0.03249121084809303\n",
      "Epoch 21, Batch 21/32 : Loss = 0.011805688962340355\n",
      "Epoch 21, Batch 22/32 : Loss = 0.09768784791231155\n",
      "Epoch 21, Batch 23/32 : Loss = 0.017921164631843567\n",
      "Epoch 21, Batch 24/32 : Loss = 0.056418344378471375\n",
      "Epoch 21, Batch 25/32 : Loss = 0.052612271159887314\n",
      "Epoch 21, Batch 26/32 : Loss = 0.22826853394508362\n",
      "Epoch 21, Batch 27/32 : Loss = 0.008095035329461098\n",
      "Epoch 21, Batch 28/32 : Loss = 0.14909245073795319\n",
      "Epoch 21, Batch 29/32 : Loss = 0.10034655034542084\n",
      "Epoch 21, Batch 30/32 : Loss = 0.018991410732269287\n",
      "Epoch 21, Batch 31/32 : Loss = 0.20101535320281982\n",
      "Epoch 21 finished in 0.0524368683497111 minutes\n",
      "Epoch 21 training_loss = 0.04928385838866234\n",
      "--BB-----.----Y----I---W--------66-----F-----hh-----X----'--YY-----2----- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----WW------=----22---+----E-----1---nn----T----X----r--C-----a---I------ => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "----EE-------0-----[----x-----)---RR------8-----ii---P------w-------))--- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "--.---cC----OO------w------uu---`---uu----.--RR----{{---3----\"---##------ => .cCOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 21 val_loss = 0.49198973178863525, word_accuracy = 0.68\n",
      "Epoch 22, Batch 0/32 : Loss = 0.012290789745748043\n",
      "Epoch 22, Batch 1/32 : Loss = 0.026863334700465202\n",
      "Epoch 22, Batch 2/32 : Loss = 0.015780676156282425\n",
      "Epoch 22, Batch 3/32 : Loss = 0.08567316830158234\n",
      "Epoch 22, Batch 4/32 : Loss = 0.010652096942067146\n",
      "Epoch 22, Batch 5/32 : Loss = 0.09155057370662689\n",
      "Epoch 22, Batch 6/32 : Loss = 0.009071378037333488\n",
      "Epoch 22, Batch 7/32 : Loss = 0.011447117663919926\n",
      "Epoch 22, Batch 8/32 : Loss = 0.011814262717962265\n",
      "Epoch 22, Batch 9/32 : Loss = 0.016384251415729523\n",
      "Epoch 22, Batch 10/32 : Loss = 0.016959557309746742\n",
      "Epoch 22, Batch 11/32 : Loss = 0.015430466271936893\n",
      "Epoch 22, Batch 12/32 : Loss = 0.01065048761665821\n",
      "Epoch 22, Batch 13/32 : Loss = 0.08178238570690155\n",
      "Epoch 22, Batch 14/32 : Loss = 0.10314048826694489\n",
      "Epoch 22, Batch 15/32 : Loss = 0.014297625049948692\n",
      "Epoch 22, Batch 16/32 : Loss = 0.08159387111663818\n",
      "Epoch 22, Batch 17/32 : Loss = 0.010465962812304497\n",
      "Epoch 22, Batch 18/32 : Loss = 0.0463130883872509\n",
      "Epoch 22, Batch 19/32 : Loss = 0.027777189388871193\n",
      "Epoch 22, Batch 20/32 : Loss = 0.027256224304437637\n",
      "Epoch 22, Batch 21/32 : Loss = 0.018283311277627945\n",
      "Epoch 22, Batch 22/32 : Loss = 0.01626121997833252\n",
      "Epoch 22, Batch 23/32 : Loss = 0.06027510389685631\n",
      "Epoch 22, Batch 24/32 : Loss = 0.029178958386182785\n",
      "Epoch 22, Batch 25/32 : Loss = 0.022501103579998016\n",
      "Epoch 22, Batch 26/32 : Loss = 0.017128176987171173\n",
      "Epoch 22, Batch 27/32 : Loss = 0.008103022351861\n",
      "Epoch 22, Batch 28/32 : Loss = 0.026545872911810875\n",
      "Epoch 22, Batch 29/32 : Loss = 0.016543468460440636\n",
      "Epoch 22, Batch 30/32 : Loss = 0.008470410481095314\n",
      "Epoch 22, Batch 31/32 : Loss = 0.01712029054760933\n",
      "Epoch 22 finished in 0.05643672943115234 minutes\n",
      "Epoch 22 training_loss = 0.030606446787714958\n",
      "---<<-----v---O------T---`--4----VV---[--0--------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---o---\"\"--}--!--BB----r--&-----9---;-`---O-----w----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---k---$---|---'---9----Y----N----W------m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----0---Q---66--<<---<----(--T---N---5---=---P-----mm----- => 0Q6<<(TN5=Pm, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 22 val_loss = 0.5305034518241882, word_accuracy = 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 0/32 : Loss = 0.03818224370479584\n",
      "Epoch 23, Batch 1/32 : Loss = 0.07820677012205124\n",
      "Epoch 23, Batch 2/32 : Loss = 0.012581863440573215\n",
      "Epoch 23, Batch 3/32 : Loss = 0.041458554565906525\n",
      "Epoch 23, Batch 4/32 : Loss = 0.02442537061870098\n",
      "Epoch 23, Batch 5/32 : Loss = 0.01649521104991436\n",
      "Epoch 23, Batch 6/32 : Loss = 0.0758679211139679\n",
      "Epoch 23, Batch 7/32 : Loss = 0.014184996485710144\n",
      "Epoch 23, Batch 8/32 : Loss = 0.014537306502461433\n",
      "Epoch 23, Batch 9/32 : Loss = 0.010837051086127758\n",
      "Epoch 23, Batch 10/32 : Loss = 0.021531714126467705\n",
      "Epoch 23, Batch 11/32 : Loss = 0.01597418636083603\n",
      "Epoch 23, Batch 12/32 : Loss = 0.06683403998613358\n",
      "Epoch 23, Batch 13/32 : Loss = 0.04703419655561447\n",
      "Epoch 23, Batch 14/32 : Loss = 0.02030612900853157\n",
      "Epoch 23, Batch 15/32 : Loss = 0.08196072280406952\n",
      "Epoch 23, Batch 16/32 : Loss = 0.008339101448655128\n",
      "Epoch 23, Batch 17/32 : Loss = 0.009539766237139702\n",
      "Epoch 23, Batch 18/32 : Loss = 0.018520314246416092\n",
      "Epoch 23, Batch 19/32 : Loss = 0.010395016521215439\n",
      "Epoch 23, Batch 20/32 : Loss = 0.04816792905330658\n",
      "Epoch 23, Batch 21/32 : Loss = 0.011268128640949726\n",
      "Epoch 23, Batch 22/32 : Loss = 0.03144007548689842\n",
      "Epoch 23, Batch 23/32 : Loss = 0.014884142205119133\n",
      "Epoch 23, Batch 24/32 : Loss = 0.12075438350439072\n",
      "Epoch 23, Batch 25/32 : Loss = 0.0076388209126889706\n",
      "Epoch 23, Batch 26/32 : Loss = 0.015982721000909805\n",
      "Epoch 23, Batch 27/32 : Loss = 0.10500450432300568\n",
      "Epoch 23, Batch 28/32 : Loss = 0.016333408653736115\n",
      "Epoch 23, Batch 29/32 : Loss = 0.00791996344923973\n",
      "Epoch 23, Batch 30/32 : Loss = 0.009072771295905113\n",
      "Epoch 23, Batch 31/32 : Loss = 0.006045712623745203\n",
      "Epoch 23 finished in 0.05305443207422892 minutes\n",
      "Epoch 23 training_loss = 0.03265654295682907\n",
      "--------3-----\\----$$---->>-----SS-----\\----MM------ii--B------ => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "---J--;--qq----+---/---z---yy---U-------%----U-----1---x---.--- => J;q+/zyU%U1x., Ground Truth is J;q+/zyU%U1x_\n",
      "---0-----c----++-----b-----I--\"----b-----6----.----Q----------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----d----!---N------r---A----j--*----$----3----h-----5----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 23 val_loss = 0.5092963576316833, word_accuracy = 0.7\n",
      "Epoch 24, Batch 0/32 : Loss = 0.015160176903009415\n",
      "Epoch 24, Batch 1/32 : Loss = 0.029094405472278595\n",
      "Epoch 24, Batch 2/32 : Loss = 0.007512436248362064\n",
      "Epoch 24, Batch 3/32 : Loss = 0.036160215735435486\n",
      "Epoch 24, Batch 4/32 : Loss = 0.0093804020434618\n",
      "Epoch 24, Batch 5/32 : Loss = 0.13661901652812958\n",
      "Epoch 24, Batch 6/32 : Loss = 0.017648685723543167\n",
      "Epoch 24, Batch 7/32 : Loss = 0.19297397136688232\n",
      "Epoch 24, Batch 8/32 : Loss = 0.023213747888803482\n",
      "Epoch 24, Batch 9/32 : Loss = 0.0076013426296412945\n",
      "Epoch 24, Batch 10/32 : Loss = 0.02690153941512108\n",
      "Epoch 24, Batch 11/32 : Loss = 0.01322674285620451\n",
      "Epoch 24, Batch 12/32 : Loss = 0.12181627750396729\n",
      "Epoch 24, Batch 13/32 : Loss = 0.015312641859054565\n",
      "Epoch 24, Batch 14/32 : Loss = 0.03640030324459076\n",
      "Epoch 24, Batch 15/32 : Loss = 0.012659132480621338\n",
      "Epoch 24, Batch 16/32 : Loss = 0.009952140972018242\n",
      "Epoch 24, Batch 17/32 : Loss = 0.01082908920943737\n",
      "Epoch 24, Batch 18/32 : Loss = 0.028605487197637558\n",
      "Epoch 24, Batch 19/32 : Loss = 0.01659301668405533\n",
      "Epoch 24, Batch 20/32 : Loss = 0.025173239409923553\n",
      "Epoch 24, Batch 21/32 : Loss = 0.03161608427762985\n",
      "Epoch 24, Batch 22/32 : Loss = 0.010696899145841599\n",
      "Epoch 24, Batch 23/32 : Loss = 0.024534132331609726\n",
      "Epoch 24, Batch 24/32 : Loss = 0.008453762158751488\n",
      "Epoch 24, Batch 25/32 : Loss = 0.07733742147684097\n",
      "Epoch 24, Batch 26/32 : Loss = 0.030521180480718613\n",
      "Epoch 24, Batch 27/32 : Loss = 0.011881103739142418\n",
      "Epoch 24, Batch 28/32 : Loss = 0.014526411890983582\n",
      "Epoch 24, Batch 29/32 : Loss = 0.027523767203092575\n",
      "Epoch 24, Batch 30/32 : Loss = 0.009630870074033737\n",
      "Epoch 24, Batch 31/32 : Loss = 0.09066135436296463\n",
      "Epoch 24 finished in 0.0554313858350118 minutes\n",
      "Epoch 24 training_loss = 0.03376347944140434\n",
      "-----c----RR----;--99----y----?----2----d-----i--O------{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---#-----G----99---E----I-=----h---5----#----2----J---)--k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---BB---.---YY---I--W-------6----F----h-----X---'--Y----2----- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----4----r---{-----%%---/--'`-)---w-----&----NN-----+----P---- => 4r{%/'`)w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 24 val_loss = 0.5440417528152466, word_accuracy = 0.64\n",
      "Epoch 25, Batch 0/32 : Loss = 0.014391240663826466\n",
      "Epoch 25, Batch 1/32 : Loss = 0.015144634060561657\n",
      "Epoch 25, Batch 2/32 : Loss = 0.012707686051726341\n",
      "Epoch 25, Batch 3/32 : Loss = 0.046258196234703064\n",
      "Epoch 25, Batch 4/32 : Loss = 0.09618819504976273\n",
      "Epoch 25, Batch 5/32 : Loss = 0.015438230708241463\n",
      "Epoch 25, Batch 6/32 : Loss = 0.013363663107156754\n",
      "Epoch 25, Batch 7/32 : Loss = 0.0291571207344532\n",
      "Epoch 25, Batch 8/32 : Loss = 0.011351879686117172\n",
      "Epoch 25, Batch 9/32 : Loss = 0.009716820903122425\n",
      "Epoch 25, Batch 10/32 : Loss = 0.0091147655621171\n",
      "Epoch 25, Batch 11/32 : Loss = 0.031344376504421234\n",
      "Epoch 25, Batch 12/32 : Loss = 0.021712591871619225\n",
      "Epoch 25, Batch 13/32 : Loss = 0.01586252637207508\n",
      "Epoch 25, Batch 14/32 : Loss = 0.007910669781267643\n",
      "Epoch 25, Batch 15/32 : Loss = 0.01811593770980835\n",
      "Epoch 25, Batch 16/32 : Loss = 0.0490858219563961\n",
      "Epoch 25, Batch 17/32 : Loss = 0.014647092670202255\n",
      "Epoch 25, Batch 18/32 : Loss = 0.027130551636219025\n",
      "Epoch 25, Batch 19/32 : Loss = 0.009011199697852135\n",
      "Epoch 25, Batch 20/32 : Loss = 0.02571844309568405\n",
      "Epoch 25, Batch 21/32 : Loss = 0.013522814027965069\n",
      "Epoch 25, Batch 22/32 : Loss = 0.019215917214751244\n",
      "Epoch 25, Batch 23/32 : Loss = 0.08818599581718445\n",
      "Epoch 25, Batch 24/32 : Loss = 0.01279384270310402\n",
      "Epoch 25, Batch 25/32 : Loss = 0.010534597560763359\n",
      "Epoch 25, Batch 26/32 : Loss = 0.06640610843896866\n",
      "Epoch 25, Batch 27/32 : Loss = 0.07446969300508499\n",
      "Epoch 25, Batch 28/32 : Loss = 0.0844418928027153\n",
      "Epoch 25, Batch 29/32 : Loss = 0.01242376770824194\n",
      "Epoch 25, Batch 30/32 : Loss = 0.03157512843608856\n",
      "Epoch 25, Batch 31/32 : Loss = 0.01156122051179409\n",
      "Epoch 25 finished in 0.054506142934163414 minutes\n",
      "Epoch 25 training_loss = 0.029185112565755844\n",
      "--.---c----O-----w-----u---``--u---.--RR----{--3----\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----QQ-----3-----g----I--z---#-----YY---:--]---q-----+----*-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "-----d---:---X----9----ee----a----FF--,--8--------V----R------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----5----9-----g----m-------J---uu----x---c---x---.--d----\\--- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 25 val_loss = 0.5265985131263733, word_accuracy = 0.7\n",
      "Epoch 26, Batch 0/32 : Loss = 0.02696714736521244\n",
      "Epoch 26, Batch 1/32 : Loss = 0.009217673912644386\n",
      "Epoch 26, Batch 2/32 : Loss = 0.03277987986803055\n",
      "Epoch 26, Batch 3/32 : Loss = 0.007293505594134331\n",
      "Epoch 26, Batch 4/32 : Loss = 0.018069680780172348\n",
      "Epoch 26, Batch 5/32 : Loss = 0.019112478941679\n",
      "Epoch 26, Batch 6/32 : Loss = 0.018015455454587936\n",
      "Epoch 26, Batch 7/32 : Loss = 0.06093703210353851\n",
      "Epoch 26, Batch 8/32 : Loss = 0.04244881868362427\n",
      "Epoch 26, Batch 9/32 : Loss = 0.04288201406598091\n",
      "Epoch 26, Batch 10/32 : Loss = 0.023359455168247223\n",
      "Epoch 26, Batch 11/32 : Loss = 0.039978668093681335\n",
      "Epoch 26, Batch 12/32 : Loss = 0.03908628970384598\n",
      "Epoch 26, Batch 13/32 : Loss = 0.006738264113664627\n",
      "Epoch 26, Batch 14/32 : Loss = 0.005779988598078489\n",
      "Epoch 26, Batch 15/32 : Loss = 0.013342862017452717\n",
      "Epoch 26, Batch 16/32 : Loss = 0.12882274389266968\n",
      "Epoch 26, Batch 17/32 : Loss = 0.02181156910955906\n",
      "Epoch 26, Batch 18/32 : Loss = 0.008698156103491783\n",
      "Epoch 26, Batch 19/32 : Loss = 0.00844498910009861\n",
      "Epoch 26, Batch 20/32 : Loss = 0.007798358798027039\n",
      "Epoch 26, Batch 21/32 : Loss = 0.009391102939844131\n",
      "Epoch 26, Batch 22/32 : Loss = 0.009496260434389114\n",
      "Epoch 26, Batch 23/32 : Loss = 0.11501961201429367\n",
      "Epoch 26, Batch 24/32 : Loss = 0.013275750912725925\n",
      "Epoch 26, Batch 25/32 : Loss = 0.01932513527572155\n",
      "Epoch 26, Batch 26/32 : Loss = 0.013773001730442047\n",
      "Epoch 26, Batch 27/32 : Loss = 0.06369927525520325\n",
      "Epoch 26, Batch 28/32 : Loss = 0.0809590220451355\n",
      "Epoch 26, Batch 29/32 : Loss = 0.0499703511595726\n",
      "Epoch 26, Batch 30/32 : Loss = 0.013289953581988811\n",
      "Epoch 26, Batch 31/32 : Loss = 0.018878962844610214\n",
      "Epoch 26 finished in 0.05456969738006592 minutes\n",
      "Epoch 26 training_loss = 0.03123355284333229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Q------33-----gg-----I--z-----#------Y----::--]]---q-----++-----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "------O------JJ----!---((--;---AA------3-----,--''--)----r----r----77----- => OJ!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "-----C-----DD----E-----g----m------\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "---------3------\\\\-----$------>-------SS------\\\\----M---------i---BB------ => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 26 val_loss = 0.5209324359893799, word_accuracy = 0.66\n",
      "Epoch 27, Batch 0/32 : Loss = 0.008743764832615852\n",
      "Epoch 27, Batch 1/32 : Loss = 0.0088022630661726\n",
      "Epoch 27, Batch 2/32 : Loss = 0.010068073868751526\n",
      "Epoch 27, Batch 3/32 : Loss = 0.010056122206151485\n",
      "Epoch 27, Batch 4/32 : Loss = 0.020919781178236008\n",
      "Epoch 27, Batch 5/32 : Loss = 0.010610717348754406\n",
      "Epoch 27, Batch 6/32 : Loss = 0.01768273487687111\n",
      "Epoch 27, Batch 7/32 : Loss = 0.01447210367769003\n",
      "Epoch 27, Batch 8/32 : Loss = 0.01186547800898552\n",
      "Epoch 27, Batch 9/32 : Loss = 0.03269340097904205\n",
      "Epoch 27, Batch 10/32 : Loss = 0.0645797997713089\n",
      "Epoch 27, Batch 11/32 : Loss = 0.006080199498683214\n",
      "Epoch 27, Batch 12/32 : Loss = 0.00966192688792944\n",
      "Epoch 27, Batch 13/32 : Loss = 0.17429772019386292\n",
      "Epoch 27, Batch 14/32 : Loss = 0.014641276560723782\n",
      "Epoch 27, Batch 15/32 : Loss = 0.0292435884475708\n",
      "Epoch 27, Batch 16/32 : Loss = 0.054899029433727264\n",
      "Epoch 27, Batch 17/32 : Loss = 0.009778113104403019\n",
      "Epoch 27, Batch 18/32 : Loss = 0.0070990100502967834\n",
      "Epoch 27, Batch 19/32 : Loss = 0.07995541393756866\n",
      "Epoch 27, Batch 20/32 : Loss = 0.11737658828496933\n",
      "Epoch 27, Batch 21/32 : Loss = 0.02354053035378456\n",
      "Epoch 27, Batch 22/32 : Loss = 0.009626873768866062\n",
      "Epoch 27, Batch 23/32 : Loss = 0.007301122881472111\n",
      "Epoch 27, Batch 24/32 : Loss = 0.006926330737769604\n",
      "Epoch 27, Batch 25/32 : Loss = 0.005027125123888254\n",
      "Epoch 27, Batch 26/32 : Loss = 0.012067289091646671\n",
      "Epoch 27, Batch 27/32 : Loss = 0.023259760811924934\n",
      "Epoch 27, Batch 28/32 : Loss = 0.005255668889731169\n",
      "Epoch 27, Batch 29/32 : Loss = 0.011291654780507088\n",
      "Epoch 27, Batch 30/32 : Loss = 0.024836763739585876\n",
      "Epoch 27, Batch 31/32 : Loss = 0.32721999287605286\n",
      "Epoch 27 finished in 0.055771330992380776 minutes\n",
      "Epoch 27 training_loss = 0.028387561440467834\n",
      "--.---c----O----ww----uu---`--u---.--R----{--3----\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----0----c-----+-----b----I--\"----b----66----.---Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----#----G----9----E---l-=----h---5---#---2----J--))--k----- => #G9El=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---o----\"--}}--!-BB----r---&-----9---;-`---O-----w-----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 27 val_loss = 0.5269187688827515, word_accuracy = 0.65\n",
      "Epoch 28, Batch 0/32 : Loss = 0.0068198684602975845\n",
      "Epoch 28, Batch 1/32 : Loss = 0.006064999848604202\n",
      "Epoch 28, Batch 2/32 : Loss = 0.06625168025493622\n",
      "Epoch 28, Batch 3/32 : Loss = 0.01027701422572136\n",
      "Epoch 28, Batch 4/32 : Loss = 0.008797023445367813\n",
      "Epoch 28, Batch 5/32 : Loss = 0.011261491104960442\n",
      "Epoch 28, Batch 6/32 : Loss = 0.03850213438272476\n",
      "Epoch 28, Batch 7/32 : Loss = 0.005551966838538647\n",
      "Epoch 28, Batch 8/32 : Loss = 0.0051720100454986095\n",
      "Epoch 28, Batch 9/32 : Loss = 0.013770969584584236\n",
      "Epoch 28, Batch 10/32 : Loss = 0.0870964527130127\n",
      "Epoch 28, Batch 11/32 : Loss = 0.05408034846186638\n",
      "Epoch 28, Batch 12/32 : Loss = 0.0510740801692009\n",
      "Epoch 28, Batch 13/32 : Loss = 0.008584843017160892\n",
      "Epoch 28, Batch 14/32 : Loss = 0.022340577095746994\n",
      "Epoch 28, Batch 15/32 : Loss = 0.008049609139561653\n",
      "Epoch 28, Batch 16/32 : Loss = 0.0072518084198236465\n",
      "Epoch 28, Batch 17/32 : Loss = 0.043154843151569366\n",
      "Epoch 28, Batch 18/32 : Loss = 0.022603347897529602\n",
      "Epoch 28, Batch 19/32 : Loss = 0.013767760246992111\n",
      "Epoch 28, Batch 20/32 : Loss = 0.044564712792634964\n",
      "Epoch 28, Batch 21/32 : Loss = 0.012703108601272106\n",
      "Epoch 28, Batch 22/32 : Loss = 0.007009332999587059\n",
      "Epoch 28, Batch 23/32 : Loss = 0.01224670559167862\n",
      "Epoch 28, Batch 24/32 : Loss = 0.020920418202877045\n",
      "Epoch 28, Batch 25/32 : Loss = 0.02041498012840748\n",
      "Epoch 28, Batch 26/32 : Loss = 0.08644527196884155\n",
      "Epoch 28, Batch 27/32 : Loss = 0.03503696620464325\n",
      "Epoch 28, Batch 28/32 : Loss = 0.012821064330637455\n",
      "Epoch 28, Batch 29/32 : Loss = 0.015372887253761292\n",
      "Epoch 28, Batch 30/32 : Loss = 0.02208833396434784\n",
      "Epoch 28, Batch 31/32 : Loss = 0.006004656665027142\n",
      "Epoch 28 finished in 0.05305243730545044 minutes\n",
      "Epoch 28 training_loss = 0.02508746087551117\n",
      "------0-----JJ----!!---(--;---AA------3----,,--'--))---r----r----77----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---;-------3------\\----$$----->-------S-----\\\\---MMM-------i--BB-------- => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----W------=-----2---++----E----11---n-----T----X---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "----+----:---z----77----8-----d------S-----v---5-----SS----JJ----BB----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 28 val_loss = 0.520291805267334, word_accuracy = 0.71\n",
      "Epoch 29, Batch 0/32 : Loss = 0.04019090533256531\n",
      "Epoch 29, Batch 1/32 : Loss = 0.008770489133894444\n",
      "Epoch 29, Batch 2/32 : Loss = 0.0058243791572749615\n",
      "Epoch 29, Batch 3/32 : Loss = 0.007635084446519613\n",
      "Epoch 29, Batch 4/32 : Loss = 0.004876194521784782\n",
      "Epoch 29, Batch 5/32 : Loss = 0.009505392983555794\n",
      "Epoch 29, Batch 6/32 : Loss = 0.007404054515063763\n",
      "Epoch 29, Batch 7/32 : Loss = 0.009812566451728344\n",
      "Epoch 29, Batch 8/32 : Loss = 0.005652913823723793\n",
      "Epoch 29, Batch 9/32 : Loss = 0.12126392871141434\n",
      "Epoch 29, Batch 10/32 : Loss = 0.03550069406628609\n",
      "Epoch 29, Batch 11/32 : Loss = 0.011040308512747288\n",
      "Epoch 29, Batch 12/32 : Loss = 0.011913664638996124\n",
      "Epoch 29, Batch 13/32 : Loss = 0.005301486700773239\n",
      "Epoch 29, Batch 14/32 : Loss = 0.038984570652246475\n",
      "Epoch 29, Batch 15/32 : Loss = 0.029793087393045425\n",
      "Epoch 29, Batch 16/32 : Loss = 0.006831372156739235\n",
      "Epoch 29, Batch 17/32 : Loss = 0.006692631635814905\n",
      "Epoch 29, Batch 18/32 : Loss = 0.0071314494125545025\n",
      "Epoch 29, Batch 19/32 : Loss = 0.017902743071317673\n",
      "Epoch 29, Batch 20/32 : Loss = 0.03820499777793884\n",
      "Epoch 29, Batch 21/32 : Loss = 0.016722839325666428\n",
      "Epoch 29, Batch 22/32 : Loss = 0.04249081388115883\n",
      "Epoch 29, Batch 23/32 : Loss = 0.006080189719796181\n",
      "Epoch 29, Batch 24/32 : Loss = 0.009511740878224373\n",
      "Epoch 29, Batch 25/32 : Loss = 0.11603095382452011\n",
      "Epoch 29, Batch 26/32 : Loss = 0.0063710506074130535\n",
      "Epoch 29, Batch 27/32 : Loss = 0.0068737235851585865\n",
      "Epoch 29, Batch 28/32 : Loss = 0.009396994486451149\n",
      "Epoch 29, Batch 29/32 : Loss = 0.17091837525367737\n",
      "Epoch 29, Batch 30/32 : Loss = 0.02029098942875862\n",
      "Epoch 29, Batch 31/32 : Loss = 0.021460629999637604\n",
      "Epoch 29 finished in 0.052121448516845706 minutes\n",
      "Epoch 29 training_loss = 0.026910938322544098\n",
      "----88----K----l--Z-----5----p----$$---a----}---w-----,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----o---\"---}--!--B----r---&----9---;--`--O-----w----}}-- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---<<----v---OO-----T---`--4-----V---[--0--------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "--BB---.---Y---I--W------6----F----h----X---'--Y----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 29 val_loss = 0.5354776978492737, word_accuracy = 0.67\n",
      "Epoch 30, Batch 0/32 : Loss = 0.018781602382659912\n",
      "Epoch 30, Batch 1/32 : Loss = 0.004915494006127119\n",
      "Epoch 30, Batch 2/32 : Loss = 0.006147878244519234\n",
      "Epoch 30, Batch 3/32 : Loss = 0.008400527760386467\n",
      "Epoch 30, Batch 4/32 : Loss = 0.0074976710602641106\n",
      "Epoch 30, Batch 5/32 : Loss = 0.006186363287270069\n",
      "Epoch 30, Batch 6/32 : Loss = 0.07195242494344711\n",
      "Epoch 30, Batch 7/32 : Loss = 0.010917900130152702\n",
      "Epoch 30, Batch 8/32 : Loss = 0.009624738246202469\n",
      "Epoch 30, Batch 9/32 : Loss = 0.009711874648928642\n",
      "Epoch 30, Batch 10/32 : Loss = 0.04743925482034683\n",
      "Epoch 30, Batch 11/32 : Loss = 0.020285604521632195\n",
      "Epoch 30, Batch 12/32 : Loss = 0.005174349062144756\n",
      "Epoch 30, Batch 13/32 : Loss = 0.008888373151421547\n",
      "Epoch 30, Batch 14/32 : Loss = 0.006580289453268051\n",
      "Epoch 30, Batch 15/32 : Loss = 0.011128418147563934\n",
      "Epoch 30, Batch 16/32 : Loss = 0.11166075617074966\n",
      "Epoch 30, Batch 17/32 : Loss = 0.00899623055011034\n",
      "Epoch 30, Batch 18/32 : Loss = 0.04204553738236427\n",
      "Epoch 30, Batch 19/32 : Loss = 0.015647731721401215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 20/32 : Loss = 0.006961419712752104\n",
      "Epoch 30, Batch 21/32 : Loss = 0.05176515132188797\n",
      "Epoch 30, Batch 22/32 : Loss = 0.06671179831027985\n",
      "Epoch 30, Batch 23/32 : Loss = 0.006092614959925413\n",
      "Epoch 30, Batch 24/32 : Loss = 0.1462799608707428\n",
      "Epoch 30, Batch 25/32 : Loss = 0.0071792034432291985\n",
      "Epoch 30, Batch 26/32 : Loss = 0.003920184448361397\n",
      "Epoch 30, Batch 27/32 : Loss = 0.015991680324077606\n",
      "Epoch 30, Batch 28/32 : Loss = 0.08743879944086075\n",
      "Epoch 30, Batch 29/32 : Loss = 0.010376794263720512\n",
      "Epoch 30, Batch 30/32 : Loss = 0.013161353766918182\n",
      "Epoch 30, Batch 31/32 : Loss = 0.590610682964325\n",
      "Epoch 30 finished in 0.05302894115447998 minutes\n",
      "Epoch 30 training_loss = 0.029612474143505096\n",
      "----55---->>----:--*----Y----A----'--OO-----DD----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---Z------0----\"\"-----GG------//---~~------$$-----c------11---jj--tt----- => Z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----Z------0-----\"-----GG------/----~~------$------c-----11---jj--tt----- => Z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---------3------\\-----$$------>-------S------\\\\----M---------i---BB------ => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 30 val_loss = 0.5442751049995422, word_accuracy = 0.64\n",
      "Epoch 31, Batch 0/32 : Loss = 0.03529823198914528\n",
      "Epoch 31, Batch 1/32 : Loss = 0.009755151346325874\n",
      "Epoch 31, Batch 2/32 : Loss = 0.02326565980911255\n",
      "Epoch 31, Batch 3/32 : Loss = 0.02431505359709263\n",
      "Epoch 31, Batch 4/32 : Loss = 0.009871693328022957\n",
      "Epoch 31, Batch 5/32 : Loss = 0.007511579431593418\n",
      "Epoch 31, Batch 6/32 : Loss = 0.011954746209084988\n",
      "Epoch 31, Batch 7/32 : Loss = 0.032543156296014786\n",
      "Epoch 31, Batch 8/32 : Loss = 0.06448691338300705\n",
      "Epoch 31, Batch 9/32 : Loss = 0.014677946455776691\n",
      "Epoch 31, Batch 10/32 : Loss = 0.15967103838920593\n",
      "Epoch 31, Batch 11/32 : Loss = 0.006936773657798767\n",
      "Epoch 31, Batch 12/32 : Loss = 0.00690749054774642\n",
      "Epoch 31, Batch 13/32 : Loss = 0.011016673408448696\n",
      "Epoch 31, Batch 14/32 : Loss = 0.03528691828250885\n",
      "Epoch 31, Batch 15/32 : Loss = 0.008139379322528839\n",
      "Epoch 31, Batch 16/32 : Loss = 0.019754700362682343\n",
      "Epoch 31, Batch 17/32 : Loss = 0.00657083373516798\n",
      "Epoch 31, Batch 18/32 : Loss = 0.11215215921401978\n",
      "Epoch 31, Batch 19/32 : Loss = 0.006117336452007294\n",
      "Epoch 31, Batch 20/32 : Loss = 0.008400910533964634\n",
      "Epoch 31, Batch 21/32 : Loss = 0.05455181747674942\n",
      "Epoch 31, Batch 22/32 : Loss = 0.02236012928187847\n",
      "Epoch 31, Batch 23/32 : Loss = 0.014536447823047638\n",
      "Epoch 31, Batch 24/32 : Loss = 0.007676337845623493\n",
      "Epoch 31, Batch 25/32 : Loss = 0.00837644748389721\n",
      "Epoch 31, Batch 26/32 : Loss = 0.006040745414793491\n",
      "Epoch 31, Batch 27/32 : Loss = 0.011550135910511017\n",
      "Epoch 31, Batch 28/32 : Loss = 0.10452675819396973\n",
      "Epoch 31, Batch 29/32 : Loss = 0.007589763030409813\n",
      "Epoch 31, Batch 30/32 : Loss = 0.01778840459883213\n",
      "Epoch 31, Batch 31/32 : Loss = 0.007556798402220011\n",
      "Epoch 31 finished in 0.05349875688552856 minutes\n",
      "Epoch 31 training_loss = 0.027970312163233757\n",
      "----X---77--00--j---@----SS---Z----L--4----C----m------MM---- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----d----!--NN-----r---A----j--**---$----3----h----55---nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----8-----K----l--ZZ-----5----pp----$$---aa----}---w-----,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---#-----G----9----E---I--=---h----5---#----2----J---)--k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 31 val_loss = 0.5085326433181763, word_accuracy = 0.65\n",
      "Epoch 32, Batch 0/32 : Loss = 0.006390615366399288\n",
      "Epoch 32, Batch 1/32 : Loss = 0.008724603801965714\n",
      "Epoch 32, Batch 2/32 : Loss = 0.05923732742667198\n",
      "Epoch 32, Batch 3/32 : Loss = 0.009857246652245522\n",
      "Epoch 32, Batch 4/32 : Loss = 0.03646541386842728\n",
      "Epoch 32, Batch 5/32 : Loss = 0.010001426562666893\n",
      "Epoch 32, Batch 6/32 : Loss = 0.014755700714886189\n",
      "Epoch 32, Batch 7/32 : Loss = 0.054508887231349945\n",
      "Epoch 32, Batch 8/32 : Loss = 0.05222659185528755\n",
      "Epoch 32, Batch 9/32 : Loss = 0.006738012656569481\n",
      "Epoch 32, Batch 10/32 : Loss = 0.008505837060511112\n",
      "Epoch 32, Batch 11/32 : Loss = 0.006018650718033314\n",
      "Epoch 32, Batch 12/32 : Loss = 0.019312027841806412\n",
      "Epoch 32, Batch 13/32 : Loss = 0.008220445364713669\n",
      "Epoch 32, Batch 14/32 : Loss = 0.033101968467235565\n",
      "Epoch 32, Batch 15/32 : Loss = 0.006466175429522991\n",
      "Epoch 32, Batch 16/32 : Loss = 0.007341649848967791\n",
      "Epoch 32, Batch 17/32 : Loss = 0.004260275978595018\n",
      "Epoch 32, Batch 18/32 : Loss = 0.03167460113763809\n",
      "Epoch 32, Batch 19/32 : Loss = 0.005952493287622929\n",
      "Epoch 32, Batch 20/32 : Loss = 0.0197958592325449\n",
      "Epoch 32, Batch 21/32 : Loss = 0.007491216063499451\n",
      "Epoch 32, Batch 22/32 : Loss = 0.007653950713574886\n",
      "Epoch 32, Batch 23/32 : Loss = 0.08384387195110321\n",
      "Epoch 32, Batch 24/32 : Loss = 0.004255177453160286\n",
      "Epoch 32, Batch 25/32 : Loss = 0.008114462718367577\n",
      "Epoch 32, Batch 26/32 : Loss = 0.012418778613209724\n",
      "Epoch 32, Batch 27/32 : Loss = 0.024901416152715683\n",
      "Epoch 32, Batch 28/32 : Loss = 0.008816763758659363\n",
      "Epoch 32, Batch 29/32 : Loss = 0.05582248419523239\n",
      "Epoch 32, Batch 30/32 : Loss = 0.003573091235011816\n",
      "Epoch 32, Batch 31/32 : Loss = 0.0133295226842165\n",
      "Epoch 32 finished in 0.052779177824656166 minutes\n",
      "Epoch 32 training_loss = 0.020180344581604004\n",
      "----5----9---g----m------J---uu---x---c---x--.--d---\\--- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "---d---:---X----9---e----a----F---,-8--------V---RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----0----J---!---(-----A-----3---,--'-))--r---r---7---- => 0J!(A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---{--B----Y----R---aa---y---h--##---22--->----E---4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 32 val_loss = 0.5408216714859009, word_accuracy = 0.65\n",
      "Epoch 33, Batch 0/32 : Loss = 0.008217043243348598\n",
      "Epoch 33, Batch 1/32 : Loss = 0.004439547657966614\n",
      "Epoch 33, Batch 2/32 : Loss = 0.031927336007356644\n",
      "Epoch 33, Batch 3/32 : Loss = 0.013796200044453144\n",
      "Epoch 33, Batch 4/32 : Loss = 0.004863945301622152\n",
      "Epoch 33, Batch 5/32 : Loss = 0.11467466503381729\n",
      "Epoch 33, Batch 6/32 : Loss = 0.008904315531253815\n",
      "Epoch 33, Batch 7/32 : Loss = 0.004197685047984123\n",
      "Epoch 33, Batch 8/32 : Loss = 0.02189243584871292\n",
      "Epoch 33, Batch 9/32 : Loss = 0.014474166557192802\n",
      "Epoch 33, Batch 10/32 : Loss = 0.007807057350873947\n",
      "Epoch 33, Batch 11/32 : Loss = 0.012582652270793915\n",
      "Epoch 33, Batch 12/32 : Loss = 0.00991363450884819\n",
      "Epoch 33, Batch 13/32 : Loss = 0.010732471942901611\n",
      "Epoch 33, Batch 14/32 : Loss = 0.007516510784626007\n",
      "Epoch 33, Batch 15/32 : Loss = 0.005857541225850582\n",
      "Epoch 33, Batch 16/32 : Loss = 0.005996644496917725\n",
      "Epoch 33, Batch 17/32 : Loss = 0.01611197181046009\n",
      "Epoch 33, Batch 18/32 : Loss = 0.009021458216011524\n",
      "Epoch 33, Batch 19/32 : Loss = 0.01197549793869257\n",
      "Epoch 33, Batch 20/32 : Loss = 0.00966782495379448\n",
      "Epoch 33, Batch 21/32 : Loss = 0.21107469499111176\n",
      "Epoch 33, Batch 22/32 : Loss = 0.02753676474094391\n",
      "Epoch 33, Batch 23/32 : Loss = 0.005910656880587339\n",
      "Epoch 33, Batch 24/32 : Loss = 0.010466396808624268\n",
      "Epoch 33, Batch 25/32 : Loss = 0.13998091220855713\n",
      "Epoch 33, Batch 26/32 : Loss = 0.017252953723073006\n",
      "Epoch 33, Batch 27/32 : Loss = 0.0044784750789403915\n",
      "Epoch 33, Batch 28/32 : Loss = 0.008089562878012657\n",
      "Epoch 33, Batch 29/32 : Loss = 0.0136734489351511\n",
      "Epoch 33, Batch 30/32 : Loss = 0.0066253384575247765\n",
      "Epoch 33, Batch 31/32 : Loss = 0.0261093582957983\n",
      "Epoch 33 finished in 0.05360150337219238 minutes\n",
      "Epoch 33 training_loss = 0.02515416219830513\n",
      "--77---n----DD----P----n---tt--w-----d---\\----Q----a----RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "---------MM-----o-----E----^---33----x---/----&----66----X------ => MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---z----0-----\"----GG-----//---~------$-----C-----1---jj--t----- => z0\"G/~$C1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----X---7---0---j---@----SS---Z----L--4----C----mm-----MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 33 val_loss = 0.5598175525665283, word_accuracy = 0.65\n",
      "Epoch 34, Batch 0/32 : Loss = 0.005844327621161938\n",
      "Epoch 34, Batch 1/32 : Loss = 0.07834330946207047\n",
      "Epoch 34, Batch 2/32 : Loss = 0.019785717129707336\n",
      "Epoch 34, Batch 3/32 : Loss = 0.004400790203362703\n",
      "Epoch 34, Batch 4/32 : Loss = 0.014454549178481102\n",
      "Epoch 34, Batch 5/32 : Loss = 0.011387797072529793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 6/32 : Loss = 0.017868628725409508\n",
      "Epoch 34, Batch 7/32 : Loss = 0.004513351246714592\n",
      "Epoch 34, Batch 8/32 : Loss = 0.008496474474668503\n",
      "Epoch 34, Batch 9/32 : Loss = 0.11245279759168625\n",
      "Epoch 34, Batch 10/32 : Loss = 0.012731514871120453\n",
      "Epoch 34, Batch 11/32 : Loss = 0.008831389248371124\n",
      "Epoch 34, Batch 12/32 : Loss = 0.03145235776901245\n",
      "Epoch 34, Batch 13/32 : Loss = 0.01587716117501259\n",
      "Epoch 34, Batch 14/32 : Loss = 0.030185813084244728\n",
      "Epoch 34, Batch 15/32 : Loss = 0.00619178730994463\n",
      "Epoch 34, Batch 16/32 : Loss = 0.00822723563760519\n",
      "Epoch 34, Batch 17/32 : Loss = 0.006184381432831287\n",
      "Epoch 34, Batch 18/32 : Loss = 0.02951342612504959\n",
      "Epoch 34, Batch 19/32 : Loss = 0.007388772442936897\n",
      "Epoch 34, Batch 20/32 : Loss = 0.02654423750936985\n",
      "Epoch 34, Batch 21/32 : Loss = 0.01086423173546791\n",
      "Epoch 34, Batch 22/32 : Loss = 0.00549921952188015\n",
      "Epoch 34, Batch 23/32 : Loss = 0.015534458681941032\n",
      "Epoch 34, Batch 24/32 : Loss = 0.07966077327728271\n",
      "Epoch 34, Batch 25/32 : Loss = 0.013393130153417587\n",
      "Epoch 34, Batch 26/32 : Loss = 0.0062861694023013115\n",
      "Epoch 34, Batch 27/32 : Loss = 0.012862732633948326\n",
      "Epoch 34, Batch 28/32 : Loss = 0.04437774047255516\n",
      "Epoch 34, Batch 29/32 : Loss = 0.003895476460456848\n",
      "Epoch 34, Batch 30/32 : Loss = 0.019649803638458252\n",
      "Epoch 34, Batch 31/32 : Loss = 0.03874783217906952\n",
      "Epoch 34 finished in 0.051336145401000975 minutes\n",
      "Epoch 34 training_loss = 0.021768448874354362\n",
      "---/----MM------o-----E----^^---3-----x---/----&-----6-----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----00----Q---66---<----<----(--TT---N----5---=----P---(--mm------ => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----YY----W-------]--i--\\---|-----MM-----<<-----MM-----8-----8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----0----JJ----!!--(--:----A-----33---,---'--)----r---r----7----- => 0J!(:A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 34 val_loss = 0.5417451858520508, word_accuracy = 0.65\n",
      "Epoch 35, Batch 0/32 : Loss = 0.020256077870726585\n",
      "Epoch 35, Batch 1/32 : Loss = 0.03874952346086502\n",
      "Epoch 35, Batch 2/32 : Loss = 0.010329912416636944\n",
      "Epoch 35, Batch 3/32 : Loss = 0.005087141413241625\n",
      "Epoch 35, Batch 4/32 : Loss = 0.00761360302567482\n",
      "Epoch 35, Batch 5/32 : Loss = 0.0061272792518138885\n",
      "Epoch 35, Batch 6/32 : Loss = 0.02232646569609642\n",
      "Epoch 35, Batch 7/32 : Loss = 0.011047390289604664\n",
      "Epoch 35, Batch 8/32 : Loss = 0.00808254349976778\n",
      "Epoch 35, Batch 9/32 : Loss = 0.01420239545404911\n",
      "Epoch 35, Batch 10/32 : Loss = 0.005138781853020191\n",
      "Epoch 35, Batch 11/32 : Loss = 0.03194988891482353\n",
      "Epoch 35, Batch 12/32 : Loss = 0.021060604602098465\n",
      "Epoch 35, Batch 13/32 : Loss = 0.00393036101013422\n",
      "Epoch 35, Batch 14/32 : Loss = 0.05238020792603493\n",
      "Epoch 35, Batch 15/32 : Loss = 0.03986058384180069\n",
      "Epoch 35, Batch 16/32 : Loss = 0.044901687651872635\n",
      "Epoch 35, Batch 17/32 : Loss = 0.004873447120189667\n",
      "Epoch 35, Batch 18/32 : Loss = 0.010461699217557907\n",
      "Epoch 35, Batch 19/32 : Loss = 0.010405603796243668\n",
      "Epoch 35, Batch 20/32 : Loss = 0.004515899810940027\n",
      "Epoch 35, Batch 21/32 : Loss = 0.007893422618508339\n",
      "Epoch 35, Batch 22/32 : Loss = 0.02739638462662697\n",
      "Epoch 35, Batch 23/32 : Loss = 0.01058730948716402\n",
      "Epoch 35, Batch 24/32 : Loss = 0.004839933477342129\n",
      "Epoch 35, Batch 25/32 : Loss = 0.004022710025310516\n",
      "Epoch 35, Batch 26/32 : Loss = 0.004275506362318993\n",
      "Epoch 35, Batch 27/32 : Loss = 0.006363095715641975\n",
      "Epoch 35, Batch 28/32 : Loss = 0.04545307531952858\n",
      "Epoch 35, Batch 29/32 : Loss = 0.004203808028250933\n",
      "Epoch 35, Batch 30/32 : Loss = 0.0063784453086555\n",
      "Epoch 35, Batch 31/32 : Loss = 0.3079972267150879\n",
      "Epoch 35 finished in 0.05250922441482544 minutes\n",
      "Epoch 35 training_loss = 0.017131386324763298\n",
      "----z-----0----\"-----G-----//---~~-----$-----c-----1---j---t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----7----n----D----PP---nn---t--w-----d----\\---Q-----a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "------0----JJ----!--((--:--AA-----3----,--'--)---r----r---77---- => 0J!(:A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "-----+---:--z----7---8----dd----S-----v---5----S----J----B------ => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 35 val_loss = 0.5742146968841553, word_accuracy = 0.66\n",
      "Epoch 36, Batch 0/32 : Loss = 0.018897179514169693\n",
      "Epoch 36, Batch 1/32 : Loss = 0.02037597820162773\n",
      "Epoch 36, Batch 2/32 : Loss = 0.012115506455302238\n",
      "Epoch 36, Batch 3/32 : Loss = 0.004513057880103588\n",
      "Epoch 36, Batch 4/32 : Loss = 0.0046950154937803745\n",
      "Epoch 36, Batch 5/32 : Loss = 0.05303654074668884\n",
      "Epoch 36, Batch 6/32 : Loss = 0.013813622295856476\n",
      "Epoch 36, Batch 7/32 : Loss = 0.009058075025677681\n",
      "Epoch 36, Batch 8/32 : Loss = 0.00813889130949974\n",
      "Epoch 36, Batch 9/32 : Loss = 0.015333587303757668\n",
      "Epoch 36, Batch 10/32 : Loss = 0.0036129956133663654\n",
      "Epoch 36, Batch 11/32 : Loss = 0.020537670701742172\n",
      "Epoch 36, Batch 12/32 : Loss = 0.027182508260011673\n",
      "Epoch 36, Batch 13/32 : Loss = 0.024924758821725845\n",
      "Epoch 36, Batch 14/32 : Loss = 0.02693486213684082\n",
      "Epoch 36, Batch 15/32 : Loss = 0.02741139568388462\n",
      "Epoch 36, Batch 16/32 : Loss = 0.013947851955890656\n",
      "Epoch 36, Batch 17/32 : Loss = 0.008688661269843578\n",
      "Epoch 36, Batch 18/32 : Loss = 0.0046027470380067825\n",
      "Epoch 36, Batch 19/32 : Loss = 0.02622048370540142\n",
      "Epoch 36, Batch 20/32 : Loss = 0.006247083656489849\n",
      "Epoch 36, Batch 21/32 : Loss = 0.024397598579525948\n",
      "Epoch 36, Batch 22/32 : Loss = 0.016782766208052635\n",
      "Epoch 36, Batch 23/32 : Loss = 0.004755488596856594\n",
      "Epoch 36, Batch 24/32 : Loss = 0.011052761226892471\n",
      "Epoch 36, Batch 25/32 : Loss = 0.010979635640978813\n",
      "Epoch 36, Batch 26/32 : Loss = 0.0026933420449495316\n",
      "Epoch 36, Batch 27/32 : Loss = 0.014653528109192848\n",
      "Epoch 36, Batch 28/32 : Loss = 0.053764648735523224\n",
      "Epoch 36, Batch 29/32 : Loss = 0.012990512885153294\n",
      "Epoch 36, Batch 30/32 : Loss = 0.004683151841163635\n",
      "Epoch 36, Batch 31/32 : Loss = 0.0035643514711409807\n",
      "Epoch 36 finished in 0.05269325574239095 minutes\n",
      "Epoch 36 training_loss = 0.01630481518805027\n",
      "----X----7----0---j---@-----S----Z----L---44----C----mm------MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---7----n----DD-----P----nn---tt--w-----dd---\\\\----Q----aa----R------- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----55---->----:--*---YY----A---'---O-----DD----*---#-----O-----g----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---\"----]---t---44-----e----^^----W---------Q------4----->>-----gg---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 36 val_loss = 0.5587448477745056, word_accuracy = 0.68\n",
      "Epoch 37, Batch 0/32 : Loss = 0.005716477055102587\n",
      "Epoch 37, Batch 1/32 : Loss = 0.005983403418213129\n",
      "Epoch 37, Batch 2/32 : Loss = 0.03507927805185318\n",
      "Epoch 37, Batch 3/32 : Loss = 0.017397398129105568\n",
      "Epoch 37, Batch 4/32 : Loss = 0.00866473838686943\n",
      "Epoch 37, Batch 5/32 : Loss = 0.004160101525485516\n",
      "Epoch 37, Batch 6/32 : Loss = 0.005902615841478109\n",
      "Epoch 37, Batch 7/32 : Loss = 0.04279221221804619\n",
      "Epoch 37, Batch 8/32 : Loss = 0.004494273103773594\n",
      "Epoch 37, Batch 9/32 : Loss = 0.024223677814006805\n",
      "Epoch 37, Batch 10/32 : Loss = 0.0026550707407295704\n",
      "Epoch 37, Batch 11/32 : Loss = 0.0049388413317501545\n",
      "Epoch 37, Batch 12/32 : Loss = 0.07101795822381973\n",
      "Epoch 37, Batch 13/32 : Loss = 0.0039534782990813255\n",
      "Epoch 37, Batch 14/32 : Loss = 0.004386816173791885\n",
      "Epoch 37, Batch 15/32 : Loss = 0.0049977852031588554\n",
      "Epoch 37, Batch 16/32 : Loss = 0.01615065708756447\n",
      "Epoch 37, Batch 17/32 : Loss = 0.04335515946149826\n",
      "Epoch 37, Batch 18/32 : Loss = 0.05347980558872223\n",
      "Epoch 37, Batch 19/32 : Loss = 0.006301784422248602\n",
      "Epoch 37, Batch 20/32 : Loss = 0.005555341951549053\n",
      "Epoch 37, Batch 21/32 : Loss = 0.003124926472082734\n",
      "Epoch 37, Batch 22/32 : Loss = 0.038688722997903824\n",
      "Epoch 37, Batch 23/32 : Loss = 0.012692534364759922\n",
      "Epoch 37, Batch 24/32 : Loss = 0.012779061682522297\n",
      "Epoch 37, Batch 25/32 : Loss = 0.005725994240492582\n",
      "Epoch 37, Batch 26/32 : Loss = 0.012089837342500687\n",
      "Epoch 37, Batch 27/32 : Loss = 0.0035847905091941357\n",
      "Epoch 37, Batch 28/32 : Loss = 0.006865926552563906\n",
      "Epoch 37, Batch 29/32 : Loss = 0.0058638956397771835\n",
      "Epoch 37, Batch 30/32 : Loss = 0.004921827465295792\n",
      "Epoch 37, Batch 31/32 : Loss = 0.012490523979067802\n",
      "Epoch 37 finished in 0.05315903027852376 minutes\n",
      "Epoch 37 training_loss = 0.015392955392599106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----d----!--N-----r--A---j--**---$---3---hh----5---nn---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---77---n---DD---P---nn--tt-ww---dd---\\---Q---a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----k---$$--l--F----DD---ee--hh---]--kk--0----\\--XX----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "------Q----33----g---I--z---#----Y----:-]--q----++---**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 37 val_loss = 0.5796266198158264, word_accuracy = 0.71\n",
      "Epoch 38, Batch 0/32 : Loss = 0.009189756587147713\n",
      "Epoch 38, Batch 1/32 : Loss = 0.03478145971894264\n",
      "Epoch 38, Batch 2/32 : Loss = 0.02980325184762478\n",
      "Epoch 38, Batch 3/32 : Loss = 0.004654939752072096\n",
      "Epoch 38, Batch 4/32 : Loss = 0.028305981308221817\n",
      "Epoch 38, Batch 5/32 : Loss = 0.005561436526477337\n",
      "Epoch 38, Batch 6/32 : Loss = 0.004982637707144022\n",
      "Epoch 38, Batch 7/32 : Loss = 0.0035420239437371492\n",
      "Epoch 38, Batch 8/32 : Loss = 0.0032807758543640375\n",
      "Epoch 38, Batch 9/32 : Loss = 0.007125215604901314\n",
      "Epoch 38, Batch 10/32 : Loss = 0.00525642978027463\n",
      "Epoch 38, Batch 11/32 : Loss = 0.06100008264183998\n",
      "Epoch 38, Batch 12/32 : Loss = 0.010528406128287315\n",
      "Epoch 38, Batch 13/32 : Loss = 0.0031126406975090504\n",
      "Epoch 38, Batch 14/32 : Loss = 0.0046966420486569405\n",
      "Epoch 38, Batch 15/32 : Loss = 0.01488346979022026\n",
      "Epoch 38, Batch 16/32 : Loss = 0.0033670684788376093\n",
      "Epoch 38, Batch 17/32 : Loss = 0.01536712609231472\n",
      "Epoch 38, Batch 18/32 : Loss = 0.005589927546679974\n",
      "Epoch 38, Batch 19/32 : Loss = 0.011099981144070625\n",
      "Epoch 38, Batch 20/32 : Loss = 0.011728750541806221\n",
      "Epoch 38, Batch 21/32 : Loss = 0.04515320807695389\n",
      "Epoch 38, Batch 22/32 : Loss = 0.02342347614467144\n",
      "Epoch 38, Batch 23/32 : Loss = 0.012325585819780827\n",
      "Epoch 38, Batch 24/32 : Loss = 0.004626487381756306\n",
      "Epoch 38, Batch 25/32 : Loss = 0.02141774632036686\n",
      "Epoch 38, Batch 26/32 : Loss = 0.01018700934946537\n",
      "Epoch 38, Batch 27/32 : Loss = 0.006828569807112217\n",
      "Epoch 38, Batch 28/32 : Loss = 0.006586585193872452\n",
      "Epoch 38, Batch 29/32 : Loss = 0.0027493336237967014\n",
      "Epoch 38, Batch 30/32 : Loss = 0.005341437645256519\n",
      "Epoch 38, Batch 31/32 : Loss = 0.005402306094765663\n",
      "Epoch 38 finished in 0.052657397588094075 minutes\n",
      "Epoch 38 training_loss = 0.013403138145804405\n",
      "------z----0----\"----GG-----/---~~----$$----cc----11--j---t--- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---kk----O-----/---,--y---c----*----1----P----}--##----BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----+++--:--z----7---88----d----SS----v---5---SS---JJ---BB---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "---BB---.---YY---I--W-------6----FF---h-----X---'--Y----2----- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 38 val_loss = 0.5330468416213989, word_accuracy = 0.75\n",
      "Epoch 39, Batch 0/32 : Loss = 0.017556333914399147\n",
      "Epoch 39, Batch 1/32 : Loss = 0.004187966696918011\n",
      "Epoch 39, Batch 2/32 : Loss = 0.004214360378682613\n",
      "Epoch 39, Batch 3/32 : Loss = 0.005660463124513626\n",
      "Epoch 39, Batch 4/32 : Loss = 0.009849704802036285\n",
      "Epoch 39, Batch 5/32 : Loss = 0.007611565291881561\n",
      "Epoch 39, Batch 6/32 : Loss = 0.006315567530691624\n",
      "Epoch 39, Batch 7/32 : Loss = 0.003719345200806856\n",
      "Epoch 39, Batch 8/32 : Loss = 0.014809481799602509\n",
      "Epoch 39, Batch 9/32 : Loss = 0.006500173360109329\n",
      "Epoch 39, Batch 10/32 : Loss = 0.004992369096726179\n",
      "Epoch 39, Batch 11/32 : Loss = 0.004060431849211454\n",
      "Epoch 39, Batch 12/32 : Loss = 0.004966889042407274\n",
      "Epoch 39, Batch 13/32 : Loss = 0.08209039270877838\n",
      "Epoch 39, Batch 14/32 : Loss = 0.0800056979060173\n",
      "Epoch 39, Batch 15/32 : Loss = 0.0030016815289855003\n",
      "Epoch 39, Batch 16/32 : Loss = 0.010981875471770763\n",
      "Epoch 39, Batch 17/32 : Loss = 0.00830702856183052\n",
      "Epoch 39, Batch 18/32 : Loss = 0.028509963303804398\n",
      "Epoch 39, Batch 19/32 : Loss = 0.009926462545990944\n",
      "Epoch 39, Batch 20/32 : Loss = 0.023668568581342697\n",
      "Epoch 39, Batch 21/32 : Loss = 0.012361720204353333\n",
      "Epoch 39, Batch 22/32 : Loss = 0.009103205986320972\n",
      "Epoch 39, Batch 23/32 : Loss = 0.09314736723899841\n",
      "Epoch 39, Batch 24/32 : Loss = 0.006893182173371315\n",
      "Epoch 39, Batch 25/32 : Loss = 0.05114717781543732\n",
      "Epoch 39, Batch 26/32 : Loss = 0.008941540494561195\n",
      "Epoch 39, Batch 27/32 : Loss = 0.00452608335763216\n",
      "Epoch 39, Batch 28/32 : Loss = 0.005854504648596048\n",
      "Epoch 39, Batch 29/32 : Loss = 0.009280884638428688\n",
      "Epoch 39, Batch 30/32 : Loss = 0.0147191621363163\n",
      "Epoch 39, Batch 31/32 : Loss = 0.015416216105222702\n",
      "Epoch 39 finished in 0.05256251096725464 minutes\n",
      "Epoch 39 training_loss = 0.017954640090465546\n",
      "----<<-----v---O------T---`---4----V----[--0--------Q----- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "--BB---.---Y---I--W-------6----F----h----X---'--Y----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----++---:-z----7---8----d----S----v---5---SS---JJ---B---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      ";------3-----\\----$----->-----SS-----\\---MM------i--BB---- => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 39 val_loss = 0.5742745995521545, word_accuracy = 0.66\n",
      "Epoch 40, Batch 0/32 : Loss = 0.00808456540107727\n",
      "Epoch 40, Batch 1/32 : Loss = 0.022226247936487198\n",
      "Epoch 40, Batch 2/32 : Loss = 0.029074542224407196\n",
      "Epoch 40, Batch 3/32 : Loss = 0.042270831763744354\n",
      "Epoch 40, Batch 4/32 : Loss = 0.006353397853672504\n",
      "Epoch 40, Batch 5/32 : Loss = 0.007820675149559975\n",
      "Epoch 40, Batch 6/32 : Loss = 0.0049349116161465645\n",
      "Epoch 40, Batch 7/32 : Loss = 0.2718789279460907\n",
      "Epoch 40, Batch 8/32 : Loss = 0.004007329698652029\n",
      "Epoch 40, Batch 9/32 : Loss = 0.003178378799930215\n",
      "Epoch 40, Batch 10/32 : Loss = 0.02942131832242012\n",
      "Epoch 40, Batch 11/32 : Loss = 0.00620015524327755\n",
      "Epoch 40, Batch 12/32 : Loss = 0.10512322187423706\n",
      "Epoch 40, Batch 13/32 : Loss = 0.018764609470963478\n",
      "Epoch 40, Batch 14/32 : Loss = 0.03202880918979645\n",
      "Epoch 40, Batch 15/32 : Loss = 0.016454612836241722\n",
      "Epoch 40, Batch 16/32 : Loss = 0.0068023111671209335\n",
      "Epoch 40, Batch 17/32 : Loss = 0.006246157921850681\n",
      "Epoch 40, Batch 18/32 : Loss = 0.0063203806057572365\n",
      "Epoch 40, Batch 19/32 : Loss = 0.005719965323805809\n",
      "Epoch 40, Batch 20/32 : Loss = 0.007237541489303112\n",
      "Epoch 40, Batch 21/32 : Loss = 0.0068635493516922\n",
      "Epoch 40, Batch 22/32 : Loss = 0.024047015234827995\n",
      "Epoch 40, Batch 23/32 : Loss = 0.007277300115674734\n",
      "Epoch 40, Batch 24/32 : Loss = 0.005372567102313042\n",
      "Epoch 40, Batch 25/32 : Loss = 0.004518561996519566\n",
      "Epoch 40, Batch 26/32 : Loss = 0.2896111011505127\n",
      "Epoch 40, Batch 27/32 : Loss = 0.019546132534742355\n",
      "Epoch 40, Batch 28/32 : Loss = 0.009169474244117737\n",
      "Epoch 40, Batch 29/32 : Loss = 0.011885669082403183\n",
      "Epoch 40, Batch 30/32 : Loss = 0.010594887658953667\n",
      "Epoch 40, Batch 31/32 : Loss = 0.031516317278146744\n",
      "Epoch 40 finished in 0.053247034549713135 minutes\n",
      "Epoch 40 training_loss = 0.03318794071674347\n",
      "--77---n----DD----P----n----t--w-----d---\\\\---Q----a----RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----Zz----0----\"-----G-----/----~-----$$----cc----11---j--t---- => Zz0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----5---->---'--*---YY---A---'--O-----DD---**--#----O-----g----- => 5>'*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----EE------0----[---x----)---RR-----8----i---P-----w------)---- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "Epoch 40 val_loss = 0.5637231469154358, word_accuracy = 0.69\n",
      "Epoch 41, Batch 0/32 : Loss = 0.007412823382765055\n",
      "Epoch 41, Batch 1/32 : Loss = 0.0089267548173666\n",
      "Epoch 41, Batch 2/32 : Loss = 0.016984038054943085\n",
      "Epoch 41, Batch 3/32 : Loss = 0.008543201722204685\n",
      "Epoch 41, Batch 4/32 : Loss = 0.006280512548983097\n",
      "Epoch 41, Batch 5/32 : Loss = 0.015822933986783028\n",
      "Epoch 41, Batch 6/32 : Loss = 0.0037347860634326935\n",
      "Epoch 41, Batch 7/32 : Loss = 0.003369214478880167\n",
      "Epoch 41, Batch 8/32 : Loss = 0.007380378432571888\n",
      "Epoch 41, Batch 9/32 : Loss = 0.007453617174178362\n",
      "Epoch 41, Batch 10/32 : Loss = 0.0077187973074615\n",
      "Epoch 41, Batch 11/32 : Loss = 0.024232501164078712\n",
      "Epoch 41, Batch 12/32 : Loss = 0.003658633679151535\n",
      "Epoch 41, Batch 13/32 : Loss = 0.005662711337208748\n",
      "Epoch 41, Batch 14/32 : Loss = 0.004311767406761646\n",
      "Epoch 41, Batch 15/32 : Loss = 0.00425021443516016\n",
      "Epoch 41, Batch 16/32 : Loss = 0.005119678098708391\n",
      "Epoch 41, Batch 17/32 : Loss = 0.0051199328154325485\n",
      "Epoch 41, Batch 18/32 : Loss = 0.012365748174488544\n",
      "Epoch 41, Batch 19/32 : Loss = 0.015649059787392616\n",
      "Epoch 41, Batch 20/32 : Loss = 0.03348724544048309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Batch 21/32 : Loss = 0.03509436920285225\n",
      "Epoch 41, Batch 22/32 : Loss = 0.0059936074540019035\n",
      "Epoch 41, Batch 23/32 : Loss = 0.10520338267087936\n",
      "Epoch 41, Batch 24/32 : Loss = 0.004443338140845299\n",
      "Epoch 41, Batch 25/32 : Loss = 0.04670215770602226\n",
      "Epoch 41, Batch 26/32 : Loss = 0.05895983800292015\n",
      "Epoch 41, Batch 27/32 : Loss = 0.003931185696274042\n",
      "Epoch 41, Batch 28/32 : Loss = 0.04167965054512024\n",
      "Epoch 41, Batch 29/32 : Loss = 0.004371126648038626\n",
      "Epoch 41, Batch 30/32 : Loss = 0.013115635141730309\n",
      "Epoch 41, Batch 31/32 : Loss = 0.03473540395498276\n",
      "Epoch 41 finished in 0.18974773089090982 minutes\n",
      "Epoch 41 training_loss = 0.017070544883608818\n",
      "----0----Q---6----<----<---(---T---N----5---=----P---(-mm----- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "---JJ--;---q----+---/----z---y---U-------%%---U-----11---x---_ => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "-----0-----c----+++----b----I--\"----bb----66----.---Q--------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----7---n----DD----P----n---t--w-----d----\\---Q----a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 41 val_loss = 0.5542125105857849, word_accuracy = 0.7\n",
      "Epoch 42, Batch 0/32 : Loss = 0.012647666037082672\n",
      "Epoch 42, Batch 1/32 : Loss = 0.004472322762012482\n",
      "Epoch 42, Batch 2/32 : Loss = 0.0037238546647131443\n",
      "Epoch 42, Batch 3/32 : Loss = 0.013008465059101582\n",
      "Epoch 42, Batch 4/32 : Loss = 0.005766195245087147\n",
      "Epoch 42, Batch 5/32 : Loss = 0.02174278534948826\n",
      "Epoch 42, Batch 6/32 : Loss = 0.006200723350048065\n",
      "Epoch 42, Batch 7/32 : Loss = 0.019570739939808846\n",
      "Epoch 42, Batch 8/32 : Loss = 0.011741182766854763\n",
      "Epoch 42, Batch 9/32 : Loss = 0.020923033356666565\n",
      "Epoch 42, Batch 10/32 : Loss = 0.09516690671443939\n",
      "Epoch 42, Batch 11/32 : Loss = 0.02816089242696762\n",
      "Epoch 42, Batch 12/32 : Loss = 0.03433715179562569\n",
      "Epoch 42, Batch 13/32 : Loss = 0.05051710084080696\n",
      "Epoch 42, Batch 14/32 : Loss = 0.004544440656900406\n",
      "Epoch 42, Batch 15/32 : Loss = 0.009743474423885345\n",
      "Epoch 42, Batch 16/32 : Loss = 0.00936930626630783\n",
      "Epoch 42, Batch 17/32 : Loss = 0.005655017215758562\n",
      "Epoch 42, Batch 18/32 : Loss = 0.005077673122286797\n",
      "Epoch 42, Batch 19/32 : Loss = 0.028491869568824768\n",
      "Epoch 42, Batch 20/32 : Loss = 0.0055335876531898975\n",
      "Epoch 42, Batch 21/32 : Loss = 0.00817251019179821\n",
      "Epoch 42, Batch 22/32 : Loss = 0.03903799131512642\n",
      "Epoch 42, Batch 23/32 : Loss = 0.026944546028971672\n",
      "Epoch 42, Batch 24/32 : Loss = 0.010706259869039059\n",
      "Epoch 42, Batch 25/32 : Loss = 0.013835245743393898\n",
      "Epoch 42, Batch 26/32 : Loss = 0.00491740508005023\n",
      "Epoch 42, Batch 27/32 : Loss = 0.0139548871666193\n",
      "Epoch 42, Batch 28/32 : Loss = 0.03029363788664341\n",
      "Epoch 42, Batch 29/32 : Loss = 0.012436112388968468\n",
      "Epoch 42, Batch 30/32 : Loss = 0.00935943704098463\n",
      "Epoch 42, Batch 31/32 : Loss = 0.0066327378153800964\n",
      "Epoch 42 finished in 0.05353389978408814 minutes\n",
      "Epoch 42 training_loss = 0.0182130616158247\n",
      "-----WW------=-----2---++---EE----11---n----T----X----r---C----a---nn----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "---kk----$$----I---F-----DD-----e-----hh----]]---k----0-----\\----XX------- => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----o----\"\"---}}--!!--BB-----r----&------9----;--`----O------w------}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----4-----r---{-------%%---/--''--)---w-------&------N------+-----P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 42 val_loss = 0.5624465346336365, word_accuracy = 0.66\n",
      "Epoch 43, Batch 0/32 : Loss = 0.04156480357050896\n",
      "Epoch 43, Batch 1/32 : Loss = 0.010960194282233715\n",
      "Epoch 43, Batch 2/32 : Loss = 0.024527885019779205\n",
      "Epoch 43, Batch 3/32 : Loss = 0.0069200266152620316\n",
      "Epoch 43, Batch 4/32 : Loss = 0.00729078333824873\n",
      "Epoch 43, Batch 5/32 : Loss = 0.007738914806395769\n",
      "Epoch 43, Batch 6/32 : Loss = 0.006204144563525915\n",
      "Epoch 43, Batch 7/32 : Loss = 0.015837889164686203\n",
      "Epoch 43, Batch 8/32 : Loss = 0.011951858177781105\n",
      "Epoch 43, Batch 9/32 : Loss = 0.00826993677765131\n",
      "Epoch 43, Batch 10/32 : Loss = 0.00529474625363946\n",
      "Epoch 43, Batch 11/32 : Loss = 0.12896353006362915\n",
      "Epoch 43, Batch 12/32 : Loss = 0.012322675436735153\n",
      "Epoch 43, Batch 13/32 : Loss = 0.009294765070080757\n",
      "Epoch 43, Batch 14/32 : Loss = 0.007058080285787582\n",
      "Epoch 43, Batch 15/32 : Loss = 0.015059276483952999\n",
      "Epoch 43, Batch 16/32 : Loss = 0.0038282517343759537\n",
      "Epoch 43, Batch 17/32 : Loss = 0.03517831116914749\n",
      "Epoch 43, Batch 18/32 : Loss = 0.02232973463833332\n",
      "Epoch 43, Batch 19/32 : Loss = 0.004648812115192413\n",
      "Epoch 43, Batch 20/32 : Loss = 0.011262629181146622\n",
      "Epoch 43, Batch 21/32 : Loss = 0.008452851325273514\n",
      "Epoch 43, Batch 22/32 : Loss = 0.016900669783353806\n",
      "Epoch 43, Batch 23/32 : Loss = 0.01244414784014225\n",
      "Epoch 43, Batch 24/32 : Loss = 0.014878666028380394\n",
      "Epoch 43, Batch 25/32 : Loss = 0.003726260969415307\n",
      "Epoch 43, Batch 26/32 : Loss = 0.00865107774734497\n",
      "Epoch 43, Batch 27/32 : Loss = 0.02122405171394348\n",
      "Epoch 43, Batch 28/32 : Loss = 0.004775196313858032\n",
      "Epoch 43, Batch 29/32 : Loss = 0.11596416682004929\n",
      "Epoch 43, Batch 30/32 : Loss = 0.005724185146391392\n",
      "Epoch 43, Batch 31/32 : Loss = 0.025680826976895332\n",
      "Epoch 43 finished in 0.05285145441691081 minutes\n",
      "Epoch 43 training_loss = 0.019677385687828064\n",
      "--2----p---:--mm------X---a----z---n----@------C----y------%------%--- => 2p:mXazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----Z------0----\"\"----GG------/----~------$$-----c-----11---jj--t----- => Z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----X----7----0---j---@-----S----Z----L---44----C----mm------MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----BB----..--YY----I--WW-------6-----F-----h-----X----'--YY----2----- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 43 val_loss = 0.5695557594299316, word_accuracy = 0.69\n",
      "Epoch 44, Batch 0/32 : Loss = 0.04276037588715553\n",
      "Epoch 44, Batch 1/32 : Loss = 0.0038925111293792725\n",
      "Epoch 44, Batch 2/32 : Loss = 0.0077232010662555695\n",
      "Epoch 44, Batch 3/32 : Loss = 0.004032553639262915\n",
      "Epoch 44, Batch 4/32 : Loss = 0.006143324077129364\n",
      "Epoch 44, Batch 5/32 : Loss = 0.01847158372402191\n",
      "Epoch 44, Batch 6/32 : Loss = 0.03392580896615982\n",
      "Epoch 44, Batch 7/32 : Loss = 0.047251224517822266\n",
      "Epoch 44, Batch 8/32 : Loss = 0.004767928272485733\n",
      "Epoch 44, Batch 9/32 : Loss = 0.005622765049338341\n",
      "Epoch 44, Batch 10/32 : Loss = 0.0030954512767493725\n",
      "Epoch 44, Batch 11/32 : Loss = 0.0036298760678619146\n",
      "Epoch 44, Batch 12/32 : Loss = 0.005142789334058762\n",
      "Epoch 44, Batch 13/32 : Loss = 0.004341187886893749\n",
      "Epoch 44, Batch 14/32 : Loss = 0.09703566879034042\n",
      "Epoch 44, Batch 15/32 : Loss = 0.005996427498757839\n",
      "Epoch 44, Batch 16/32 : Loss = 0.0048576961271464825\n",
      "Epoch 44, Batch 17/32 : Loss = 0.05933073163032532\n",
      "Epoch 44, Batch 18/32 : Loss = 0.08548550307750702\n",
      "Epoch 44, Batch 19/32 : Loss = 0.0044410619884729385\n",
      "Epoch 44, Batch 20/32 : Loss = 0.008200506679713726\n",
      "Epoch 44, Batch 21/32 : Loss = 0.005458060652017593\n",
      "Epoch 44, Batch 22/32 : Loss = 0.0052297161892056465\n",
      "Epoch 44, Batch 23/32 : Loss = 0.03290560096502304\n",
      "Epoch 44, Batch 24/32 : Loss = 0.005987686105072498\n",
      "Epoch 44, Batch 25/32 : Loss = 0.20229315757751465\n",
      "Epoch 44, Batch 26/32 : Loss = 0.010781029239296913\n",
      "Epoch 44, Batch 27/32 : Loss = 0.015322275459766388\n",
      "Epoch 44, Batch 28/32 : Loss = 0.0249545406550169\n",
      "Epoch 44, Batch 29/32 : Loss = 0.004398731514811516\n",
      "Epoch 44, Batch 30/32 : Loss = 0.007120511494576931\n",
      "Epoch 44, Batch 31/32 : Loss = 0.0033218953758478165\n",
      "Epoch 44 finished in 0.053729180494944254 minutes\n",
      "Epoch 44 training_loss = 0.0247715562582016\n",
      "----0-----c----++---bb---I--\"----b----6----.--Q--------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "---dd---!---N-----r--AA---j--*---$---3----h----5----n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-------]--t---4----e---^---WW------Q-----4--->>----g---- => -]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---'---]--t--4----e----^---W-------Q-----4--->>----g---- => ']t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 44 val_loss = 0.5728908181190491, word_accuracy = 0.63\n",
      "Epoch 45, Batch 0/32 : Loss = 0.005546961911022663\n",
      "Epoch 45, Batch 1/32 : Loss = 0.022538568824529648\n",
      "Epoch 45, Batch 2/32 : Loss = 0.013543408364057541\n",
      "Epoch 45, Batch 3/32 : Loss = 0.05703183636069298\n",
      "Epoch 45, Batch 4/32 : Loss = 0.010968917049467564\n",
      "Epoch 45, Batch 5/32 : Loss = 0.011542580090463161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Batch 6/32 : Loss = 0.005680367350578308\n",
      "Epoch 45, Batch 7/32 : Loss = 0.011023559607565403\n",
      "Epoch 45, Batch 8/32 : Loss = 0.01177543867379427\n",
      "Epoch 45, Batch 9/32 : Loss = 0.005870789289474487\n",
      "Epoch 45, Batch 10/32 : Loss = 0.010907860472798347\n",
      "Epoch 45, Batch 11/32 : Loss = 0.006115886848419905\n",
      "Epoch 45, Batch 12/32 : Loss = 0.007079331669956446\n",
      "Epoch 45, Batch 13/32 : Loss = 0.008131265640258789\n",
      "Epoch 45, Batch 14/32 : Loss = 0.004098620265722275\n",
      "Epoch 45, Batch 15/32 : Loss = 0.0034905781503766775\n",
      "Epoch 45, Batch 16/32 : Loss = 0.0064808982424438\n",
      "Epoch 45, Batch 17/32 : Loss = 0.014977526850998402\n",
      "Epoch 45, Batch 18/32 : Loss = 0.017182979732751846\n",
      "Epoch 45, Batch 19/32 : Loss = 0.009470054879784584\n",
      "Epoch 45, Batch 20/32 : Loss = 0.003120723646134138\n",
      "Epoch 45, Batch 21/32 : Loss = 0.003599804360419512\n",
      "Epoch 45, Batch 22/32 : Loss = 0.004231597762554884\n",
      "Epoch 45, Batch 23/32 : Loss = 0.20877724885940552\n",
      "Epoch 45, Batch 24/32 : Loss = 0.010077321901917458\n",
      "Epoch 45, Batch 25/32 : Loss = 0.0104847876355052\n",
      "Epoch 45, Batch 26/32 : Loss = 0.003558059222996235\n",
      "Epoch 45, Batch 27/32 : Loss = 0.013723764568567276\n",
      "Epoch 45, Batch 28/32 : Loss = 0.005883754696696997\n",
      "Epoch 45, Batch 29/32 : Loss = 0.004091016948223114\n",
      "Epoch 45, Batch 30/32 : Loss = 0.003536492120474577\n",
      "Epoch 45, Batch 31/32 : Loss = 0.2704336941242218\n",
      "Epoch 45 finished in 0.05329558849334717 minutes\n",
      "Epoch 45 training_loss = 0.017617547884583473\n",
      "------kk----$----l--FF-----DD-----e----hh----]---k-----O----\\----X------- => k$lFDeh]kO\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----55---->>----:--*----Y----AA---'--O------D-----*---#-----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "-----OO-----cc-----++-----bb-----I--\"-----bb-----66----.----Q------------ => Oc+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----0-----Q----66---<<----<<---((---T----N-----5---=----PP---(--mm------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 45 val_loss = 0.6264082193374634, word_accuracy = 0.58\n",
      "Epoch 46, Batch 0/32 : Loss = 0.005300099030137062\n",
      "Epoch 46, Batch 1/32 : Loss = 0.05101621150970459\n",
      "Epoch 46, Batch 2/32 : Loss = 0.008902564644813538\n",
      "Epoch 46, Batch 3/32 : Loss = 0.005792990792542696\n",
      "Epoch 46, Batch 4/32 : Loss = 0.005652446299791336\n",
      "Epoch 46, Batch 5/32 : Loss = 0.01341179572045803\n",
      "Epoch 46, Batch 6/32 : Loss = 0.006798368878662586\n",
      "Epoch 46, Batch 7/32 : Loss = 0.007012581918388605\n",
      "Epoch 46, Batch 8/32 : Loss = 0.004738551564514637\n",
      "Epoch 46, Batch 9/32 : Loss = 0.005053448025137186\n",
      "Epoch 46, Batch 10/32 : Loss = 0.012865958735346794\n",
      "Epoch 46, Batch 11/32 : Loss = 0.028698502108454704\n",
      "Epoch 46, Batch 12/32 : Loss = 0.03637242317199707\n",
      "Epoch 46, Batch 13/32 : Loss = 0.005508328787982464\n",
      "Epoch 46, Batch 14/32 : Loss = 0.003907034173607826\n",
      "Epoch 46, Batch 15/32 : Loss = 0.003137818071991205\n",
      "Epoch 46, Batch 16/32 : Loss = 0.01725000888109207\n",
      "Epoch 46, Batch 17/32 : Loss = 0.007467744871973991\n",
      "Epoch 46, Batch 18/32 : Loss = 0.02395240217447281\n",
      "Epoch 46, Batch 19/32 : Loss = 0.006087580695748329\n",
      "Epoch 46, Batch 20/32 : Loss = 0.016590667888522148\n",
      "Epoch 46, Batch 21/32 : Loss = 0.008390571922063828\n",
      "Epoch 46, Batch 22/32 : Loss = 0.11607513576745987\n",
      "Epoch 46, Batch 23/32 : Loss = 0.013780624605715275\n",
      "Epoch 46, Batch 24/32 : Loss = 0.007468510884791613\n",
      "Epoch 46, Batch 25/32 : Loss = 0.008301755413413048\n",
      "Epoch 46, Batch 26/32 : Loss = 0.005037473049014807\n",
      "Epoch 46, Batch 27/32 : Loss = 0.10442059487104416\n",
      "Epoch 46, Batch 28/32 : Loss = 0.013778576627373695\n",
      "Epoch 46, Batch 29/32 : Loss = 0.02476419508457184\n",
      "Epoch 46, Batch 30/32 : Loss = 0.0036917817778885365\n",
      "Epoch 46, Batch 31/32 : Loss = 0.002008363138884306\n",
      "Epoch 46 finished in 0.05540846188863118 minutes\n",
      "Epoch 46 training_loss = 0.018682017922401428\n",
      "--4----r--{-----%%--//-'--)---w-----&----N-----+----P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "-----c----R----;---9----y---?----2----d----i--OO-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---7----n---DD----P---nn--tt--w----d---\\---Q----a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----E-----00----[--xx---)---R-----8----i--P-----w-----)---- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "Epoch 46 val_loss = 0.5872267484664917, word_accuracy = 0.68\n",
      "Epoch 47, Batch 0/32 : Loss = 0.04535527899861336\n",
      "Epoch 47, Batch 1/32 : Loss = 0.003823895938694477\n",
      "Epoch 47, Batch 2/32 : Loss = 0.003762439824640751\n",
      "Epoch 47, Batch 3/32 : Loss = 0.014423578046262264\n",
      "Epoch 47, Batch 4/32 : Loss = 0.009930268861353397\n",
      "Epoch 47, Batch 5/32 : Loss = 0.006314039696007967\n",
      "Epoch 47, Batch 6/32 : Loss = 0.013448926620185375\n",
      "Epoch 47, Batch 7/32 : Loss = 0.0030004791915416718\n",
      "Epoch 47, Batch 8/32 : Loss = 0.004286039154976606\n",
      "Epoch 47, Batch 9/32 : Loss = 0.0051552364602684975\n",
      "Epoch 47, Batch 10/32 : Loss = 0.01125846616923809\n",
      "Epoch 47, Batch 11/32 : Loss = 0.009076612070202827\n",
      "Epoch 47, Batch 12/32 : Loss = 0.016096942126750946\n",
      "Epoch 47, Batch 13/32 : Loss = 0.005474839359521866\n",
      "Epoch 47, Batch 14/32 : Loss = 0.007367732934653759\n",
      "Epoch 47, Batch 15/32 : Loss = 0.009357323870062828\n",
      "Epoch 47, Batch 16/32 : Loss = 0.004703946877270937\n",
      "Epoch 47, Batch 17/32 : Loss = 0.010695971548557281\n",
      "Epoch 47, Batch 18/32 : Loss = 0.005887949373573065\n",
      "Epoch 47, Batch 19/32 : Loss = 0.004107197280973196\n",
      "Epoch 47, Batch 20/32 : Loss = 0.013621700927615166\n",
      "Epoch 47, Batch 21/32 : Loss = 0.02589295245707035\n",
      "Epoch 47, Batch 22/32 : Loss = 0.0032013212330639362\n",
      "Epoch 47, Batch 23/32 : Loss = 0.041513703763484955\n",
      "Epoch 47, Batch 24/32 : Loss = 0.01529837865382433\n",
      "Epoch 47, Batch 25/32 : Loss = 0.004518968518823385\n",
      "Epoch 47, Batch 26/32 : Loss = 0.004603461362421513\n",
      "Epoch 47, Batch 27/32 : Loss = 0.3212085962295532\n",
      "Epoch 47, Batch 28/32 : Loss = 0.07540607452392578\n",
      "Epoch 47, Batch 29/32 : Loss = 0.003671293845400214\n",
      "Epoch 47, Batch 30/32 : Loss = 0.028239917010068893\n",
      "Epoch 47, Batch 31/32 : Loss = 0.007871180772781372\n",
      "Epoch 47 finished in 0.051510179042816163 minutes\n",
      "Epoch 47 training_loss = 0.02350802905857563\n",
      "--JJ--;--qq----+---/---z---yy---U-------%----U-----1---x---_--- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "---k----$----l--F----DD----e----hh----]--k----0----\\---XX------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---\"---]---t---E----e----^^---WW-------Q------4---->-----g----- => \"]tEe^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "----7---n----DD----P---nn---t--ww----d---\\\\---Q----a----R------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 47 val_loss = 0.5912802815437317, word_accuracy = 0.69\n",
      "Epoch 48, Batch 0/32 : Loss = 0.01550711877644062\n",
      "Epoch 48, Batch 1/32 : Loss = 0.003057545982301235\n",
      "Epoch 48, Batch 2/32 : Loss = 0.008160949684679508\n",
      "Epoch 48, Batch 3/32 : Loss = 0.004630222916603088\n",
      "Epoch 48, Batch 4/32 : Loss = 0.009071744047105312\n",
      "Epoch 48, Batch 5/32 : Loss = 0.006093816831707954\n",
      "Epoch 48, Batch 6/32 : Loss = 0.013243479654192924\n",
      "Epoch 48, Batch 7/32 : Loss = 0.012811869382858276\n",
      "Epoch 48, Batch 8/32 : Loss = 0.006762942764908075\n",
      "Epoch 48, Batch 9/32 : Loss = 0.007095052860677242\n",
      "Epoch 48, Batch 10/32 : Loss = 0.02370663359761238\n",
      "Epoch 48, Batch 11/32 : Loss = 0.010050460696220398\n",
      "Epoch 48, Batch 12/32 : Loss = 0.006435200106352568\n",
      "Epoch 48, Batch 13/32 : Loss = 0.008520528674125671\n",
      "Epoch 48, Batch 14/32 : Loss = 0.008247790858149529\n",
      "Epoch 48, Batch 15/32 : Loss = 0.010086323134601116\n",
      "Epoch 48, Batch 16/32 : Loss = 0.010252976790070534\n",
      "Epoch 48, Batch 17/32 : Loss = 0.0069367229007184505\n",
      "Epoch 48, Batch 18/32 : Loss = 0.00729221198707819\n",
      "Epoch 48, Batch 19/32 : Loss = 0.10179363191127777\n",
      "Epoch 48, Batch 20/32 : Loss = 0.006960689555853605\n",
      "Epoch 48, Batch 21/32 : Loss = 0.003794575808569789\n",
      "Epoch 48, Batch 22/32 : Loss = 0.009480424225330353\n",
      "Epoch 48, Batch 23/32 : Loss = 0.004498199559748173\n",
      "Epoch 48, Batch 24/32 : Loss = 0.0045403605327010155\n",
      "Epoch 48, Batch 25/32 : Loss = 0.012815890833735466\n",
      "Epoch 48, Batch 26/32 : Loss = 0.0028873214032500982\n",
      "Epoch 48, Batch 27/32 : Loss = 0.0037940912880003452\n",
      "Epoch 48, Batch 28/32 : Loss = 0.00996326096355915\n",
      "Epoch 48, Batch 29/32 : Loss = 0.02260497771203518\n",
      "Epoch 48, Batch 30/32 : Loss = 0.002677137264981866\n",
      "Epoch 48, Batch 31/32 : Loss = 0.006794936489313841\n",
      "Epoch 48 finished in 0.05214540163675944 minutes\n",
      "Epoch 48 training_loss = 0.0117148132994771\n",
      "-----=-------0-----[----X----)----R------88----ii--P------w-------)----- => =0[X)R8iPw), Ground Truth is Err:509\n",
      "--88------K------ll--ZZ------5------p------$$-----aa-----}-----w------,- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "-----Q-------3------g-----I--z----##------Y----:---]---q------+-----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----W------=----22---++---EE----11---n----TT----X---r---C----aa---nn---- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 48 val_loss = 0.5743239521980286, word_accuracy = 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Batch 0/32 : Loss = 0.0038734986446797848\n",
      "Epoch 49, Batch 1/32 : Loss = 0.004768467508256435\n",
      "Epoch 49, Batch 2/32 : Loss = 0.007835433818399906\n",
      "Epoch 49, Batch 3/32 : Loss = 0.0061904070898890495\n",
      "Epoch 49, Batch 4/32 : Loss = 0.005297555588185787\n",
      "Epoch 49, Batch 5/32 : Loss = 0.015343969687819481\n",
      "Epoch 49, Batch 6/32 : Loss = 0.009781662374734879\n",
      "Epoch 49, Batch 7/32 : Loss = 0.0025051040574908257\n",
      "Epoch 49, Batch 8/32 : Loss = 0.0029775078874081373\n",
      "Epoch 49, Batch 9/32 : Loss = 0.004297706298530102\n",
      "Epoch 49, Batch 10/32 : Loss = 0.041315097361803055\n",
      "Epoch 49, Batch 11/32 : Loss = 0.004413934424519539\n",
      "Epoch 49, Batch 12/32 : Loss = 0.004193430300801992\n",
      "Epoch 49, Batch 13/32 : Loss = 0.007300148718059063\n",
      "Epoch 49, Batch 14/32 : Loss = 0.0021397206000983715\n",
      "Epoch 49, Batch 15/32 : Loss = 0.04741158336400986\n",
      "Epoch 49, Batch 16/32 : Loss = 0.015442326664924622\n",
      "Epoch 49, Batch 17/32 : Loss = 0.01284037809818983\n",
      "Epoch 49, Batch 18/32 : Loss = 0.004927619360387325\n",
      "Epoch 49, Batch 19/32 : Loss = 0.002465527970343828\n",
      "Epoch 49, Batch 20/32 : Loss = 0.003792174393311143\n",
      "Epoch 49, Batch 21/32 : Loss = 0.0045974720269441605\n",
      "Epoch 49, Batch 22/32 : Loss = 0.00770136434584856\n",
      "Epoch 49, Batch 23/32 : Loss = 0.002433866262435913\n",
      "Epoch 49, Batch 24/32 : Loss = 0.00289689633063972\n",
      "Epoch 49, Batch 25/32 : Loss = 0.0028950939886271954\n",
      "Epoch 49, Batch 26/32 : Loss = 0.0577397383749485\n",
      "Epoch 49, Batch 27/32 : Loss = 0.0031426381319761276\n",
      "Epoch 49, Batch 28/32 : Loss = 0.002376182936131954\n",
      "Epoch 49, Batch 29/32 : Loss = 0.003497902536764741\n",
      "Epoch 49, Batch 30/32 : Loss = 0.004794582724571228\n",
      "Epoch 49, Batch 31/32 : Loss = 0.00962540041655302\n",
      "Epoch 49 finished in 0.05379707415898641 minutes\n",
      "Epoch 49 training_loss = 0.009715409018099308\n",
      "-----0-----c----++-----b----II-\"----bb----66----.---Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "---#----G-----9----E----I-=----h----5---#----2----J---)---k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      ";-------3-----\\----$$---->>-----SS-----\\----MM------ii--BB----- => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "--JJ--;--qq----+---/---z---yy---U-------%----U-----1---x---_--- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 49 val_loss = 0.5676825046539307, word_accuracy = 0.72\n",
      "Epoch 50, Batch 0/32 : Loss = 0.0033909704070538282\n",
      "Epoch 50, Batch 1/32 : Loss = 0.0031094891019165516\n",
      "Epoch 50, Batch 2/32 : Loss = 0.00384192680940032\n",
      "Epoch 50, Batch 3/32 : Loss = 0.011626963503658772\n",
      "Epoch 50, Batch 4/32 : Loss = 0.03267201408743858\n",
      "Epoch 50, Batch 5/32 : Loss = 0.005956584587693214\n",
      "Epoch 50, Batch 6/32 : Loss = 0.012982447631657124\n",
      "Epoch 50, Batch 7/32 : Loss = 0.0025609489530324936\n",
      "Epoch 50, Batch 8/32 : Loss = 0.0045300801284611225\n",
      "Epoch 50, Batch 9/32 : Loss = 0.002283166628330946\n",
      "Epoch 50, Batch 10/32 : Loss = 0.0026830583810806274\n",
      "Epoch 50, Batch 11/32 : Loss = 0.002584385219961405\n",
      "Epoch 50, Batch 12/32 : Loss = 0.012160143814980984\n",
      "Epoch 50, Batch 13/32 : Loss = 0.025062507018446922\n",
      "Epoch 50, Batch 14/32 : Loss = 0.005072450265288353\n",
      "Epoch 50, Batch 15/32 : Loss = 0.0029574409127235413\n",
      "Epoch 50, Batch 16/32 : Loss = 0.0031954990699887276\n",
      "Epoch 50, Batch 17/32 : Loss = 0.010044529102742672\n",
      "Epoch 50, Batch 18/32 : Loss = 0.003415273269638419\n",
      "Epoch 50, Batch 19/32 : Loss = 0.010487059131264687\n",
      "Epoch 50, Batch 20/32 : Loss = 0.005175232421606779\n",
      "Epoch 50, Batch 21/32 : Loss = 0.004930436611175537\n",
      "Epoch 50, Batch 22/32 : Loss = 0.0030138217844069004\n",
      "Epoch 50, Batch 23/32 : Loss = 0.0024363400880247355\n",
      "Epoch 50, Batch 24/32 : Loss = 0.0725414976477623\n",
      "Epoch 50, Batch 25/32 : Loss = 0.001958743203431368\n",
      "Epoch 50, Batch 26/32 : Loss = 0.03166382759809494\n",
      "Epoch 50, Batch 27/32 : Loss = 0.014309733174741268\n",
      "Epoch 50, Batch 28/32 : Loss = 0.005000184755772352\n",
      "Epoch 50, Batch 29/32 : Loss = 0.008270232938230038\n",
      "Epoch 50, Batch 30/32 : Loss = 0.0081913061439991\n",
      "Epoch 50, Batch 31/32 : Loss = 0.11044351756572723\n",
      "Epoch 50 finished in 0.05941923062006633 minutes\n",
      "Epoch 50 training_loss = 0.010663894936442375\n",
      "----W------==----2----+----E-----1---nn----T----X---r---C-----a---mm----- => W=2+E1nTXrCam, Ground Truth is W=2+E1nTXrCan\n",
      "---kk-----O------//--,,--yy----c----**----1-----P-----}---#------BB------ => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "--BB----..---YY----I---W--------6------F----hh------X---'---Y-----22----- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----<-------v----O--------T----`----44-----V----[[---0----------Q-------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "Epoch 50 val_loss = 0.5770286321640015, word_accuracy = 0.71\n",
      "Epoch 51, Batch 0/32 : Loss = 0.0023839427158236504\n",
      "Epoch 51, Batch 1/32 : Loss = 0.0016911879647523165\n",
      "Epoch 51, Batch 2/32 : Loss = 0.004317878745496273\n",
      "Epoch 51, Batch 3/32 : Loss = 0.046619899570941925\n",
      "Epoch 51, Batch 4/32 : Loss = 0.027045801281929016\n",
      "Epoch 51, Batch 5/32 : Loss = 0.0298471562564373\n",
      "Epoch 51, Batch 6/32 : Loss = 0.004036240745335817\n",
      "Epoch 51, Batch 7/32 : Loss = 0.008032580837607384\n",
      "Epoch 51, Batch 8/32 : Loss = 0.0023191331420093775\n",
      "Epoch 51, Batch 9/32 : Loss = 0.004121338948607445\n",
      "Epoch 51, Batch 10/32 : Loss = 0.017029330134391785\n",
      "Epoch 51, Batch 11/32 : Loss = 0.004546772688627243\n",
      "Epoch 51, Batch 12/32 : Loss = 0.0018203234067186713\n",
      "Epoch 51, Batch 13/32 : Loss = 0.0025809803046286106\n",
      "Epoch 51, Batch 14/32 : Loss = 0.005849957466125488\n",
      "Epoch 51, Batch 15/32 : Loss = 0.0027236947789788246\n",
      "Epoch 51, Batch 16/32 : Loss = 0.0041939434595406055\n",
      "Epoch 51, Batch 17/32 : Loss = 0.0023968706373125315\n",
      "Epoch 51, Batch 18/32 : Loss = 0.004480471834540367\n",
      "Epoch 51, Batch 19/32 : Loss = 0.0038279416039586067\n",
      "Epoch 51, Batch 20/32 : Loss = 0.002509034238755703\n",
      "Epoch 51, Batch 21/32 : Loss = 0.003942643292248249\n",
      "Epoch 51, Batch 22/32 : Loss = 0.020252808928489685\n",
      "Epoch 51, Batch 23/32 : Loss = 0.0161735862493515\n",
      "Epoch 51, Batch 24/32 : Loss = 0.0022809854708611965\n",
      "Epoch 51, Batch 25/32 : Loss = 0.008059611544013023\n",
      "Epoch 51, Batch 26/32 : Loss = 0.007485048845410347\n",
      "Epoch 51, Batch 27/32 : Loss = 0.013290130533277988\n",
      "Epoch 51, Batch 28/32 : Loss = 0.0026140387635678053\n",
      "Epoch 51, Batch 29/32 : Loss = 0.005275125149637461\n",
      "Epoch 51, Batch 30/32 : Loss = 0.005718622822314501\n",
      "Epoch 51, Batch 31/32 : Loss = 0.06847013533115387\n",
      "Epoch 51 finished in 0.05267266035079956 minutes\n",
      "Epoch 51 training_loss = 0.00886830035597086\n",
      "----E#-----0----[---x---))--RR----88---ii--P-----w-----)---- => E#0[x)R8iPw), Ground Truth is Err:509\n",
      "----4---rr--{-----%%--//-''--)--w-----&----NN----+----P----- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---<<-----v---O------T---`---4----VV---[--00-------Q-------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----##---G-----9---E---I-=---hh---5---#---2---JJ--))--k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 51 val_loss = 0.6087319254875183, word_accuracy = 0.69\n",
      "Epoch 52, Batch 0/32 : Loss = 0.008236809633672237\n",
      "Epoch 52, Batch 1/32 : Loss = 0.00454614358022809\n",
      "Epoch 52, Batch 2/32 : Loss = 0.06582312285900116\n",
      "Epoch 52, Batch 3/32 : Loss = 0.020895253866910934\n",
      "Epoch 52, Batch 4/32 : Loss = 0.005293353460729122\n",
      "Epoch 52, Batch 5/32 : Loss = 0.010121063329279423\n",
      "Epoch 52, Batch 6/32 : Loss = 0.005905403755605221\n",
      "Epoch 52, Batch 7/32 : Loss = 0.002284424379467964\n",
      "Epoch 52, Batch 8/32 : Loss = 0.004597713239490986\n",
      "Epoch 52, Batch 9/32 : Loss = 0.011831790208816528\n",
      "Epoch 52, Batch 10/32 : Loss = 0.008270327933132648\n",
      "Epoch 52, Batch 11/32 : Loss = 0.007143802475184202\n",
      "Epoch 52, Batch 12/32 : Loss = 0.0072403703816235065\n",
      "Epoch 52, Batch 13/32 : Loss = 0.0034002407919615507\n",
      "Epoch 52, Batch 14/32 : Loss = 0.005698048043996096\n",
      "Epoch 52, Batch 15/32 : Loss = 0.006392406299710274\n",
      "Epoch 52, Batch 16/32 : Loss = 0.011475170031189919\n",
      "Epoch 52, Batch 17/32 : Loss = 0.002909323200583458\n",
      "Epoch 52, Batch 18/32 : Loss = 0.0031594526953995228\n",
      "Epoch 52, Batch 19/32 : Loss = 0.004892151802778244\n",
      "Epoch 52, Batch 20/32 : Loss = 0.003831433830782771\n",
      "Epoch 52, Batch 21/32 : Loss = 0.005358603782951832\n",
      "Epoch 52, Batch 22/32 : Loss = 0.0019882777705788612\n",
      "Epoch 52, Batch 23/32 : Loss = 0.003673689439892769\n",
      "Epoch 52, Batch 24/32 : Loss = 0.005410811863839626\n",
      "Epoch 52, Batch 25/32 : Loss = 0.026819108054041862\n",
      "Epoch 52, Batch 26/32 : Loss = 0.007912814617156982\n",
      "Epoch 52, Batch 27/32 : Loss = 0.002204961609095335\n",
      "Epoch 52, Batch 28/32 : Loss = 0.002798938425257802\n",
      "Epoch 52, Batch 29/32 : Loss = 0.0023295660503208637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Batch 30/32 : Loss = 0.0028535146266222\n",
      "Epoch 52, Batch 31/32 : Loss = 0.0014789284905418754\n",
      "Epoch 52 finished in 0.05265188217163086 minutes\n",
      "Epoch 52 training_loss = 0.008529571816325188\n",
      "--88-----K-----l---Z-----55----pp-----$$----a-----}---ww-----, => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "-----<<-----v---O------TT--``---4----VV---[---0--------Q------ => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----++---:--z----7---88----d----SS----v---5----S---JJ---BB---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----0---Q----6----<----<---(---T---N----5---=----P---(-mm----- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 52 val_loss = 0.623106837272644, word_accuracy = 0.69\n",
      "Epoch 53, Batch 0/32 : Loss = 0.0024452556390315294\n",
      "Epoch 53, Batch 1/32 : Loss = 0.004610225558280945\n",
      "Epoch 53, Batch 2/32 : Loss = 0.002170199528336525\n",
      "Epoch 53, Batch 3/32 : Loss = 0.00271000899374485\n",
      "Epoch 53, Batch 4/32 : Loss = 0.0026880071964114904\n",
      "Epoch 53, Batch 5/32 : Loss = 0.003076299326494336\n",
      "Epoch 53, Batch 6/32 : Loss = 0.0016709549818187952\n",
      "Epoch 53, Batch 7/32 : Loss = 0.008283467032015324\n",
      "Epoch 53, Batch 8/32 : Loss = 0.0047096144407987595\n",
      "Epoch 53, Batch 9/32 : Loss = 0.007204652763903141\n",
      "Epoch 53, Batch 10/32 : Loss = 0.01770470105111599\n",
      "Epoch 53, Batch 11/32 : Loss = 0.005770329385995865\n",
      "Epoch 53, Batch 12/32 : Loss = 0.0020556014496833086\n",
      "Epoch 53, Batch 13/32 : Loss = 0.003387060947716236\n",
      "Epoch 53, Batch 14/32 : Loss = 0.0023435528855770826\n",
      "Epoch 53, Batch 15/32 : Loss = 0.001982483547180891\n",
      "Epoch 53, Batch 16/32 : Loss = 0.002147568855434656\n",
      "Epoch 53, Batch 17/32 : Loss = 0.00473346933722496\n",
      "Epoch 53, Batch 18/32 : Loss = 0.0027349614538252354\n",
      "Epoch 53, Batch 19/32 : Loss = 0.007058017887175083\n",
      "Epoch 53, Batch 20/32 : Loss = 0.0016993815079331398\n",
      "Epoch 53, Batch 21/32 : Loss = 0.0029882690869271755\n",
      "Epoch 53, Batch 22/32 : Loss = 0.0037557524628937244\n",
      "Epoch 53, Batch 23/32 : Loss = 0.0022631881292909384\n",
      "Epoch 53, Batch 24/32 : Loss = 0.0015987259102985263\n",
      "Epoch 53, Batch 25/32 : Loss = 0.0027315570041537285\n",
      "Epoch 53, Batch 26/32 : Loss = 0.00567599106580019\n",
      "Epoch 53, Batch 27/32 : Loss = 0.005837745498865843\n",
      "Epoch 53, Batch 28/32 : Loss = 0.005829039495438337\n",
      "Epoch 53, Batch 29/32 : Loss = 0.014463885687291622\n",
      "Epoch 53, Batch 30/32 : Loss = 0.0015127463266253471\n",
      "Epoch 53, Batch 31/32 : Loss = 0.07987991720438004\n",
      "Epoch 53 finished in 0.05329508781433105 minutes\n",
      "Epoch 53 training_loss = 0.004749484360218048\n",
      "----dd---!--NN-----r---A----j--*---$----3----h----55---nn---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "----kk---$----|--'-,-99---Y----N-----W------m------T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---BB----.---Y---I--W-------6----F----h-----X--''--Y----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----kk---$----|--!----9----Y---NN----WW-----mm------T---8---- => k$|!9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 53 val_loss = 0.6333810687065125, word_accuracy = 0.71\n",
      "Epoch 54, Batch 0/32 : Loss = 0.0012757743243128061\n",
      "Epoch 54, Batch 1/32 : Loss = 0.00520742405205965\n",
      "Epoch 54, Batch 2/32 : Loss = 0.0014480496756732464\n",
      "Epoch 54, Batch 3/32 : Loss = 0.010726838372647762\n",
      "Epoch 54, Batch 4/32 : Loss = 0.005796154960989952\n",
      "Epoch 54, Batch 5/32 : Loss = 0.04327983409166336\n",
      "Epoch 54, Batch 6/32 : Loss = 0.02894717827439308\n",
      "Epoch 54, Batch 7/32 : Loss = 0.0020143024157732725\n",
      "Epoch 54, Batch 8/32 : Loss = 0.034113045781850815\n",
      "Epoch 54, Batch 9/32 : Loss = 0.00309629924595356\n",
      "Epoch 54, Batch 10/32 : Loss = 0.0023429272696375847\n",
      "Epoch 54, Batch 11/32 : Loss = 0.00302532478235662\n",
      "Epoch 54, Batch 12/32 : Loss = 0.004302861634641886\n",
      "Epoch 54, Batch 13/32 : Loss = 0.0029241470620036125\n",
      "Epoch 54, Batch 14/32 : Loss = 0.019861852750182152\n",
      "Epoch 54, Batch 15/32 : Loss = 0.0023164169397205114\n",
      "Epoch 54, Batch 16/32 : Loss = 0.01139584556221962\n",
      "Epoch 54, Batch 17/32 : Loss = 0.005487677175551653\n",
      "Epoch 54, Batch 18/32 : Loss = 0.00955058541148901\n",
      "Epoch 54, Batch 19/32 : Loss = 0.005922447890043259\n",
      "Epoch 54, Batch 20/32 : Loss = 0.0028766824398189783\n",
      "Epoch 54, Batch 21/32 : Loss = 0.01724185049533844\n",
      "Epoch 54, Batch 22/32 : Loss = 0.0026939986273646355\n",
      "Epoch 54, Batch 23/32 : Loss = 0.0019958470948040485\n",
      "Epoch 54, Batch 24/32 : Loss = 0.0028031705878674984\n",
      "Epoch 54, Batch 25/32 : Loss = 0.005246330052614212\n",
      "Epoch 54, Batch 26/32 : Loss = 0.003866567276418209\n",
      "Epoch 54, Batch 27/32 : Loss = 0.011384913697838783\n",
      "Epoch 54, Batch 28/32 : Loss = 0.002856224775314331\n",
      "Epoch 54, Batch 29/32 : Loss = 0.0023733372800052166\n",
      "Epoch 54, Batch 30/32 : Loss = 0.0018435161327943206\n",
      "Epoch 54, Batch 31/32 : Loss = 0.0018385008443146944\n",
      "Epoch 54 finished in 0.05297396183013916 minutes\n",
      "Epoch 54 training_loss = 0.008303523994982243\n",
      "---//---MM-----o----E----^--3----xx--/---&----6----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "-----<-----v---O------T---`---4----V---[---0--------Q----- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "--.--cc---O-----w----uu--``--u---.--R----{--3---\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "---'---]--tt--4----e---^----W------Q-----4---->-----g----- => ']t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 54 val_loss = 0.5945202708244324, word_accuracy = 0.69\n",
      "Epoch 55, Batch 0/32 : Loss = 0.0034095312003046274\n",
      "Epoch 55, Batch 1/32 : Loss = 0.004975738003849983\n",
      "Epoch 55, Batch 2/32 : Loss = 0.00221366249024868\n",
      "Epoch 55, Batch 3/32 : Loss = 0.006655034609138966\n",
      "Epoch 55, Batch 4/32 : Loss = 0.009707162156701088\n",
      "Epoch 55, Batch 5/32 : Loss = 0.004914031829684973\n",
      "Epoch 55, Batch 6/32 : Loss = 0.013755356892943382\n",
      "Epoch 55, Batch 7/32 : Loss = 0.006984973791986704\n",
      "Epoch 55, Batch 8/32 : Loss = 0.023275485262274742\n",
      "Epoch 55, Batch 9/32 : Loss = 0.0022073870059102774\n",
      "Epoch 55, Batch 10/32 : Loss = 0.0017970068147405982\n",
      "Epoch 55, Batch 11/32 : Loss = 0.0043035647831857204\n",
      "Epoch 55, Batch 12/32 : Loss = 0.002190803410485387\n",
      "Epoch 55, Batch 13/32 : Loss = 0.0030762001406401396\n",
      "Epoch 55, Batch 14/32 : Loss = 0.008161951787769794\n",
      "Epoch 55, Batch 15/32 : Loss = 0.0024324271362274885\n",
      "Epoch 55, Batch 16/32 : Loss = 0.0016853692941367626\n",
      "Epoch 55, Batch 17/32 : Loss = 0.025699224323034286\n",
      "Epoch 55, Batch 18/32 : Loss = 0.02097458392381668\n",
      "Epoch 55, Batch 19/32 : Loss = 0.0025463763158768415\n",
      "Epoch 55, Batch 20/32 : Loss = 0.0022418831940740347\n",
      "Epoch 55, Batch 21/32 : Loss = 0.0029044949915260077\n",
      "Epoch 55, Batch 22/32 : Loss = 0.04492739960551262\n",
      "Epoch 55, Batch 23/32 : Loss = 0.00893716886639595\n",
      "Epoch 55, Batch 24/32 : Loss = 0.008418447338044643\n",
      "Epoch 55, Batch 25/32 : Loss = 0.034062255173921585\n",
      "Epoch 55, Batch 26/32 : Loss = 0.0034931283444166183\n",
      "Epoch 55, Batch 27/32 : Loss = 0.0036410733591765165\n",
      "Epoch 55, Batch 28/32 : Loss = 0.006216671783477068\n",
      "Epoch 55, Batch 29/32 : Loss = 0.041123706847429276\n",
      "Epoch 55, Batch 30/32 : Loss = 0.004868245217949152\n",
      "Epoch 55, Batch 31/32 : Loss = 0.03270344063639641\n",
      "Epoch 55 finished in 0.05304485559463501 minutes\n",
      "Epoch 55 training_loss = 0.01014901977032423\n",
      "------d----:---X----9----ee----a----FF--,--8--------VV----R------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----c----RR----;--99----yy---?----2-----d----i---O-----{{--!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---#-----G-----9---EE---I--=----h----5---#-----2---JJ---}---k---- => #G9EI=h5#2J}k, Ground Truth is #G9EI=h5#2J)k\n",
      "----55----9----gg----m------JJ---uu----x---C----x--.--d----\\\\---- => 59gmJuxCx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 55 val_loss = 0.5808298587799072, word_accuracy = 0.66\n",
      "Epoch 56, Batch 0/32 : Loss = 0.003241377416998148\n",
      "Epoch 56, Batch 1/32 : Loss = 0.0027208346873521805\n",
      "Epoch 56, Batch 2/32 : Loss = 0.0034337351098656654\n",
      "Epoch 56, Batch 3/32 : Loss = 0.0023626526817679405\n",
      "Epoch 56, Batch 4/32 : Loss = 0.003196828532963991\n",
      "Epoch 56, Batch 5/32 : Loss = 0.004438611213117838\n",
      "Epoch 56, Batch 6/32 : Loss = 0.005551658105105162\n",
      "Epoch 56, Batch 7/32 : Loss = 0.0028650301974266768\n",
      "Epoch 56, Batch 8/32 : Loss = 0.01703019253909588\n",
      "Epoch 56, Batch 9/32 : Loss = 0.0035794838331639767\n",
      "Epoch 56, Batch 10/32 : Loss = 0.003460649400949478\n",
      "Epoch 56, Batch 11/32 : Loss = 0.0075035663321614265\n",
      "Epoch 56, Batch 12/32 : Loss = 0.021874064579606056\n",
      "Epoch 56, Batch 13/32 : Loss = 0.02322932332754135\n",
      "Epoch 56, Batch 14/32 : Loss = 0.0057906657457351685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Batch 15/32 : Loss = 0.004922901280224323\n",
      "Epoch 56, Batch 16/32 : Loss = 0.004579775035381317\n",
      "Epoch 56, Batch 17/32 : Loss = 0.00207956088706851\n",
      "Epoch 56, Batch 18/32 : Loss = 0.004086378030478954\n",
      "Epoch 56, Batch 19/32 : Loss = 0.006443322636187077\n",
      "Epoch 56, Batch 20/32 : Loss = 0.0030093658715486526\n",
      "Epoch 56, Batch 21/32 : Loss = 0.005606704391539097\n",
      "Epoch 56, Batch 22/32 : Loss = 0.0022259498946368694\n",
      "Epoch 56, Batch 23/32 : Loss = 0.005255884490907192\n",
      "Epoch 56, Batch 24/32 : Loss = 0.03632846847176552\n",
      "Epoch 56, Batch 25/32 : Loss = 0.00882849469780922\n",
      "Epoch 56, Batch 26/32 : Loss = 0.003582203760743141\n",
      "Epoch 56, Batch 27/32 : Loss = 0.009800219908356667\n",
      "Epoch 56, Batch 28/32 : Loss = 0.05783110111951828\n",
      "Epoch 56, Batch 29/32 : Loss = 0.0021124158520251513\n",
      "Epoch 56, Batch 30/32 : Loss = 0.004557764157652855\n",
      "Epoch 56, Batch 31/32 : Loss = 0.05874026566743851\n",
      "Epoch 56 finished in 0.1467458724975586 minutes\n",
      "Epoch 56 training_loss = 0.008959734812378883\n",
      "----JJ---;---q-----++---//---z----yy---UU--------%%----U------1----x---_-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "---7----nn----DD-----PP-----n----t---w------d-----\\----Q------a----RR----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----WW------=----22---++---EE----1----n----T----XX---r---C----a---In----- => W=2+E1nTXrCaIn, Ground Truth is W=2+E1nTXrCan\n",
      "---kk----$$----l---F-----DD-----ee-----h----]]---k----0-----\\----XX------- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 56 val_loss = 0.5986605286598206, word_accuracy = 0.71\n",
      "Epoch 57, Batch 0/32 : Loss = 0.016209660097956657\n",
      "Epoch 57, Batch 1/32 : Loss = 0.0023169107735157013\n",
      "Epoch 57, Batch 2/32 : Loss = 0.007537467870861292\n",
      "Epoch 57, Batch 3/32 : Loss = 0.007651301100850105\n",
      "Epoch 57, Batch 4/32 : Loss = 0.008467833511531353\n",
      "Epoch 57, Batch 5/32 : Loss = 0.005038176663219929\n",
      "Epoch 57, Batch 6/32 : Loss = 0.0028468044474720955\n",
      "Epoch 57, Batch 7/32 : Loss = 0.0026633767411112785\n",
      "Epoch 57, Batch 8/32 : Loss = 0.009180357679724693\n",
      "Epoch 57, Batch 9/32 : Loss = 0.0022855615243315697\n",
      "Epoch 57, Batch 10/32 : Loss = 0.0028651398606598377\n",
      "Epoch 57, Batch 11/32 : Loss = 0.04563445225358009\n",
      "Epoch 57, Batch 12/32 : Loss = 0.01618853770196438\n",
      "Epoch 57, Batch 13/32 : Loss = 0.003660788992419839\n",
      "Epoch 57, Batch 14/32 : Loss = 0.0018518322613090277\n",
      "Epoch 57, Batch 15/32 : Loss = 0.0067708794958889484\n",
      "Epoch 57, Batch 16/32 : Loss = 0.0031632818281650543\n",
      "Epoch 57, Batch 17/32 : Loss = 0.007040816824883223\n",
      "Epoch 57, Batch 18/32 : Loss = 0.002771038329228759\n",
      "Epoch 57, Batch 19/32 : Loss = 0.0037071325350552797\n",
      "Epoch 57, Batch 20/32 : Loss = 0.003842088393867016\n",
      "Epoch 57, Batch 21/32 : Loss = 0.005256028845906258\n",
      "Epoch 57, Batch 22/32 : Loss = 0.010145694948732853\n",
      "Epoch 57, Batch 23/32 : Loss = 0.004414192866533995\n",
      "Epoch 57, Batch 24/32 : Loss = 0.0035007940605282784\n",
      "Epoch 57, Batch 25/32 : Loss = 0.002532098675146699\n",
      "Epoch 57, Batch 26/32 : Loss = 0.0028269095346331596\n",
      "Epoch 57, Batch 27/32 : Loss = 0.006718803197145462\n",
      "Epoch 57, Batch 28/32 : Loss = 0.002920829225331545\n",
      "Epoch 57, Batch 29/32 : Loss = 0.03781445324420929\n",
      "Epoch 57, Batch 30/32 : Loss = 0.004410864785313606\n",
      "Epoch 57, Batch 31/32 : Loss = 0.002285689814016223\n",
      "Epoch 57 finished in 0.05223950544993083 minutes\n",
      "Epoch 57 training_loss = 0.007791799958795309\n",
      "---J--;---q----+---/---z---yy---U-------%----U-----1---x---_)-- => J;q+/zyU%U1x_), Ground Truth is J;q+/zyU%U1x_\n",
      "--2----p---:-mm-----x--a---z---n----@----C----y-----%-----%---- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----dd---:---XX---99----e----a----FF--,--8--------VV---RR------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "---k----$$---I--F----DD----ee----h----]--k----0----\\---XX------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 57 val_loss = 0.5785523653030396, word_accuracy = 0.69\n",
      "Epoch 58, Batch 0/32 : Loss = 0.0026011813897639513\n",
      "Epoch 58, Batch 1/32 : Loss = 0.005626823287457228\n",
      "Epoch 58, Batch 2/32 : Loss = 0.058509260416030884\n",
      "Epoch 58, Batch 3/32 : Loss = 0.004599410109221935\n",
      "Epoch 58, Batch 4/32 : Loss = 0.0023409686982631683\n",
      "Epoch 58, Batch 5/32 : Loss = 0.002547073643654585\n",
      "Epoch 58, Batch 6/32 : Loss = 0.004253367427736521\n",
      "Epoch 58, Batch 7/32 : Loss = 0.018482400104403496\n",
      "Epoch 58, Batch 8/32 : Loss = 0.011625775136053562\n",
      "Epoch 58, Batch 9/32 : Loss = 0.007421416230499744\n",
      "Epoch 58, Batch 10/32 : Loss = 0.004278652835637331\n",
      "Epoch 58, Batch 11/32 : Loss = 0.005254106130450964\n",
      "Epoch 58, Batch 12/32 : Loss = 0.003397871507331729\n",
      "Epoch 58, Batch 13/32 : Loss = 0.006467139348387718\n",
      "Epoch 58, Batch 14/32 : Loss = 0.0023275124840438366\n",
      "Epoch 58, Batch 15/32 : Loss = 0.039843104779720306\n",
      "Epoch 58, Batch 16/32 : Loss = 0.0032295763958245516\n",
      "Epoch 58, Batch 17/32 : Loss = 0.0027930699288845062\n",
      "Epoch 58, Batch 18/32 : Loss = 0.0026782623026520014\n",
      "Epoch 58, Batch 19/32 : Loss = 0.0027382029220461845\n",
      "Epoch 58, Batch 20/32 : Loss = 0.006172234192490578\n",
      "Epoch 58, Batch 21/32 : Loss = 0.0029127378948032856\n",
      "Epoch 58, Batch 22/32 : Loss = 0.0030325024854391813\n",
      "Epoch 58, Batch 23/32 : Loss = 0.008028952404856682\n",
      "Epoch 58, Batch 24/32 : Loss = 0.009338507428765297\n",
      "Epoch 58, Batch 25/32 : Loss = 0.002251631347462535\n",
      "Epoch 58, Batch 26/32 : Loss = 0.0137185612693429\n",
      "Epoch 58, Batch 27/32 : Loss = 0.003688909113407135\n",
      "Epoch 58, Batch 28/32 : Loss = 0.003102813847362995\n",
      "Epoch 58, Batch 29/32 : Loss = 0.026061827316880226\n",
      "Epoch 58, Batch 30/32 : Loss = 0.0033619683235883713\n",
      "Epoch 58, Batch 31/32 : Loss = 0.006241814233362675\n",
      "Epoch 58 finished in 0.05340514580408732 minutes\n",
      "Epoch 58 training_loss = 0.00878605805337429\n",
      "----Y----WW-----]--i--\\---|----MM----<<-----M-----8----8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "---/---MMM----o-----E----^---3----x---/----&----6----XX----- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---{--BB----Y----R----a----y--hh---#----2--->>----E----4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----##---G----99---E-----=---hh---5---#---2----J--))--k----- => #G9E=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 58 val_loss = 0.6193633675575256, word_accuracy = 0.7\n",
      "Epoch 59, Batch 0/32 : Loss = 0.0036613091360777617\n",
      "Epoch 59, Batch 1/32 : Loss = 0.002687423024326563\n",
      "Epoch 59, Batch 2/32 : Loss = 0.004353357013314962\n",
      "Epoch 59, Batch 3/32 : Loss = 0.005874115973711014\n",
      "Epoch 59, Batch 4/32 : Loss = 0.00528708565980196\n",
      "Epoch 59, Batch 5/32 : Loss = 0.005100434646010399\n",
      "Epoch 59, Batch 6/32 : Loss = 0.008966495282948017\n",
      "Epoch 59, Batch 7/32 : Loss = 0.0017980949487537146\n",
      "Epoch 59, Batch 8/32 : Loss = 0.024828024208545685\n",
      "Epoch 59, Batch 9/32 : Loss = 0.0016234500799328089\n",
      "Epoch 59, Batch 10/32 : Loss = 0.0026717972941696644\n",
      "Epoch 59, Batch 11/32 : Loss = 0.05101305991411209\n",
      "Epoch 59, Batch 12/32 : Loss = 0.0018311014864593744\n",
      "Epoch 59, Batch 13/32 : Loss = 0.006320556625723839\n",
      "Epoch 59, Batch 14/32 : Loss = 0.007813635282218456\n",
      "Epoch 59, Batch 15/32 : Loss = 0.002861777786165476\n",
      "Epoch 59, Batch 16/32 : Loss = 0.00147556746378541\n",
      "Epoch 59, Batch 17/32 : Loss = 0.002084808424115181\n",
      "Epoch 59, Batch 18/32 : Loss = 0.005253810901194811\n",
      "Epoch 59, Batch 19/32 : Loss = 0.013933577574789524\n",
      "Epoch 59, Batch 20/32 : Loss = 0.018367476761341095\n",
      "Epoch 59, Batch 21/32 : Loss = 0.03841767460107803\n",
      "Epoch 59, Batch 22/32 : Loss = 0.014412417076528072\n",
      "Epoch 59, Batch 23/32 : Loss = 0.0018926248885691166\n",
      "Epoch 59, Batch 24/32 : Loss = 0.003986662719398737\n",
      "Epoch 59, Batch 25/32 : Loss = 0.004431511741131544\n",
      "Epoch 59, Batch 26/32 : Loss = 0.0028368462808430195\n",
      "Epoch 59, Batch 27/32 : Loss = 0.0023060417734086514\n",
      "Epoch 59, Batch 28/32 : Loss = 0.017223577946424484\n",
      "Epoch 59, Batch 29/32 : Loss = 0.004640195518732071\n",
      "Epoch 59, Batch 30/32 : Loss = 0.006747681647539139\n",
      "Epoch 59, Batch 31/32 : Loss = 0.009997651912271976\n",
      "Epoch 59 finished in 0.05227424303690593 minutes\n",
      "Epoch 59 training_loss = 0.008865924552083015\n",
      "-----YY----WW-------]--ii--\\----|----MMM------<------MMM-----88----8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "__--55----->----:--*----Y----AA---'--O------DD----*---#-----O-----gg----- => _5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----J---;---q-----++---//---zz---yy---UU--------%%----U------1----x---_-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "---2----p---:--mm------x---a----z---n----@@-----C-----y-----%%------%---- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "Epoch 59 val_loss = 0.6679199934005737, word_accuracy = 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Batch 0/32 : Loss = 0.0034391451627016068\n",
      "Epoch 60, Batch 1/32 : Loss = 0.002115837298333645\n",
      "Epoch 60, Batch 2/32 : Loss = 0.0031111964490264654\n",
      "Epoch 60, Batch 3/32 : Loss = 0.008916237391531467\n",
      "Epoch 60, Batch 4/32 : Loss = 0.00603876169770956\n",
      "Epoch 60, Batch 5/32 : Loss = 0.003630797378718853\n",
      "Epoch 60, Batch 6/32 : Loss = 0.0045775603502988815\n",
      "Epoch 60, Batch 7/32 : Loss = 0.00175442302133888\n",
      "Epoch 60, Batch 8/32 : Loss = 0.11853920668363571\n",
      "Epoch 60, Batch 9/32 : Loss = 0.002908482449129224\n",
      "Epoch 60, Batch 10/32 : Loss = 0.012379434891045094\n",
      "Epoch 60, Batch 11/32 : Loss = 0.0013877952005714178\n",
      "Epoch 60, Batch 12/32 : Loss = 0.002607844304293394\n",
      "Epoch 60, Batch 13/32 : Loss = 0.0018270242726430297\n",
      "Epoch 60, Batch 14/32 : Loss = 0.4238271713256836\n",
      "Epoch 60, Batch 15/32 : Loss = 0.0024016601964831352\n",
      "Epoch 60, Batch 16/32 : Loss = 0.0038764704950153828\n",
      "Epoch 60, Batch 17/32 : Loss = 0.0024467664770781994\n",
      "Epoch 60, Batch 18/32 : Loss = 0.012909729033708572\n",
      "Epoch 60, Batch 19/32 : Loss = 0.003965286072343588\n",
      "Epoch 60, Batch 20/32 : Loss = 0.004502974916249514\n",
      "Epoch 60, Batch 21/32 : Loss = 0.04575372487306595\n",
      "Epoch 60, Batch 22/32 : Loss = 0.00581770483404398\n",
      "Epoch 60, Batch 23/32 : Loss = 0.009336590766906738\n",
      "Epoch 60, Batch 24/32 : Loss = 0.004693889059126377\n",
      "Epoch 60, Batch 25/32 : Loss = 0.004844925366342068\n",
      "Epoch 60, Batch 26/32 : Loss = 0.01875900663435459\n",
      "Epoch 60, Batch 27/32 : Loss = 0.00544427614659071\n",
      "Epoch 60, Batch 28/32 : Loss = 0.003628765232861042\n",
      "Epoch 60, Batch 29/32 : Loss = 0.01263352856040001\n",
      "Epoch 60, Batch 30/32 : Loss = 0.003303222358226776\n",
      "Epoch 60, Batch 31/32 : Loss = 0.005045644007623196\n",
      "Epoch 60 finished in 0.05135104258855184 minutes\n",
      "Epoch 60 training_loss = 0.02383967861533165\n",
      "--7----n----DD----P----n----t--w----dd---\\\\---Q----a----RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----<------v---O-------T---`---4-----V---[[--0---------Q-------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----kk---$----l--F----DD----e----hh---]]--k----0----\\---X------- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---7---n----DD----PP----n---tt--w-----d----\\---Q-----a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 60 val_loss = 0.6259875297546387, word_accuracy = 0.63\n",
      "Epoch 61, Batch 0/32 : Loss = 0.07802180200815201\n",
      "Epoch 61, Batch 1/32 : Loss = 0.10894586890935898\n",
      "Epoch 61, Batch 2/32 : Loss = 0.004951951093971729\n",
      "Epoch 61, Batch 3/32 : Loss = 0.007611767388880253\n",
      "Epoch 61, Batch 4/32 : Loss = 0.004184124059975147\n",
      "Epoch 61, Batch 5/32 : Loss = 0.005667789373546839\n",
      "Epoch 61, Batch 6/32 : Loss = 0.0055769807659089565\n",
      "Epoch 61, Batch 7/32 : Loss = 0.0077554648742079735\n",
      "Epoch 61, Batch 8/32 : Loss = 0.025219300761818886\n",
      "Epoch 61, Batch 9/32 : Loss = 0.01827305182814598\n",
      "Epoch 61, Batch 10/32 : Loss = 0.01119286846369505\n",
      "Epoch 61, Batch 11/32 : Loss = 0.011008644476532936\n",
      "Epoch 61, Batch 12/32 : Loss = 0.10628378391265869\n",
      "Epoch 61, Batch 13/32 : Loss = 0.005025838501751423\n",
      "Epoch 61, Batch 14/32 : Loss = 0.011029833927750587\n",
      "Epoch 61, Batch 15/32 : Loss = 0.0385565459728241\n",
      "Epoch 61, Batch 16/32 : Loss = 0.018599733710289\n",
      "Epoch 61, Batch 17/32 : Loss = 0.0039031284395605326\n",
      "Epoch 61, Batch 18/32 : Loss = 0.005979565903544426\n",
      "Epoch 61, Batch 19/32 : Loss = 0.0031204666011035442\n",
      "Epoch 61, Batch 20/32 : Loss = 0.004127751104533672\n",
      "Epoch 61, Batch 21/32 : Loss = 0.006258425302803516\n",
      "Epoch 61, Batch 22/32 : Loss = 0.004155246540904045\n",
      "Epoch 61, Batch 23/32 : Loss = 0.004765687510371208\n",
      "Epoch 61, Batch 24/32 : Loss = 0.004790514707565308\n",
      "Epoch 61, Batch 25/32 : Loss = 0.0048295981250703335\n",
      "Epoch 61, Batch 26/32 : Loss = 0.0024435194209218025\n",
      "Epoch 61, Batch 27/32 : Loss = 0.004248912446200848\n",
      "Epoch 61, Batch 28/32 : Loss = 0.02969023585319519\n",
      "Epoch 61, Batch 29/32 : Loss = 0.01365802250802517\n",
      "Epoch 61, Batch 30/32 : Loss = 0.005826442502439022\n",
      "Epoch 61, Batch 31/32 : Loss = 0.013020041398704052\n",
      "Epoch 61 finished in 0.05256947676340739 minutes\n",
      "Epoch 61 training_loss = 0.018227476626634598\n",
      "----o---\"\"---}--!--BB----r---&-----9---;--`---O-----w-----}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----8-----K-----l--Z-----5-----p----$$---aa----}---ww----,--- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----J--;---q----+---|---zz--yy---U-------%----U-----1---x---_- => J;q+|zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "---r---GG----I---TT----#----33----PP-----Q------w-----'-i----- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "Epoch 61 val_loss = 0.6227542757987976, word_accuracy = 0.66\n",
      "Epoch 62, Batch 0/32 : Loss = 0.0077100349590182304\n",
      "Epoch 62, Batch 1/32 : Loss = 0.0035677170380949974\n",
      "Epoch 62, Batch 2/32 : Loss = 0.005441633984446526\n",
      "Epoch 62, Batch 3/32 : Loss = 0.011944230645895004\n",
      "Epoch 62, Batch 4/32 : Loss = 0.001861085882410407\n",
      "Epoch 62, Batch 5/32 : Loss = 0.0026463945396244526\n",
      "Epoch 62, Batch 6/32 : Loss = 0.0023349905386567116\n",
      "Epoch 62, Batch 7/32 : Loss = 0.011128270998597145\n",
      "Epoch 62, Batch 8/32 : Loss = 0.0041900090873241425\n",
      "Epoch 62, Batch 9/32 : Loss = 0.004748109728097916\n",
      "Epoch 62, Batch 10/32 : Loss = 0.006600695196539164\n",
      "Epoch 62, Batch 11/32 : Loss = 0.002863204339519143\n",
      "Epoch 62, Batch 12/32 : Loss = 0.004995432216674089\n",
      "Epoch 62, Batch 13/32 : Loss = 0.006917386315762997\n",
      "Epoch 62, Batch 14/32 : Loss = 0.0041355229914188385\n",
      "Epoch 62, Batch 15/32 : Loss = 0.04765643551945686\n",
      "Epoch 62, Batch 16/32 : Loss = 0.022346870973706245\n",
      "Epoch 62, Batch 17/32 : Loss = 0.001911807688884437\n",
      "Epoch 62, Batch 18/32 : Loss = 0.11398953199386597\n",
      "Epoch 62, Batch 19/32 : Loss = 0.0022179516963660717\n",
      "Epoch 62, Batch 20/32 : Loss = 0.01826118677854538\n",
      "Epoch 62, Batch 21/32 : Loss = 0.013932605274021626\n",
      "Epoch 62, Batch 22/32 : Loss = 0.009893681854009628\n",
      "Epoch 62, Batch 23/32 : Loss = 0.012145239859819412\n",
      "Epoch 62, Batch 24/32 : Loss = 0.04001530259847641\n",
      "Epoch 62, Batch 25/32 : Loss = 0.005036959424614906\n",
      "Epoch 62, Batch 26/32 : Loss = 0.0035795920994132757\n",
      "Epoch 62, Batch 27/32 : Loss = 0.02242288924753666\n",
      "Epoch 62, Batch 28/32 : Loss = 0.009093954227864742\n",
      "Epoch 62, Batch 29/32 : Loss = 0.00805704016238451\n",
      "Epoch 62, Batch 30/32 : Loss = 0.013573326170444489\n",
      "Epoch 62, Batch 31/32 : Loss = 0.0059892116114497185\n",
      "Epoch 62 finished in 0.05178673267364502 minutes\n",
      "Epoch 62 training_loss = 0.013685709796845913\n",
      "----{---BB-----Y----R----aa----y---h----#----22---->----EE----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "------kk---$$---l--FF----DD-----e----h-----]---k---0----\\\\---XX----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----#----GG-----9----E----I--=----h----5----#----22---JJ---)---k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----kk----O------/---,---y---c-----*----1----PP----}---#-----BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 62 val_loss = 0.6287175416946411, word_accuracy = 0.64\n",
      "Epoch 63, Batch 0/32 : Loss = 0.01562640443444252\n",
      "Epoch 63, Batch 1/32 : Loss = 0.005398678593337536\n",
      "Epoch 63, Batch 2/32 : Loss = 0.006375129334628582\n",
      "Epoch 63, Batch 3/32 : Loss = 0.029646333307027817\n",
      "Epoch 63, Batch 4/32 : Loss = 0.005746356677263975\n",
      "Epoch 63, Batch 5/32 : Loss = 0.003208273323252797\n",
      "Epoch 63, Batch 6/32 : Loss = 0.003769498085603118\n",
      "Epoch 63, Batch 7/32 : Loss = 0.004677272401750088\n",
      "Epoch 63, Batch 8/32 : Loss = 0.003861217526718974\n",
      "Epoch 63, Batch 9/32 : Loss = 0.00319092720746994\n",
      "Epoch 63, Batch 10/32 : Loss = 0.004433587659150362\n",
      "Epoch 63, Batch 11/32 : Loss = 0.009375645779073238\n",
      "Epoch 63, Batch 12/32 : Loss = 0.021546928212046623\n",
      "Epoch 63, Batch 13/32 : Loss = 0.02548876218497753\n",
      "Epoch 63, Batch 14/32 : Loss = 0.004101605154573917\n",
      "Epoch 63, Batch 15/32 : Loss = 0.004358198493719101\n",
      "Epoch 63, Batch 16/32 : Loss = 0.05437484383583069\n",
      "Epoch 63, Batch 17/32 : Loss = 0.00499112019315362\n",
      "Epoch 63, Batch 18/32 : Loss = 0.3118474781513214\n",
      "Epoch 63, Batch 19/32 : Loss = 0.0034995349124073982\n",
      "Epoch 63, Batch 20/32 : Loss = 0.01360756903886795\n",
      "Epoch 63, Batch 21/32 : Loss = 0.11556872725486755\n",
      "Epoch 63, Batch 22/32 : Loss = 0.00408425647765398\n",
      "Epoch 63, Batch 23/32 : Loss = 0.005234203301370144\n",
      "Epoch 63, Batch 24/32 : Loss = 0.008789502084255219\n",
      "Epoch 63, Batch 25/32 : Loss = 0.009055436588823795\n",
      "Epoch 63, Batch 26/32 : Loss = 0.033010344952344894\n",
      "Epoch 63, Batch 27/32 : Loss = 0.047917112708091736\n",
      "Epoch 63, Batch 28/32 : Loss = 0.056115590035915375\n",
      "Epoch 63, Batch 29/32 : Loss = 0.02399875782430172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Batch 30/32 : Loss = 0.11523924767971039\n",
      "Epoch 63, Batch 31/32 : Loss = 0.013165061362087727\n",
      "Epoch 63 finished in 0.05210262934366862 minutes\n",
      "Epoch 63 training_loss = 0.0308364350348711\n",
      "---<<-----v---O------TT---`---44----V----[--0--------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----o---\"\"---}--!--BB----r---&-----9---;-``--O------w----}}-- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----0-----cc----++----bb----I--\"---bb----66----.--Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----kk---$----|--'---9----Y----N-----W------m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 63 val_loss = 0.5707680583000183, word_accuracy = 0.7\n",
      "Epoch 64, Batch 0/32 : Loss = 0.012908447533845901\n",
      "Epoch 64, Batch 1/32 : Loss = 0.021414700895547867\n",
      "Epoch 64, Batch 2/32 : Loss = 0.005192740820348263\n",
      "Epoch 64, Batch 3/32 : Loss = 0.0060526286251842976\n",
      "Epoch 64, Batch 4/32 : Loss = 0.008078180253505707\n",
      "Epoch 64, Batch 5/32 : Loss = 0.007975448854267597\n",
      "Epoch 64, Batch 6/32 : Loss = 0.008433900773525238\n",
      "Epoch 64, Batch 7/32 : Loss = 0.006980897393077612\n",
      "Epoch 64, Batch 8/32 : Loss = 0.04000917077064514\n",
      "Epoch 64, Batch 9/32 : Loss = 0.1960061937570572\n",
      "Epoch 64, Batch 10/32 : Loss = 0.019177421927452087\n",
      "Epoch 64, Batch 11/32 : Loss = 0.014348810538649559\n",
      "Epoch 64, Batch 12/32 : Loss = 0.004828281234949827\n",
      "Epoch 64, Batch 13/32 : Loss = 0.006828729063272476\n",
      "Epoch 64, Batch 14/32 : Loss = 0.00614074245095253\n",
      "Epoch 64, Batch 15/32 : Loss = 0.0646287053823471\n",
      "Epoch 64, Batch 16/32 : Loss = 0.031028123572468758\n",
      "Epoch 64, Batch 17/32 : Loss = 0.01210204791277647\n",
      "Epoch 64, Batch 18/32 : Loss = 0.016475262120366096\n",
      "Epoch 64, Batch 19/32 : Loss = 0.00737895630300045\n",
      "Epoch 64, Batch 20/32 : Loss = 0.008687648922204971\n",
      "Epoch 64, Batch 21/32 : Loss = 0.019290635362267494\n",
      "Epoch 64, Batch 22/32 : Loss = 0.0064268275164067745\n",
      "Epoch 64, Batch 23/32 : Loss = 0.022614380344748497\n",
      "Epoch 64, Batch 24/32 : Loss = 0.014839351177215576\n",
      "Epoch 64, Batch 25/32 : Loss = 0.007417508400976658\n",
      "Epoch 64, Batch 26/32 : Loss = 0.059997718781232834\n",
      "Epoch 64, Batch 27/32 : Loss = 0.01272937748581171\n",
      "Epoch 64, Batch 28/32 : Loss = 0.03065720945596695\n",
      "Epoch 64, Batch 29/32 : Loss = 0.014300521463155746\n",
      "Epoch 64, Batch 30/32 : Loss = 0.010003272444009781\n",
      "Epoch 64, Batch 31/32 : Loss = 0.04040501266717911\n",
      "Epoch 64 finished in 0.0517848531405131 minutes\n",
      "Epoch 64 training_loss = 0.022747129201889038\n",
      "---55---->---:--*---Y----A---'-O------D----*--#----0-----g----- => 5>:*YA'OD*#0g, Ground Truth is 5>:*YA'OD*#Og\n",
      "---XX---7---0---j--@-----S----Z---L---4---C----mm------M------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "-----Q------3-----g----I--z----#-----Y---:--]]--q-----+----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "------k---$$---l--F----DD----e----hh---]---k---0----\\---XX----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 64 val_loss = 0.5972490310668945, word_accuracy = 0.75\n",
      "Epoch 65, Batch 0/32 : Loss = 0.005932703148573637\n",
      "Epoch 65, Batch 1/32 : Loss = 0.01072159968316555\n",
      "Epoch 65, Batch 2/32 : Loss = 0.02910306118428707\n",
      "Epoch 65, Batch 3/32 : Loss = 0.004316258244216442\n",
      "Epoch 65, Batch 4/32 : Loss = 0.010574491694569588\n",
      "Epoch 65, Batch 5/32 : Loss = 0.016276296228170395\n",
      "Epoch 65, Batch 6/32 : Loss = 0.009496379643678665\n",
      "Epoch 65, Batch 7/32 : Loss = 0.00955006293952465\n",
      "Epoch 65, Batch 8/32 : Loss = 0.00855470821261406\n",
      "Epoch 65, Batch 9/32 : Loss = 0.021820219233632088\n",
      "Epoch 65, Batch 10/32 : Loss = 0.0065974947065114975\n",
      "Epoch 65, Batch 11/32 : Loss = 0.0068883756175637245\n",
      "Epoch 65, Batch 12/32 : Loss = 0.003664386924356222\n",
      "Epoch 65, Batch 13/32 : Loss = 0.003657725639641285\n",
      "Epoch 65, Batch 14/32 : Loss = 0.14593017101287842\n",
      "Epoch 65, Batch 15/32 : Loss = 0.002461283002048731\n",
      "Epoch 65, Batch 16/32 : Loss = 0.07363802939653397\n",
      "Epoch 65, Batch 17/32 : Loss = 0.004385635256767273\n",
      "Epoch 65, Batch 18/32 : Loss = 0.04679281264543533\n",
      "Epoch 65, Batch 19/32 : Loss = 0.009021705016493797\n",
      "Epoch 65, Batch 20/32 : Loss = 0.005383406765758991\n",
      "Epoch 65, Batch 21/32 : Loss = 0.0031860549934208393\n",
      "Epoch 65, Batch 22/32 : Loss = 0.15418852865695953\n",
      "Epoch 65, Batch 23/32 : Loss = 0.01447894237935543\n",
      "Epoch 65, Batch 24/32 : Loss = 0.004387364722788334\n",
      "Epoch 65, Batch 25/32 : Loss = 0.010294519364833832\n",
      "Epoch 65, Batch 26/32 : Loss = 0.004978922661393881\n",
      "Epoch 65, Batch 27/32 : Loss = 0.020162886008620262\n",
      "Epoch 65, Batch 28/32 : Loss = 0.008288117125630379\n",
      "Epoch 65, Batch 29/32 : Loss = 0.007092365995049477\n",
      "Epoch 65, Batch 30/32 : Loss = 0.005557130090892315\n",
      "Epoch 65, Batch 31/32 : Loss = 0.004915341734886169\n",
      "Epoch 65 finished in 0.0521828293800354 minutes\n",
      "Epoch 65 training_loss = 0.021461717784404755\n",
      "----J---;;--q-----+----/---zz---yy---U--------%%---UU------1---X----- => J;q+/zyU%U1X, Ground Truth is J;q+/zyU%U1x_\n",
      "--------33------|----$------>>------S-------\\----MM-------ii--BB----- => -3|$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "-..--C-----O-------w-----uu---`'--u----.--RR-----{--3-----\"--##------ => .COwu`'u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----55----9-----g-----m--------J----uu----x---C-----x---.--d-----\\--- => 59gmJuxCx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 65 val_loss = 0.6571829915046692, word_accuracy = 0.62\n",
      "Epoch 66, Batch 0/32 : Loss = 0.010850605554878712\n",
      "Epoch 66, Batch 1/32 : Loss = 0.20968885719776154\n",
      "Epoch 66, Batch 2/32 : Loss = 0.006755746901035309\n",
      "Epoch 66, Batch 3/32 : Loss = 0.08198832720518112\n",
      "Epoch 66, Batch 4/32 : Loss = 0.004033410456031561\n",
      "Epoch 66, Batch 5/32 : Loss = 0.013736020773649216\n",
      "Epoch 66, Batch 6/32 : Loss = 0.0031586370896548033\n",
      "Epoch 66, Batch 7/32 : Loss = 0.026532860472798347\n",
      "Epoch 66, Batch 8/32 : Loss = 0.005401990842074156\n",
      "Epoch 66, Batch 9/32 : Loss = 0.08008001744747162\n",
      "Epoch 66, Batch 10/32 : Loss = 0.004389957524836063\n",
      "Epoch 66, Batch 11/32 : Loss = 0.0426466166973114\n",
      "Epoch 66, Batch 12/32 : Loss = 0.005666359327733517\n",
      "Epoch 66, Batch 13/32 : Loss = 0.02995140105485916\n",
      "Epoch 66, Batch 14/32 : Loss = 0.0732550397515297\n",
      "Epoch 66, Batch 15/32 : Loss = 0.020447392016649246\n",
      "Epoch 66, Batch 16/32 : Loss = 0.004272346384823322\n",
      "Epoch 66, Batch 17/32 : Loss = 0.01686425320804119\n",
      "Epoch 66, Batch 18/32 : Loss = 0.0964735820889473\n",
      "Epoch 66, Batch 19/32 : Loss = 0.04251975938677788\n",
      "Epoch 66, Batch 20/32 : Loss = 0.0053214034996926785\n",
      "Epoch 66, Batch 21/32 : Loss = 0.004075036384165287\n",
      "Epoch 66, Batch 22/32 : Loss = 0.0036587808281183243\n",
      "Epoch 66, Batch 23/32 : Loss = 0.05505505949258804\n",
      "Epoch 66, Batch 24/32 : Loss = 0.008373066782951355\n",
      "Epoch 66, Batch 25/32 : Loss = 0.021257739514112473\n",
      "Epoch 66, Batch 26/32 : Loss = 0.011322764679789543\n",
      "Epoch 66, Batch 27/32 : Loss = 0.00790821947157383\n",
      "Epoch 66, Batch 28/32 : Loss = 0.007980477064847946\n",
      "Epoch 66, Batch 29/32 : Loss = 0.012805900536477566\n",
      "Epoch 66, Batch 30/32 : Loss = 0.04565632343292236\n",
      "Epoch 66, Batch 31/32 : Loss = 0.007772210985422134\n",
      "Epoch 66 finished in 0.051678025722503663 minutes\n",
      "Epoch 66 training_loss = 0.030942954123020172\n",
      "----++---:--z---77---8----d----S-----v--5----S----J----B---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "---7---n----D-----P---nn---t--w-----d----\\--Q-----a----RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----4----r--{------%---/--'--)--w-----&-----N-----+---P----- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---;-----33----\\---$$---->>-----S----\\----MM-----ii--B------ => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 66 val_loss = 0.6850629448890686, word_accuracy = 0.55\n",
      "Epoch 67, Batch 0/32 : Loss = 0.039547599852085114\n",
      "Epoch 67, Batch 1/32 : Loss = 0.02340267226099968\n",
      "Epoch 67, Batch 2/32 : Loss = 0.029576536267995834\n",
      "Epoch 67, Batch 3/32 : Loss = 0.005480979569256306\n",
      "Epoch 67, Batch 4/32 : Loss = 0.014355997554957867\n",
      "Epoch 67, Batch 5/32 : Loss = 0.1335323005914688\n",
      "Epoch 67, Batch 6/32 : Loss = 0.026633456349372864\n",
      "Epoch 67, Batch 7/32 : Loss = 0.03586243465542793\n",
      "Epoch 67, Batch 8/32 : Loss = 0.0050073470920324326\n",
      "Epoch 67, Batch 9/32 : Loss = 0.01642911694943905\n",
      "Epoch 67, Batch 10/32 : Loss = 0.005578192416578531\n",
      "Epoch 67, Batch 11/32 : Loss = 0.0169056486338377\n",
      "Epoch 67, Batch 12/32 : Loss = 0.010873215273022652\n",
      "Epoch 67, Batch 13/32 : Loss = 0.022173183038830757\n",
      "Epoch 67, Batch 14/32 : Loss = 0.01188715174794197\n",
      "Epoch 67, Batch 15/32 : Loss = 0.01907062530517578\n",
      "Epoch 67, Batch 16/32 : Loss = 0.017640363425016403\n",
      "Epoch 67, Batch 17/32 : Loss = 0.007538268342614174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Batch 18/32 : Loss = 0.06359221041202545\n",
      "Epoch 67, Batch 19/32 : Loss = 0.00447734585031867\n",
      "Epoch 67, Batch 20/32 : Loss = 0.006840015761554241\n",
      "Epoch 67, Batch 21/32 : Loss = 0.009110604412853718\n",
      "Epoch 67, Batch 22/32 : Loss = 0.006911462172865868\n",
      "Epoch 67, Batch 23/32 : Loss = 0.011491725221276283\n",
      "Epoch 67, Batch 24/32 : Loss = 0.0034848088398575783\n",
      "Epoch 67, Batch 25/32 : Loss = 0.01249811239540577\n",
      "Epoch 67, Batch 26/32 : Loss = 0.0031382364686578512\n",
      "Epoch 67, Batch 27/32 : Loss = 0.026906859129667282\n",
      "Epoch 67, Batch 28/32 : Loss = 0.00532892532646656\n",
      "Epoch 67, Batch 29/32 : Loss = 0.004690123721957207\n",
      "Epoch 67, Batch 30/32 : Loss = 0.034053023904561996\n",
      "Epoch 67, Batch 31/32 : Loss = 0.01440301164984703\n",
      "Epoch 67 finished in 0.05241703987121582 minutes\n",
      "Epoch 67 training_loss = 0.020427916198968887\n",
      "--88------K------l---Z------55-----p------$$-----aa-----}----W------,- => 8KlZ5p$a}W,, Ground Truth is 8KIZ5p$a}w,\n",
      "----X----7---00---j---@-----S----Z----L---44---C-----mm------M-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "-----0------c-----+++-----b-----I--\"-----b------6----..---Q----------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----YY----WW-------]--i--\\\\----|----M-------<------MM-----88----8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 67 val_loss = 0.621252715587616, word_accuracy = 0.65\n",
      "Epoch 68, Batch 0/32 : Loss = 0.0049933744594454765\n",
      "Epoch 68, Batch 1/32 : Loss = 0.0038817240856587887\n",
      "Epoch 68, Batch 2/32 : Loss = 0.04858868196606636\n",
      "Epoch 68, Batch 3/32 : Loss = 0.007767342962324619\n",
      "Epoch 68, Batch 4/32 : Loss = 0.03370339050889015\n",
      "Epoch 68, Batch 5/32 : Loss = 0.0040613822638988495\n",
      "Epoch 68, Batch 6/32 : Loss = 0.002763087395578623\n",
      "Epoch 68, Batch 7/32 : Loss = 0.003855318995192647\n",
      "Epoch 68, Batch 8/32 : Loss = 0.028674354776740074\n",
      "Epoch 68, Batch 9/32 : Loss = 0.006898637395352125\n",
      "Epoch 68, Batch 10/32 : Loss = 0.19603291153907776\n",
      "Epoch 68, Batch 11/32 : Loss = 0.006518581882119179\n",
      "Epoch 68, Batch 12/32 : Loss = 0.05416736751794815\n",
      "Epoch 68, Batch 13/32 : Loss = 0.0037755088414996862\n",
      "Epoch 68, Batch 14/32 : Loss = 0.004455240909010172\n",
      "Epoch 68, Batch 15/32 : Loss = 0.0028941696509718895\n",
      "Epoch 68, Batch 16/32 : Loss = 0.008349251933395863\n",
      "Epoch 68, Batch 17/32 : Loss = 0.00319862668402493\n",
      "Epoch 68, Batch 18/32 : Loss = 0.009972263127565384\n",
      "Epoch 68, Batch 19/32 : Loss = 0.012256693094968796\n",
      "Epoch 68, Batch 20/32 : Loss = 0.01218140497803688\n",
      "Epoch 68, Batch 21/32 : Loss = 0.004012109246104956\n",
      "Epoch 68, Batch 22/32 : Loss = 0.004787243437021971\n",
      "Epoch 68, Batch 23/32 : Loss = 0.003581112716346979\n",
      "Epoch 68, Batch 24/32 : Loss = 0.004077865742146969\n",
      "Epoch 68, Batch 25/32 : Loss = 0.10425838828086853\n",
      "Epoch 68, Batch 26/32 : Loss = 0.004180781543254852\n",
      "Epoch 68, Batch 27/32 : Loss = 0.019707849249243736\n",
      "Epoch 68, Batch 28/32 : Loss = 0.05908609926700592\n",
      "Epoch 68, Batch 29/32 : Loss = 0.004555671010166407\n",
      "Epoch 68, Batch 30/32 : Loss = 0.002180854557082057\n",
      "Epoch 68, Batch 31/32 : Loss = 0.0034789706114679575\n",
      "Epoch 68 finished in 0.052608005205790204 minutes\n",
      "Epoch 68 training_loss = 0.021521350368857384\n",
      "---k---$----|--'---9----Y----N----W------m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----o---\"\"--}--!--BB----r--&&----9---;-``--O-----w-----}-- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-:-----33----\\----$---->>-----S-----\\----MM-----ii--B----- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----d----!--NN----rr--AA---j--*---$----3---hh---55---nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 68 val_loss = 0.6463155746459961, word_accuracy = 0.68\n",
      "Epoch 69, Batch 0/32 : Loss = 0.004799200221896172\n",
      "Epoch 69, Batch 1/32 : Loss = 0.0033600199967622757\n",
      "Epoch 69, Batch 2/32 : Loss = 0.016368882730603218\n",
      "Epoch 69, Batch 3/32 : Loss = 0.0027968892827630043\n",
      "Epoch 69, Batch 4/32 : Loss = 0.014756152406334877\n",
      "Epoch 69, Batch 5/32 : Loss = 0.008582452312111855\n",
      "Epoch 69, Batch 6/32 : Loss = 0.023455698043107986\n",
      "Epoch 69, Batch 7/32 : Loss = 0.008242965675890446\n",
      "Epoch 69, Batch 8/32 : Loss = 0.17922909557819366\n",
      "Epoch 69, Batch 9/32 : Loss = 0.02834564447402954\n",
      "Epoch 69, Batch 10/32 : Loss = 0.002587499562650919\n",
      "Epoch 69, Batch 11/32 : Loss = 0.002402485813945532\n",
      "Epoch 69, Batch 12/32 : Loss = 0.002145468257367611\n",
      "Epoch 69, Batch 13/32 : Loss = 0.015412979759275913\n",
      "Epoch 69, Batch 14/32 : Loss = 0.00497175008058548\n",
      "Epoch 69, Batch 15/32 : Loss = 0.03614822402596474\n",
      "Epoch 69, Batch 16/32 : Loss = 0.01126226969063282\n",
      "Epoch 69, Batch 17/32 : Loss = 0.03985709324479103\n",
      "Epoch 69, Batch 18/32 : Loss = 0.00565411476418376\n",
      "Epoch 69, Batch 19/32 : Loss = 0.016255056485533714\n",
      "Epoch 69, Batch 20/32 : Loss = 0.010887730866670609\n",
      "Epoch 69, Batch 21/32 : Loss = 0.018880926072597504\n",
      "Epoch 69, Batch 22/32 : Loss = 0.015720486640930176\n",
      "Epoch 69, Batch 23/32 : Loss = 0.014836045913398266\n",
      "Epoch 69, Batch 24/32 : Loss = 0.011097215116024017\n",
      "Epoch 69, Batch 25/32 : Loss = 0.0034658974036574364\n",
      "Epoch 69, Batch 26/32 : Loss = 0.005897252820432186\n",
      "Epoch 69, Batch 27/32 : Loss = 0.009488982148468494\n",
      "Epoch 69, Batch 28/32 : Loss = 0.012980879284441471\n",
      "Epoch 69, Batch 29/32 : Loss = 0.0034497776068747044\n",
      "Epoch 69, Batch 30/32 : Loss = 0.010330493561923504\n",
      "Epoch 69, Batch 31/32 : Loss = 0.031241418793797493\n",
      "Epoch 69 finished in 0.055249249935150145 minutes\n",
      "Epoch 69 training_loss = 0.017592767253518105\n",
      "----55----9----g-----m------JJ---uu----X---c----X--..-d-----\\---- => 59gmJuXcX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "-:------33-----\\----$$---->>------S-----\\\\---MM-------i---B------ => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "--{---B-----Y----RR----a----y---h----#----2---->-----E---44------ => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "---{---B-----Y----RR----a----y---h---##----22--->-----E----44---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 69 val_loss = 0.6288753151893616, word_accuracy = 0.65\n",
      "Epoch 70, Batch 0/32 : Loss = 0.0034490711987018585\n",
      "Epoch 70, Batch 1/32 : Loss = 0.008530810475349426\n",
      "Epoch 70, Batch 2/32 : Loss = 0.002855489030480385\n",
      "Epoch 70, Batch 3/32 : Loss = 0.006508341524749994\n",
      "Epoch 70, Batch 4/32 : Loss = 0.002657391829416156\n",
      "Epoch 70, Batch 5/32 : Loss = 0.004676230251789093\n",
      "Epoch 70, Batch 6/32 : Loss = 0.00501267472282052\n",
      "Epoch 70, Batch 7/32 : Loss = 0.010282924398779869\n",
      "Epoch 70, Batch 8/32 : Loss = 0.003943875432014465\n",
      "Epoch 70, Batch 9/32 : Loss = 0.008665074594318867\n",
      "Epoch 70, Batch 10/32 : Loss = 0.0026001816149801016\n",
      "Epoch 70, Batch 11/32 : Loss = 0.012553180567920208\n",
      "Epoch 70, Batch 12/32 : Loss = 0.005278770811855793\n",
      "Epoch 70, Batch 13/32 : Loss = 0.13203197717666626\n",
      "Epoch 70, Batch 14/32 : Loss = 0.007430661469697952\n",
      "Epoch 70, Batch 15/32 : Loss = 0.005766559392213821\n",
      "Epoch 70, Batch 16/32 : Loss = 0.006737818010151386\n",
      "Epoch 70, Batch 17/32 : Loss = 0.010221831500530243\n",
      "Epoch 70, Batch 18/32 : Loss = 0.0033526266925036907\n",
      "Epoch 70, Batch 19/32 : Loss = 0.008566809818148613\n",
      "Epoch 70, Batch 20/32 : Loss = 0.0252764243632555\n",
      "Epoch 70, Batch 21/32 : Loss = 0.012180410325527191\n",
      "Epoch 70, Batch 22/32 : Loss = 0.009746821597218513\n",
      "Epoch 70, Batch 23/32 : Loss = 0.006431246176362038\n",
      "Epoch 70, Batch 24/32 : Loss = 0.009031135588884354\n",
      "Epoch 70, Batch 25/32 : Loss = 0.019572336226701736\n",
      "Epoch 70, Batch 26/32 : Loss = 0.006101552397012711\n",
      "Epoch 70, Batch 27/32 : Loss = 0.010379962623119354\n",
      "Epoch 70, Batch 28/32 : Loss = 0.009071852080523968\n",
      "Epoch 70, Batch 29/32 : Loss = 0.036430273205041885\n",
      "Epoch 70, Batch 30/32 : Loss = 0.010558882728219032\n",
      "Epoch 70, Batch 31/32 : Loss = 0.003887932049110532\n",
      "Epoch 70 finished in 0.053290927410125734 minutes\n",
      "Epoch 70 training_loss = 0.013056680560112\n",
      "-----c----R----;--9----yy---??---2---dd----i--O-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----4---rr--{-----%---/--'-))--w-----&----N-----+---P----- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----o----\"--}--!--BB----r--&-----9---;-``--O-----w----}}-- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---k---$----|--'---9----Y----N----W------m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 70 val_loss = 0.6881672739982605, word_accuracy = 0.62\n",
      "Epoch 71, Batch 0/32 : Loss = 0.013488920405507088\n",
      "Epoch 71, Batch 1/32 : Loss = 0.020538434386253357\n",
      "Epoch 71, Batch 2/32 : Loss = 0.0026508886367082596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Batch 3/32 : Loss = 0.013418466784060001\n",
      "Epoch 71, Batch 4/32 : Loss = 0.004200344905257225\n",
      "Epoch 71, Batch 5/32 : Loss = 0.0030237017199397087\n",
      "Epoch 71, Batch 6/32 : Loss = 0.002525043673813343\n",
      "Epoch 71, Batch 7/32 : Loss = 0.003759728977456689\n",
      "Epoch 71, Batch 8/32 : Loss = 0.03489254042506218\n",
      "Epoch 71, Batch 9/32 : Loss = 0.003564103739336133\n",
      "Epoch 71, Batch 10/32 : Loss = 0.028310269117355347\n",
      "Epoch 71, Batch 11/32 : Loss = 0.017784861847758293\n",
      "Epoch 71, Batch 12/32 : Loss = 0.027797238901257515\n",
      "Epoch 71, Batch 13/32 : Loss = 0.014433611184358597\n",
      "Epoch 71, Batch 14/32 : Loss = 0.0018688482232391834\n",
      "Epoch 71, Batch 15/32 : Loss = 0.013873501680791378\n",
      "Epoch 71, Batch 16/32 : Loss = 0.010899508371949196\n",
      "Epoch 71, Batch 17/32 : Loss = 0.0026959795504808426\n",
      "Epoch 71, Batch 18/32 : Loss = 0.01370445266366005\n",
      "Epoch 71, Batch 19/32 : Loss = 0.00239908741787076\n",
      "Epoch 71, Batch 20/32 : Loss = 0.010075433179736137\n",
      "Epoch 71, Batch 21/32 : Loss = 0.005847529042512178\n",
      "Epoch 71, Batch 22/32 : Loss = 0.013135256245732307\n",
      "Epoch 71, Batch 23/32 : Loss = 0.011086282320320606\n",
      "Epoch 71, Batch 24/32 : Loss = 0.06391364336013794\n",
      "Epoch 71, Batch 25/32 : Loss = 0.011011209338903427\n",
      "Epoch 71, Batch 26/32 : Loss = 0.02331739291548729\n",
      "Epoch 71, Batch 27/32 : Loss = 0.005172251723706722\n",
      "Epoch 71, Batch 28/32 : Loss = 0.008073410950601101\n",
      "Epoch 71, Batch 29/32 : Loss = 0.006365098059177399\n",
      "Epoch 71, Batch 30/32 : Loss = 0.003241890110075474\n",
      "Epoch 71, Batch 31/32 : Loss = 0.004859871231019497\n",
      "Epoch 71 finished in 0.13315172990163168 minutes\n",
      "Epoch 71 training_loss = 0.012776752933859825\n",
      "--4----r--{-----%%--//--'-)---w-----&----N-----+----P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---/----M------o----E----^---3----x---/---&----6-----X----- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---8-----K----ll--Z----5----p-----$$---a----}---w-----,---- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---<<-----v---O------T---`--44----V---[---0-------Q-------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "Epoch 71 val_loss = 0.6031875610351562, word_accuracy = 0.72\n",
      "Epoch 72, Batch 0/32 : Loss = 0.004281234927475452\n",
      "Epoch 72, Batch 1/32 : Loss = 0.0024691238068044186\n",
      "Epoch 72, Batch 2/32 : Loss = 0.0049215019680559635\n",
      "Epoch 72, Batch 3/32 : Loss = 0.005222306586802006\n",
      "Epoch 72, Batch 4/32 : Loss = 0.005480514839291573\n",
      "Epoch 72, Batch 5/32 : Loss = 0.0025236429646611214\n",
      "Epoch 72, Batch 6/32 : Loss = 0.0018556667491793633\n",
      "Epoch 72, Batch 7/32 : Loss = 0.007015855051577091\n",
      "Epoch 72, Batch 8/32 : Loss = 0.004801912698894739\n",
      "Epoch 72, Batch 9/32 : Loss = 0.002415314083918929\n",
      "Epoch 72, Batch 10/32 : Loss = 0.004418753087520599\n",
      "Epoch 72, Batch 11/32 : Loss = 0.003058105008676648\n",
      "Epoch 72, Batch 12/32 : Loss = 0.002163311466574669\n",
      "Epoch 72, Batch 13/32 : Loss = 0.0039322818629443645\n",
      "Epoch 72, Batch 14/32 : Loss = 0.0018603235948830843\n",
      "Epoch 72, Batch 15/32 : Loss = 0.009014849551022053\n",
      "Epoch 72, Batch 16/32 : Loss = 0.003708248259499669\n",
      "Epoch 72, Batch 17/32 : Loss = 0.0024997578002512455\n",
      "Epoch 72, Batch 18/32 : Loss = 0.0020910929888486862\n",
      "Epoch 72, Batch 19/32 : Loss = 0.003228588495403528\n",
      "Epoch 72, Batch 20/32 : Loss = 0.005323249381035566\n",
      "Epoch 72, Batch 21/32 : Loss = 0.07212767004966736\n",
      "Epoch 72, Batch 22/32 : Loss = 0.04103482887148857\n",
      "Epoch 72, Batch 23/32 : Loss = 0.0023130769841372967\n",
      "Epoch 72, Batch 24/32 : Loss = 0.004353512544184923\n",
      "Epoch 72, Batch 25/32 : Loss = 0.004424384795129299\n",
      "Epoch 72, Batch 26/32 : Loss = 0.002056045923382044\n",
      "Epoch 72, Batch 27/32 : Loss = 0.026630785316228867\n",
      "Epoch 72, Batch 28/32 : Loss = 0.004774018656462431\n",
      "Epoch 72, Batch 29/32 : Loss = 0.007595960050821304\n",
      "Epoch 72, Batch 30/32 : Loss = 0.001610548235476017\n",
      "Epoch 72, Batch 31/32 : Loss = 0.002378829289227724\n",
      "Epoch 72 finished in 0.05467104911804199 minutes\n",
      "Epoch 72 training_loss = 0.008016186766326427\n",
      "-------3----\\\\---$---->>-----S-----\\---MM-----i---B---- => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----E----00---[[--x----)--RR----8----i--P-----w-----)-- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "----0---JJ----!-((--;--AA----3---,--'-))--r---r---7---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----\"--]--t--4----e---^---WW-----Q-----4---->----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 72 val_loss = 0.6464481353759766, word_accuracy = 0.7\n",
      "Epoch 73, Batch 0/32 : Loss = 0.0163163710385561\n",
      "Epoch 73, Batch 1/32 : Loss = 0.0015568749513477087\n",
      "Epoch 73, Batch 2/32 : Loss = 0.004048325587064028\n",
      "Epoch 73, Batch 3/32 : Loss = 0.02836804836988449\n",
      "Epoch 73, Batch 4/32 : Loss = 0.0026801247149705887\n",
      "Epoch 73, Batch 5/32 : Loss = 0.003444985020905733\n",
      "Epoch 73, Batch 6/32 : Loss = 0.004846537485718727\n",
      "Epoch 73, Batch 7/32 : Loss = 0.002738913055509329\n",
      "Epoch 73, Batch 8/32 : Loss = 0.004447205923497677\n",
      "Epoch 73, Batch 9/32 : Loss = 0.03722461685538292\n",
      "Epoch 73, Batch 10/32 : Loss = 0.0075822556391358376\n",
      "Epoch 73, Batch 11/32 : Loss = 0.0027941716834902763\n",
      "Epoch 73, Batch 12/32 : Loss = 0.002878563478589058\n",
      "Epoch 73, Batch 13/32 : Loss = 0.009414918720722198\n",
      "Epoch 73, Batch 14/32 : Loss = 0.003735048696398735\n",
      "Epoch 73, Batch 15/32 : Loss = 0.002942266408354044\n",
      "Epoch 73, Batch 16/32 : Loss = 0.0022582404781132936\n",
      "Epoch 73, Batch 17/32 : Loss = 0.036497555673122406\n",
      "Epoch 73, Batch 18/32 : Loss = 0.009724628180265427\n",
      "Epoch 73, Batch 19/32 : Loss = 0.0033302835654467344\n",
      "Epoch 73, Batch 20/32 : Loss = 0.004204798024147749\n",
      "Epoch 73, Batch 21/32 : Loss = 0.004401789046823978\n",
      "Epoch 73, Batch 22/32 : Loss = 0.11545568704605103\n",
      "Epoch 73, Batch 23/32 : Loss = 0.021949248388409615\n",
      "Epoch 73, Batch 24/32 : Loss = 0.006494936998933554\n",
      "Epoch 73, Batch 25/32 : Loss = 0.002002333290874958\n",
      "Epoch 73, Batch 26/32 : Loss = 0.00576774450019002\n",
      "Epoch 73, Batch 27/32 : Loss = 0.024163663387298584\n",
      "Epoch 73, Batch 28/32 : Loss = 0.005435708910226822\n",
      "Epoch 73, Batch 29/32 : Loss = 0.010224019177258015\n",
      "Epoch 73, Batch 30/32 : Loss = 0.00393294170498848\n",
      "Epoch 73, Batch 31/32 : Loss = 0.03838355466723442\n",
      "Epoch 73 finished in 0.05231192906697591 minutes\n",
      "Epoch 73 training_loss = 0.012711991555988789\n",
      "----cc----R---;---9----y---?---22---d----i--O-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---'---]--t--4----e----^---W------QQ----44---->----g---- => ']t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---<<----v---O------T--``--4----V---[---0-------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "-----0---JJ---!!-((-;---A----33---,--'-))--r---r---7---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 73 val_loss = 0.6960062980651855, word_accuracy = 0.67\n",
      "Epoch 74, Batch 0/32 : Loss = 0.02456623502075672\n",
      "Epoch 74, Batch 1/32 : Loss = 0.05246178060770035\n",
      "Epoch 74, Batch 2/32 : Loss = 0.014318609610199928\n",
      "Epoch 74, Batch 3/32 : Loss = 0.017807142809033394\n",
      "Epoch 74, Batch 4/32 : Loss = 0.004205870442092419\n",
      "Epoch 74, Batch 5/32 : Loss = 0.02242118865251541\n",
      "Epoch 74, Batch 6/32 : Loss = 0.0038688096683472395\n",
      "Epoch 74, Batch 7/32 : Loss = 0.004975829739123583\n",
      "Epoch 74, Batch 8/32 : Loss = 0.028554659336805344\n",
      "Epoch 74, Batch 9/32 : Loss = 0.006905000656843185\n",
      "Epoch 74, Batch 10/32 : Loss = 0.005656654946506023\n",
      "Epoch 74, Batch 11/32 : Loss = 0.002892593387514353\n",
      "Epoch 74, Batch 12/32 : Loss = 0.0034150569699704647\n",
      "Epoch 74, Batch 13/32 : Loss = 0.21526707708835602\n",
      "Epoch 74, Batch 14/32 : Loss = 0.001891273190267384\n",
      "Epoch 74, Batch 15/32 : Loss = 0.0071818712167441845\n",
      "Epoch 74, Batch 16/32 : Loss = 0.006587384268641472\n",
      "Epoch 74, Batch 17/32 : Loss = 0.003891326254233718\n",
      "Epoch 74, Batch 18/32 : Loss = 0.0162360817193985\n",
      "Epoch 74, Batch 19/32 : Loss = 0.0027398383244872093\n",
      "Epoch 74, Batch 20/32 : Loss = 0.02346660941839218\n",
      "Epoch 74, Batch 21/32 : Loss = 0.02800877019762993\n",
      "Epoch 74, Batch 22/32 : Loss = 0.009488514624536037\n",
      "Epoch 74, Batch 23/32 : Loss = 0.004826103337109089\n",
      "Epoch 74, Batch 24/32 : Loss = 0.015345580875873566\n",
      "Epoch 74, Batch 25/32 : Loss = 0.008713983930647373\n",
      "Epoch 74, Batch 26/32 : Loss = 0.017144935205578804\n",
      "Epoch 74, Batch 27/32 : Loss = 0.01052052341401577\n",
      "Epoch 74, Batch 28/32 : Loss = 0.003752366406843066\n",
      "Epoch 74, Batch 29/32 : Loss = 0.015041482634842396\n",
      "Epoch 74, Batch 30/32 : Loss = 0.00573575496673584\n",
      "Epoch 74, Batch 31/32 : Loss = 0.005362290423363447\n",
      "Epoch 74 finished in 0.05287394920984904 minutes\n",
      "Epoch 74 training_loss = 0.018909530714154243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---8------K------l---Z------5-----p-------$-----a------}----W------. => 8KlZ5p$a}W., Ground Truth is 8KIZ5p$a}w,\n",
      "----55----9-----g-----mm------JJ----u----x----C----X---.--d-----\\--- => 59gmJuxCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "-:-------3------\\----$$----->-------S-----\\\\----M--------i---B------ => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----{---B-----YY----R----a-----y---h----#-----2---->-----E----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 74 val_loss = 0.6138960719108582, word_accuracy = 0.59\n",
      "Epoch 75, Batch 0/32 : Loss = 0.00992821715772152\n",
      "Epoch 75, Batch 1/32 : Loss = 0.004142729565501213\n",
      "Epoch 75, Batch 2/32 : Loss = 0.015605633147060871\n",
      "Epoch 75, Batch 3/32 : Loss = 0.007855304516851902\n",
      "Epoch 75, Batch 4/32 : Loss = 0.005410379264503717\n",
      "Epoch 75, Batch 5/32 : Loss = 0.006699133198708296\n",
      "Epoch 75, Batch 6/32 : Loss = 0.09476788341999054\n",
      "Epoch 75, Batch 7/32 : Loss = 0.006658419035375118\n",
      "Epoch 75, Batch 8/32 : Loss = 0.00790313072502613\n",
      "Epoch 75, Batch 9/32 : Loss = 0.007126355543732643\n",
      "Epoch 75, Batch 10/32 : Loss = 0.0730176642537117\n",
      "Epoch 75, Batch 11/32 : Loss = 0.006351981777697802\n",
      "Epoch 75, Batch 12/32 : Loss = 0.2036406695842743\n",
      "Epoch 75, Batch 13/32 : Loss = 0.0020765517838299274\n",
      "Epoch 75, Batch 14/32 : Loss = 0.11862218379974365\n",
      "Epoch 75, Batch 15/32 : Loss = 0.0033037650864571333\n",
      "Epoch 75, Batch 16/32 : Loss = 0.011258823797106743\n",
      "Epoch 75, Batch 17/32 : Loss = 0.01165359653532505\n",
      "Epoch 75, Batch 18/32 : Loss = 0.012446843087673187\n",
      "Epoch 75, Batch 19/32 : Loss = 0.013312187045812607\n",
      "Epoch 75, Batch 20/32 : Loss = 0.008847717195749283\n",
      "Epoch 75, Batch 21/32 : Loss = 0.14679881930351257\n",
      "Epoch 75, Batch 22/32 : Loss = 0.02621329575777054\n",
      "Epoch 75, Batch 23/32 : Loss = 0.00770103745162487\n",
      "Epoch 75, Batch 24/32 : Loss = 0.003393169492483139\n",
      "Epoch 75, Batch 25/32 : Loss = 0.0034799608401954174\n",
      "Epoch 75, Batch 26/32 : Loss = 0.025478072464466095\n",
      "Epoch 75, Batch 27/32 : Loss = 0.00445745512843132\n",
      "Epoch 75, Batch 28/32 : Loss = 0.0037474778946489096\n",
      "Epoch 75, Batch 29/32 : Loss = 0.004349636845290661\n",
      "Epoch 75, Batch 30/32 : Loss = 0.006893022917211056\n",
      "Epoch 75, Batch 31/32 : Loss = 0.0022305776365101337\n",
      "Epoch 75 finished in 0.05206932624181112 minutes\n",
      "Epoch 75 training_loss = 0.027740396559238434\n",
      "-----Q-----3----g----I--z--#-----Y---:--]--q----++---*-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---55----9---g----mm-----J---uu---x---c---x--.--d---\\--- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "---<<-----v--O------T--``--4-----V--[[--0-------@------- => <vOT`4V[0-@, Ground Truth is <vOT`4V[0-Q\n",
      "---0----c----++----b----I--\"---b----66---.---Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 75 val_loss = 0.6178541779518127, word_accuracy = 0.67\n",
      "Epoch 76, Batch 0/32 : Loss = 0.0041739740408957005\n",
      "Epoch 76, Batch 1/32 : Loss = 0.033470626920461655\n",
      "Epoch 76, Batch 2/32 : Loss = 0.005930173210799694\n",
      "Epoch 76, Batch 3/32 : Loss = 0.07326631247997284\n",
      "Epoch 76, Batch 4/32 : Loss = 0.022547952830791473\n",
      "Epoch 76, Batch 5/32 : Loss = 0.09359414875507355\n",
      "Epoch 76, Batch 6/32 : Loss = 0.004285529255867004\n",
      "Epoch 76, Batch 7/32 : Loss = 0.0037303194403648376\n",
      "Epoch 76, Batch 8/32 : Loss = 0.005679719150066376\n",
      "Epoch 76, Batch 9/32 : Loss = 0.004008351359516382\n",
      "Epoch 76, Batch 10/32 : Loss = 0.0037756769452244043\n",
      "Epoch 76, Batch 11/32 : Loss = 0.011136773973703384\n",
      "Epoch 76, Batch 12/32 : Loss = 0.025254618376493454\n",
      "Epoch 76, Batch 13/32 : Loss = 0.004274264443665743\n",
      "Epoch 76, Batch 14/32 : Loss = 0.010012386366724968\n",
      "Epoch 76, Batch 15/32 : Loss = 0.010714955627918243\n",
      "Epoch 76, Batch 16/32 : Loss = 0.025587210431694984\n",
      "Epoch 76, Batch 17/32 : Loss = 0.012676174752414227\n",
      "Epoch 76, Batch 18/32 : Loss = 0.00913102738559246\n",
      "Epoch 76, Batch 19/32 : Loss = 0.0024115138221532106\n",
      "Epoch 76, Batch 20/32 : Loss = 0.003116547130048275\n",
      "Epoch 76, Batch 21/32 : Loss = 0.0031918250024318695\n",
      "Epoch 76, Batch 22/32 : Loss = 0.015493215061724186\n",
      "Epoch 76, Batch 23/32 : Loss = 0.054689306765794754\n",
      "Epoch 76, Batch 24/32 : Loss = 0.05122219771146774\n",
      "Epoch 76, Batch 25/32 : Loss = 0.010126162320375443\n",
      "Epoch 76, Batch 26/32 : Loss = 0.005027415230870247\n",
      "Epoch 76, Batch 27/32 : Loss = 0.008615726605057716\n",
      "Epoch 76, Batch 28/32 : Loss = 0.005009178537875414\n",
      "Epoch 76, Batch 29/32 : Loss = 0.0092301731929183\n",
      "Epoch 76, Batch 30/32 : Loss = 0.016965461894869804\n",
      "Epoch 76, Batch 31/32 : Loss = 0.0028745410963892937\n",
      "Epoch 76 finished in 0.05218935410181681 minutes\n",
      "Epoch 76 training_loss = 0.01762918010354042\n",
      "--.--CC----O-------w-----uu---`---uu---.--RR----{---3----\"\"---#------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----#-----G-----9----EE---I--=----hh----5---##----22---JJ---)---k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-----4----r---{-------%---//--'--))---w------&-----NN-----++----P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----\"----]---t---4-----e-----^----W--------Q-------4----->-----gg---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 76 val_loss = 0.6139814257621765, word_accuracy = 0.75\n",
      "Epoch 77, Batch 0/32 : Loss = 0.005683396942913532\n",
      "Epoch 77, Batch 1/32 : Loss = 0.002634401200339198\n",
      "Epoch 77, Batch 2/32 : Loss = 0.001477455603890121\n",
      "Epoch 77, Batch 3/32 : Loss = 0.007076197303831577\n",
      "Epoch 77, Batch 4/32 : Loss = 0.003272998845204711\n",
      "Epoch 77, Batch 5/32 : Loss = 0.008057763800024986\n",
      "Epoch 77, Batch 6/32 : Loss = 0.0025181600358337164\n",
      "Epoch 77, Batch 7/32 : Loss = 0.0032235048711299896\n",
      "Epoch 77, Batch 8/32 : Loss = 0.0073819030076265335\n",
      "Epoch 77, Batch 9/32 : Loss = 0.0035219062119722366\n",
      "Epoch 77, Batch 10/32 : Loss = 0.002828731667250395\n",
      "Epoch 77, Batch 11/32 : Loss = 0.0034671607427299023\n",
      "Epoch 77, Batch 12/32 : Loss = 0.002098832745105028\n",
      "Epoch 77, Batch 13/32 : Loss = 0.0020439799409359694\n",
      "Epoch 77, Batch 14/32 : Loss = 0.0018700093496590853\n",
      "Epoch 77, Batch 15/32 : Loss = 0.0031470241956412792\n",
      "Epoch 77, Batch 16/32 : Loss = 0.019304394721984863\n",
      "Epoch 77, Batch 17/32 : Loss = 0.020341508090496063\n",
      "Epoch 77, Batch 18/32 : Loss = 0.0022786620538681746\n",
      "Epoch 77, Batch 19/32 : Loss = 0.004026408307254314\n",
      "Epoch 77, Batch 20/32 : Loss = 0.006243834272027016\n",
      "Epoch 77, Batch 21/32 : Loss = 0.0017820736393332481\n",
      "Epoch 77, Batch 22/32 : Loss = 0.001995411003008485\n",
      "Epoch 77, Batch 23/32 : Loss = 0.01117316447198391\n",
      "Epoch 77, Batch 24/32 : Loss = 0.004566015675663948\n",
      "Epoch 77, Batch 25/32 : Loss = 0.004679176025092602\n",
      "Epoch 77, Batch 26/32 : Loss = 0.0020245322957634926\n",
      "Epoch 77, Batch 27/32 : Loss = 0.021697035059332848\n",
      "Epoch 77, Batch 28/32 : Loss = 0.002329715993255377\n",
      "Epoch 77, Batch 29/32 : Loss = 0.0038284806068986654\n",
      "Epoch 77, Batch 30/32 : Loss = 0.0037154117599129677\n",
      "Epoch 77, Batch 31/32 : Loss = 0.02383074164390564\n",
      "Epoch 77 finished in 0.05358275572458903 minutes\n",
      "Epoch 77 training_loss = 0.005566846113651991\n",
      "---2---p--::-mm----x---a---z--nn---@-----C---yy----%-----%---- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----4----r--{{-----%%---/--'--)---w-----&-----N-----+----P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----o----\"---}--!---B----r---&-----9---;--`--OO-----ww----}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----J--;;--q----+---|----z--yy---U-------%%---U-----1---x----- => J;q+|zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 77 val_loss = 0.6502784490585327, word_accuracy = 0.7\n",
      "Epoch 78, Batch 0/32 : Loss = 0.003160054562613368\n",
      "Epoch 78, Batch 1/32 : Loss = 0.00869591161608696\n",
      "Epoch 78, Batch 2/32 : Loss = 0.3580804467201233\n",
      "Epoch 78, Batch 3/32 : Loss = 0.00226282118819654\n",
      "Epoch 78, Batch 4/32 : Loss = 0.006055735982954502\n",
      "Epoch 78, Batch 5/32 : Loss = 0.0024296678602695465\n",
      "Epoch 78, Batch 6/32 : Loss = 0.01578228734433651\n",
      "Epoch 78, Batch 7/32 : Loss = 0.0022832145914435387\n",
      "Epoch 78, Batch 8/32 : Loss = 0.014598563313484192\n",
      "Epoch 78, Batch 9/32 : Loss = 0.008196219801902771\n",
      "Epoch 78, Batch 10/32 : Loss = 0.009539535269141197\n",
      "Epoch 78, Batch 11/32 : Loss = 0.008685078471899033\n",
      "Epoch 78, Batch 12/32 : Loss = 0.06038067489862442\n",
      "Epoch 78, Batch 13/32 : Loss = 0.005754552315920591\n",
      "Epoch 78, Batch 14/32 : Loss = 0.005157901905477047\n",
      "Epoch 78, Batch 15/32 : Loss = 0.004837319254875183\n",
      "Epoch 78, Batch 16/32 : Loss = 0.0042740823701024055\n",
      "Epoch 78, Batch 17/32 : Loss = 0.010563591495156288\n",
      "Epoch 78, Batch 18/32 : Loss = 0.004649709910154343\n",
      "Epoch 78, Batch 19/32 : Loss = 0.005742340348660946\n",
      "Epoch 78, Batch 20/32 : Loss = 0.010496633127331734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Batch 21/32 : Loss = 0.006910921540111303\n",
      "Epoch 78, Batch 22/32 : Loss = 0.0028200792148709297\n",
      "Epoch 78, Batch 23/32 : Loss = 0.01301844883710146\n",
      "Epoch 78, Batch 24/32 : Loss = 0.010514707304537296\n",
      "Epoch 78, Batch 25/32 : Loss = 0.0051507605239748955\n",
      "Epoch 78, Batch 26/32 : Loss = 0.0027271697763353586\n",
      "Epoch 78, Batch 27/32 : Loss = 0.002728804014623165\n",
      "Epoch 78, Batch 28/32 : Loss = 0.003782701212912798\n",
      "Epoch 78, Batch 29/32 : Loss = 0.011304257437586784\n",
      "Epoch 78, Batch 30/32 : Loss = 0.011020956560969353\n",
      "Epoch 78, Batch 31/32 : Loss = 0.0749882385134697\n",
      "Epoch 78 finished in 0.05252495209376017 minutes\n",
      "Epoch 78 training_loss = 0.020272405818104744\n",
      "----JJ--;;--qq-----+---//----z---yy---U---------%----UU-----1----x---_-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "----W------=-----2----+----E----11---n----TT----X---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "---J---;---q-----+----/----z---yy---UU--------%----UU-----1----x---__--- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "-:-------3-----\\\\----$$------>>------SS-----\\-----MMM------ii---BB------ => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 78 val_loss = 0.6075511574745178, word_accuracy = 0.76\n",
      "Epoch 79, Batch 0/32 : Loss = 0.0042483191937208176\n",
      "Epoch 79, Batch 1/32 : Loss = 0.020031750202178955\n",
      "Epoch 79, Batch 2/32 : Loss = 0.0036654965952038765\n",
      "Epoch 79, Batch 3/32 : Loss = 0.0023337630555033684\n",
      "Epoch 79, Batch 4/32 : Loss = 0.007629032712429762\n",
      "Epoch 79, Batch 5/32 : Loss = 0.0035499557852745056\n",
      "Epoch 79, Batch 6/32 : Loss = 0.010969942435622215\n",
      "Epoch 79, Batch 7/32 : Loss = 0.006836947984993458\n",
      "Epoch 79, Batch 8/32 : Loss = 0.001838517957367003\n",
      "Epoch 79, Batch 9/32 : Loss = 0.003017003647983074\n",
      "Epoch 79, Batch 10/32 : Loss = 0.02934359386563301\n",
      "Epoch 79, Batch 11/32 : Loss = 0.0024214242585003376\n",
      "Epoch 79, Batch 12/32 : Loss = 0.004705196712166071\n",
      "Epoch 79, Batch 13/32 : Loss = 0.0019939234480261803\n",
      "Epoch 79, Batch 14/32 : Loss = 0.0033248737454414368\n",
      "Epoch 79, Batch 15/32 : Loss = 0.02842058055102825\n",
      "Epoch 79, Batch 16/32 : Loss = 0.0026648035272955894\n",
      "Epoch 79, Batch 17/32 : Loss = 0.006360508967190981\n",
      "Epoch 79, Batch 18/32 : Loss = 0.0022838558070361614\n",
      "Epoch 79, Batch 19/32 : Loss = 0.0017888587899506092\n",
      "Epoch 79, Batch 20/32 : Loss = 0.001918721478432417\n",
      "Epoch 79, Batch 21/32 : Loss = 0.005579415708780289\n",
      "Epoch 79, Batch 22/32 : Loss = 0.005074118729680777\n",
      "Epoch 79, Batch 23/32 : Loss = 0.002741302363574505\n",
      "Epoch 79, Batch 24/32 : Loss = 0.0016761964652687311\n",
      "Epoch 79, Batch 25/32 : Loss = 0.009442264214158058\n",
      "Epoch 79, Batch 26/32 : Loss = 0.003173272591084242\n",
      "Epoch 79, Batch 27/32 : Loss = 0.010885193012654781\n",
      "Epoch 79, Batch 28/32 : Loss = 0.01632031798362732\n",
      "Epoch 79, Batch 29/32 : Loss = 0.002061540260910988\n",
      "Epoch 79, Batch 30/32 : Loss = 0.00919192936271429\n",
      "Epoch 79, Batch 31/32 : Loss = 0.00554612185806036\n",
      "Epoch 79 finished in 0.053238693873087564 minutes\n",
      "Epoch 79 training_loss = 0.006945731583982706\n",
      "-----+---:---z----77---8-----d-----S------v--55----S----JJ----B------- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----/----MMM------o-----E-----^---33----xx---//---&-----66-----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----XX---77---0---j---@-----S----ZZ---L---44----C----mm------MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---r----GG-----II---T-----#-----33-----PP------Q-------w-----''-i----- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "Epoch 79 val_loss = 0.6224204301834106, word_accuracy = 0.75\n",
      "Epoch 80, Batch 0/32 : Loss = 0.0011593806557357311\n",
      "Epoch 80, Batch 1/32 : Loss = 0.002064578700810671\n",
      "Epoch 80, Batch 2/32 : Loss = 0.0034553944133222103\n",
      "Epoch 80, Batch 3/32 : Loss = 0.006997299380600452\n",
      "Epoch 80, Batch 4/32 : Loss = 0.0023122774437069893\n",
      "Epoch 80, Batch 5/32 : Loss = 0.0027241709176450968\n",
      "Epoch 80, Batch 6/32 : Loss = 0.002598921535536647\n",
      "Epoch 80, Batch 7/32 : Loss = 0.007712093181908131\n",
      "Epoch 80, Batch 8/32 : Loss = 0.002512603998184204\n",
      "Epoch 80, Batch 9/32 : Loss = 0.0026718417648226023\n",
      "Epoch 80, Batch 10/32 : Loss = 0.0022420722525566816\n",
      "Epoch 80, Batch 11/32 : Loss = 0.002677168697118759\n",
      "Epoch 80, Batch 12/32 : Loss = 0.0017143429722636938\n",
      "Epoch 80, Batch 13/32 : Loss = 0.02821342833340168\n",
      "Epoch 80, Batch 14/32 : Loss = 0.003579466138035059\n",
      "Epoch 80, Batch 15/32 : Loss = 0.0015724273398518562\n",
      "Epoch 80, Batch 16/32 : Loss = 0.0035696898121386766\n",
      "Epoch 80, Batch 17/32 : Loss = 0.0012382888235151768\n",
      "Epoch 80, Batch 18/32 : Loss = 0.0011044528800994158\n",
      "Epoch 80, Batch 19/32 : Loss = 0.013543623499572277\n",
      "Epoch 80, Batch 20/32 : Loss = 0.002261381596326828\n",
      "Epoch 80, Batch 21/32 : Loss = 0.002199630718678236\n",
      "Epoch 80, Batch 22/32 : Loss = 0.0017609621863812208\n",
      "Epoch 80, Batch 23/32 : Loss = 0.0020775236189365387\n",
      "Epoch 80, Batch 24/32 : Loss = 0.002996613970026374\n",
      "Epoch 80, Batch 25/32 : Loss = 0.0038262398447841406\n",
      "Epoch 80, Batch 26/32 : Loss = 0.004945487715303898\n",
      "Epoch 80, Batch 27/32 : Loss = 0.003966988064348698\n",
      "Epoch 80, Batch 28/32 : Loss = 0.001197858713567257\n",
      "Epoch 80, Batch 29/32 : Loss = 0.001563478261232376\n",
      "Epoch 80, Batch 30/32 : Loss = 0.001923111965879798\n",
      "Epoch 80, Batch 31/32 : Loss = 0.023896988481283188\n",
      "Epoch 80 finished in 0.05270789464314778 minutes\n",
      "Epoch 80 training_loss = 0.004027948714792728\n",
      "---{---B------Y----RR----a-----y---h----#-----2---->-----EE----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----0----Q-----6----<----<<---(---T----N----5---==---P----(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----2---p----:-mm-----x---a----z--nn----@-----C----y------%-----%---- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "------Q------3-----gg----I---z---##-----Y----:--]]---q-----++---**--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 80 val_loss = 0.6475046277046204, word_accuracy = 0.76\n",
      "Epoch 81, Batch 0/32 : Loss = 0.0015875013777986169\n",
      "Epoch 81, Batch 1/32 : Loss = 0.008963355794548988\n",
      "Epoch 81, Batch 2/32 : Loss = 0.0023565057199448347\n",
      "Epoch 81, Batch 3/32 : Loss = 0.002579310443252325\n",
      "Epoch 81, Batch 4/32 : Loss = 0.003349300939589739\n",
      "Epoch 81, Batch 5/32 : Loss = 0.005879545118659735\n",
      "Epoch 81, Batch 6/32 : Loss = 0.001499649602919817\n",
      "Epoch 81, Batch 7/32 : Loss = 0.002349197631701827\n",
      "Epoch 81, Batch 8/32 : Loss = 0.0008454069611616433\n",
      "Epoch 81, Batch 9/32 : Loss = 0.0060448190197348595\n",
      "Epoch 81, Batch 10/32 : Loss = 0.0017320923507213593\n",
      "Epoch 81, Batch 11/32 : Loss = 0.00168828503228724\n",
      "Epoch 81, Batch 12/32 : Loss = 0.002000350272282958\n",
      "Epoch 81, Batch 13/32 : Loss = 0.0015761185204610229\n",
      "Epoch 81, Batch 14/32 : Loss = 0.0011403544340282679\n",
      "Epoch 81, Batch 15/32 : Loss = 0.0025023669004440308\n",
      "Epoch 81, Batch 16/32 : Loss = 0.08785264194011688\n",
      "Epoch 81, Batch 17/32 : Loss = 0.0012243042001500726\n",
      "Epoch 81, Batch 18/32 : Loss = 0.002428930252790451\n",
      "Epoch 81, Batch 19/32 : Loss = 0.002437042538076639\n",
      "Epoch 81, Batch 20/32 : Loss = 0.0022161100059747696\n",
      "Epoch 81, Batch 21/32 : Loss = 0.0009256685152649879\n",
      "Epoch 81, Batch 22/32 : Loss = 0.0023946219589561224\n",
      "Epoch 81, Batch 23/32 : Loss = 0.0014945808798074722\n",
      "Epoch 81, Batch 24/32 : Loss = 0.003088469849899411\n",
      "Epoch 81, Batch 25/32 : Loss = 0.0010328483767807484\n",
      "Epoch 81, Batch 26/32 : Loss = 0.02084115520119667\n",
      "Epoch 81, Batch 27/32 : Loss = 0.022797085344791412\n",
      "Epoch 81, Batch 28/32 : Loss = 0.0009992819977924228\n",
      "Epoch 81, Batch 29/32 : Loss = 0.0050261239521205425\n",
      "Epoch 81, Batch 30/32 : Loss = 0.0043080514296889305\n",
      "Epoch 81, Batch 31/32 : Loss = 0.02224155142903328\n",
      "Epoch 81 finished in 0.05227942069371541 minutes\n",
      "Epoch 81 training_loss = 0.006680844351649284\n",
      "-----+----:---z----7----8-----d-----SS-----v---5-----S----JJ----BB------- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "------0-----JJ-----!--((---;---A------33----,--'\"--))---r----r----77----- => 0J!(;A3,'\")rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----WW------=----2----+----EE----1---nn----T----X---r---C-----a---nn----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "----XX----7---0----j---@-----S-----Z----L---4----CC----mm------MM-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 81 val_loss = 0.6393009424209595, word_accuracy = 0.76\n",
      "Epoch 82, Batch 0/32 : Loss = 0.0015194705920293927\n",
      "Epoch 82, Batch 1/32 : Loss = 0.05196832865476608\n",
      "Epoch 82, Batch 2/32 : Loss = 0.0009090556995943189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Batch 3/32 : Loss = 0.0035120511893182993\n",
      "Epoch 82, Batch 4/32 : Loss = 0.001328007667325437\n",
      "Epoch 82, Batch 5/32 : Loss = 0.0013182045659050345\n",
      "Epoch 82, Batch 6/32 : Loss = 0.0037558753974735737\n",
      "Epoch 82, Batch 7/32 : Loss = 0.001913214335218072\n",
      "Epoch 82, Batch 8/32 : Loss = 0.0012191245332360268\n",
      "Epoch 82, Batch 9/32 : Loss = 0.1531297117471695\n",
      "Epoch 82, Batch 10/32 : Loss = 0.0011066584847867489\n",
      "Epoch 82, Batch 11/32 : Loss = 0.24758535623550415\n",
      "Epoch 82, Batch 12/32 : Loss = 0.001259160926565528\n",
      "Epoch 82, Batch 13/32 : Loss = 0.00163193978369236\n",
      "Epoch 82, Batch 14/32 : Loss = 0.004373685922473669\n",
      "Epoch 82, Batch 15/32 : Loss = 0.004190109204500914\n",
      "Epoch 82, Batch 16/32 : Loss = 0.03840221092104912\n",
      "Epoch 82, Batch 17/32 : Loss = 0.009527877904474735\n",
      "Epoch 82, Batch 18/32 : Loss = 0.007396688684821129\n",
      "Epoch 82, Batch 19/32 : Loss = 0.004696585237979889\n",
      "Epoch 82, Batch 20/32 : Loss = 0.02206672728061676\n",
      "Epoch 82, Batch 21/32 : Loss = 0.009307876229286194\n",
      "Epoch 82, Batch 22/32 : Loss = 0.04164842888712883\n",
      "Epoch 82, Batch 23/32 : Loss = 0.007901856675744057\n",
      "Epoch 82, Batch 24/32 : Loss = 0.03142116218805313\n",
      "Epoch 82, Batch 25/32 : Loss = 0.10549119114875793\n",
      "Epoch 82, Batch 26/32 : Loss = 0.007284590974450111\n",
      "Epoch 82, Batch 27/32 : Loss = 0.00809293519705534\n",
      "Epoch 82, Batch 28/32 : Loss = 0.11327651143074036\n",
      "Epoch 82, Batch 29/32 : Loss = 0.007852314971387386\n",
      "Epoch 82, Batch 30/32 : Loss = 0.019635967910289764\n",
      "Epoch 82, Batch 31/32 : Loss = 0.005974742118269205\n",
      "Epoch 82 finished in 0.05200192133585612 minutes\n",
      "Epoch 82 training_loss = 0.029412679374217987\n",
      "----Y----WW-----]]-ii-\\----|----M------<-----MM-----8----8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----d---!!--N-----r---AA---j--*----$----3---hh----5----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----<<-----v---O------TT--``---z----VV---[---0--------Q------ => <vOT`zV[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "--2---p---:--mm----x---a---zz--n----@----C----y-----%-----%--- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "Epoch 82 val_loss = 0.6319736838340759, word_accuracy = 0.72\n",
      "Epoch 83, Batch 0/32 : Loss = 0.0031638978980481625\n",
      "Epoch 83, Batch 1/32 : Loss = 0.22969353199005127\n",
      "Epoch 83, Batch 2/32 : Loss = 0.009801099076867104\n",
      "Epoch 83, Batch 3/32 : Loss = 0.0065323226153850555\n",
      "Epoch 83, Batch 4/32 : Loss = 0.025245096534490585\n",
      "Epoch 83, Batch 5/32 : Loss = 0.01912628673017025\n",
      "Epoch 83, Batch 6/32 : Loss = 0.01327426452189684\n",
      "Epoch 83, Batch 7/32 : Loss = 0.012405376881361008\n",
      "Epoch 83, Batch 8/32 : Loss = 0.015789171680808067\n",
      "Epoch 83, Batch 9/32 : Loss = 0.01174368616193533\n",
      "Epoch 83, Batch 10/32 : Loss = 0.021738242357969284\n",
      "Epoch 83, Batch 11/32 : Loss = 0.018315071240067482\n",
      "Epoch 83, Batch 12/32 : Loss = 0.00856779981404543\n",
      "Epoch 83, Batch 13/32 : Loss = 0.0060100071132183075\n",
      "Epoch 83, Batch 14/32 : Loss = 0.007631564512848854\n",
      "Epoch 83, Batch 15/32 : Loss = 0.006095360033214092\n",
      "Epoch 83, Batch 16/32 : Loss = 0.008892384357750416\n",
      "Epoch 83, Batch 17/32 : Loss = 0.007946733385324478\n",
      "Epoch 83, Batch 18/32 : Loss = 0.003641050308942795\n",
      "Epoch 83, Batch 19/32 : Loss = 0.00475082965567708\n",
      "Epoch 83, Batch 20/32 : Loss = 0.1780676692724228\n",
      "Epoch 83, Batch 21/32 : Loss = 0.011444984935224056\n",
      "Epoch 83, Batch 22/32 : Loss = 0.015566807240247726\n",
      "Epoch 83, Batch 23/32 : Loss = 0.004631033167243004\n",
      "Epoch 83, Batch 24/32 : Loss = 0.007404643576592207\n",
      "Epoch 83, Batch 25/32 : Loss = 0.04367993026971817\n",
      "Epoch 83, Batch 26/32 : Loss = 0.004864094313234091\n",
      "Epoch 83, Batch 27/32 : Loss = 0.0060193478129804134\n",
      "Epoch 83, Batch 28/32 : Loss = 0.11130911856889725\n",
      "Epoch 83, Batch 29/32 : Loss = 0.0035569516476243734\n",
      "Epoch 83, Batch 30/32 : Loss = 0.008794493973255157\n",
      "Epoch 83, Batch 31/32 : Loss = 0.05200113728642464\n",
      "Epoch 83 finished in 0.05220819314320882 minutes\n",
      "Epoch 83 training_loss = 0.027058729901909828\n",
      "------8------K------l--ZZ-----55-----p-----$$-----a-----}----w------,--- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----W------==---22----+----E----11---n-----T---XX---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "-----0----Q----66----<----<<---(---TT---NN----5---==---P---((--m-------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----0o----Q----66----<----<<---(----T----N----5----=----P---(--mm------- => 0oQ6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 83 val_loss = 0.6482922434806824, word_accuracy = 0.66\n",
      "Epoch 84, Batch 0/32 : Loss = 0.006005933508276939\n",
      "Epoch 84, Batch 1/32 : Loss = 0.014515168033540249\n",
      "Epoch 84, Batch 2/32 : Loss = 0.024841610342264175\n",
      "Epoch 84, Batch 3/32 : Loss = 0.013846916146576405\n",
      "Epoch 84, Batch 4/32 : Loss = 0.0074362982995808125\n",
      "Epoch 84, Batch 5/32 : Loss = 0.0041753267869353294\n",
      "Epoch 84, Batch 6/32 : Loss = 0.017147861421108246\n",
      "Epoch 84, Batch 7/32 : Loss = 0.006492068525403738\n",
      "Epoch 84, Batch 8/32 : Loss = 0.018538735806941986\n",
      "Epoch 84, Batch 9/32 : Loss = 0.004333755001425743\n",
      "Epoch 84, Batch 10/32 : Loss = 0.008389167487621307\n",
      "Epoch 84, Batch 11/32 : Loss = 0.02478678897023201\n",
      "Epoch 84, Batch 12/32 : Loss = 0.021342307329177856\n",
      "Epoch 84, Batch 13/32 : Loss = 0.004478201735764742\n",
      "Epoch 84, Batch 14/32 : Loss = 0.016418179497122765\n",
      "Epoch 84, Batch 15/32 : Loss = 0.003814307041466236\n",
      "Epoch 84, Batch 16/32 : Loss = 0.028380470350384712\n",
      "Epoch 84, Batch 17/32 : Loss = 0.056130263954401016\n",
      "Epoch 84, Batch 18/32 : Loss = 0.0021364963613450527\n",
      "Epoch 84, Batch 19/32 : Loss = 0.00470747472718358\n",
      "Epoch 84, Batch 20/32 : Loss = 0.04684937372803688\n",
      "Epoch 84, Batch 21/32 : Loss = 0.0077505153603851795\n",
      "Epoch 84, Batch 22/32 : Loss = 0.002222859300673008\n",
      "Epoch 84, Batch 23/32 : Loss = 0.002872193930670619\n",
      "Epoch 84, Batch 24/32 : Loss = 0.009158222004771233\n",
      "Epoch 84, Batch 25/32 : Loss = 0.004626614507287741\n",
      "Epoch 84, Batch 26/32 : Loss = 0.06713363528251648\n",
      "Epoch 84, Batch 27/32 : Loss = 0.002576373517513275\n",
      "Epoch 84, Batch 28/32 : Loss = 0.005334754008799791\n",
      "Epoch 84, Batch 29/32 : Loss = 0.004987374413758516\n",
      "Epoch 84, Batch 30/32 : Loss = 0.009379087015986443\n",
      "Epoch 84, Batch 31/32 : Loss = 0.0027343668043613434\n",
      "Epoch 84 finished in 0.05352017879486084 minutes\n",
      "Epoch 84 training_loss = 0.01449478417634964\n",
      "-----d---:---X----9-----e----a-----F--,--8--------V----RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "------#----0-----[---x---))---R-----8----i---P-----w-----))--- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "-----z---00----\"---GGG-----/---~~----$$----c-----11---j--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----+---;--z-----7---8----d-----S-----v--55---SS---JJ----B---- => +;z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 84 val_loss = 0.6344051957130432, word_accuracy = 0.69\n",
      "Epoch 85, Batch 0/32 : Loss = 0.0036248182877898216\n",
      "Epoch 85, Batch 1/32 : Loss = 0.0035378113389015198\n",
      "Epoch 85, Batch 2/32 : Loss = 0.02265200950205326\n",
      "Epoch 85, Batch 3/32 : Loss = 0.002240297384560108\n",
      "Epoch 85, Batch 4/32 : Loss = 0.0026705774944275618\n",
      "Epoch 85, Batch 5/32 : Loss = 0.013557006604969501\n",
      "Epoch 85, Batch 6/32 : Loss = 0.005567873362451792\n",
      "Epoch 85, Batch 7/32 : Loss = 0.01662697270512581\n",
      "Epoch 85, Batch 8/32 : Loss = 0.11294865608215332\n",
      "Epoch 85, Batch 9/32 : Loss = 0.0688013955950737\n",
      "Epoch 85, Batch 10/32 : Loss = 0.003443884663283825\n",
      "Epoch 85, Batch 11/32 : Loss = 0.002812914550304413\n",
      "Epoch 85, Batch 12/32 : Loss = 0.003140309825539589\n",
      "Epoch 85, Batch 13/32 : Loss = 0.0025581959635019302\n",
      "Epoch 85, Batch 14/32 : Loss = 0.002070230897516012\n",
      "Epoch 85, Batch 15/32 : Loss = 0.003934875596314669\n",
      "Epoch 85, Batch 16/32 : Loss = 0.008656489662826061\n",
      "Epoch 85, Batch 17/32 : Loss = 0.001923270057886839\n",
      "Epoch 85, Batch 18/32 : Loss = 0.020375095307826996\n",
      "Epoch 85, Batch 19/32 : Loss = 0.023812144994735718\n",
      "Epoch 85, Batch 20/32 : Loss = 0.007650258485227823\n",
      "Epoch 85, Batch 21/32 : Loss = 0.09109999984502792\n",
      "Epoch 85, Batch 22/32 : Loss = 0.00779027258977294\n",
      "Epoch 85, Batch 23/32 : Loss = 0.009416066110134125\n",
      "Epoch 85, Batch 24/32 : Loss = 0.014528227038681507\n",
      "Epoch 85, Batch 25/32 : Loss = 0.0027772458270192146\n",
      "Epoch 85, Batch 26/32 : Loss = 0.00818025041371584\n",
      "Epoch 85, Batch 27/32 : Loss = 0.0063430145382881165\n",
      "Epoch 85, Batch 28/32 : Loss = 0.07597282528877258\n",
      "Epoch 85, Batch 29/32 : Loss = 0.0026849722489714622\n",
      "Epoch 85, Batch 30/32 : Loss = 0.011971988715231419\n",
      "Epoch 85, Batch 31/32 : Loss = 0.0161700788885355\n",
      "Epoch 85 finished in 0.05237917105356852 minutes\n",
      "Epoch 85 training_loss = 0.01816518045961857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_------3-----\\----$----->-----S-----\\-----M-----ii--BB---- => _-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----d----!--NN----r---A---j--*----$---3----hh---5----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---<<----v---OO------T---`---4----VV---[--0--------Q------ => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----B---.---Y---I--WW------6----F---hh----X---'--Y---22--- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 85 val_loss = 0.6465634107589722, word_accuracy = 0.67\n",
      "Epoch 86, Batch 0/32 : Loss = 0.006487096659839153\n",
      "Epoch 86, Batch 1/32 : Loss = 0.02427261509001255\n",
      "Epoch 86, Batch 2/32 : Loss = 0.0021634732838720083\n",
      "Epoch 86, Batch 3/32 : Loss = 0.003043974284082651\n",
      "Epoch 86, Batch 4/32 : Loss = 0.053140558302402496\n",
      "Epoch 86, Batch 5/32 : Loss = 0.014199460856616497\n",
      "Epoch 86, Batch 6/32 : Loss = 0.00855947658419609\n",
      "Epoch 86, Batch 7/32 : Loss = 0.010107488371431828\n",
      "Epoch 86, Batch 8/32 : Loss = 0.0015614585718140006\n",
      "Epoch 86, Batch 9/32 : Loss = 0.008680627681314945\n",
      "Epoch 86, Batch 10/32 : Loss = 0.0044794450514018536\n",
      "Epoch 86, Batch 11/32 : Loss = 0.0063113924115896225\n",
      "Epoch 86, Batch 12/32 : Loss = 0.010520050302147865\n",
      "Epoch 86, Batch 13/32 : Loss = 0.0021272674202919006\n",
      "Epoch 86, Batch 14/32 : Loss = 0.013063536956906319\n",
      "Epoch 86, Batch 15/32 : Loss = 0.0030467198230326176\n",
      "Epoch 86, Batch 16/32 : Loss = 0.006685096304863691\n",
      "Epoch 86, Batch 17/32 : Loss = 0.001984110102057457\n",
      "Epoch 86, Batch 18/32 : Loss = 0.0026904032565653324\n",
      "Epoch 86, Batch 19/32 : Loss = 0.02933523617684841\n",
      "Epoch 86, Batch 20/32 : Loss = 0.0021776873618364334\n",
      "Epoch 86, Batch 21/32 : Loss = 0.01069738157093525\n",
      "Epoch 86, Batch 22/32 : Loss = 0.013927368447184563\n",
      "Epoch 86, Batch 23/32 : Loss = 0.06772924214601517\n",
      "Epoch 86, Batch 24/32 : Loss = 0.011780931614339352\n",
      "Epoch 86, Batch 25/32 : Loss = 0.05256720632314682\n",
      "Epoch 86, Batch 26/32 : Loss = 0.002117599593475461\n",
      "Epoch 86, Batch 27/32 : Loss = 0.002245864365249872\n",
      "Epoch 86, Batch 28/32 : Loss = 0.0025235957000404596\n",
      "Epoch 86, Batch 29/32 : Loss = 0.0043814219534397125\n",
      "Epoch 86, Batch 30/32 : Loss = 0.004663211293518543\n",
      "Epoch 86, Batch 31/32 : Loss = 0.008811932988464832\n",
      "Epoch 86 finished in 0.052811233202616374 minutes\n",
      "Epoch 86 training_loss = 0.012477829121053219\n",
      "--..--c----O-----w-----u---`---u---.--R----{--3----\"---=----- => .cOwu`u.R{3\"=, Ground Truth is .cOwu`u.R{3\"#\n",
      "----kk---$---|--'',--99---Y----NN----W------mm-----T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---++---:--z----7---88---dd----S-----v---5---SS---JJ----B---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "---{--BB----Y----RR---a-----y--hh--##----2--->>----E----4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 86 val_loss = 0.6793546676635742, word_accuracy = 0.65\n",
      "Epoch 87, Batch 0/32 : Loss = 0.0015261624939739704\n",
      "Epoch 87, Batch 1/32 : Loss = 0.010432573966681957\n",
      "Epoch 87, Batch 2/32 : Loss = 0.009252119809389114\n",
      "Epoch 87, Batch 3/32 : Loss = 0.03756466507911682\n",
      "Epoch 87, Batch 4/32 : Loss = 0.009295455180108547\n",
      "Epoch 87, Batch 5/32 : Loss = 0.004280037712305784\n",
      "Epoch 87, Batch 6/32 : Loss = 0.003757019294425845\n",
      "Epoch 87, Batch 7/32 : Loss = 0.017070339992642403\n",
      "Epoch 87, Batch 8/32 : Loss = 0.003245832398533821\n",
      "Epoch 87, Batch 9/32 : Loss = 0.0023789964616298676\n",
      "Epoch 87, Batch 10/32 : Loss = 0.005078385584056377\n",
      "Epoch 87, Batch 11/32 : Loss = 0.004055622033774853\n",
      "Epoch 87, Batch 12/32 : Loss = 0.003513946430757642\n",
      "Epoch 87, Batch 13/32 : Loss = 0.04709751904010773\n",
      "Epoch 87, Batch 14/32 : Loss = 0.0018252970185130835\n",
      "Epoch 87, Batch 15/32 : Loss = 0.0026814087759703398\n",
      "Epoch 87, Batch 16/32 : Loss = 0.00993959978222847\n",
      "Epoch 87, Batch 17/32 : Loss = 0.0018537130672484636\n",
      "Epoch 87, Batch 18/32 : Loss = 0.0021693822927773\n",
      "Epoch 87, Batch 19/32 : Loss = 0.01611761935055256\n",
      "Epoch 87, Batch 20/32 : Loss = 0.01690864935517311\n",
      "Epoch 87, Batch 21/32 : Loss = 0.0013459637993946671\n",
      "Epoch 87, Batch 22/32 : Loss = 0.0016805697232484818\n",
      "Epoch 87, Batch 23/32 : Loss = 0.0015667208936065435\n",
      "Epoch 87, Batch 24/32 : Loss = 0.006069849245250225\n",
      "Epoch 87, Batch 25/32 : Loss = 0.0403149351477623\n",
      "Epoch 87, Batch 26/32 : Loss = 0.0017715897411108017\n",
      "Epoch 87, Batch 27/32 : Loss = 0.003215775592252612\n",
      "Epoch 87, Batch 28/32 : Loss = 0.0021901847794651985\n",
      "Epoch 87, Batch 29/32 : Loss = 0.0013682874850928783\n",
      "Epoch 87, Batch 30/32 : Loss = 0.0051240259781479836\n",
      "Epoch 87, Batch 31/32 : Loss = 0.010329489596188068\n",
      "Epoch 87 finished in 0.05233765443166097 minutes\n",
      "Epoch 87 training_loss = 0.008866937831044197\n",
      "---c-----R---;---9-----y---?----2----d----i--OO-----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---++---:--z---77--88---d-----S----v---5---SS---JJ---B---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "---o---\"\"--}--!--BB----r--&&----9---;-``--O-----w----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----Y----W-----]--i-\\---|----MM----<-----MM----8---8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 87 val_loss = 0.6732755899429321, word_accuracy = 0.66\n",
      "Epoch 88, Batch 0/32 : Loss = 0.0033478972036391497\n",
      "Epoch 88, Batch 1/32 : Loss = 0.0026316021103411913\n",
      "Epoch 88, Batch 2/32 : Loss = 0.0028150780126452446\n",
      "Epoch 88, Batch 3/32 : Loss = 0.0013223171699792147\n",
      "Epoch 88, Batch 4/32 : Loss = 0.003125468734651804\n",
      "Epoch 88, Batch 5/32 : Loss = 0.0019198314985260367\n",
      "Epoch 88, Batch 6/32 : Loss = 0.005110754165798426\n",
      "Epoch 88, Batch 7/32 : Loss = 0.013707718811929226\n",
      "Epoch 88, Batch 8/32 : Loss = 0.017292633652687073\n",
      "Epoch 88, Batch 9/32 : Loss = 0.0013154000043869019\n",
      "Epoch 88, Batch 10/32 : Loss = 0.004990838468074799\n",
      "Epoch 88, Batch 11/32 : Loss = 0.007273321971297264\n",
      "Epoch 88, Batch 12/32 : Loss = 0.001578566269017756\n",
      "Epoch 88, Batch 13/32 : Loss = 0.007847664877772331\n",
      "Epoch 88, Batch 14/32 : Loss = 0.0017346132081001997\n",
      "Epoch 88, Batch 15/32 : Loss = 0.01789979264140129\n",
      "Epoch 88, Batch 16/32 : Loss = 0.0015473137609660625\n",
      "Epoch 88, Batch 17/32 : Loss = 0.003971027210354805\n",
      "Epoch 88, Batch 18/32 : Loss = 0.004250047728419304\n",
      "Epoch 88, Batch 19/32 : Loss = 0.02008928172290325\n",
      "Epoch 88, Batch 20/32 : Loss = 0.016838161274790764\n",
      "Epoch 88, Batch 21/32 : Loss = 0.002465558471158147\n",
      "Epoch 88, Batch 22/32 : Loss = 0.0033614428248256445\n",
      "Epoch 88, Batch 23/32 : Loss = 0.0018362930277362466\n",
      "Epoch 88, Batch 24/32 : Loss = 0.0385747030377388\n",
      "Epoch 88, Batch 25/32 : Loss = 0.008776340633630753\n",
      "Epoch 88, Batch 26/32 : Loss = 0.013573901727795601\n",
      "Epoch 88, Batch 27/32 : Loss = 0.004199485294520855\n",
      "Epoch 88, Batch 28/32 : Loss = 0.003152694785967469\n",
      "Epoch 88, Batch 29/32 : Loss = 0.003264001803472638\n",
      "Epoch 88, Batch 30/32 : Loss = 0.02224303036928177\n",
      "Epoch 88, Batch 31/32 : Loss = 0.0009411380742676556\n",
      "Epoch 88 finished in 0.05200626850128174 minutes\n",
      "Epoch 88 training_loss = 0.007780705112963915\n",
      "-----kk----O------//--,---y----c----*-----1----PP----}---#-----BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----CC----DD----E----g----mm-----\"---m------F---<<----Q-----8---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----k----$-----|--'',,--9-----Y-----N-----W-------m-------T----8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "------zz----0-----\"-----G------//----~-----$$-----c------1----j---t--- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 88 val_loss = 0.6612330675125122, word_accuracy = 0.72\n",
      "Epoch 89, Batch 0/32 : Loss = 0.01658789813518524\n",
      "Epoch 89, Batch 1/32 : Loss = 0.0212055966258049\n",
      "Epoch 89, Batch 2/32 : Loss = 0.0033759200014173985\n",
      "Epoch 89, Batch 3/32 : Loss = 0.0029821940697729588\n",
      "Epoch 89, Batch 4/32 : Loss = 0.008570914156734943\n",
      "Epoch 89, Batch 5/32 : Loss = 0.003369677346199751\n",
      "Epoch 89, Batch 6/32 : Loss = 0.003848881460726261\n",
      "Epoch 89, Batch 7/32 : Loss = 0.021020326763391495\n",
      "Epoch 89, Batch 8/32 : Loss = 0.0021044143941253424\n",
      "Epoch 89, Batch 9/32 : Loss = 0.0020762598142027855\n",
      "Epoch 89, Batch 10/32 : Loss = 0.004946122877299786\n",
      "Epoch 89, Batch 11/32 : Loss = 0.012180454097688198\n",
      "Epoch 89, Batch 12/32 : Loss = 0.004534055944532156\n",
      "Epoch 89, Batch 13/32 : Loss = 0.005165104754269123\n",
      "Epoch 89, Batch 14/32 : Loss = 0.05062612518668175\n",
      "Epoch 89, Batch 15/32 : Loss = 0.0018314938060939312\n",
      "Epoch 89, Batch 16/32 : Loss = 0.014903577044606209\n",
      "Epoch 89, Batch 17/32 : Loss = 0.01505228877067566\n",
      "Epoch 89, Batch 18/32 : Loss = 0.00619499571621418\n",
      "Epoch 89, Batch 19/32 : Loss = 0.005542594939470291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Batch 20/32 : Loss = 0.0024613637942820787\n",
      "Epoch 89, Batch 21/32 : Loss = 0.0019015655852854252\n",
      "Epoch 89, Batch 22/32 : Loss = 0.014554268680512905\n",
      "Epoch 89, Batch 23/32 : Loss = 0.003442874876782298\n",
      "Epoch 89, Batch 24/32 : Loss = 0.010366709902882576\n",
      "Epoch 89, Batch 25/32 : Loss = 0.0012494722614064813\n",
      "Epoch 89, Batch 26/32 : Loss = 0.007924389094114304\n",
      "Epoch 89, Batch 27/32 : Loss = 0.0009248254355043173\n",
      "Epoch 89, Batch 28/32 : Loss = 0.0020136707462370396\n",
      "Epoch 89, Batch 29/32 : Loss = 0.019964709877967834\n",
      "Epoch 89, Batch 30/32 : Loss = 0.03239622339606285\n",
      "Epoch 89, Batch 31/32 : Loss = 0.000840077584143728\n",
      "Epoch 89 finished in 0.05625929832458496 minutes\n",
      "Epoch 89 training_loss = 0.009748561307787895\n",
      "----\"--]--t--4----e---^----W------Q----4---->----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "----d----!--N-----r--A----j-*---$$---3---hh---5----n--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "----E-----0----[--x---))---R----8----i--PP----w-----)-- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "-----d---:---X---9----e---a----FF-,--8-------V----R---- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 89 val_loss = 0.5968471169471741, word_accuracy = 0.82\n",
      "Epoch 90, Batch 0/32 : Loss = 0.0013284182641655207\n",
      "Epoch 90, Batch 1/32 : Loss = 0.0013351624365895987\n",
      "Epoch 90, Batch 2/32 : Loss = 0.001813830342143774\n",
      "Epoch 90, Batch 3/32 : Loss = 0.002612716518342495\n",
      "Epoch 90, Batch 4/32 : Loss = 0.010853618383407593\n",
      "Epoch 90, Batch 5/32 : Loss = 0.007283843122422695\n",
      "Epoch 90, Batch 6/32 : Loss = 0.0022120897192507982\n",
      "Epoch 90, Batch 7/32 : Loss = 0.034459397196769714\n",
      "Epoch 90, Batch 8/32 : Loss = 0.0018017420079559088\n",
      "Epoch 90, Batch 9/32 : Loss = 0.0009406388271600008\n",
      "Epoch 90, Batch 10/32 : Loss = 0.014235524460673332\n",
      "Epoch 90, Batch 11/32 : Loss = 0.026019545271992683\n",
      "Epoch 90, Batch 12/32 : Loss = 0.0097962012514472\n",
      "Epoch 90, Batch 13/32 : Loss = 0.007935691624879837\n",
      "Epoch 90, Batch 14/32 : Loss = 0.0019323228625580668\n",
      "Epoch 90, Batch 15/32 : Loss = 0.0029990377370268106\n",
      "Epoch 90, Batch 16/32 : Loss = 0.0023031237069517374\n",
      "Epoch 90, Batch 17/32 : Loss = 0.00354883074760437\n",
      "Epoch 90, Batch 18/32 : Loss = 0.0035423331428319216\n",
      "Epoch 90, Batch 19/32 : Loss = 0.003757953178137541\n",
      "Epoch 90, Batch 20/32 : Loss = 0.0015646594110876322\n",
      "Epoch 90, Batch 21/32 : Loss = 0.07979216426610947\n",
      "Epoch 90, Batch 22/32 : Loss = 0.001829901011660695\n",
      "Epoch 90, Batch 23/32 : Loss = 0.004752509295940399\n",
      "Epoch 90, Batch 24/32 : Loss = 0.005288147833198309\n",
      "Epoch 90, Batch 25/32 : Loss = 0.003129035234451294\n",
      "Epoch 90, Batch 26/32 : Loss = 0.0015575039433315396\n",
      "Epoch 90, Batch 27/32 : Loss = 0.0033054668456315994\n",
      "Epoch 90, Batch 28/32 : Loss = 0.04095687344670296\n",
      "Epoch 90, Batch 29/32 : Loss = 0.003040309762582183\n",
      "Epoch 90, Batch 30/32 : Loss = 0.002543193520978093\n",
      "Epoch 90, Batch 31/32 : Loss = 0.00384255382232368\n",
      "Epoch 90 finished in 0.05149089097976685 minutes\n",
      "Epoch 90 training_loss = 0.009283601306378841\n",
      "----W------=-----2----+----E----11---n-----T----X---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "----#-----G------9----EE---I--=----hh----5----#-----2----J----)---k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-------Q------3------g-----I---z---##-----Y----:---]---q-----++----**--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "-----d-----!---NN-----r----A----j---*----$$----3----hhh----5-----n------ => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 90 val_loss = 0.6528651118278503, word_accuracy = 0.73\n",
      "Epoch 91, Batch 0/32 : Loss = 0.006869356147944927\n",
      "Epoch 91, Batch 1/32 : Loss = 0.003248185385018587\n",
      "Epoch 91, Batch 2/32 : Loss = 0.0018843456637114286\n",
      "Epoch 91, Batch 3/32 : Loss = 0.009721960872411728\n",
      "Epoch 91, Batch 4/32 : Loss = 0.005581083707511425\n",
      "Epoch 91, Batch 5/32 : Loss = 0.0022506697569042444\n",
      "Epoch 91, Batch 6/32 : Loss = 0.008663903921842575\n",
      "Epoch 91, Batch 7/32 : Loss = 0.003062688745558262\n",
      "Epoch 91, Batch 8/32 : Loss = 0.0046565718948841095\n",
      "Epoch 91, Batch 9/32 : Loss = 0.07549719512462616\n",
      "Epoch 91, Batch 10/32 : Loss = 0.0034372578375041485\n",
      "Epoch 91, Batch 11/32 : Loss = 0.009210716933012009\n",
      "Epoch 91, Batch 12/32 : Loss = 0.0019071371061727405\n",
      "Epoch 91, Batch 13/32 : Loss = 0.0028623032849282026\n",
      "Epoch 91, Batch 14/32 : Loss = 0.013711021281778812\n",
      "Epoch 91, Batch 15/32 : Loss = 0.0038824263028800488\n",
      "Epoch 91, Batch 16/32 : Loss = 0.0030142455361783504\n",
      "Epoch 91, Batch 17/32 : Loss = 0.026457592844963074\n",
      "Epoch 91, Batch 18/32 : Loss = 0.0051286970265209675\n",
      "Epoch 91, Batch 19/32 : Loss = 0.0054458314552903175\n",
      "Epoch 91, Batch 20/32 : Loss = 0.0019129092106595635\n",
      "Epoch 91, Batch 21/32 : Loss = 0.021222872659564018\n",
      "Epoch 91, Batch 22/32 : Loss = 0.053172387182712555\n",
      "Epoch 91, Batch 23/32 : Loss = 0.0015919201541692019\n",
      "Epoch 91, Batch 24/32 : Loss = 0.012198850512504578\n",
      "Epoch 91, Batch 25/32 : Loss = 0.0020630406215786934\n",
      "Epoch 91, Batch 26/32 : Loss = 0.0021273535676300526\n",
      "Epoch 91, Batch 27/32 : Loss = 0.003247865242883563\n",
      "Epoch 91, Batch 28/32 : Loss = 0.001399795524775982\n",
      "Epoch 91, Batch 29/32 : Loss = 0.009422463364899158\n",
      "Epoch 91, Batch 30/32 : Loss = 0.0013136491179466248\n",
      "Epoch 91, Batch 31/32 : Loss = 0.01054148655384779\n",
      "Epoch 91 finished in 0.050785462061564125 minutes\n",
      "Epoch 91 training_loss = 0.009879001416265965\n",
      "-----z-----0----\"\"----GG------/----~------$$-----c-----11---jj--t----- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----XX---77---0---j---@-----S----ZZ---LL--44----C----mm------MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----88-----K-----ll--ZZ-----5-----p------$-----a-----}----w-----,----- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "-----0-----JJ-----!--(--;;----A-----3-----,--'---)----r---rr----7----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 91 val_loss = 0.6564872860908508, word_accuracy = 0.72\n",
      "Epoch 92, Batch 0/32 : Loss = 0.00375946587882936\n",
      "Epoch 92, Batch 1/32 : Loss = 0.00452017318457365\n",
      "Epoch 92, Batch 2/32 : Loss = 0.0023277425207197666\n",
      "Epoch 92, Batch 3/32 : Loss = 0.0074254777282476425\n",
      "Epoch 92, Batch 4/32 : Loss = 0.0011499172542244196\n",
      "Epoch 92, Batch 5/32 : Loss = 0.002211370738223195\n",
      "Epoch 92, Batch 6/32 : Loss = 0.003079561050981283\n",
      "Epoch 92, Batch 7/32 : Loss = 0.001965465024113655\n",
      "Epoch 92, Batch 8/32 : Loss = 0.002941695274785161\n",
      "Epoch 92, Batch 9/32 : Loss = 0.0036502936854958534\n",
      "Epoch 92, Batch 10/32 : Loss = 0.012579687871038914\n",
      "Epoch 92, Batch 11/32 : Loss = 0.0033973893150687218\n",
      "Epoch 92, Batch 12/32 : Loss = 0.0012989267706871033\n",
      "Epoch 92, Batch 13/32 : Loss = 0.0022577387280762196\n",
      "Epoch 92, Batch 14/32 : Loss = 0.0015630272682756186\n",
      "Epoch 92, Batch 15/32 : Loss = 0.0016163234831765294\n",
      "Epoch 92, Batch 16/32 : Loss = 0.0017949971370398998\n",
      "Epoch 92, Batch 17/32 : Loss = 0.001629444072023034\n",
      "Epoch 92, Batch 18/32 : Loss = 0.0026528697926551104\n",
      "Epoch 92, Batch 19/32 : Loss = 0.002619566395878792\n",
      "Epoch 92, Batch 20/32 : Loss = 0.0018445895984768867\n",
      "Epoch 92, Batch 21/32 : Loss = 0.002728994470089674\n",
      "Epoch 92, Batch 22/32 : Loss = 0.002439269330352545\n",
      "Epoch 92, Batch 23/32 : Loss = 0.02274155244231224\n",
      "Epoch 92, Batch 24/32 : Loss = 0.006122648250311613\n",
      "Epoch 92, Batch 25/32 : Loss = 0.004868504591286182\n",
      "Epoch 92, Batch 26/32 : Loss = 0.0029822150245308876\n",
      "Epoch 92, Batch 27/32 : Loss = 0.0015815256629139185\n",
      "Epoch 92, Batch 28/32 : Loss = 0.0016136099584400654\n",
      "Epoch 92, Batch 29/32 : Loss = 0.004650544840842485\n",
      "Epoch 92, Batch 30/32 : Loss = 0.025608569383621216\n",
      "Epoch 92, Batch 31/32 : Loss = 0.007674478925764561\n",
      "Epoch 92 finished in 0.05174841086069743 minutes\n",
      "Epoch 92 training_loss = 0.004580962937325239\n",
      "-----J---;---q-----++---//----z---y-----U---------%----U------1----X---_-- => J;q+/zyU%U1X_, Ground Truth is J;q+/zyU%U1x_\n",
      "---8-------K------ll--ZZ------5------p-------$$-----a------}-----w------,- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "-----C-----DD----E-----g----mm-----\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----o----\"\"---}}--!!---B-----r----&------9----;--`----O-------w-----}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 92 val_loss = 0.6984773278236389, word_accuracy = 0.76\n",
      "Epoch 93, Batch 0/32 : Loss = 0.0044867293909192085\n",
      "Epoch 93, Batch 1/32 : Loss = 0.002167460508644581\n",
      "Epoch 93, Batch 2/32 : Loss = 0.0007883473881520331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Batch 3/32 : Loss = 0.002665234263986349\n",
      "Epoch 93, Batch 4/32 : Loss = 0.007799353450536728\n",
      "Epoch 93, Batch 5/32 : Loss = 0.0030690894927829504\n",
      "Epoch 93, Batch 6/32 : Loss = 0.0014200705336406827\n",
      "Epoch 93, Batch 7/32 : Loss = 0.004829606506973505\n",
      "Epoch 93, Batch 8/32 : Loss = 0.0011321984929963946\n",
      "Epoch 93, Batch 9/32 : Loss = 0.0024836407974362373\n",
      "Epoch 93, Batch 10/32 : Loss = 0.0026807787362486124\n",
      "Epoch 93, Batch 11/32 : Loss = 0.002092592651024461\n",
      "Epoch 93, Batch 12/32 : Loss = 0.0008861031383275986\n",
      "Epoch 93, Batch 13/32 : Loss = 0.00218680570833385\n",
      "Epoch 93, Batch 14/32 : Loss = 0.0012325473362579942\n",
      "Epoch 93, Batch 15/32 : Loss = 0.0009413115330971777\n",
      "Epoch 93, Batch 16/32 : Loss = 0.0009270617738366127\n",
      "Epoch 93, Batch 17/32 : Loss = 0.0010905478848144412\n",
      "Epoch 93, Batch 18/32 : Loss = 0.0014813562156632543\n",
      "Epoch 93, Batch 19/32 : Loss = 0.0007684347219765186\n",
      "Epoch 93, Batch 20/32 : Loss = 0.000975134433247149\n",
      "Epoch 93, Batch 21/32 : Loss = 0.0018927848432213068\n",
      "Epoch 93, Batch 22/32 : Loss = 0.001257814117707312\n",
      "Epoch 93, Batch 23/32 : Loss = 0.013894191011786461\n",
      "Epoch 93, Batch 24/32 : Loss = 0.0008145524770952761\n",
      "Epoch 93, Batch 25/32 : Loss = 0.0012796130031347275\n",
      "Epoch 93, Batch 26/32 : Loss = 0.0006580232875421643\n",
      "Epoch 93, Batch 27/32 : Loss = 0.007073516491800547\n",
      "Epoch 93, Batch 28/32 : Loss = 0.01118486188352108\n",
      "Epoch 93, Batch 29/32 : Loss = 0.0013438630849123001\n",
      "Epoch 93, Batch 30/32 : Loss = 0.0009341499535366893\n",
      "Epoch 93, Batch 31/32 : Loss = 0.002631618408486247\n",
      "Epoch 93 finished in 0.050710125764211016 minutes\n",
      "Epoch 93 training_loss = 0.0027876857202500105\n",
      "----/---MM-----o----E---^---3---xx--/---&----6----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---d---:---X----9----e----a----F--,--8-------VV---RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----d---::--X----9---ee---a----FF-,--8--------V---RR---- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----5----9---gg----m------J---u---x---c---Xx--.--d---\\--- => 59gmJuxcXx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 93 val_loss = 0.6354397535324097, word_accuracy = 0.71\n",
      "Epoch 94, Batch 0/32 : Loss = 0.0008493861532770097\n",
      "Epoch 94, Batch 1/32 : Loss = 0.024417860433459282\n",
      "Epoch 94, Batch 2/32 : Loss = 0.044222068041563034\n",
      "Epoch 94, Batch 3/32 : Loss = 0.0007445947267115116\n",
      "Epoch 94, Batch 4/32 : Loss = 0.0009399906848557293\n",
      "Epoch 94, Batch 5/32 : Loss = 0.0010960260406136513\n",
      "Epoch 94, Batch 6/32 : Loss = 0.005433522164821625\n",
      "Epoch 94, Batch 7/32 : Loss = 0.0007971527520567179\n",
      "Epoch 94, Batch 8/32 : Loss = 0.006214790511876345\n",
      "Epoch 94, Batch 9/32 : Loss = 0.000773867592215538\n",
      "Epoch 94, Batch 10/32 : Loss = 0.0017948613967746496\n",
      "Epoch 94, Batch 11/32 : Loss = 0.0010319668799638748\n",
      "Epoch 94, Batch 12/32 : Loss = 0.00101750367321074\n",
      "Epoch 94, Batch 13/32 : Loss = 0.017133064568042755\n",
      "Epoch 94, Batch 14/32 : Loss = 0.0032058649230748415\n",
      "Epoch 94, Batch 15/32 : Loss = 0.0013938003685325384\n",
      "Epoch 94, Batch 16/32 : Loss = 0.0007037103059701622\n",
      "Epoch 94, Batch 17/32 : Loss = 0.02029462344944477\n",
      "Epoch 94, Batch 18/32 : Loss = 0.0018225617241114378\n",
      "Epoch 94, Batch 19/32 : Loss = 0.003927368670701981\n",
      "Epoch 94, Batch 20/32 : Loss = 0.0008361989166587591\n",
      "Epoch 94, Batch 21/32 : Loss = 0.02234027162194252\n",
      "Epoch 94, Batch 22/32 : Loss = 0.003342781215906143\n",
      "Epoch 94, Batch 23/32 : Loss = 0.0008130880305543542\n",
      "Epoch 94, Batch 24/32 : Loss = 0.0011748919496312737\n",
      "Epoch 94, Batch 25/32 : Loss = 0.0009694962645880878\n",
      "Epoch 94, Batch 26/32 : Loss = 0.0012265292461961508\n",
      "Epoch 94, Batch 27/32 : Loss = 0.002669348381459713\n",
      "Epoch 94, Batch 28/32 : Loss = 0.0010558828944340348\n",
      "Epoch 94, Batch 29/32 : Loss = 0.002902423497289419\n",
      "Epoch 94, Batch 30/32 : Loss = 0.0008944666478782892\n",
      "Epoch 94, Batch 31/32 : Loss = 0.0005894851638004184\n",
      "Epoch 94 finished in 0.05064690907796224 minutes\n",
      "Epoch 94 training_loss = 0.005658269394189119\n",
      "----\"---]--t--4----e----^----W-------Q----44---->----gg----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "--4----r--{------%---/--'--)---w-----&-----N----++---PP----- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----4----r--{------%--//--'-))--w-----&-----NN----+---PP---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---XX---7---0--j---@----S---ZZ---L--4----C---mm-----MM------ => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 94 val_loss = 0.6833839416503906, word_accuracy = 0.69\n",
      "Epoch 95, Batch 0/32 : Loss = 0.005314715206623077\n",
      "Epoch 95, Batch 1/32 : Loss = 0.0011610772926360369\n",
      "Epoch 95, Batch 2/32 : Loss = 0.0019959036726504564\n",
      "Epoch 95, Batch 3/32 : Loss = 0.0011296195443719625\n",
      "Epoch 95, Batch 4/32 : Loss = 0.008334203623235226\n",
      "Epoch 95, Batch 5/32 : Loss = 0.0039055235683918\n",
      "Epoch 95, Batch 6/32 : Loss = 0.00249432772397995\n",
      "Epoch 95, Batch 7/32 : Loss = 0.0028959056362509727\n",
      "Epoch 95, Batch 8/32 : Loss = 0.003590693697333336\n",
      "Epoch 95, Batch 9/32 : Loss = 0.016583042219281197\n",
      "Epoch 95, Batch 10/32 : Loss = 0.0006749082822352648\n",
      "Epoch 95, Batch 11/32 : Loss = 0.0011041942052543163\n",
      "Epoch 95, Batch 12/32 : Loss = 0.0008841762319207191\n",
      "Epoch 95, Batch 13/32 : Loss = 0.0017354253213852644\n",
      "Epoch 95, Batch 14/32 : Loss = 0.016324946656823158\n",
      "Epoch 95, Batch 15/32 : Loss = 0.0026501649990677834\n",
      "Epoch 95, Batch 16/32 : Loss = 0.003936820197850466\n",
      "Epoch 95, Batch 17/32 : Loss = 0.003954225219786167\n",
      "Epoch 95, Batch 18/32 : Loss = 0.001824584905989468\n",
      "Epoch 95, Batch 19/32 : Loss = 0.006247438490390778\n",
      "Epoch 95, Batch 20/32 : Loss = 0.003208179958164692\n",
      "Epoch 95, Batch 21/32 : Loss = 0.0011662670876830816\n",
      "Epoch 95, Batch 22/32 : Loss = 0.0009923307225108147\n",
      "Epoch 95, Batch 23/32 : Loss = 0.0016009787796065211\n",
      "Epoch 95, Batch 24/32 : Loss = 0.0019212517654523253\n",
      "Epoch 95, Batch 25/32 : Loss = 0.0012376653030514717\n",
      "Epoch 95, Batch 26/32 : Loss = 0.0008205457124859095\n",
      "Epoch 95, Batch 27/32 : Loss = 0.0009057715069502592\n",
      "Epoch 95, Batch 28/32 : Loss = 0.0008706665830686688\n",
      "Epoch 95, Batch 29/32 : Loss = 0.021572045981884003\n",
      "Epoch 95, Batch 30/32 : Loss = 0.002406417392194271\n",
      "Epoch 95, Batch 31/32 : Loss = 0.0010798948351293802\n",
      "Epoch 95 finished in 0.051254558563232425 minutes\n",
      "Epoch 95 training_loss = 0.003970410209149122\n",
      "----\"---]--t---4----e----^----W-------Q-----4----->-----g------ => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---J--;---q----+---/---zz--yy---U-------%%---U-----1---x---_)-- => J;q+/zyU%U1x_), Ground Truth is J;q+/zyU%U1x_\n",
      "-----d---!!--NN----rr--AA---j--**---$----3----hh---55----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "------QQ----33----g----I---z---#-----Y---:--]---q----++----*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 95 val_loss = 0.6978651881217957, word_accuracy = 0.7\n",
      "Epoch 96, Batch 0/32 : Loss = 0.0012910891091451049\n",
      "Epoch 96, Batch 1/32 : Loss = 0.0008588398341089487\n",
      "Epoch 96, Batch 2/32 : Loss = 0.0013885628432035446\n",
      "Epoch 96, Batch 3/32 : Loss = 0.0022808783687651157\n",
      "Epoch 96, Batch 4/32 : Loss = 0.0063980091363191605\n",
      "Epoch 96, Batch 5/32 : Loss = 0.00897242221981287\n",
      "Epoch 96, Batch 6/32 : Loss = 0.0009022129233926535\n",
      "Epoch 96, Batch 7/32 : Loss = 0.0009759491076692939\n",
      "Epoch 96, Batch 8/32 : Loss = 0.0014612539671361446\n",
      "Epoch 96, Batch 9/32 : Loss = 0.0006979621248319745\n",
      "Epoch 96, Batch 10/32 : Loss = 0.0020930541213601828\n",
      "Epoch 96, Batch 11/32 : Loss = 0.0008165918989107013\n",
      "Epoch 96, Batch 12/32 : Loss = 0.0058824447914958\n",
      "Epoch 96, Batch 13/32 : Loss = 0.002618851140141487\n",
      "Epoch 96, Batch 14/32 : Loss = 0.030777975916862488\n",
      "Epoch 96, Batch 15/32 : Loss = 0.0014391372678801417\n",
      "Epoch 96, Batch 16/32 : Loss = 0.0010890072444453835\n",
      "Epoch 96, Batch 17/32 : Loss = 0.002556505147367716\n",
      "Epoch 96, Batch 18/32 : Loss = 0.0010587278520688415\n",
      "Epoch 96, Batch 19/32 : Loss = 0.0016185101121664047\n",
      "Epoch 96, Batch 20/32 : Loss = 0.0009616686147637665\n",
      "Epoch 96, Batch 21/32 : Loss = 0.000888550712261349\n",
      "Epoch 96, Batch 22/32 : Loss = 0.0007127515273168683\n",
      "Epoch 96, Batch 23/32 : Loss = 0.013864480890333652\n",
      "Epoch 96, Batch 24/32 : Loss = 0.0014719339087605476\n",
      "Epoch 96, Batch 25/32 : Loss = 0.002464971039444208\n",
      "Epoch 96, Batch 26/32 : Loss = 0.0027473936788737774\n",
      "Epoch 96, Batch 27/32 : Loss = 0.0014214222319424152\n",
      "Epoch 96, Batch 28/32 : Loss = 0.0016448632813990116\n",
      "Epoch 96, Batch 29/32 : Loss = 0.0014061522670090199\n",
      "Epoch 96, Batch 30/32 : Loss = 0.001849832944571972\n",
      "Epoch 96, Batch 31/32 : Loss = 0.0006128482054919004\n",
      "Epoch 96 finished in 0.051802925268809 minutes\n",
      "Epoch 96 training_loss = 0.0033634896390140057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----88----K----l--Z----5----pp---$$---a----}---w----,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "-----d---:---X---9----e---a----FF----8-------V---RR---- => d:X9eaF8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----\"--]--t--4----e---^----W------Q----4---->----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---o---\"---}--!--B----r--&----9---;--`--O-----w---}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 96 val_loss = 0.6843323111534119, word_accuracy = 0.72\n",
      "Epoch 97, Batch 0/32 : Loss = 0.0014377122279256582\n",
      "Epoch 97, Batch 1/32 : Loss = 0.0006248928839340806\n",
      "Epoch 97, Batch 2/32 : Loss = 0.0014171876246109605\n",
      "Epoch 97, Batch 3/32 : Loss = 0.0011789551936089993\n",
      "Epoch 97, Batch 4/32 : Loss = 0.0010701637947931886\n",
      "Epoch 97, Batch 5/32 : Loss = 0.001642933115363121\n",
      "Epoch 97, Batch 6/32 : Loss = 0.0014310079859569669\n",
      "Epoch 97, Batch 7/32 : Loss = 0.0027840491384267807\n",
      "Epoch 97, Batch 8/32 : Loss = 0.01267799362540245\n",
      "Epoch 97, Batch 9/32 : Loss = 0.0021776012144982815\n",
      "Epoch 97, Batch 10/32 : Loss = 0.0007454874576069415\n",
      "Epoch 97, Batch 11/32 : Loss = 0.0008392834570258856\n",
      "Epoch 97, Batch 12/32 : Loss = 0.0009592519490979612\n",
      "Epoch 97, Batch 13/32 : Loss = 0.002208470832556486\n",
      "Epoch 97, Batch 14/32 : Loss = 0.0008003828697837889\n",
      "Epoch 97, Batch 15/32 : Loss = 0.00896591879427433\n",
      "Epoch 97, Batch 16/32 : Loss = 0.0009314558701589704\n",
      "Epoch 97, Batch 17/32 : Loss = 0.001584883313626051\n",
      "Epoch 97, Batch 18/32 : Loss = 0.001082787523046136\n",
      "Epoch 97, Batch 19/32 : Loss = 0.0009350603213533759\n",
      "Epoch 97, Batch 20/32 : Loss = 0.0014653797261416912\n",
      "Epoch 97, Batch 21/32 : Loss = 0.0008328989497385919\n",
      "Epoch 97, Batch 22/32 : Loss = 0.0015999863389879465\n",
      "Epoch 97, Batch 23/32 : Loss = 0.001113025937229395\n",
      "Epoch 97, Batch 24/32 : Loss = 0.0027183862403035164\n",
      "Epoch 97, Batch 25/32 : Loss = 0.000805903400760144\n",
      "Epoch 97, Batch 26/32 : Loss = 0.0006881073350086808\n",
      "Epoch 97, Batch 27/32 : Loss = 0.001694980775937438\n",
      "Epoch 97, Batch 28/32 : Loss = 0.0035702825989574194\n",
      "Epoch 97, Batch 29/32 : Loss = 0.0005216097342781723\n",
      "Epoch 97, Batch 30/32 : Loss = 0.0014267483493313193\n",
      "Epoch 97, Batch 31/32 : Loss = 0.0006705785635858774\n",
      "Epoch 97 finished in 0.05342063109079997 minutes\n",
      "Epoch 97 training_loss = 0.0019925017841160297\n",
      "---o---\"---}-!!-BB---rr--&----99--;--`--O-----w----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----E-----0----[--xx---)---R----8----i--PP----w----))--- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "----00----c----+----bb---I--\"----b----6---..---Q-------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "-----Q-----3----g----I--z--#-----Y--:--]]--q----+---**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 97 val_loss = 0.7351148128509521, word_accuracy = 0.71\n",
      "Epoch 98, Batch 0/32 : Loss = 0.0007707882905378938\n",
      "Epoch 98, Batch 1/32 : Loss = 0.0009202397195622325\n",
      "Epoch 98, Batch 2/32 : Loss = 0.0006507107755169272\n",
      "Epoch 98, Batch 3/32 : Loss = 0.007245007902383804\n",
      "Epoch 98, Batch 4/32 : Loss = 0.0009189666016027331\n",
      "Epoch 98, Batch 5/32 : Loss = 0.0006874201353639364\n",
      "Epoch 98, Batch 6/32 : Loss = 0.0006828028708696365\n",
      "Epoch 98, Batch 7/32 : Loss = 0.0011117482790723443\n",
      "Epoch 98, Batch 8/32 : Loss = 0.004229727201163769\n",
      "Epoch 98, Batch 9/32 : Loss = 0.0022260278929024935\n",
      "Epoch 98, Batch 10/32 : Loss = 0.0013719433918595314\n",
      "Epoch 98, Batch 11/32 : Loss = 0.0030841303523629904\n",
      "Epoch 98, Batch 12/32 : Loss = 0.0015022455481812358\n",
      "Epoch 98, Batch 13/32 : Loss = 0.0006154143484309316\n",
      "Epoch 98, Batch 14/32 : Loss = 0.0015260935761034489\n",
      "Epoch 98, Batch 15/32 : Loss = 0.029627058655023575\n",
      "Epoch 98, Batch 16/32 : Loss = 0.0005526402383111417\n",
      "Epoch 98, Batch 17/32 : Loss = 0.06302499026060104\n",
      "Epoch 98, Batch 18/32 : Loss = 0.0006730749737471342\n",
      "Epoch 98, Batch 19/32 : Loss = 0.004844410344958305\n",
      "Epoch 98, Batch 20/32 : Loss = 0.01047796756029129\n",
      "Epoch 98, Batch 21/32 : Loss = 0.0013499415945261717\n",
      "Epoch 98, Batch 22/32 : Loss = 0.011618240736424923\n",
      "Epoch 98, Batch 23/32 : Loss = 0.002861320273950696\n",
      "Epoch 98, Batch 24/32 : Loss = 0.14687734842300415\n",
      "Epoch 98, Batch 25/32 : Loss = 0.0018724348628893495\n",
      "Epoch 98, Batch 26/32 : Loss = 0.0006477088900282979\n",
      "Epoch 98, Batch 27/32 : Loss = 0.0010578017681837082\n",
      "Epoch 98, Batch 28/32 : Loss = 0.0031623244285583496\n",
      "Epoch 98, Batch 29/32 : Loss = 0.0016795177944004536\n",
      "Epoch 98, Batch 30/32 : Loss = 0.004686917643994093\n",
      "Epoch 98, Batch 31/32 : Loss = 0.00486481748521328\n",
      "Epoch 98 finished in 0.05080704689025879 minutes\n",
      "Epoch 98 training_loss = 0.010061527602374554\n",
      "----=-----0----[--x---)---RR----8---i---P----w-----)--- => =0[x)R8iPw), Ground Truth is Err:509\n",
      "-----0---JJ---!--(--;---A----3---,--'--)--r---r---77--- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---{----33---\\\\---$---->-----S----\\----M-----i--BB----- => {-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "--{--B----Y----R---a----y--h---#---22--->----E---4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 98 val_loss = 0.704282820224762, word_accuracy = 0.71\n",
      "Epoch 99, Batch 0/32 : Loss = 0.004354003816843033\n",
      "Epoch 99, Batch 1/32 : Loss = 0.009138529188930988\n",
      "Epoch 99, Batch 2/32 : Loss = 0.02407096140086651\n",
      "Epoch 99, Batch 3/32 : Loss = 0.0057075489312410355\n",
      "Epoch 99, Batch 4/32 : Loss = 0.0035280734300613403\n",
      "Epoch 99, Batch 5/32 : Loss = 0.0030445766169577837\n",
      "Epoch 99, Batch 6/32 : Loss = 0.0068014999851584435\n",
      "Epoch 99, Batch 7/32 : Loss = 0.008245665580034256\n",
      "Epoch 99, Batch 8/32 : Loss = 0.01931324042379856\n",
      "Epoch 99, Batch 9/32 : Loss = 0.08120252937078476\n",
      "Epoch 99, Batch 10/32 : Loss = 0.006372876465320587\n",
      "Epoch 99, Batch 11/32 : Loss = 0.0028226939029991627\n",
      "Epoch 99, Batch 12/32 : Loss = 0.0017374798189848661\n",
      "Epoch 99, Batch 13/32 : Loss = 0.009076513350009918\n",
      "Epoch 99, Batch 14/32 : Loss = 0.003936165943741798\n",
      "Epoch 99, Batch 15/32 : Loss = 0.0026585664600133896\n",
      "Epoch 99, Batch 16/32 : Loss = 0.031391412019729614\n",
      "Epoch 99, Batch 17/32 : Loss = 0.021344251930713654\n",
      "Epoch 99, Batch 18/32 : Loss = 0.0017091208137571812\n",
      "Epoch 99, Batch 19/32 : Loss = 0.004855146165937185\n",
      "Epoch 99, Batch 20/32 : Loss = 0.1104731410741806\n",
      "Epoch 99, Batch 21/32 : Loss = 0.006223537027835846\n",
      "Epoch 99, Batch 22/32 : Loss = 0.0030776492785662413\n",
      "Epoch 99, Batch 23/32 : Loss = 0.04225669428706169\n",
      "Epoch 99, Batch 24/32 : Loss = 0.009525910019874573\n",
      "Epoch 99, Batch 25/32 : Loss = 0.00980030931532383\n",
      "Epoch 99, Batch 26/32 : Loss = 0.025592103600502014\n",
      "Epoch 99, Batch 27/32 : Loss = 0.0030971753876656294\n",
      "Epoch 99, Batch 28/32 : Loss = 0.011927861720323563\n",
      "Epoch 99, Batch 29/32 : Loss = 0.005319227464497089\n",
      "Epoch 99, Batch 30/32 : Loss = 0.004811080172657967\n",
      "Epoch 99, Batch 31/32 : Loss = 0.0033094899263232946\n",
      "Epoch 99 finished in 0.051003336906433105 minutes\n",
      "Epoch 99 training_loss = 0.015544713474810123\n",
      "----XX----7----0---j---@------S----Z----LL---4----CC----mm-------MM----- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "------d-----R----;;--99-----y-----?----2-----d----ii---O-------{--!----- => dR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----0------c-----++------bb----||--\"----bb------6----..----Q------------ => 0c+b|\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----W------=-----2----+----E----11---n----TT---XX---r---C-----a---n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 99 val_loss = 0.7087094187736511, word_accuracy = 0.58\n",
      "Epoch 100, Batch 0/32 : Loss = 0.011752487160265446\n",
      "Epoch 100, Batch 1/32 : Loss = 0.0029158280231058598\n",
      "Epoch 100, Batch 2/32 : Loss = 0.014981063082814217\n",
      "Epoch 100, Batch 3/32 : Loss = 0.004243956878781319\n",
      "Epoch 100, Batch 4/32 : Loss = 0.0020651347003877163\n",
      "Epoch 100, Batch 5/32 : Loss = 0.015078671276569366\n",
      "Epoch 100, Batch 6/32 : Loss = 0.14040254056453705\n",
      "Epoch 100, Batch 7/32 : Loss = 0.0014901121612638235\n",
      "Epoch 100, Batch 8/32 : Loss = 0.011239845305681229\n",
      "Epoch 100, Batch 9/32 : Loss = 0.009470957331359386\n",
      "Epoch 100, Batch 10/32 : Loss = 0.0016343671595677733\n",
      "Epoch 100, Batch 11/32 : Loss = 0.007811460644006729\n",
      "Epoch 100, Batch 12/32 : Loss = 0.014067055657505989\n",
      "Epoch 100, Batch 13/32 : Loss = 0.015800373628735542\n",
      "Epoch 100, Batch 14/32 : Loss = 0.001975767547264695\n",
      "Epoch 100, Batch 15/32 : Loss = 0.09352165460586548\n",
      "Epoch 100, Batch 16/32 : Loss = 0.026610685512423515\n",
      "Epoch 100, Batch 17/32 : Loss = 0.0012465791078284383\n",
      "Epoch 100, Batch 18/32 : Loss = 0.012018172070384026\n",
      "Epoch 100, Batch 19/32 : Loss = 0.0023013614118099213\n",
      "Epoch 100, Batch 20/32 : Loss = 0.004148324951529503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Batch 21/32 : Loss = 0.0015327977016568184\n",
      "Epoch 100, Batch 22/32 : Loss = 0.002872074954211712\n",
      "Epoch 100, Batch 23/32 : Loss = 0.0021462328732013702\n",
      "Epoch 100, Batch 24/32 : Loss = 0.001925953896716237\n",
      "Epoch 100, Batch 25/32 : Loss = 0.0019004838541150093\n",
      "Epoch 100, Batch 26/32 : Loss = 0.010788707062602043\n",
      "Epoch 100, Batch 27/32 : Loss = 0.004675566218793392\n",
      "Epoch 100, Batch 28/32 : Loss = 0.015099914744496346\n",
      "Epoch 100, Batch 29/32 : Loss = 0.002642218489199877\n",
      "Epoch 100, Batch 30/32 : Loss = 0.01653282158076763\n",
      "Epoch 100, Batch 31/32 : Loss = 0.21664747595787048\n",
      "Epoch 100 finished in 0.051302238305409746 minutes\n",
      "Epoch 100 training_loss = 0.015485110692679882\n",
      "---kk---O-------,-y---c--**---1---P--}}--#---BB---- => kO,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----c----R---;--9----y---?--2----d---i--O-----{-!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "--zz---0---\"----G----/---~----$----c---11--j--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---\"--]--tt--4---e---^---W------Q----4--->>---g---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 100 val_loss = 0.6842512488365173, word_accuracy = 0.68\n",
      "Epoch 101, Batch 0/32 : Loss = 0.00452306866645813\n",
      "Epoch 101, Batch 1/32 : Loss = 0.002720981603488326\n",
      "Epoch 101, Batch 2/32 : Loss = 0.0018766135908663273\n",
      "Epoch 101, Batch 3/32 : Loss = 0.0012416356476023793\n",
      "Epoch 101, Batch 4/32 : Loss = 0.003154242876917124\n",
      "Epoch 101, Batch 5/32 : Loss = 0.0016096492763608694\n",
      "Epoch 101, Batch 6/32 : Loss = 0.013919144868850708\n",
      "Epoch 101, Batch 7/32 : Loss = 0.004210935905575752\n",
      "Epoch 101, Batch 8/32 : Loss = 0.0038131149485707283\n",
      "Epoch 101, Batch 9/32 : Loss = 0.0038385428488254547\n",
      "Epoch 101, Batch 10/32 : Loss = 0.003488203976303339\n",
      "Epoch 101, Batch 11/32 : Loss = 0.0013963994570076466\n",
      "Epoch 101, Batch 12/32 : Loss = 0.0033482746221125126\n",
      "Epoch 101, Batch 13/32 : Loss = 0.0015735914930701256\n",
      "Epoch 101, Batch 14/32 : Loss = 0.0022525186650455\n",
      "Epoch 101, Batch 15/32 : Loss = 0.001595031819306314\n",
      "Epoch 101, Batch 16/32 : Loss = 0.0013507097028195858\n",
      "Epoch 101, Batch 17/32 : Loss = 0.005227839574217796\n",
      "Epoch 101, Batch 18/32 : Loss = 0.0009629162377677858\n",
      "Epoch 101, Batch 19/32 : Loss = 0.0010178094962611794\n",
      "Epoch 101, Batch 20/32 : Loss = 0.004216835834085941\n",
      "Epoch 101, Batch 21/32 : Loss = 0.023979710415005684\n",
      "Epoch 101, Batch 22/32 : Loss = 0.0028728509787470102\n",
      "Epoch 101, Batch 23/32 : Loss = 0.001038751215673983\n",
      "Epoch 101, Batch 24/32 : Loss = 0.0027626666706055403\n",
      "Epoch 101, Batch 25/32 : Loss = 0.0015384184662252665\n",
      "Epoch 101, Batch 26/32 : Loss = 0.0022401362657546997\n",
      "Epoch 101, Batch 27/32 : Loss = 0.0026507964357733727\n",
      "Epoch 101, Batch 28/32 : Loss = 0.05467671900987625\n",
      "Epoch 101, Batch 29/32 : Loss = 0.010939637199044228\n",
      "Epoch 101, Batch 30/32 : Loss = 0.0017278464511036873\n",
      "Epoch 101, Batch 31/32 : Loss = 0.23236221075057983\n",
      "Epoch 101 finished in 0.05033793846766154 minutes\n",
      "Epoch 101 training_loss = 0.006451754830777645\n",
      "--kk----$---ll-FF----DD----e----h----]--kk---0---\\----X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "--BB---..--Y----I--WW------6-----F---hh----XX--'--Y-----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "--.---c----O-----w-----uu--`---u---.--R----{--33---\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----##-----0----[[--x----)---RR----88---i---P-----w-----))--- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "Epoch 101 val_loss = 0.7281078696250916, word_accuracy = 0.64\n",
      "Epoch 102, Batch 0/32 : Loss = 0.0010591773316264153\n",
      "Epoch 102, Batch 1/32 : Loss = 0.10717102140188217\n",
      "Epoch 102, Batch 2/32 : Loss = 0.0014237769646570086\n",
      "Epoch 102, Batch 3/32 : Loss = 0.0013026853557676077\n",
      "Epoch 102, Batch 4/32 : Loss = 0.002705763792619109\n",
      "Epoch 102, Batch 5/32 : Loss = 0.0011791354045271873\n",
      "Epoch 102, Batch 6/32 : Loss = 0.0014675440033897758\n",
      "Epoch 102, Batch 7/32 : Loss = 0.0026513529010117054\n",
      "Epoch 102, Batch 8/32 : Loss = 0.003176685655489564\n",
      "Epoch 102, Batch 9/32 : Loss = 0.0024107948411256075\n",
      "Epoch 102, Batch 10/32 : Loss = 0.002941008424386382\n",
      "Epoch 102, Batch 11/32 : Loss = 0.003011258551850915\n",
      "Epoch 102, Batch 12/32 : Loss = 0.0016475196462124586\n",
      "Epoch 102, Batch 13/32 : Loss = 0.042893774807453156\n",
      "Epoch 102, Batch 14/32 : Loss = 0.003219038248062134\n",
      "Epoch 102, Batch 15/32 : Loss = 0.0018754408229142427\n",
      "Epoch 102, Batch 16/32 : Loss = 0.0013454922009259462\n",
      "Epoch 102, Batch 17/32 : Loss = 0.002705030143260956\n",
      "Epoch 102, Batch 18/32 : Loss = 0.003758504753932357\n",
      "Epoch 102, Batch 19/32 : Loss = 0.021266289055347443\n",
      "Epoch 102, Batch 20/32 : Loss = 0.02124151587486267\n",
      "Epoch 102, Batch 21/32 : Loss = 0.0015212052967399359\n",
      "Epoch 102, Batch 22/32 : Loss = 0.00404135137796402\n",
      "Epoch 102, Batch 23/32 : Loss = 0.008821835741400719\n",
      "Epoch 102, Batch 24/32 : Loss = 0.0014656343264505267\n",
      "Epoch 102, Batch 25/32 : Loss = 0.0015789588214829564\n",
      "Epoch 102, Batch 26/32 : Loss = 0.003159594489261508\n",
      "Epoch 102, Batch 27/32 : Loss = 0.0028408102225512266\n",
      "Epoch 102, Batch 28/32 : Loss = 0.0031362101435661316\n",
      "Epoch 102, Batch 29/32 : Loss = 0.001751311938278377\n",
      "Epoch 102, Batch 30/32 : Loss = 0.0023120110854506493\n",
      "Epoch 102, Batch 31/32 : Loss = 0.16885167360305786\n",
      "Epoch 102 finished in 0.05052881638209025 minutes\n",
      "Epoch 102 training_loss = 0.009066285565495491\n",
      "---7---n----DD----P----n----t--w-----d---\\----Q----a----RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----d----:--XX----9----e----aa----F--,--88--------V----RR------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----#------0----[---x----)---RR-----8----i---P-----w-----))---- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "---##----G-----9----E---I--=---hh---55---#----2----J---)--kk---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 102 val_loss = 0.7018342614173889, word_accuracy = 0.66\n",
      "Epoch 103, Batch 0/32 : Loss = 0.0027946080081164837\n",
      "Epoch 103, Batch 1/32 : Loss = 0.002159467665478587\n",
      "Epoch 103, Batch 2/32 : Loss = 0.0013218993553891778\n",
      "Epoch 103, Batch 3/32 : Loss = 0.0046918317675590515\n",
      "Epoch 103, Batch 4/32 : Loss = 0.004634169861674309\n",
      "Epoch 103, Batch 5/32 : Loss = 0.002493638778105378\n",
      "Epoch 103, Batch 6/32 : Loss = 0.01858387514948845\n",
      "Epoch 103, Batch 7/32 : Loss = 0.0010444503277540207\n",
      "Epoch 103, Batch 8/32 : Loss = 0.0018342578550800681\n",
      "Epoch 103, Batch 9/32 : Loss = 0.002767442259937525\n",
      "Epoch 103, Batch 10/32 : Loss = 0.0015003676526248455\n",
      "Epoch 103, Batch 11/32 : Loss = 0.0010464077349752188\n",
      "Epoch 103, Batch 12/32 : Loss = 0.00223382655531168\n",
      "Epoch 103, Batch 13/32 : Loss = 0.000872075033839792\n",
      "Epoch 103, Batch 14/32 : Loss = 0.0012488799402490258\n",
      "Epoch 103, Batch 15/32 : Loss = 0.004491005092859268\n",
      "Epoch 103, Batch 16/32 : Loss = 0.0025087969843298197\n",
      "Epoch 103, Batch 17/32 : Loss = 0.0011000547092407942\n",
      "Epoch 103, Batch 18/32 : Loss = 0.008534379303455353\n",
      "Epoch 103, Batch 19/32 : Loss = 0.0022839526645839214\n",
      "Epoch 103, Batch 20/32 : Loss = 0.010237859562039375\n",
      "Epoch 103, Batch 21/32 : Loss = 0.021933849900960922\n",
      "Epoch 103, Batch 22/32 : Loss = 0.002641725353896618\n",
      "Epoch 103, Batch 23/32 : Loss = 0.005707605741918087\n",
      "Epoch 103, Batch 24/32 : Loss = 0.0019882344640791416\n",
      "Epoch 103, Batch 25/32 : Loss = 0.018715115264058113\n",
      "Epoch 103, Batch 26/32 : Loss = 0.0024238559417426586\n",
      "Epoch 103, Batch 27/32 : Loss = 0.0010857294546440244\n",
      "Epoch 103, Batch 28/32 : Loss = 0.0012379379477351904\n",
      "Epoch 103, Batch 29/32 : Loss = 0.0012058106949552894\n",
      "Epoch 103, Batch 30/32 : Loss = 0.001648541889153421\n",
      "Epoch 103, Batch 31/32 : Loss = 0.0017035033088177443\n",
      "Epoch 103 finished in 0.05533058643341064 minutes\n",
      "Epoch 103 training_loss = 0.004407537169754505\n",
      "-----0----cc---++----bb----I--\"---bb----66---.---Q--------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "---2--pp--:--m----xx--a---z--n----@----C---yy----%-----%--- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "--4----r--{------%---/--'-)---w-----&-----N----++---P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---++---:--z----7---8----d----S----vv--5----S---JJ----B---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 103 val_loss = 0.6858910322189331, word_accuracy = 0.66\n",
      "Epoch 104, Batch 0/32 : Loss = 0.010520688258111477\n",
      "Epoch 104, Batch 1/32 : Loss = 0.013260368257761002\n",
      "Epoch 104, Batch 2/32 : Loss = 0.005801726598292589\n",
      "Epoch 104, Batch 3/32 : Loss = 0.000800853711552918\n",
      "Epoch 104, Batch 4/32 : Loss = 0.0014318187022581697\n",
      "Epoch 104, Batch 5/32 : Loss = 0.0032325652427971363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104, Batch 6/32 : Loss = 0.002350343856960535\n",
      "Epoch 104, Batch 7/32 : Loss = 0.0009909069631248713\n",
      "Epoch 104, Batch 8/32 : Loss = 0.0017373454757034779\n",
      "Epoch 104, Batch 9/32 : Loss = 0.0019290503114461899\n",
      "Epoch 104, Batch 10/32 : Loss = 0.002829590579494834\n",
      "Epoch 104, Batch 11/32 : Loss = 0.0011284388601779938\n",
      "Epoch 104, Batch 12/32 : Loss = 0.0009604513179510832\n",
      "Epoch 104, Batch 13/32 : Loss = 0.0006294106133282185\n",
      "Epoch 104, Batch 14/32 : Loss = 0.0027287527918815613\n",
      "Epoch 104, Batch 15/32 : Loss = 0.0009076533606275916\n",
      "Epoch 104, Batch 16/32 : Loss = 0.00070114282425493\n",
      "Epoch 104, Batch 17/32 : Loss = 0.024264592677354813\n",
      "Epoch 104, Batch 18/32 : Loss = 0.0007605243008583784\n",
      "Epoch 104, Batch 19/32 : Loss = 0.0005770260468125343\n",
      "Epoch 104, Batch 20/32 : Loss = 0.0016101936344057322\n",
      "Epoch 104, Batch 21/32 : Loss = 0.0007802075706422329\n",
      "Epoch 104, Batch 22/32 : Loss = 0.0008459729142487049\n",
      "Epoch 104, Batch 23/32 : Loss = 0.0038311867974698544\n",
      "Epoch 104, Batch 24/32 : Loss = 0.0008803193341009319\n",
      "Epoch 104, Batch 25/32 : Loss = 0.0008082121494226158\n",
      "Epoch 104, Batch 26/32 : Loss = 0.03992428258061409\n",
      "Epoch 104, Batch 27/32 : Loss = 0.00873736385256052\n",
      "Epoch 104, Batch 28/32 : Loss = 0.0017336653545498848\n",
      "Epoch 104, Batch 29/32 : Loss = 0.005855274386703968\n",
      "Epoch 104, Batch 30/32 : Loss = 0.000949070556089282\n",
      "Epoch 104, Batch 31/32 : Loss = 0.0037494238931685686\n",
      "Epoch 104 finished in 0.05308258533477783 minutes\n",
      "Epoch 104 training_loss = 0.0046254671178758144\n",
      "-----C------D----EE----g----mm-----\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----YY-----WW-------]--ii-\\\\----|-----MM------<<------MM------8-----8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----55----99-----g-----mm-------J-----u----X----CC---X---..--d----\\\\----- => 59gmJuXCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "---77---nn----DD-----P-----n----t---w------d----\\----QQ----aa----RR------- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 104 val_loss = 0.6958944797515869, word_accuracy = 0.68\n",
      "Epoch 105, Batch 0/32 : Loss = 0.0005952623905614018\n",
      "Epoch 105, Batch 1/32 : Loss = 0.0026162504218518734\n",
      "Epoch 105, Batch 2/32 : Loss = 0.01771918497979641\n",
      "Epoch 105, Batch 3/32 : Loss = 0.001160971587523818\n",
      "Epoch 105, Batch 4/32 : Loss = 0.002632997464388609\n",
      "Epoch 105, Batch 5/32 : Loss = 0.001500924932770431\n",
      "Epoch 105, Batch 6/32 : Loss = 0.001485141459852457\n",
      "Epoch 105, Batch 7/32 : Loss = 0.004585543181747198\n",
      "Epoch 105, Batch 8/32 : Loss = 0.0006682143430225551\n",
      "Epoch 105, Batch 9/32 : Loss = 0.001879235845990479\n",
      "Epoch 105, Batch 10/32 : Loss = 0.0012028573546558619\n",
      "Epoch 105, Batch 11/32 : Loss = 0.002778154332190752\n",
      "Epoch 105, Batch 12/32 : Loss = 0.0012428452027961612\n",
      "Epoch 105, Batch 13/32 : Loss = 0.0024699661880731583\n",
      "Epoch 105, Batch 14/32 : Loss = 0.0024070164654403925\n",
      "Epoch 105, Batch 15/32 : Loss = 0.0010267621837556362\n",
      "Epoch 105, Batch 16/32 : Loss = 0.0005679135792888701\n",
      "Epoch 105, Batch 17/32 : Loss = 0.0030002964194864035\n",
      "Epoch 105, Batch 18/32 : Loss = 0.01184689998626709\n",
      "Epoch 105, Batch 19/32 : Loss = 0.0008225105702877045\n",
      "Epoch 105, Batch 20/32 : Loss = 0.001010286738164723\n",
      "Epoch 105, Batch 21/32 : Loss = 0.002746538259088993\n",
      "Epoch 105, Batch 22/32 : Loss = 0.0018581573385745287\n",
      "Epoch 105, Batch 23/32 : Loss = 0.0015089062508195639\n",
      "Epoch 105, Batch 24/32 : Loss = 0.0008992509683594108\n",
      "Epoch 105, Batch 25/32 : Loss = 0.002089587040245533\n",
      "Epoch 105, Batch 26/32 : Loss = 0.0007287992630153894\n",
      "Epoch 105, Batch 27/32 : Loss = 0.0006758535164408386\n",
      "Epoch 105, Batch 28/32 : Loss = 0.0010612939950078726\n",
      "Epoch 105, Batch 29/32 : Loss = 0.0009313392220064998\n",
      "Epoch 105, Batch 30/32 : Loss = 0.0019010421819984913\n",
      "Epoch 105, Batch 31/32 : Loss = 0.0008102165884338319\n",
      "Epoch 105 finished in 0.05316779613494873 minutes\n",
      "Epoch 105 training_loss = 0.002497069537639618\n",
      "----55----9-----g----mm-------J----u----X----C---X---.---d---\\\\--- => 59gmJuXCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "---k----$----|---'----9----Y-----N-----W------mm------TT---8------ => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "-----o----\"---}--!!--BB----r---&&----99---;--`---O------w-----}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----0----Q---66---<----<<---(---T---N----5---=----P---(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 105 val_loss = 0.7057995200157166, word_accuracy = 0.68\n",
      "Epoch 106, Batch 0/32 : Loss = 0.027119122445583344\n",
      "Epoch 106, Batch 1/32 : Loss = 0.00300017511472106\n",
      "Epoch 106, Batch 2/32 : Loss = 0.0020817758049815893\n",
      "Epoch 106, Batch 3/32 : Loss = 0.0013945682439953089\n",
      "Epoch 106, Batch 4/32 : Loss = 0.0007934966706670821\n",
      "Epoch 106, Batch 5/32 : Loss = 0.006853091064840555\n",
      "Epoch 106, Batch 6/32 : Loss = 0.0018056603148579597\n",
      "Epoch 106, Batch 7/32 : Loss = 0.0018320364179089665\n",
      "Epoch 106, Batch 8/32 : Loss = 0.001163858687505126\n",
      "Epoch 106, Batch 9/32 : Loss = 0.006312400568276644\n",
      "Epoch 106, Batch 10/32 : Loss = 0.00864957645535469\n",
      "Epoch 106, Batch 11/32 : Loss = 0.028184272348880768\n",
      "Epoch 106, Batch 12/32 : Loss = 0.0005751335411332548\n",
      "Epoch 106, Batch 13/32 : Loss = 0.043347738683223724\n",
      "Epoch 106, Batch 14/32 : Loss = 0.000938839977607131\n",
      "Epoch 106, Batch 15/32 : Loss = 0.0009252959280274808\n",
      "Epoch 106, Batch 16/32 : Loss = 0.0007974867476150393\n",
      "Epoch 106, Batch 17/32 : Loss = 0.0023480020463466644\n",
      "Epoch 106, Batch 18/32 : Loss = 0.001206760061904788\n",
      "Epoch 106, Batch 19/32 : Loss = 0.007222211919724941\n",
      "Epoch 106, Batch 20/32 : Loss = 0.005069694947451353\n",
      "Epoch 106, Batch 21/32 : Loss = 0.001201688777655363\n",
      "Epoch 106, Batch 22/32 : Loss = 0.0007397452718578279\n",
      "Epoch 106, Batch 23/32 : Loss = 0.0007792870746925473\n",
      "Epoch 106, Batch 24/32 : Loss = 0.0009486131020821631\n",
      "Epoch 106, Batch 25/32 : Loss = 0.0014479286037385464\n",
      "Epoch 106, Batch 26/32 : Loss = 0.0014971006894484162\n",
      "Epoch 106, Batch 27/32 : Loss = 0.0007014740258455276\n",
      "Epoch 106, Batch 28/32 : Loss = 0.004150863736867905\n",
      "Epoch 106, Batch 29/32 : Loss = 0.0036829409655183554\n",
      "Epoch 106, Batch 30/32 : Loss = 0.0011341898934915662\n",
      "Epoch 106, Batch 31/32 : Loss = 0.0008338785264641047\n",
      "Epoch 106 finished in 0.05167125463485718 minutes\n",
      "Epoch 106 training_loss = 0.005397886969149113\n",
      "----kk---$---||--!----9---Y----NN----W------m------T---8---- => k$|!9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---XX---7---0--j---@----S----Z---L--4----C----m------M------ => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "---7---n----DD----P----n---t--ww----d---\\\\---Q----a----RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----/----MM----oo----E----^--33---xx--//--&&----6----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "Epoch 106 val_loss = 0.7810036540031433, word_accuracy = 0.61\n",
      "Epoch 107, Batch 0/32 : Loss = 0.003951050341129303\n",
      "Epoch 107, Batch 1/32 : Loss = 0.0025383918546140194\n",
      "Epoch 107, Batch 2/32 : Loss = 0.02638251520693302\n",
      "Epoch 107, Batch 3/32 : Loss = 0.0009724822011776268\n",
      "Epoch 107, Batch 4/32 : Loss = 0.0025915801525115967\n",
      "Epoch 107, Batch 5/32 : Loss = 0.0016830053646117449\n",
      "Epoch 107, Batch 6/32 : Loss = 0.0010916540632024407\n",
      "Epoch 107, Batch 7/32 : Loss = 0.0008311566198244691\n",
      "Epoch 107, Batch 8/32 : Loss = 0.001387275056913495\n",
      "Epoch 107, Batch 9/32 : Loss = 0.0008536588866263628\n",
      "Epoch 107, Batch 10/32 : Loss = 0.0007162883994169533\n",
      "Epoch 107, Batch 11/32 : Loss = 0.05702943727374077\n",
      "Epoch 107, Batch 12/32 : Loss = 0.0007553021423518658\n",
      "Epoch 107, Batch 13/32 : Loss = 0.0016847066581249237\n",
      "Epoch 107, Batch 14/32 : Loss = 0.00156348233576864\n",
      "Epoch 107, Batch 15/32 : Loss = 0.0029856450855731964\n",
      "Epoch 107, Batch 16/32 : Loss = 0.0012922758469358087\n",
      "Epoch 107, Batch 17/32 : Loss = 0.0017995971720665693\n",
      "Epoch 107, Batch 18/32 : Loss = 0.001340360613539815\n",
      "Epoch 107, Batch 19/32 : Loss = 0.0012445864267647266\n",
      "Epoch 107, Batch 20/32 : Loss = 0.002004382200539112\n",
      "Epoch 107, Batch 21/32 : Loss = 0.001676349900662899\n",
      "Epoch 107, Batch 22/32 : Loss = 0.0007723772432655096\n",
      "Epoch 107, Batch 23/32 : Loss = 0.0014495330397039652\n",
      "Epoch 107, Batch 24/32 : Loss = 0.0020181587897241116\n",
      "Epoch 107, Batch 25/32 : Loss = 0.00095095403958112\n",
      "Epoch 107, Batch 26/32 : Loss = 0.005037910304963589\n",
      "Epoch 107, Batch 27/32 : Loss = 0.0009958362206816673\n",
      "Epoch 107, Batch 28/32 : Loss = 0.0014723739586770535\n",
      "Epoch 107, Batch 29/32 : Loss = 0.0026671772357076406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107, Batch 30/32 : Loss = 0.011925753206014633\n",
      "Epoch 107, Batch 31/32 : Loss = 0.003995377570390701\n",
      "Epoch 107 finished in 0.052651158968607586 minutes\n",
      "Epoch 107 training_loss = 0.0046317968517541885\n",
      "-----C----DD----E----g----mm-----'---m------F----<-----Q----8---22---- => CDEgm'mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----44----rr--{-------%----/--''-)----w------&-----NN-----++----P----- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---.---c-----O------w------u---``---u---.---R-----{--33----\"---=------ => .cOwu`u.R{3\"=, Ground Truth is .cOwu`u.R{3\"#\n",
      "------d-----:---X-----9-----e-----a-----F--,---8---------VV----RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 107 val_loss = 0.7418314814567566, word_accuracy = 0.66\n",
      "Epoch 108, Batch 0/32 : Loss = 0.000564592017326504\n",
      "Epoch 108, Batch 1/32 : Loss = 0.000576216378249228\n",
      "Epoch 108, Batch 2/32 : Loss = 0.0009606508538126945\n",
      "Epoch 108, Batch 3/32 : Loss = 0.00048177127609960735\n",
      "Epoch 108, Batch 4/32 : Loss = 0.0027772225439548492\n",
      "Epoch 108, Batch 5/32 : Loss = 0.0005887186853215098\n",
      "Epoch 108, Batch 6/32 : Loss = 0.0005859966622665524\n",
      "Epoch 108, Batch 7/32 : Loss = 0.0007978116627782583\n",
      "Epoch 108, Batch 8/32 : Loss = 0.001494233962148428\n",
      "Epoch 108, Batch 9/32 : Loss = 0.0016439524479210377\n",
      "Epoch 108, Batch 10/32 : Loss = 0.000590369338169694\n",
      "Epoch 108, Batch 11/32 : Loss = 0.0013378029689192772\n",
      "Epoch 108, Batch 12/32 : Loss = 0.0005272041889838874\n",
      "Epoch 108, Batch 13/32 : Loss = 0.0012396140955388546\n",
      "Epoch 108, Batch 14/32 : Loss = 0.0008487445884384215\n",
      "Epoch 108, Batch 15/32 : Loss = 0.00368898781016469\n",
      "Epoch 108, Batch 16/32 : Loss = 0.0021333154290914536\n",
      "Epoch 108, Batch 17/32 : Loss = 0.031788893043994904\n",
      "Epoch 108, Batch 18/32 : Loss = 0.0008989181951619685\n",
      "Epoch 108, Batch 19/32 : Loss = 0.0012214642483741045\n",
      "Epoch 108, Batch 20/32 : Loss = 0.0006655043689534068\n",
      "Epoch 108, Batch 21/32 : Loss = 0.0006212524604052305\n",
      "Epoch 108, Batch 22/32 : Loss = 0.0019478087779134512\n",
      "Epoch 108, Batch 23/32 : Loss = 0.0014381243381649256\n",
      "Epoch 108, Batch 24/32 : Loss = 0.0006592270219698548\n",
      "Epoch 108, Batch 25/32 : Loss = 0.001163365668617189\n",
      "Epoch 108, Batch 26/32 : Loss = 0.0017188177444040775\n",
      "Epoch 108, Batch 27/32 : Loss = 0.002233137609437108\n",
      "Epoch 108, Batch 28/32 : Loss = 0.0008357954793609679\n",
      "Epoch 108, Batch 29/32 : Loss = 0.000699673080816865\n",
      "Epoch 108, Batch 30/32 : Loss = 0.003945706412196159\n",
      "Epoch 108, Batch 31/32 : Loss = 0.0008830591687001288\n",
      "Epoch 108 finished in 0.05320511658986409 minutes\n",
      "Epoch 108 training_loss = 0.002274226164445281\n",
      "----++---:--z---7---8----d----SS----v--5----S---JJ---BB--- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----5----9----g----m------J----u--x----C--X---.--d---\\---- => 59gmJuxCX.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "----\"--]---t--4----e---^^---W-------Q----4---->>----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "-----Y----W-----]]-i-\\\\--|----MM----<<----MM----8---88---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 108 val_loss = 0.7606686353683472, word_accuracy = 0.67\n",
      "Epoch 109, Batch 0/32 : Loss = 0.012215610593557358\n",
      "Epoch 109, Batch 1/32 : Loss = 0.0026443256065249443\n",
      "Epoch 109, Batch 2/32 : Loss = 0.0037211854942142963\n",
      "Epoch 109, Batch 3/32 : Loss = 0.0034005464985966682\n",
      "Epoch 109, Batch 4/32 : Loss = 0.0004911748692393303\n",
      "Epoch 109, Batch 5/32 : Loss = 0.0008755429880693555\n",
      "Epoch 109, Batch 6/32 : Loss = 0.0009239588398486376\n",
      "Epoch 109, Batch 7/32 : Loss = 0.0008940268307924271\n",
      "Epoch 109, Batch 8/32 : Loss = 0.0014801484066992998\n",
      "Epoch 109, Batch 9/32 : Loss = 0.0006728684529662132\n",
      "Epoch 109, Batch 10/32 : Loss = 0.0006846309988759458\n",
      "Epoch 109, Batch 11/32 : Loss = 0.0007300388533622026\n",
      "Epoch 109, Batch 12/32 : Loss = 0.001914530759677291\n",
      "Epoch 109, Batch 13/32 : Loss = 0.0006606210372410715\n",
      "Epoch 109, Batch 14/32 : Loss = 0.0006011095829308033\n",
      "Epoch 109, Batch 15/32 : Loss = 0.0009222713997587562\n",
      "Epoch 109, Batch 16/32 : Loss = 0.0007690146449021995\n",
      "Epoch 109, Batch 17/32 : Loss = 0.0007593624177388847\n",
      "Epoch 109, Batch 18/32 : Loss = 0.0012207005638629198\n",
      "Epoch 109, Batch 19/32 : Loss = 0.0005487541784532368\n",
      "Epoch 109, Batch 20/32 : Loss = 0.0013768820790573955\n",
      "Epoch 109, Batch 21/32 : Loss = 0.0004066437832079828\n",
      "Epoch 109, Batch 22/32 : Loss = 0.0014277081936597824\n",
      "Epoch 109, Batch 23/32 : Loss = 0.0007480325875803828\n",
      "Epoch 109, Batch 24/32 : Loss = 0.0010272662620991468\n",
      "Epoch 109, Batch 25/32 : Loss = 0.0005173836834728718\n",
      "Epoch 109, Batch 26/32 : Loss = 0.0018280481453984976\n",
      "Epoch 109, Batch 27/32 : Loss = 0.0006864027236588299\n",
      "Epoch 109, Batch 28/32 : Loss = 0.0009377419482916594\n",
      "Epoch 109, Batch 29/32 : Loss = 0.020582864060997963\n",
      "Epoch 109, Batch 30/32 : Loss = 0.002073362236842513\n",
      "Epoch 109, Batch 31/32 : Loss = 0.0009050205699168146\n",
      "Epoch 109 finished in 0.052411643664042155 minutes\n",
      "Epoch 109 training_loss = 0.0021801088005304337\n",
      "-----#-----0----[[--x----)---RR----88---i---P-----w-----))--- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "----0----Q---6----<----<---(--TT---N---5---=----P-----m------ => 0Q6<<(TN5=Pm, Ground Truth is 0Q6<<(TN5=P(m\n",
      "--.---c----O-----w-----uu--`---u---.--R----{---3---\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----kk----O-----/---,-yy---c---**---1----P----}--##----BB---- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 109 val_loss = 0.7291952967643738, word_accuracy = 0.68\n",
      "Epoch 110, Batch 0/32 : Loss = 0.0004404305946081877\n",
      "Epoch 110, Batch 1/32 : Loss = 0.0008980011334642768\n",
      "Epoch 110, Batch 2/32 : Loss = 0.010279462672770023\n",
      "Epoch 110, Batch 3/32 : Loss = 0.0006561402697116137\n",
      "Epoch 110, Batch 4/32 : Loss = 0.000714498630259186\n",
      "Epoch 110, Batch 5/32 : Loss = 0.001665311399847269\n",
      "Epoch 110, Batch 6/32 : Loss = 0.0009273330215364695\n",
      "Epoch 110, Batch 7/32 : Loss = 0.000688987027388066\n",
      "Epoch 110, Batch 8/32 : Loss = 0.0015120906755328178\n",
      "Epoch 110, Batch 9/32 : Loss = 0.0006568807293660939\n",
      "Epoch 110, Batch 10/32 : Loss = 0.002002923283725977\n",
      "Epoch 110, Batch 11/32 : Loss = 0.0004742485180031508\n",
      "Epoch 110, Batch 12/32 : Loss = 0.0006286119460128248\n",
      "Epoch 110, Batch 13/32 : Loss = 0.0003676807682495564\n",
      "Epoch 110, Batch 14/32 : Loss = 0.002509386744350195\n",
      "Epoch 110, Batch 15/32 : Loss = 0.11318923532962799\n",
      "Epoch 110, Batch 16/32 : Loss = 0.0007444844231940806\n",
      "Epoch 110, Batch 17/32 : Loss = 0.004632467869669199\n",
      "Epoch 110, Batch 18/32 : Loss = 0.002062346786260605\n",
      "Epoch 110, Batch 19/32 : Loss = 0.00036854343488812447\n",
      "Epoch 110, Batch 20/32 : Loss = 0.0509914830327034\n",
      "Epoch 110, Batch 21/32 : Loss = 0.002626397181302309\n",
      "Epoch 110, Batch 22/32 : Loss = 0.014460927806794643\n",
      "Epoch 110, Batch 23/32 : Loss = 0.0007805041968822479\n",
      "Epoch 110, Batch 24/32 : Loss = 0.23380908370018005\n",
      "Epoch 110, Batch 25/32 : Loss = 0.0006165751256048679\n",
      "Epoch 110, Batch 26/32 : Loss = 0.0029767025262117386\n",
      "Epoch 110, Batch 27/32 : Loss = 0.0005941687850281596\n",
      "Epoch 110, Batch 28/32 : Loss = 0.000581281550694257\n",
      "Epoch 110, Batch 29/32 : Loss = 0.005775256082415581\n",
      "Epoch 110, Batch 30/32 : Loss = 0.0009126114891842008\n",
      "Epoch 110, Batch 31/32 : Loss = 0.002762040589004755\n",
      "Epoch 110 finished in 0.0523162047068278 minutes\n",
      "Epoch 110 training_loss = 0.014775561168789864\n",
      "------Q------3-----g----I---z---##----YY---:---]---q-----+----**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---;------33----\\-----$----->------S-----\\----MMM-----i---BB------ => ;-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "-----0----Q---66---<----<<---(---T---N----5---=----P---(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----5---->---:---*---Y----AA---'--O-----DD---**--##----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 110 val_loss = 0.7584465146064758, word_accuracy = 0.66\n",
      "Epoch 111, Batch 0/32 : Loss = 0.00093887432012707\n",
      "Epoch 111, Batch 1/32 : Loss = 0.0014881480019539595\n",
      "Epoch 111, Batch 2/32 : Loss = 0.0011657739523798227\n",
      "Epoch 111, Batch 3/32 : Loss = 0.001959097571671009\n",
      "Epoch 111, Batch 4/32 : Loss = 0.0015923988539725542\n",
      "Epoch 111, Batch 5/32 : Loss = 0.002866494469344616\n",
      "Epoch 111, Batch 6/32 : Loss = 0.0013451927807182074\n",
      "Epoch 111, Batch 7/32 : Loss = 0.0010011405684053898\n",
      "Epoch 111, Batch 8/32 : Loss = 0.0010675209341570735\n",
      "Epoch 111, Batch 9/32 : Loss = 0.0012032643426209688\n",
      "Epoch 111, Batch 10/32 : Loss = 0.0006990908295847476\n",
      "Epoch 111, Batch 11/32 : Loss = 0.0034526491072028875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111, Batch 12/32 : Loss = 0.002795228734612465\n",
      "Epoch 111, Batch 13/32 : Loss = 0.11373226344585419\n",
      "Epoch 111, Batch 14/32 : Loss = 0.0021681389771401882\n",
      "Epoch 111, Batch 15/32 : Loss = 0.004171560052782297\n",
      "Epoch 111, Batch 16/32 : Loss = 0.026617972180247307\n",
      "Epoch 111, Batch 17/32 : Loss = 0.000984351383522153\n",
      "Epoch 111, Batch 18/32 : Loss = 0.10601397603750229\n",
      "Epoch 111, Batch 19/32 : Loss = 0.0033426634036004543\n",
      "Epoch 111, Batch 20/32 : Loss = 0.0006193373119458556\n",
      "Epoch 111, Batch 21/32 : Loss = 0.0017571108182892203\n",
      "Epoch 111, Batch 22/32 : Loss = 0.003543111262843013\n",
      "Epoch 111, Batch 23/32 : Loss = 0.026759861037135124\n",
      "Epoch 111, Batch 24/32 : Loss = 0.001766541157849133\n",
      "Epoch 111, Batch 25/32 : Loss = 0.003819109406322241\n",
      "Epoch 111, Batch 26/32 : Loss = 0.001091119833290577\n",
      "Epoch 111, Batch 27/32 : Loss = 0.007144606672227383\n",
      "Epoch 111, Batch 28/32 : Loss = 0.053722258657217026\n",
      "Epoch 111, Batch 29/32 : Loss = 0.0040198699571192265\n",
      "Epoch 111, Batch 30/32 : Loss = 0.0011793153826147318\n",
      "Epoch 111, Batch 31/32 : Loss = 0.0009503398323431611\n",
      "Epoch 111 finished in 0.05198107560475667 minutes\n",
      "Epoch 111 training_loss = 0.012342065572738647\n",
      "----kk---$$---l--F-----D----ee---hh---]]--kk---0---\\\\---X------- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----Y-----W------]--ii-\\---||----MM-----<------M-----8----8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "--..--c----OO-----w-----uu--``--uu--.---R----{--3----\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----C----D----E----g----m-----\"--mm-----F---<<----Q----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 111 val_loss = 0.7749975919723511, word_accuracy = 0.67\n",
      "Epoch 112, Batch 0/32 : Loss = 0.0013611926697194576\n",
      "Epoch 112, Batch 1/32 : Loss = 0.02059916779398918\n",
      "Epoch 112, Batch 2/32 : Loss = 0.02004258893430233\n",
      "Epoch 112, Batch 3/32 : Loss = 0.0022032142151147127\n",
      "Epoch 112, Batch 4/32 : Loss = 0.0007788412040099502\n",
      "Epoch 112, Batch 5/32 : Loss = 0.00540123600512743\n",
      "Epoch 112, Batch 6/32 : Loss = 0.0008417211938649416\n",
      "Epoch 112, Batch 7/32 : Loss = 0.0006923783803358674\n",
      "Epoch 112, Batch 8/32 : Loss = 0.003058707807213068\n",
      "Epoch 112, Batch 9/32 : Loss = 0.0009128438541665673\n",
      "Epoch 112, Batch 10/32 : Loss = 0.07240120321512222\n",
      "Epoch 112, Batch 11/32 : Loss = 0.013206163421273232\n",
      "Epoch 112, Batch 12/32 : Loss = 0.0012566519435495138\n",
      "Epoch 112, Batch 13/32 : Loss = 0.014113367535173893\n",
      "Epoch 112, Batch 14/32 : Loss = 0.01998250186443329\n",
      "Epoch 112, Batch 15/32 : Loss = 0.0029589051846414804\n",
      "Epoch 112, Batch 16/32 : Loss = 0.0008324552327394485\n",
      "Epoch 112, Batch 17/32 : Loss = 0.0008725705556571484\n",
      "Epoch 112, Batch 18/32 : Loss = 0.002949954941868782\n",
      "Epoch 112, Batch 19/32 : Loss = 0.007112601306289434\n",
      "Epoch 112, Batch 20/32 : Loss = 0.0014247451908886433\n",
      "Epoch 112, Batch 21/32 : Loss = 0.0015484150499105453\n",
      "Epoch 112, Batch 22/32 : Loss = 0.0019631506875157356\n",
      "Epoch 112, Batch 23/32 : Loss = 0.0019164062105119228\n",
      "Epoch 112, Batch 24/32 : Loss = 0.0009281989187002182\n",
      "Epoch 112, Batch 25/32 : Loss = 0.0015603352803736925\n",
      "Epoch 112, Batch 26/32 : Loss = 0.0008069543982855976\n",
      "Epoch 112, Batch 27/32 : Loss = 0.0010677960235625505\n",
      "Epoch 112, Batch 28/32 : Loss = 0.0016693283105269074\n",
      "Epoch 112, Batch 29/32 : Loss = 0.0014643820468336344\n",
      "Epoch 112, Batch 30/32 : Loss = 0.0009563058847561479\n",
      "Epoch 112, Batch 31/32 : Loss = 0.0005906576989218593\n",
      "Epoch 112 finished in 0.05126177469889323 minutes\n",
      "Epoch 112 training_loss = 0.0066492571495473385\n",
      "---2---p---:-mm----x---a---z--nn---@@----C---yy----%-----%---- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----J--;---q----++--//---z--yy---U-------%----U-----1---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "---oo---\"---}--!---B----r---&-----9---;--`--OO-----w----}----- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "--..--c----O-----ww-----u--``--uu--.--RR----{--3---\"\"--#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 112 val_loss = 0.7532596588134766, word_accuracy = 0.69\n",
      "Epoch 113, Batch 0/32 : Loss = 0.0011212180834263563\n",
      "Epoch 113, Batch 1/32 : Loss = 0.004338996950536966\n",
      "Epoch 113, Batch 2/32 : Loss = 0.0012207216350361705\n",
      "Epoch 113, Batch 3/32 : Loss = 0.0007986564887687564\n",
      "Epoch 113, Batch 4/32 : Loss = 0.0009950000094249845\n",
      "Epoch 113, Batch 5/32 : Loss = 0.005871964152902365\n",
      "Epoch 113, Batch 6/32 : Loss = 0.0008522143471054733\n",
      "Epoch 113, Batch 7/32 : Loss = 0.002362903906032443\n",
      "Epoch 113, Batch 8/32 : Loss = 0.002233523642644286\n",
      "Epoch 113, Batch 9/32 : Loss = 0.001730641582980752\n",
      "Epoch 113, Batch 10/32 : Loss = 0.0007969702128320932\n",
      "Epoch 113, Batch 11/32 : Loss = 0.0027647740207612514\n",
      "Epoch 113, Batch 12/32 : Loss = 0.0009039548458531499\n",
      "Epoch 113, Batch 13/32 : Loss = 0.0019605413544923067\n",
      "Epoch 113, Batch 14/32 : Loss = 0.0009801201522350311\n",
      "Epoch 113, Batch 15/32 : Loss = 0.06995919346809387\n",
      "Epoch 113, Batch 16/32 : Loss = 0.0010075496975332499\n",
      "Epoch 113, Batch 17/32 : Loss = 0.005390272941440344\n",
      "Epoch 113, Batch 18/32 : Loss = 0.004843258764594793\n",
      "Epoch 113, Batch 19/32 : Loss = 0.0012965187197551131\n",
      "Epoch 113, Batch 20/32 : Loss = 0.0012146010994911194\n",
      "Epoch 113, Batch 21/32 : Loss = 0.0008600138826295733\n",
      "Epoch 113, Batch 22/32 : Loss = 0.0016362759051844478\n",
      "Epoch 113, Batch 23/32 : Loss = 0.0007653174689039588\n",
      "Epoch 113, Batch 24/32 : Loss = 0.0013818322913721204\n",
      "Epoch 113, Batch 25/32 : Loss = 0.003121475223451853\n",
      "Epoch 113, Batch 26/32 : Loss = 0.0011875381460413337\n",
      "Epoch 113, Batch 27/32 : Loss = 0.0005760759813711047\n",
      "Epoch 113, Batch 28/32 : Loss = 0.0013758349232375622\n",
      "Epoch 113, Batch 29/32 : Loss = 0.0008488771854899824\n",
      "Epoch 113, Batch 30/32 : Loss = 0.0020979344844818115\n",
      "Epoch 113, Batch 31/32 : Loss = 0.013008283451199532\n",
      "Epoch 113 finished in 0.05256386995315552 minutes\n",
      "Epoch 113 training_loss = 0.00411633076146245\n",
      "-----55----99----gg-----m-------JJ----uu---X-----C---xx--.---d----\\\\----- => 59gmJuXCx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "-----E-------0-----[[---x----))---RR------88----ii---P------w-------))--- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "-_--55---->>----:--**---Y----AA---'---O-----DD----*---##----O-----gg----- => _5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----J---;---q------+----/----z---yy----U---------%----UU-----1----x---_-- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 113 val_loss = 0.7772974371910095, word_accuracy = 0.63\n",
      "Epoch 114, Batch 0/32 : Loss = 0.003045962890610099\n",
      "Epoch 114, Batch 1/32 : Loss = 0.0012088968651369214\n",
      "Epoch 114, Batch 2/32 : Loss = 0.0008757452014833689\n",
      "Epoch 114, Batch 3/32 : Loss = 0.00525532616302371\n",
      "Epoch 114, Batch 4/32 : Loss = 0.0014110766351222992\n",
      "Epoch 114, Batch 5/32 : Loss = 0.0007070287829264998\n",
      "Epoch 114, Batch 6/32 : Loss = 0.05003584921360016\n",
      "Epoch 114, Batch 7/32 : Loss = 0.0017276036087423563\n",
      "Epoch 114, Batch 8/32 : Loss = 0.00181647262070328\n",
      "Epoch 114, Batch 9/32 : Loss = 0.001666433410719037\n",
      "Epoch 114, Batch 10/32 : Loss = 0.0017802217043936253\n",
      "Epoch 114, Batch 11/32 : Loss = 0.00680726021528244\n",
      "Epoch 114, Batch 12/32 : Loss = 0.001129569485783577\n",
      "Epoch 114, Batch 13/32 : Loss = 0.0012545485515147448\n",
      "Epoch 114, Batch 14/32 : Loss = 0.003704969072714448\n",
      "Epoch 114, Batch 15/32 : Loss = 0.001652086735703051\n",
      "Epoch 114, Batch 16/32 : Loss = 0.002937961369752884\n",
      "Epoch 114, Batch 17/32 : Loss = 0.011609183624386787\n",
      "Epoch 114, Batch 18/32 : Loss = 0.0011560388375073671\n",
      "Epoch 114, Batch 19/32 : Loss = 0.0016227040905505419\n",
      "Epoch 114, Batch 20/32 : Loss = 0.0013027845416218042\n",
      "Epoch 114, Batch 21/32 : Loss = 0.0010087464470416307\n",
      "Epoch 114, Batch 22/32 : Loss = 0.001106776064261794\n",
      "Epoch 114, Batch 23/32 : Loss = 0.0010137991048395634\n",
      "Epoch 114, Batch 24/32 : Loss = 0.0008843644172884524\n",
      "Epoch 114, Batch 25/32 : Loss = 0.004636233672499657\n",
      "Epoch 114, Batch 26/32 : Loss = 0.0015153439017012715\n",
      "Epoch 114, Batch 27/32 : Loss = 0.0011171691585332155\n",
      "Epoch 114, Batch 28/32 : Loss = 0.0008232452673837543\n",
      "Epoch 114, Batch 29/32 : Loss = 0.0008215464767999947\n",
      "Epoch 114, Batch 30/32 : Loss = 0.0012303685070946813\n",
      "Epoch 114, Batch 31/32 : Loss = 0.0015822534915059805\n",
      "Epoch 114 finished in 0.0518691102663676 minutes\n",
      "Epoch 114 training_loss = 0.003761063562706113\n",
      "-------z-----0-----\"-----G------//----~------$$-----c------1----j---t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "-------QQ------3-----g-----I---z----#------Y----:--]]---q-----++----**--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----W-------=----22---++---EE----1---nn----T----X---r----C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "---kk----$----||--'--,--99----Y-----NN-----W-------m--------T----8------- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 114 val_loss = 0.7549235224723816, word_accuracy = 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Batch 0/32 : Loss = 0.005859833210706711\n",
      "Epoch 115, Batch 1/32 : Loss = 0.0009165027877315879\n",
      "Epoch 115, Batch 2/32 : Loss = 0.0007304681930691004\n",
      "Epoch 115, Batch 3/32 : Loss = 0.007591896690428257\n",
      "Epoch 115, Batch 4/32 : Loss = 0.014101128093898296\n",
      "Epoch 115, Batch 5/32 : Loss = 0.0010724372696131468\n",
      "Epoch 115, Batch 6/32 : Loss = 0.0013683128636330366\n",
      "Epoch 115, Batch 7/32 : Loss = 0.001739327097311616\n",
      "Epoch 115, Batch 8/32 : Loss = 0.011267859488725662\n",
      "Epoch 115, Batch 9/32 : Loss = 0.0006839815760031343\n",
      "Epoch 115, Batch 10/32 : Loss = 0.0007714851526543498\n",
      "Epoch 115, Batch 11/32 : Loss = 0.0007557418430224061\n",
      "Epoch 115, Batch 12/32 : Loss = 0.0007546249544247985\n",
      "Epoch 115, Batch 13/32 : Loss = 0.0007321811863221228\n",
      "Epoch 115, Batch 14/32 : Loss = 0.0008641361491754651\n",
      "Epoch 115, Batch 15/32 : Loss = 0.0007175264181569219\n",
      "Epoch 115, Batch 16/32 : Loss = 0.0009344862774014473\n",
      "Epoch 115, Batch 17/32 : Loss = 0.001514220959506929\n",
      "Epoch 115, Batch 18/32 : Loss = 0.0011453244369477034\n",
      "Epoch 115, Batch 19/32 : Loss = 0.0016193955671042204\n",
      "Epoch 115, Batch 20/32 : Loss = 0.0005665847565978765\n",
      "Epoch 115, Batch 21/32 : Loss = 0.002531287260353565\n",
      "Epoch 115, Batch 22/32 : Loss = 0.007736856117844582\n",
      "Epoch 115, Batch 23/32 : Loss = 0.0015955715207383037\n",
      "Epoch 115, Batch 24/32 : Loss = 0.0005367969279177487\n",
      "Epoch 115, Batch 25/32 : Loss = 0.0018725015688687563\n",
      "Epoch 115, Batch 26/32 : Loss = 0.0005098902620375156\n",
      "Epoch 115, Batch 27/32 : Loss = 0.005052379332482815\n",
      "Epoch 115, Batch 28/32 : Loss = 0.0014096328523010015\n",
      "Epoch 115, Batch 29/32 : Loss = 0.020672855898737907\n",
      "Epoch 115, Batch 30/32 : Loss = 0.0042496370151638985\n",
      "Epoch 115, Batch 31/32 : Loss = 0.0009011678048409522\n",
      "Epoch 115 finished in 0.19779977798461915 minutes\n",
      "Epoch 115 training_loss = 0.00327670737169683\n",
      "------8------K------l---Z------5-----pp-----$$----aa-----}----w-----,---- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---kk----$----|---'-----99----Y-----NN-----W-------m--------T----8------- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----55---->>----:--*----Y----AA---'---O-----DD----*---##----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "-----#-------0-----[[---x----))---RR------88----ii---P------w-------)---- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "Epoch 115 val_loss = 0.7180168032646179, word_accuracy = 0.68\n",
      "Epoch 116, Batch 0/32 : Loss = 0.001038622809574008\n",
      "Epoch 116, Batch 1/32 : Loss = 0.0017950511537492275\n",
      "Epoch 116, Batch 2/32 : Loss = 0.001452307216823101\n",
      "Epoch 116, Batch 3/32 : Loss = 0.0020114565268158913\n",
      "Epoch 116, Batch 4/32 : Loss = 0.000934625044465065\n",
      "Epoch 116, Batch 5/32 : Loss = 0.001013228204101324\n",
      "Epoch 116, Batch 6/32 : Loss = 0.0011133326916024089\n",
      "Epoch 116, Batch 7/32 : Loss = 0.0004446157836355269\n",
      "Epoch 116, Batch 8/32 : Loss = 0.0023786365054547787\n",
      "Epoch 116, Batch 9/32 : Loss = 0.001134649384766817\n",
      "Epoch 116, Batch 10/32 : Loss = 0.010472963564097881\n",
      "Epoch 116, Batch 11/32 : Loss = 0.001156525919213891\n",
      "Epoch 116, Batch 12/32 : Loss = 0.00043857970740646124\n",
      "Epoch 116, Batch 13/32 : Loss = 0.0010672463104128838\n",
      "Epoch 116, Batch 14/32 : Loss = 0.0012577720917761326\n",
      "Epoch 116, Batch 15/32 : Loss = 0.0019568565767258406\n",
      "Epoch 116, Batch 16/32 : Loss = 0.0005900990217924118\n",
      "Epoch 116, Batch 17/32 : Loss = 0.0006529574748128653\n",
      "Epoch 116, Batch 18/32 : Loss = 0.0007888973341323435\n",
      "Epoch 116, Batch 19/32 : Loss = 0.00046463750186376274\n",
      "Epoch 116, Batch 20/32 : Loss = 0.0009915003320202231\n",
      "Epoch 116, Batch 21/32 : Loss = 0.0010926456889137626\n",
      "Epoch 116, Batch 22/32 : Loss = 0.0012255208566784859\n",
      "Epoch 116, Batch 23/32 : Loss = 0.003988477401435375\n",
      "Epoch 116, Batch 24/32 : Loss = 0.0005687727825716138\n",
      "Epoch 116, Batch 25/32 : Loss = 0.001065557124093175\n",
      "Epoch 116, Batch 26/32 : Loss = 0.0008815007167868316\n",
      "Epoch 116, Batch 27/32 : Loss = 0.0011688254307955503\n",
      "Epoch 116, Batch 28/32 : Loss = 0.009210516698658466\n",
      "Epoch 116, Batch 29/32 : Loss = 0.00045110139762982726\n",
      "Epoch 116, Batch 30/32 : Loss = 0.0009193253936246037\n",
      "Epoch 116, Batch 31/32 : Loss = 0.3526732325553894\n",
      "Epoch 116 finished in 0.053278199831644696 minutes\n",
      "Epoch 116 training_loss = 0.003142520785331726\n",
      "------d-----!---NN------r---AA----j---*-----$----3-----hh-----5-----nn---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---kk----$-----|--''-,---9----Y------N-----WW-------mm-------T----8------- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "-----WW------==----2----+----E----11---n----T----X----r---C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "-----++---:---z-----7---88-----d-----S-----vv---5-----S----JJ-----B------- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 116 val_loss = 0.7201564908027649, word_accuracy = 0.75\n",
      "Epoch 117, Batch 0/32 : Loss = 0.0005697004962712526\n",
      "Epoch 117, Batch 1/32 : Loss = 0.0008202447206713259\n",
      "Epoch 117, Batch 2/32 : Loss = 0.01225088257342577\n",
      "Epoch 117, Batch 3/32 : Loss = 0.002236589090898633\n",
      "Epoch 117, Batch 4/32 : Loss = 0.0010673182550817728\n",
      "Epoch 117, Batch 5/32 : Loss = 0.0010167662985622883\n",
      "Epoch 117, Batch 6/32 : Loss = 0.0007901513017714024\n",
      "Epoch 117, Batch 7/32 : Loss = 0.0007144295377656817\n",
      "Epoch 117, Batch 8/32 : Loss = 0.0007978269131854177\n",
      "Epoch 117, Batch 9/32 : Loss = 0.002816735301166773\n",
      "Epoch 117, Batch 10/32 : Loss = 0.004725300706923008\n",
      "Epoch 117, Batch 11/32 : Loss = 0.0005934177897870541\n",
      "Epoch 117, Batch 12/32 : Loss = 0.0010054868180304766\n",
      "Epoch 117, Batch 13/32 : Loss = 0.0032011610455811024\n",
      "Epoch 117, Batch 14/32 : Loss = 0.000588175025768578\n",
      "Epoch 117, Batch 15/32 : Loss = 0.0004989613080397248\n",
      "Epoch 117, Batch 16/32 : Loss = 0.0009466445771977305\n",
      "Epoch 117, Batch 17/32 : Loss = 0.0007446688832715154\n",
      "Epoch 117, Batch 18/32 : Loss = 0.0010918412590399384\n",
      "Epoch 117, Batch 19/32 : Loss = 0.0006544676725752652\n",
      "Epoch 117, Batch 20/32 : Loss = 0.005608223844319582\n",
      "Epoch 117, Batch 21/32 : Loss = 0.007146082818508148\n",
      "Epoch 117, Batch 22/32 : Loss = 0.00718969851732254\n",
      "Epoch 117, Batch 23/32 : Loss = 0.0007186622824519873\n",
      "Epoch 117, Batch 24/32 : Loss = 0.0005458774976432323\n",
      "Epoch 117, Batch 25/32 : Loss = 0.0005448814481496811\n",
      "Epoch 117, Batch 26/32 : Loss = 0.0005738907493650913\n",
      "Epoch 117, Batch 27/32 : Loss = 0.0011228860821574926\n",
      "Epoch 117, Batch 28/32 : Loss = 0.0028755590319633484\n",
      "Epoch 117, Batch 29/32 : Loss = 0.0007690014317631721\n",
      "Epoch 117, Batch 30/32 : Loss = 0.0007808441296219826\n",
      "Epoch 117, Batch 31/32 : Loss = 0.001214721822179854\n",
      "Epoch 117 finished in 0.05134820540746053 minutes\n",
      "Epoch 117 training_loss = 0.0020934368949383497\n",
      "----55----9----g-----m------JJ---uu---x---c----x--.--dd---\\----- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "-----c-----R----;--99----y----??---2----dd---ii--O------{--!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----++--::--z----7---8----dd----S-----v--55----S----J----B------ => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "-----d----!--NN-----r---A----j--**---$----3----hh----5----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 117 val_loss = 0.7259765267372131, word_accuracy = 0.63\n",
      "Epoch 118, Batch 0/32 : Loss = 0.0007169374730437994\n",
      "Epoch 118, Batch 1/32 : Loss = 0.03651168569922447\n",
      "Epoch 118, Batch 2/32 : Loss = 0.0005139115964993834\n",
      "Epoch 118, Batch 3/32 : Loss = 0.0009290255256928504\n",
      "Epoch 118, Batch 4/32 : Loss = 0.0008914259960874915\n",
      "Epoch 118, Batch 5/32 : Loss = 0.0006236222106963396\n",
      "Epoch 118, Batch 6/32 : Loss = 0.004174896515905857\n",
      "Epoch 118, Batch 7/32 : Loss = 0.0025570327416062355\n",
      "Epoch 118, Batch 8/32 : Loss = 0.0008165379404090345\n",
      "Epoch 118, Batch 9/32 : Loss = 0.0006784067372791469\n",
      "Epoch 118, Batch 10/32 : Loss = 0.0005167514318600297\n",
      "Epoch 118, Batch 11/32 : Loss = 0.0005291844718158245\n",
      "Epoch 118, Batch 12/32 : Loss = 0.001711412682197988\n",
      "Epoch 118, Batch 13/32 : Loss = 0.0006007523043081164\n",
      "Epoch 118, Batch 14/32 : Loss = 0.0007580063538625836\n",
      "Epoch 118, Batch 15/32 : Loss = 0.0006299457745626569\n",
      "Epoch 118, Batch 16/32 : Loss = 0.0007117593195289373\n",
      "Epoch 118, Batch 17/32 : Loss = 0.0009683697717264295\n",
      "Epoch 118, Batch 18/32 : Loss = 0.0009308089502155781\n",
      "Epoch 118, Batch 19/32 : Loss = 0.0008562757866457105\n",
      "Epoch 118, Batch 20/32 : Loss = 0.00049277872312814\n",
      "Epoch 118, Batch 21/32 : Loss = 0.0009343089768663049\n",
      "Epoch 118, Batch 22/32 : Loss = 0.001138012739829719\n",
      "Epoch 118, Batch 23/32 : Loss = 0.0014950461918488145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Batch 24/32 : Loss = 0.0007655303925275803\n",
      "Epoch 118, Batch 25/32 : Loss = 0.019870316609740257\n",
      "Epoch 118, Batch 26/32 : Loss = 0.002696247771382332\n",
      "Epoch 118, Batch 27/32 : Loss = 0.0004907005932182074\n",
      "Epoch 118, Batch 28/32 : Loss = 0.0006491661770269275\n",
      "Epoch 118, Batch 29/32 : Loss = 0.0011477320222184062\n",
      "Epoch 118, Batch 30/32 : Loss = 0.0006678183563053608\n",
      "Epoch 118, Batch 31/32 : Loss = 0.0004808974335901439\n",
      "Epoch 118 finished in 0.05089157025019328 minutes\n",
      "Epoch 118 training_loss = 0.002796289511024952\n",
      "-----c-----R----;---9-----y----??---2-----d---ii---O------{--!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----YY----WW------]--ii-\\----/----MM------<------MM-----8----8---- => YW]i\\/M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----0----Q---66---<----<<---(--TT---N----5---==---P---(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "-----0------c----+++----bb----I--\"\"---bb-----66---..---Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 118 val_loss = 0.7330173850059509, word_accuracy = 0.67\n",
      "Epoch 119, Batch 0/32 : Loss = 0.0009192618308588862\n",
      "Epoch 119, Batch 1/32 : Loss = 0.0009698222856968641\n",
      "Epoch 119, Batch 2/32 : Loss = 0.0009522865293547511\n",
      "Epoch 119, Batch 3/32 : Loss = 0.0009280822123400867\n",
      "Epoch 119, Batch 4/32 : Loss = 0.0006866974290460348\n",
      "Epoch 119, Batch 5/32 : Loss = 0.00046987508540041745\n",
      "Epoch 119, Batch 6/32 : Loss = 0.002871924079954624\n",
      "Epoch 119, Batch 7/32 : Loss = 0.0009853749070316553\n",
      "Epoch 119, Batch 8/32 : Loss = 0.001435338519513607\n",
      "Epoch 119, Batch 9/32 : Loss = 0.007240935228765011\n",
      "Epoch 119, Batch 10/32 : Loss = 0.0008258514571934938\n",
      "Epoch 119, Batch 11/32 : Loss = 0.009460857138037682\n",
      "Epoch 119, Batch 12/32 : Loss = 0.0007013094727881253\n",
      "Epoch 119, Batch 13/32 : Loss = 0.0009136712178587914\n",
      "Epoch 119, Batch 14/32 : Loss = 0.000739390030503273\n",
      "Epoch 119, Batch 15/32 : Loss = 0.0011586272157728672\n",
      "Epoch 119, Batch 16/32 : Loss = 0.006302365101873875\n",
      "Epoch 119, Batch 17/32 : Loss = 0.0011697917943820357\n",
      "Epoch 119, Batch 18/32 : Loss = 0.0017302031628787518\n",
      "Epoch 119, Batch 19/32 : Loss = 0.0005425043636932969\n",
      "Epoch 119, Batch 20/32 : Loss = 0.0007932124426588416\n",
      "Epoch 119, Batch 21/32 : Loss = 0.0005322670913301408\n",
      "Epoch 119, Batch 22/32 : Loss = 0.006229894235730171\n",
      "Epoch 119, Batch 23/32 : Loss = 0.0008947253227233887\n",
      "Epoch 119, Batch 24/32 : Loss = 0.002131642773747444\n",
      "Epoch 119, Batch 25/32 : Loss = 0.0019489787518978119\n",
      "Epoch 119, Batch 26/32 : Loss = 0.0013150183949619532\n",
      "Epoch 119, Batch 27/32 : Loss = 0.0005132368532940745\n",
      "Epoch 119, Batch 28/32 : Loss = 0.005398724693804979\n",
      "Epoch 119, Batch 29/32 : Loss = 0.0020493920892477036\n",
      "Epoch 119, Batch 30/32 : Loss = 0.0007493200246244669\n",
      "Epoch 119, Batch 31/32 : Loss = 0.000298085535177961\n",
      "Epoch 119 finished in 0.05006863673528036 minutes\n",
      "Epoch 119 training_loss = 0.0020433038007467985\n",
      "--{--BB----Y----R---aa---y---h---#----2--->>----E---4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----<-----v---O------T--``--44---VV---[---0-------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---#----G----9----E---I-==---h---55---#---2----J--))--k--- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----k---$$---|--!!---9---Y----N-----W-----mm-----T---8---- => k$|!9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 119 val_loss = 0.7333452701568604, word_accuracy = 0.63\n",
      "Epoch 120, Batch 0/32 : Loss = 0.0005890864413231611\n",
      "Epoch 120, Batch 1/32 : Loss = 0.0005324989324435592\n",
      "Epoch 120, Batch 2/32 : Loss = 0.00046467475476674736\n",
      "Epoch 120, Batch 3/32 : Loss = 0.00112244114279747\n",
      "Epoch 120, Batch 4/32 : Loss = 0.0007932896260172129\n",
      "Epoch 120, Batch 5/32 : Loss = 0.0006788893369957805\n",
      "Epoch 120, Batch 6/32 : Loss = 0.002812990453094244\n",
      "Epoch 120, Batch 7/32 : Loss = 0.0007112360908649862\n",
      "Epoch 120, Batch 8/32 : Loss = 0.0004830957914236933\n",
      "Epoch 120, Batch 9/32 : Loss = 0.00040517558227293193\n",
      "Epoch 120, Batch 10/32 : Loss = 0.0010634732898324728\n",
      "Epoch 120, Batch 11/32 : Loss = 0.001245595165528357\n",
      "Epoch 120, Batch 12/32 : Loss = 0.0006533042760565877\n",
      "Epoch 120, Batch 13/32 : Loss = 0.000891771400347352\n",
      "Epoch 120, Batch 14/32 : Loss = 0.00047397188609465957\n",
      "Epoch 120, Batch 15/32 : Loss = 0.0009864314924925566\n",
      "Epoch 120, Batch 16/32 : Loss = 0.0009127605007961392\n",
      "Epoch 120, Batch 17/32 : Loss = 0.0012744759442284703\n",
      "Epoch 120, Batch 18/32 : Loss = 0.0008536011446267366\n",
      "Epoch 120, Batch 19/32 : Loss = 0.0006618766346946359\n",
      "Epoch 120, Batch 20/32 : Loss = 0.0006015681428834796\n",
      "Epoch 120, Batch 21/32 : Loss = 0.0011601995211094618\n",
      "Epoch 120, Batch 22/32 : Loss = 0.000550426309928298\n",
      "Epoch 120, Batch 23/32 : Loss = 0.000949360488448292\n",
      "Epoch 120, Batch 24/32 : Loss = 0.0006124255014583468\n",
      "Epoch 120, Batch 25/32 : Loss = 0.0017708268715068698\n",
      "Epoch 120, Batch 26/32 : Loss = 0.0004726081097032875\n",
      "Epoch 120, Batch 27/32 : Loss = 0.0006154691800475121\n",
      "Epoch 120, Batch 28/32 : Loss = 0.0005474386271089315\n",
      "Epoch 120, Batch 29/32 : Loss = 0.0024495539255440235\n",
      "Epoch 120, Batch 30/32 : Loss = 0.00048483151476830244\n",
      "Epoch 120, Batch 31/32 : Loss = 0.0004168933955952525\n",
      "Epoch 120 finished in 0.04964160521825155 minutes\n",
      "Epoch 120 training_loss = 0.0008956613019108772\n",
      "--BB---.--Y----I--W------6----F----h----X---'--Y----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "---o---\"\"--}--!--B----r---&----9---;-``--O-----w----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---cc---RR---;--99----y----?---2----d----i---O----{{-!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----dd---!--NN----r---A---j--**---$---3---hh----5---nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 120 val_loss = 0.7014492750167847, word_accuracy = 0.67\n",
      "Epoch 121, Batch 0/32 : Loss = 0.0004122521495446563\n",
      "Epoch 121, Batch 1/32 : Loss = 0.0004402951162774116\n",
      "Epoch 121, Batch 2/32 : Loss = 0.0006452229572460055\n",
      "Epoch 121, Batch 3/32 : Loss = 0.0004944135434925556\n",
      "Epoch 121, Batch 4/32 : Loss = 0.0014586197212338448\n",
      "Epoch 121, Batch 5/32 : Loss = 0.000463956908788532\n",
      "Epoch 121, Batch 6/32 : Loss = 0.0003949155507143587\n",
      "Epoch 121, Batch 7/32 : Loss = 0.003990645054727793\n",
      "Epoch 121, Batch 8/32 : Loss = 0.00043694349005818367\n",
      "Epoch 121, Batch 9/32 : Loss = 0.00045852019684389234\n",
      "Epoch 121, Batch 10/32 : Loss = 0.014472516253590584\n",
      "Epoch 121, Batch 11/32 : Loss = 0.0005244403146207333\n",
      "Epoch 121, Batch 12/32 : Loss = 0.0006507468642666936\n",
      "Epoch 121, Batch 13/32 : Loss = 0.0007373682456091046\n",
      "Epoch 121, Batch 14/32 : Loss = 0.0033719672355800867\n",
      "Epoch 121, Batch 15/32 : Loss = 0.0014645049814134836\n",
      "Epoch 121, Batch 16/32 : Loss = 0.00152557622641325\n",
      "Epoch 121, Batch 17/32 : Loss = 0.0005579175194725394\n",
      "Epoch 121, Batch 18/32 : Loss = 0.004352557472884655\n",
      "Epoch 121, Batch 19/32 : Loss = 0.010538005270063877\n",
      "Epoch 121, Batch 20/32 : Loss = 0.0006634693127125502\n",
      "Epoch 121, Batch 21/32 : Loss = 0.0010305029572919011\n",
      "Epoch 121, Batch 22/32 : Loss = 0.0012871988583356142\n",
      "Epoch 121, Batch 23/32 : Loss = 0.0022559515200555325\n",
      "Epoch 121, Batch 24/32 : Loss = 0.0014572791988030076\n",
      "Epoch 121, Batch 25/32 : Loss = 0.001118024461902678\n",
      "Epoch 121, Batch 26/32 : Loss = 0.0011351332068443298\n",
      "Epoch 121, Batch 27/32 : Loss = 0.001069012563675642\n",
      "Epoch 121, Batch 28/32 : Loss = 0.0005170368822291493\n",
      "Epoch 121, Batch 29/32 : Loss = 0.0008780845673754811\n",
      "Epoch 121, Batch 30/32 : Loss = 0.0006572785205207765\n",
      "Epoch 121, Batch 31/32 : Loss = 0.0002951787901110947\n",
      "Epoch 121 finished in 0.05069185098012288 minutes\n",
      "Epoch 121 training_loss = 0.0019115583272650838\n",
      "--BB---.---Y----I--W-------6----F----h-----X---'--Y----22--- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----kk---$$--ll-F-----D----e----h---]]--kk---0--\\\\---X----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---7---n----DD----P---nn---t--ww----d----\\---Q----a----RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-:------3----\\\\----$----->------S----\\\\----MM-----ii--BB---- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 121 val_loss = 0.7302745580673218, word_accuracy = 0.74\n",
      "Epoch 122, Batch 0/32 : Loss = 0.0020416532643139362\n",
      "Epoch 122, Batch 1/32 : Loss = 0.00088068115292117\n",
      "Epoch 122, Batch 2/32 : Loss = 0.0006655059987679124\n",
      "Epoch 122, Batch 3/32 : Loss = 0.001006465288810432\n",
      "Epoch 122, Batch 4/32 : Loss = 0.0007468239637091756\n",
      "Epoch 122, Batch 5/32 : Loss = 0.001363102812319994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122, Batch 6/32 : Loss = 0.0008636347483843565\n",
      "Epoch 122, Batch 7/32 : Loss = 0.0010732675436884165\n",
      "Epoch 122, Batch 8/32 : Loss = 0.00047359586460515857\n",
      "Epoch 122, Batch 9/32 : Loss = 0.001007283222861588\n",
      "Epoch 122, Batch 10/32 : Loss = 0.0007345077465288341\n",
      "Epoch 122, Batch 11/32 : Loss = 0.0007930616266094148\n",
      "Epoch 122, Batch 12/32 : Loss = 0.0005822323728352785\n",
      "Epoch 122, Batch 13/32 : Loss = 0.0011149633210152388\n",
      "Epoch 122, Batch 14/32 : Loss = 0.0006561029003933072\n",
      "Epoch 122, Batch 15/32 : Loss = 0.0006013024831190705\n",
      "Epoch 122, Batch 16/32 : Loss = 0.0008317430620081723\n",
      "Epoch 122, Batch 17/32 : Loss = 0.00031758638215251267\n",
      "Epoch 122, Batch 18/32 : Loss = 0.0009429557248950005\n",
      "Epoch 122, Batch 19/32 : Loss = 0.000646699802018702\n",
      "Epoch 122, Batch 20/32 : Loss = 0.000411762623116374\n",
      "Epoch 122, Batch 21/32 : Loss = 0.008846954442560673\n",
      "Epoch 122, Batch 22/32 : Loss = 0.0012209825217723846\n",
      "Epoch 122, Batch 23/32 : Loss = 0.00291684758849442\n",
      "Epoch 122, Batch 24/32 : Loss = 0.0009130993275903165\n",
      "Epoch 122, Batch 25/32 : Loss = 0.003657818539068103\n",
      "Epoch 122, Batch 26/32 : Loss = 0.0006177228642627597\n",
      "Epoch 122, Batch 27/32 : Loss = 0.0003828216576948762\n",
      "Epoch 122, Batch 28/32 : Loss = 0.003763514570891857\n",
      "Epoch 122, Batch 29/32 : Loss = 0.00045414402848109603\n",
      "Epoch 122, Batch 30/32 : Loss = 0.0005318967159837484\n",
      "Epoch 122, Batch 31/32 : Loss = 0.0009550006361678243\n",
      "Epoch 122 finished in 0.051586687564849854 minutes\n",
      "Epoch 122 training_loss = 0.0013230557087808847\n",
      "---'--]]--t--4----e----^---W-------Q-----4---->----g---- => ']t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "-------3----\\----$$---->-----S-----\\----MM----ii--BB---- => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "---88----K----l--Z----55---p-----$---a----}---w----,---- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----Y----W-----]--i-\\---|----MM----<<----M-----8---8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 122 val_loss = 0.7308184504508972, word_accuracy = 0.61\n",
      "Epoch 123, Batch 0/32 : Loss = 0.03271155059337616\n",
      "Epoch 123, Batch 1/32 : Loss = 0.0005999839631840587\n",
      "Epoch 123, Batch 2/32 : Loss = 0.010497404262423515\n",
      "Epoch 123, Batch 3/32 : Loss = 0.0008489015744999051\n",
      "Epoch 123, Batch 4/32 : Loss = 0.0004891797434538603\n",
      "Epoch 123, Batch 5/32 : Loss = 0.0004889381816610694\n",
      "Epoch 123, Batch 6/32 : Loss = 0.0020298389717936516\n",
      "Epoch 123, Batch 7/32 : Loss = 0.002071651164442301\n",
      "Epoch 123, Batch 8/32 : Loss = 0.0004315120168030262\n",
      "Epoch 123, Batch 9/32 : Loss = 0.005463901441544294\n",
      "Epoch 123, Batch 10/32 : Loss = 0.0007363978656940162\n",
      "Epoch 123, Batch 11/32 : Loss = 0.0003611039719544351\n",
      "Epoch 123, Batch 12/32 : Loss = 0.005558950360864401\n",
      "Epoch 123, Batch 13/32 : Loss = 0.0005883295089006424\n",
      "Epoch 123, Batch 14/32 : Loss = 0.0007419480825774372\n",
      "Epoch 123, Batch 15/32 : Loss = 0.00044053568853996694\n",
      "Epoch 123, Batch 16/32 : Loss = 0.0007115440093912184\n",
      "Epoch 123, Batch 17/32 : Loss = 0.00036753364838659763\n",
      "Epoch 123, Batch 18/32 : Loss = 0.0007887192186899483\n",
      "Epoch 123, Batch 19/32 : Loss = 0.0011561663122847676\n",
      "Epoch 123, Batch 20/32 : Loss = 0.0010339220752939582\n",
      "Epoch 123, Batch 21/32 : Loss = 0.0010871613631024957\n",
      "Epoch 123, Batch 22/32 : Loss = 0.0008319076150655746\n",
      "Epoch 123, Batch 23/32 : Loss = 0.002342375461012125\n",
      "Epoch 123, Batch 24/32 : Loss = 0.00032735831337049603\n",
      "Epoch 123, Batch 25/32 : Loss = 0.0007769765215925872\n",
      "Epoch 123, Batch 26/32 : Loss = 0.0008579142158851027\n",
      "Epoch 123, Batch 27/32 : Loss = 0.00949794054031372\n",
      "Epoch 123, Batch 28/32 : Loss = 0.0006698910146951675\n",
      "Epoch 123, Batch 29/32 : Loss = 0.0005971578066237271\n",
      "Epoch 123, Batch 30/32 : Loss = 0.0005637657595798373\n",
      "Epoch 123, Batch 31/32 : Loss = 0.0038282014429569244\n",
      "Epoch 123 finished in 0.05065386692682902 minutes\n",
      "Epoch 123 training_loss = 0.0027678387705236673\n",
      "----k---$$--I--F----DD---ee---h----]--k---0---\\---X------ => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----dd---:--X----9---ee---aa----F--,-8--------V---RR---- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----0----c----++----bb---I--\"---bb----6----.---Q--------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "------Q-----3----g---II--z--##---YY---:--]--q----+----*-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 123 val_loss = 0.7777161002159119, word_accuracy = 0.69\n",
      "Epoch 124, Batch 0/32 : Loss = 0.0013504610396921635\n",
      "Epoch 124, Batch 1/32 : Loss = 0.0006151853012852371\n",
      "Epoch 124, Batch 2/32 : Loss = 0.0023153265938162804\n",
      "Epoch 124, Batch 3/32 : Loss = 0.001086752163246274\n",
      "Epoch 124, Batch 4/32 : Loss = 0.0007196017540991306\n",
      "Epoch 124, Batch 5/32 : Loss = 0.0004593477351590991\n",
      "Epoch 124, Batch 6/32 : Loss = 0.0005638226866722107\n",
      "Epoch 124, Batch 7/32 : Loss = 0.0005996946711093187\n",
      "Epoch 124, Batch 8/32 : Loss = 0.0018413732759654522\n",
      "Epoch 124, Batch 9/32 : Loss = 0.0007919849595054984\n",
      "Epoch 124, Batch 10/32 : Loss = 0.0007660518167540431\n",
      "Epoch 124, Batch 11/32 : Loss = 0.0007055508904159069\n",
      "Epoch 124, Batch 12/32 : Loss = 0.0009148146491497755\n",
      "Epoch 124, Batch 13/32 : Loss = 0.004420048091560602\n",
      "Epoch 124, Batch 14/32 : Loss = 0.0004869801923632622\n",
      "Epoch 124, Batch 15/32 : Loss = 0.0005534057272598147\n",
      "Epoch 124, Batch 16/32 : Loss = 0.0007889533881098032\n",
      "Epoch 124, Batch 17/32 : Loss = 0.00045997186680324376\n",
      "Epoch 124, Batch 18/32 : Loss = 0.0010331369703635573\n",
      "Epoch 124, Batch 19/32 : Loss = 0.0023594230879098177\n",
      "Epoch 124, Batch 20/32 : Loss = 0.0007501504151150584\n",
      "Epoch 124, Batch 21/32 : Loss = 0.0004029140982311219\n",
      "Epoch 124, Batch 22/32 : Loss = 0.0003388055192772299\n",
      "Epoch 124, Batch 23/32 : Loss = 0.00048357632476836443\n",
      "Epoch 124, Batch 24/32 : Loss = 0.0009341147379018366\n",
      "Epoch 124, Batch 25/32 : Loss = 0.0006981354672461748\n",
      "Epoch 124, Batch 26/32 : Loss = 0.0008773630252107978\n",
      "Epoch 124, Batch 27/32 : Loss = 0.00035529310116544366\n",
      "Epoch 124, Batch 28/32 : Loss = 0.00031363486777991056\n",
      "Epoch 124, Batch 29/32 : Loss = 0.000383360602427274\n",
      "Epoch 124, Batch 30/32 : Loss = 0.0005718992906622589\n",
      "Epoch 124, Batch 31/32 : Loss = 0.0009569368558004498\n",
      "Epoch 124 finished in 0.051749984423319496 minutes\n",
      "Epoch 124 training_loss = 0.0009336787625215948\n",
      "---7---nn----D-----P----n---tt--w-----d----\\---QQ----a----RR---- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----oo---\"\"--}---!--BB----r---&-----9----;-`---O------w-----}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "---CC---DD----E----g----m-----\"--mm-----F---<<----Q----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----c-----R---;;--99----y----?----2----d----i---O-----{{--!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "Epoch 124 val_loss = 0.8027470707893372, word_accuracy = 0.67\n",
      "Epoch 125, Batch 0/32 : Loss = 0.0003353033389430493\n",
      "Epoch 125, Batch 1/32 : Loss = 0.0012177028693258762\n",
      "Epoch 125, Batch 2/32 : Loss = 0.0005648164078593254\n",
      "Epoch 125, Batch 3/32 : Loss = 0.0022215365897864103\n",
      "Epoch 125, Batch 4/32 : Loss = 0.005710562225431204\n",
      "Epoch 125, Batch 5/32 : Loss = 0.0005716468440368772\n",
      "Epoch 125, Batch 6/32 : Loss = 0.0005400844966061413\n",
      "Epoch 125, Batch 7/32 : Loss = 0.0004956279881298542\n",
      "Epoch 125, Batch 8/32 : Loss = 0.0003380691632628441\n",
      "Epoch 125, Batch 9/32 : Loss = 0.0006718231597915292\n",
      "Epoch 125, Batch 10/32 : Loss = 0.0019222761038690805\n",
      "Epoch 125, Batch 11/32 : Loss = 0.0007128765573725104\n",
      "Epoch 125, Batch 12/32 : Loss = 0.0003427400952205062\n",
      "Epoch 125, Batch 13/32 : Loss = 0.00043381823343224823\n",
      "Epoch 125, Batch 14/32 : Loss = 0.0005598353454843163\n",
      "Epoch 125, Batch 15/32 : Loss = 0.00038609636249020696\n",
      "Epoch 125, Batch 16/32 : Loss = 0.0004044621891807765\n",
      "Epoch 125, Batch 17/32 : Loss = 0.0025200725067406893\n",
      "Epoch 125, Batch 18/32 : Loss = 0.00078801647759974\n",
      "Epoch 125, Batch 19/32 : Loss = 0.000614612246863544\n",
      "Epoch 125, Batch 20/32 : Loss = 0.0014544498408213258\n",
      "Epoch 125, Batch 21/32 : Loss = 0.0009582302300259471\n",
      "Epoch 125, Batch 22/32 : Loss = 0.028682341799139977\n",
      "Epoch 125, Batch 23/32 : Loss = 0.0014622585149481893\n",
      "Epoch 125, Batch 24/32 : Loss = 0.003937554080039263\n",
      "Epoch 125, Batch 25/32 : Loss = 0.0005742199718952179\n",
      "Epoch 125, Batch 26/32 : Loss = 0.0006573260761797428\n",
      "Epoch 125, Batch 27/32 : Loss = 0.0008264158968813717\n",
      "Epoch 125, Batch 28/32 : Loss = 0.0007136344793252647\n",
      "Epoch 125, Batch 29/32 : Loss = 0.0005631830426864326\n",
      "Epoch 125, Batch 30/32 : Loss = 0.010430965572595596\n",
      "Epoch 125, Batch 31/32 : Loss = 0.0004661360289901495\n",
      "Epoch 125 finished in 0.05418258508046468 minutes\n",
      "Epoch 125 training_loss = 0.002302676672115922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----kk---O-----/---,-yy---c---*---11---PP---}--##----B---- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "-----d----R---;---9----y----?---2----d---ii--O-----{--!--- => dR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---'-----3----\\----$$---->-----S----\\----MM----ii--BB----- => '-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----Y-----W-----]]-i-\\---|----MM----<<----MM----8---8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 125 val_loss = 0.7086252570152283, word_accuracy = 0.68\n",
      "Epoch 126, Batch 0/32 : Loss = 0.0008148076012730598\n",
      "Epoch 126, Batch 1/32 : Loss = 0.0007071845466271043\n",
      "Epoch 126, Batch 2/32 : Loss = 0.0004046065150760114\n",
      "Epoch 126, Batch 3/32 : Loss = 0.001927237375639379\n",
      "Epoch 126, Batch 4/32 : Loss = 0.000528971548192203\n",
      "Epoch 126, Batch 5/32 : Loss = 0.00035792303970083594\n",
      "Epoch 126, Batch 6/32 : Loss = 0.0025215742643922567\n",
      "Epoch 126, Batch 7/32 : Loss = 0.0008464431157335639\n",
      "Epoch 126, Batch 8/32 : Loss = 0.00038025426329113543\n",
      "Epoch 126, Batch 9/32 : Loss = 0.013159533962607384\n",
      "Epoch 126, Batch 10/32 : Loss = 0.00033696257742121816\n",
      "Epoch 126, Batch 11/32 : Loss = 0.000324091175571084\n",
      "Epoch 126, Batch 12/32 : Loss = 0.00036091875517740846\n",
      "Epoch 126, Batch 13/32 : Loss = 0.0005762704531662166\n",
      "Epoch 126, Batch 14/32 : Loss = 0.0005433803307823837\n",
      "Epoch 126, Batch 15/32 : Loss = 0.00041870801942422986\n",
      "Epoch 126, Batch 16/32 : Loss = 0.022441966459155083\n",
      "Epoch 126, Batch 17/32 : Loss = 0.013542452827095985\n",
      "Epoch 126, Batch 18/32 : Loss = 0.00030834245262667537\n",
      "Epoch 126, Batch 19/32 : Loss = 0.0003778163227252662\n",
      "Epoch 126, Batch 20/32 : Loss = 0.0005189908551983535\n",
      "Epoch 126, Batch 21/32 : Loss = 0.0004886476090177894\n",
      "Epoch 126, Batch 22/32 : Loss = 0.0017387133557349443\n",
      "Epoch 126, Batch 23/32 : Loss = 0.0005856463685631752\n",
      "Epoch 126, Batch 24/32 : Loss = 0.0004389907990116626\n",
      "Epoch 126, Batch 25/32 : Loss = 0.00033520188299007714\n",
      "Epoch 126, Batch 26/32 : Loss = 0.0004367985820863396\n",
      "Epoch 126, Batch 27/32 : Loss = 0.0027458590921014547\n",
      "Epoch 126, Batch 28/32 : Loss = 0.0005626226193271577\n",
      "Epoch 126, Batch 29/32 : Loss = 0.0005776563193649054\n",
      "Epoch 126, Batch 30/32 : Loss = 0.0005947063909843564\n",
      "Epoch 126, Batch 31/32 : Loss = 0.0004558691871352494\n",
      "Epoch 126 finished in 0.05218988259633382 minutes\n",
      "Epoch 126 training_loss = 0.002247719094157219\n",
      "---/----MM-----o----E---^---3----x---/---&----6----X----- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "-----Q----3-----g----I--z---#----Y---::-]]--q----++---**- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---//---MM-----o----E---^---3---xx--/---&----6----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "-----8----K----ll--Z----5----pp----$---aa----}---w----,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "Epoch 126 val_loss = 0.7452921867370605, word_accuracy = 0.71\n",
      "Epoch 127, Batch 0/32 : Loss = 0.0002883612469304353\n",
      "Epoch 127, Batch 1/32 : Loss = 0.03345918282866478\n",
      "Epoch 127, Batch 2/32 : Loss = 0.0003460733569227159\n",
      "Epoch 127, Batch 3/32 : Loss = 0.0010029410477727652\n",
      "Epoch 127, Batch 4/32 : Loss = 0.008836572989821434\n",
      "Epoch 127, Batch 5/32 : Loss = 0.0006095290300436318\n",
      "Epoch 127, Batch 6/32 : Loss = 0.0005366582190617919\n",
      "Epoch 127, Batch 7/32 : Loss = 0.001025081379339099\n",
      "Epoch 127, Batch 8/32 : Loss = 0.0006327802548184991\n",
      "Epoch 127, Batch 9/32 : Loss = 0.0005064444849267602\n",
      "Epoch 127, Batch 10/32 : Loss = 0.0012018847046419978\n",
      "Epoch 127, Batch 11/32 : Loss = 0.0005517137469723821\n",
      "Epoch 127, Batch 12/32 : Loss = 0.0006070666713640094\n",
      "Epoch 127, Batch 13/32 : Loss = 0.004122976213693619\n",
      "Epoch 127, Batch 14/32 : Loss = 0.0004654133808799088\n",
      "Epoch 127, Batch 15/32 : Loss = 0.0020769378170371056\n",
      "Epoch 127, Batch 16/32 : Loss = 0.0004242888535372913\n",
      "Epoch 127, Batch 17/32 : Loss = 0.0026276237331330776\n",
      "Epoch 127, Batch 18/32 : Loss = 0.0005321918870322406\n",
      "Epoch 127, Batch 19/32 : Loss = 0.0003980161272920668\n",
      "Epoch 127, Batch 20/32 : Loss = 0.0006405602907761931\n",
      "Epoch 127, Batch 21/32 : Loss = 0.0005885002319701016\n",
      "Epoch 127, Batch 22/32 : Loss = 0.00046964449575170875\n",
      "Epoch 127, Batch 23/32 : Loss = 0.0007826706860214472\n",
      "Epoch 127, Batch 24/32 : Loss = 0.00040548780816607177\n",
      "Epoch 127, Batch 25/32 : Loss = 0.0009275706252083182\n",
      "Epoch 127, Batch 26/32 : Loss = 0.0003078510635532439\n",
      "Epoch 127, Batch 27/32 : Loss = 0.0004600176471285522\n",
      "Epoch 127, Batch 28/32 : Loss = 0.0010060406057164073\n",
      "Epoch 127, Batch 29/32 : Loss = 0.001708170515485108\n",
      "Epoch 127, Batch 30/32 : Loss = 0.0004851983394473791\n",
      "Epoch 127, Batch 31/32 : Loss = 0.000341708364430815\n",
      "Epoch 127 finished in 0.05206384261449178 minutes\n",
      "Epoch 127 training_loss = 0.002187185687944293\n",
      "---k----OO-----/--,--y---cc---*----1----P----}--##----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "---7---n----DD----P----n---t--ww----d---\\\\---Q----a----RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "---#----GG----9----E--I---=---h---55---#----2---JJ--)---k--- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-----Q-----3-----g----I--z---#-----Y----:--]--q-----++---**- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 127 val_loss = 0.7695512175559998, word_accuracy = 0.66\n",
      "Epoch 128, Batch 0/32 : Loss = 0.0010802662000060081\n",
      "Epoch 128, Batch 1/32 : Loss = 0.0010159501107409596\n",
      "Epoch 128, Batch 2/32 : Loss = 0.0014758208999410272\n",
      "Epoch 128, Batch 3/32 : Loss = 0.0019253657665103674\n",
      "Epoch 128, Batch 4/32 : Loss = 0.002167439553886652\n",
      "Epoch 128, Batch 5/32 : Loss = 0.002298974897712469\n",
      "Epoch 128, Batch 6/32 : Loss = 0.0032193141523748636\n",
      "Epoch 128, Batch 7/32 : Loss = 0.0006350868497975171\n",
      "Epoch 128, Batch 8/32 : Loss = 0.0003704699338413775\n",
      "Epoch 128, Batch 9/32 : Loss = 0.011756387539207935\n",
      "Epoch 128, Batch 10/32 : Loss = 0.005470414645969868\n",
      "Epoch 128, Batch 11/32 : Loss = 0.0024835504591464996\n",
      "Epoch 128, Batch 12/32 : Loss = 0.0006833224906586111\n",
      "Epoch 128, Batch 13/32 : Loss = 0.0006383669096976519\n",
      "Epoch 128, Batch 14/32 : Loss = 0.0008602023008279502\n",
      "Epoch 128, Batch 15/32 : Loss = 0.000761799979954958\n",
      "Epoch 128, Batch 16/32 : Loss = 0.026573050767183304\n",
      "Epoch 128, Batch 17/32 : Loss = 0.0008038092637434602\n",
      "Epoch 128, Batch 18/32 : Loss = 0.00032006134279072285\n",
      "Epoch 128, Batch 19/32 : Loss = 0.0005036006914451718\n",
      "Epoch 128, Batch 20/32 : Loss = 0.00037112622521817684\n",
      "Epoch 128, Batch 21/32 : Loss = 0.000378443393856287\n",
      "Epoch 128, Batch 22/32 : Loss = 0.00042007718002423644\n",
      "Epoch 128, Batch 23/32 : Loss = 0.000573866069316864\n",
      "Epoch 128, Batch 24/32 : Loss = 0.0008763899677433074\n",
      "Epoch 128, Batch 25/32 : Loss = 0.00039602004108019173\n",
      "Epoch 128, Batch 26/32 : Loss = 0.0006525054341182113\n",
      "Epoch 128, Batch 27/32 : Loss = 0.0003887329949066043\n",
      "Epoch 128, Batch 28/32 : Loss = 0.0015397933311760426\n",
      "Epoch 128, Batch 29/32 : Loss = 0.00043583352817222476\n",
      "Epoch 128, Batch 30/32 : Loss = 0.0008405016269534826\n",
      "Epoch 128, Batch 31/32 : Loss = 0.0011317479657009244\n",
      "Epoch 128 finished in 0.0519748330116272 minutes\n",
      "Epoch 128 training_loss = 0.002315117046236992\n",
      "----4----r---{------%---/--\"--)---w-----&&----NN-----+---PP----- => 4r{%/\")w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---J--;---q----++---/---z---y---UU-------%----U-----1---x---_--- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "--2---p---:--mm-----X--aa---z--nn----@-----C---yy----%%-----%--- => 2p:mXazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "----C----D----E----g----m-----\"--mm-----F---<<----Q----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 128 val_loss = 0.7626640796661377, word_accuracy = 0.65\n",
      "Epoch 129, Batch 0/32 : Loss = 0.0008590407087467611\n",
      "Epoch 129, Batch 1/32 : Loss = 0.0010467862011864781\n",
      "Epoch 129, Batch 2/32 : Loss = 0.005006433930248022\n",
      "Epoch 129, Batch 3/32 : Loss = 0.0029728685040026903\n",
      "Epoch 129, Batch 4/32 : Loss = 0.0017069149762392044\n",
      "Epoch 129, Batch 5/32 : Loss = 0.0048682489432394505\n",
      "Epoch 129, Batch 6/32 : Loss = 0.0016858149319887161\n",
      "Epoch 129, Batch 7/32 : Loss = 0.0005151901277713478\n",
      "Epoch 129, Batch 8/32 : Loss = 0.0007689056219533086\n",
      "Epoch 129, Batch 9/32 : Loss = 0.003751529846340418\n",
      "Epoch 129, Batch 10/32 : Loss = 0.00280631217174232\n",
      "Epoch 129, Batch 11/32 : Loss = 0.0005468998569995165\n",
      "Epoch 129, Batch 12/32 : Loss = 0.0004102302191313356\n",
      "Epoch 129, Batch 13/32 : Loss = 0.0003596630413085222\n",
      "Epoch 129, Batch 14/32 : Loss = 0.0026938512455672026\n",
      "Epoch 129, Batch 15/32 : Loss = 0.00029015273321419954\n",
      "Epoch 129, Batch 16/32 : Loss = 0.0010823322227224708\n",
      "Epoch 129, Batch 17/32 : Loss = 0.005338751245290041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Batch 18/32 : Loss = 0.0005004566628485918\n",
      "Epoch 129, Batch 19/32 : Loss = 0.0004438743053469807\n",
      "Epoch 129, Batch 20/32 : Loss = 0.0005595815600827336\n",
      "Epoch 129, Batch 21/32 : Loss = 0.001514146919362247\n",
      "Epoch 129, Batch 22/32 : Loss = 0.000558776780962944\n",
      "Epoch 129, Batch 23/32 : Loss = 0.00132027268409729\n",
      "Epoch 129, Batch 24/32 : Loss = 0.0002968873886857182\n",
      "Epoch 129, Batch 25/32 : Loss = 0.00026895321207121015\n",
      "Epoch 129, Batch 26/32 : Loss = 0.000320867111440748\n",
      "Epoch 129, Batch 27/32 : Loss = 0.00034275755751878023\n",
      "Epoch 129, Batch 28/32 : Loss = 0.00043878209544345737\n",
      "Epoch 129, Batch 29/32 : Loss = 0.000414413952967152\n",
      "Epoch 129, Batch 30/32 : Loss = 0.003300288924947381\n",
      "Epoch 129, Batch 31/32 : Loss = 0.015017124824225903\n",
      "Epoch 129 finished in 0.05185506741205851 minutes\n",
      "Epoch 129 training_loss = 0.001570028136484325\n",
      "---k---$---|---'---99---Y----N----WW-----m------T---8----- => k$|'9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----5----9----g----m------J---u---x---c---x-----d----\\---- => 59gmJuxcxd\\, Ground Truth is 59gmJuxcx.d\\\n",
      "-----QQ----3-----g---I---z---#----Y---:--]---q----+---**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----k----O-----/--,--y---c---*----1----P---}---#----BB---- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 129 val_loss = 0.7655270099639893, word_accuracy = 0.63\n",
      "Epoch 130, Batch 0/32 : Loss = 0.00992697011679411\n",
      "Epoch 130, Batch 1/32 : Loss = 0.0006078448495827615\n",
      "Epoch 130, Batch 2/32 : Loss = 0.0007247576140798628\n",
      "Epoch 130, Batch 3/32 : Loss = 0.014731363393366337\n",
      "Epoch 130, Batch 4/32 : Loss = 0.0015879394486546516\n",
      "Epoch 130, Batch 5/32 : Loss = 0.002791629172861576\n",
      "Epoch 130, Batch 6/32 : Loss = 0.00040247762808576226\n",
      "Epoch 130, Batch 7/32 : Loss = 0.0003286392311565578\n",
      "Epoch 130, Batch 8/32 : Loss = 0.00037752429489046335\n",
      "Epoch 130, Batch 9/32 : Loss = 0.0072333249263465405\n",
      "Epoch 130, Batch 10/32 : Loss = 0.00042609398951753974\n",
      "Epoch 130, Batch 11/32 : Loss = 0.00048002589028328657\n",
      "Epoch 130, Batch 12/32 : Loss = 0.0005066413432359695\n",
      "Epoch 130, Batch 13/32 : Loss = 0.00028975727036595345\n",
      "Epoch 130, Batch 14/32 : Loss = 0.0033164082560688257\n",
      "Epoch 130, Batch 15/32 : Loss = 0.0005806765984743834\n",
      "Epoch 130, Batch 16/32 : Loss = 0.01679246500134468\n",
      "Epoch 130, Batch 17/32 : Loss = 0.0011966369347646832\n",
      "Epoch 130, Batch 18/32 : Loss = 0.0007090354338288307\n",
      "Epoch 130, Batch 19/32 : Loss = 0.0010755985276773572\n",
      "Epoch 130, Batch 20/32 : Loss = 0.0008715493604540825\n",
      "Epoch 130, Batch 21/32 : Loss = 0.00032451574224978685\n",
      "Epoch 130, Batch 22/32 : Loss = 0.0007917035836726427\n",
      "Epoch 130, Batch 23/32 : Loss = 0.0007020692573860288\n",
      "Epoch 130, Batch 24/32 : Loss = 0.0010967173147946596\n",
      "Epoch 130, Batch 25/32 : Loss = 0.000597128237131983\n",
      "Epoch 130, Batch 26/32 : Loss = 0.0008449269807897508\n",
      "Epoch 130, Batch 27/32 : Loss = 0.0005940976552665234\n",
      "Epoch 130, Batch 28/32 : Loss = 0.0014251787215471268\n",
      "Epoch 130, Batch 29/32 : Loss = 0.006166844163089991\n",
      "Epoch 130, Batch 30/32 : Loss = 0.008675006218254566\n",
      "Epoch 130, Batch 31/32 : Loss = 0.0004057817277498543\n",
      "Epoch 130 finished in 0.05340321063995361 minutes\n",
      "Epoch 130 training_loss = 0.002770321909338236\n",
      "----5---->---:--*---YY----A--''--O-----D----*---#----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "---\"----]---t---4----e-----^---WW--------Q-----4----->-----g---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "-----k----O------/--,--yy---c----*----1----PP---}---#-----BB---- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----YY----W-------]-ii-\\\\---|----MM------<-----MM-----88---88--- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 130 val_loss = 0.7687264680862427, word_accuracy = 0.63\n",
      "Epoch 131, Batch 0/32 : Loss = 0.0007945782854221761\n",
      "Epoch 131, Batch 1/32 : Loss = 0.0007086815312504768\n",
      "Epoch 131, Batch 2/32 : Loss = 0.001023361925035715\n",
      "Epoch 131, Batch 3/32 : Loss = 0.036627888679504395\n",
      "Epoch 131, Batch 4/32 : Loss = 0.0030686582904309034\n",
      "Epoch 131, Batch 5/32 : Loss = 0.004674557596445084\n",
      "Epoch 131, Batch 6/32 : Loss = 0.0006418194388970733\n",
      "Epoch 131, Batch 7/32 : Loss = 0.0004374212003313005\n",
      "Epoch 131, Batch 8/32 : Loss = 0.0010137881617993116\n",
      "Epoch 131, Batch 9/32 : Loss = 0.0005187197821214795\n",
      "Epoch 131, Batch 10/32 : Loss = 0.0020873812027275562\n",
      "Epoch 131, Batch 11/32 : Loss = 0.0014629274373874068\n",
      "Epoch 131, Batch 12/32 : Loss = 0.0010070475982502103\n",
      "Epoch 131, Batch 13/32 : Loss = 0.0009137752931565046\n",
      "Epoch 131, Batch 14/32 : Loss = 0.0007478061597794294\n",
      "Epoch 131, Batch 15/32 : Loss = 0.00034334929659962654\n",
      "Epoch 131, Batch 16/32 : Loss = 0.006739540956914425\n",
      "Epoch 131, Batch 17/32 : Loss = 0.0003378234396222979\n",
      "Epoch 131, Batch 18/32 : Loss = 0.0034213298931717873\n",
      "Epoch 131, Batch 19/32 : Loss = 0.00036599853774532676\n",
      "Epoch 131, Batch 20/32 : Loss = 0.0010609272867441177\n",
      "Epoch 131, Batch 21/32 : Loss = 0.0003582919598557055\n",
      "Epoch 131, Batch 22/32 : Loss = 0.0004672154027502984\n",
      "Epoch 131, Batch 23/32 : Loss = 0.0007537328056059778\n",
      "Epoch 131, Batch 24/32 : Loss = 0.0004981296369805932\n",
      "Epoch 131, Batch 25/32 : Loss = 0.0003882278106175363\n",
      "Epoch 131, Batch 26/32 : Loss = 0.0006902585737407207\n",
      "Epoch 131, Batch 27/32 : Loss = 0.0005383972311392426\n",
      "Epoch 131, Batch 28/32 : Loss = 0.00044344901107251644\n",
      "Epoch 131, Batch 29/32 : Loss = 0.003940793685615063\n",
      "Epoch 131, Batch 30/32 : Loss = 0.0007158159860409796\n",
      "Epoch 131, Batch 31/32 : Loss = 0.00045484627480618656\n",
      "Epoch 131 finished in 0.052739469210306804 minutes\n",
      "Epoch 131 training_loss = 0.0024690295103937387\n",
      "--88-----K-----ll--Z-----55----p------$----aa----}----w-----,- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "----kk---$---l--FF----D----ee---hh---]---k---0----\\---X------- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---'------3----\\----$$---->-----SS----\\----MM-----ii--BB------ => '-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----J--;---q----+---/|---z--yy---U-------%----U----11---x---_- => J;q+/|zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 131 val_loss = 0.8325909376144409, word_accuracy = 0.65\n",
      "Epoch 132, Batch 0/32 : Loss = 0.0014543356373906136\n",
      "Epoch 132, Batch 1/32 : Loss = 0.0004989660810679197\n",
      "Epoch 132, Batch 2/32 : Loss = 0.00035877199843525887\n",
      "Epoch 132, Batch 3/32 : Loss = 0.002078611170873046\n",
      "Epoch 132, Batch 4/32 : Loss = 0.0003998858737759292\n",
      "Epoch 132, Batch 5/32 : Loss = 0.0002957141841761768\n",
      "Epoch 132, Batch 6/32 : Loss = 0.013399378396570683\n",
      "Epoch 132, Batch 7/32 : Loss = 0.0006005889736115932\n",
      "Epoch 132, Batch 8/32 : Loss = 0.0017786701209843159\n",
      "Epoch 132, Batch 9/32 : Loss = 0.0006034101243130863\n",
      "Epoch 132, Batch 10/32 : Loss = 0.0004499933565966785\n",
      "Epoch 132, Batch 11/32 : Loss = 0.0005629718070849776\n",
      "Epoch 132, Batch 12/32 : Loss = 0.0006592384888790548\n",
      "Epoch 132, Batch 13/32 : Loss = 0.00793834961950779\n",
      "Epoch 132, Batch 14/32 : Loss = 0.00089817657135427\n",
      "Epoch 132, Batch 15/32 : Loss = 0.0005515021039173007\n",
      "Epoch 132, Batch 16/32 : Loss = 0.00041135266656056046\n",
      "Epoch 132, Batch 17/32 : Loss = 0.0022164597176015377\n",
      "Epoch 132, Batch 18/32 : Loss = 0.0003703168185893446\n",
      "Epoch 132, Batch 19/32 : Loss = 0.0006125784711912274\n",
      "Epoch 132, Batch 20/32 : Loss = 0.001244541141204536\n",
      "Epoch 132, Batch 21/32 : Loss = 0.0004513237508945167\n",
      "Epoch 132, Batch 22/32 : Loss = 0.0006054126424714923\n",
      "Epoch 132, Batch 23/32 : Loss = 0.002410334534943104\n",
      "Epoch 132, Batch 24/32 : Loss = 0.0004914902383461595\n",
      "Epoch 132, Batch 25/32 : Loss = 0.00040705272112973034\n",
      "Epoch 132, Batch 26/32 : Loss = 0.0010095350444316864\n",
      "Epoch 132, Batch 27/32 : Loss = 0.0031861907336860895\n",
      "Epoch 132, Batch 28/32 : Loss = 0.0005902012344449759\n",
      "Epoch 132, Batch 29/32 : Loss = 0.000468853279016912\n",
      "Epoch 132, Batch 30/32 : Loss = 0.0003124761860817671\n",
      "Epoch 132, Batch 31/32 : Loss = 0.0003162520588375628\n",
      "Epoch 132 finished in 0.052216347058614096 minutes\n",
      "Epoch 132 training_loss = 0.0015214846935123205\n",
      "-----d----:---X-----9----e-----a-----F--,--8---------V----RR------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----0---QQ---66---<----<<---(--TT---N----5---==---P---(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----0----Q----66---<----<<---(---T----N----5---=----P--(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "--'-/----MM------o-----E----^----3----x----/---&&----66----X------ => '/MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "Epoch 132 val_loss = 0.7754359245300293, word_accuracy = 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133, Batch 0/32 : Loss = 0.000475698325317353\n",
      "Epoch 133, Batch 1/32 : Loss = 0.0018424440640956163\n",
      "Epoch 133, Batch 2/32 : Loss = 0.000520544417668134\n",
      "Epoch 133, Batch 3/32 : Loss = 0.0019534954335540533\n",
      "Epoch 133, Batch 4/32 : Loss = 0.0003755811194423586\n",
      "Epoch 133, Batch 5/32 : Loss = 0.0010917948093265295\n",
      "Epoch 133, Batch 6/32 : Loss = 0.000965382088907063\n",
      "Epoch 133, Batch 7/32 : Loss = 0.0003459998406469822\n",
      "Epoch 133, Batch 8/32 : Loss = 0.0011717448942363262\n",
      "Epoch 133, Batch 9/32 : Loss = 0.00028675032081082463\n",
      "Epoch 133, Batch 10/32 : Loss = 0.11532635986804962\n",
      "Epoch 133, Batch 11/32 : Loss = 0.0012062714667990804\n",
      "Epoch 133, Batch 12/32 : Loss = 0.0004740238655358553\n",
      "Epoch 133, Batch 13/32 : Loss = 0.0008218265138566494\n",
      "Epoch 133, Batch 14/32 : Loss = 0.0033543251920491457\n",
      "Epoch 133, Batch 15/32 : Loss = 0.0007985237170942128\n",
      "Epoch 133, Batch 16/32 : Loss = 0.0036839835811406374\n",
      "Epoch 133, Batch 17/32 : Loss = 0.0010042873909696937\n",
      "Epoch 133, Batch 18/32 : Loss = 0.000405109632993117\n",
      "Epoch 133, Batch 19/32 : Loss = 0.0009983661584556103\n",
      "Epoch 133, Batch 20/32 : Loss = 0.0004667975299526006\n",
      "Epoch 133, Batch 21/32 : Loss = 0.0020978942047804594\n",
      "Epoch 133, Batch 22/32 : Loss = 0.022212013602256775\n",
      "Epoch 133, Batch 23/32 : Loss = 0.005719945766031742\n",
      "Epoch 133, Batch 24/32 : Loss = 0.005644279066473246\n",
      "Epoch 133, Batch 25/32 : Loss = 0.02857678383588791\n",
      "Epoch 133, Batch 26/32 : Loss = 0.18590877950191498\n",
      "Epoch 133, Batch 27/32 : Loss = 0.0011203079484403133\n",
      "Epoch 133, Batch 28/32 : Loss = 0.0003940481401514262\n",
      "Epoch 133, Batch 29/32 : Loss = 0.0005943718715570867\n",
      "Epoch 133, Batch 30/32 : Loss = 0.013577848672866821\n",
      "Epoch 133, Batch 31/32 : Loss = 0.002827854361385107\n",
      "Epoch 133 finished in 0.05343369245529175 minutes\n",
      "Epoch 133 training_loss = 0.012972499243915081\n",
      "--.---c----O------w-----uu--``--uu---.--R----{---3----\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----B---..--YY---I---W-------6-----F---h-----X----'-YY----2----- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "----++--:---z----7---8----dd----S-----v--55----S----J----B------ => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----k-----O-----/--,---y----c---**----1----P----}---#-----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 133 val_loss = 0.7809514999389648, word_accuracy = 0.66\n",
      "Epoch 134, Batch 0/32 : Loss = 0.0019891199190169573\n",
      "Epoch 134, Batch 1/32 : Loss = 0.007639518938958645\n",
      "Epoch 134, Batch 2/32 : Loss = 0.0021871260832995176\n",
      "Epoch 134, Batch 3/32 : Loss = 0.0008510432671755552\n",
      "Epoch 134, Batch 4/32 : Loss = 0.0016131007578223944\n",
      "Epoch 134, Batch 5/32 : Loss = 0.002218674635514617\n",
      "Epoch 134, Batch 6/32 : Loss = 0.005629691760987043\n",
      "Epoch 134, Batch 7/32 : Loss = 0.0008705452783033252\n",
      "Epoch 134, Batch 8/32 : Loss = 0.0004604571149684489\n",
      "Epoch 134, Batch 9/32 : Loss = 0.004117344040423632\n",
      "Epoch 134, Batch 10/32 : Loss = 0.0005004752310924232\n",
      "Epoch 134, Batch 11/32 : Loss = 0.006948474794626236\n",
      "Epoch 134, Batch 12/32 : Loss = 0.03294426202774048\n",
      "Epoch 134, Batch 13/32 : Loss = 0.0016207172302529216\n",
      "Epoch 134, Batch 14/32 : Loss = 0.0013656611554324627\n",
      "Epoch 134, Batch 15/32 : Loss = 0.0017682863399386406\n",
      "Epoch 134, Batch 16/32 : Loss = 0.0010909457923844457\n",
      "Epoch 134, Batch 17/32 : Loss = 0.0003927228390239179\n",
      "Epoch 134, Batch 18/32 : Loss = 0.0030978298746049404\n",
      "Epoch 134, Batch 19/32 : Loss = 0.0005226287757977843\n",
      "Epoch 134, Batch 20/32 : Loss = 0.0005833775503560901\n",
      "Epoch 134, Batch 21/32 : Loss = 0.0005799980135634542\n",
      "Epoch 134, Batch 22/32 : Loss = 0.000635595410130918\n",
      "Epoch 134, Batch 23/32 : Loss = 0.0006016712868586183\n",
      "Epoch 134, Batch 24/32 : Loss = 0.0005267598899081349\n",
      "Epoch 134, Batch 25/32 : Loss = 0.005847240798175335\n",
      "Epoch 134, Batch 26/32 : Loss = 0.0016526683466508985\n",
      "Epoch 134, Batch 27/32 : Loss = 0.0007898861076682806\n",
      "Epoch 134, Batch 28/32 : Loss = 0.020261969417333603\n",
      "Epoch 134, Batch 29/32 : Loss = 0.00038251985097303987\n",
      "Epoch 134, Batch 30/32 : Loss = 0.00044834031723439693\n",
      "Epoch 134, Batch 31/32 : Loss = 0.0012388942996039987\n",
      "Epoch 134 finished in 0.0513633926709493 minutes\n",
      "Epoch 134 training_loss = 0.0035435666795819998\n",
      "---/----MM-----o-----E----^---3----xx---/---&-----6----XX----- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---BB---.---Y----I--WW------6-----F---h-----X---'--Y-----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "--2---p---:--mm----x---a----z--n----@-----C---yy----%%-----%-- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "-----d---::--X----9----ee----a-----F--,--8--------V----RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "Epoch 134 val_loss = 0.8235148191452026, word_accuracy = 0.73\n",
      "Epoch 135, Batch 0/32 : Loss = 0.0009144247742369771\n",
      "Epoch 135, Batch 1/32 : Loss = 0.0004467443795874715\n",
      "Epoch 135, Batch 2/32 : Loss = 0.0005065916338935494\n",
      "Epoch 135, Batch 3/32 : Loss = 0.0007031350396573544\n",
      "Epoch 135, Batch 4/32 : Loss = 0.0011636597337201238\n",
      "Epoch 135, Batch 5/32 : Loss = 0.00041825708467513323\n",
      "Epoch 135, Batch 6/32 : Loss = 0.000424595782533288\n",
      "Epoch 135, Batch 7/32 : Loss = 0.0021345834247767925\n",
      "Epoch 135, Batch 8/32 : Loss = 0.00031517486786469817\n",
      "Epoch 135, Batch 9/32 : Loss = 0.0017630783841013908\n",
      "Epoch 135, Batch 10/32 : Loss = 0.0004867035895586014\n",
      "Epoch 135, Batch 11/32 : Loss = 0.0010043102083727717\n",
      "Epoch 135, Batch 12/32 : Loss = 0.0007032136782072484\n",
      "Epoch 135, Batch 13/32 : Loss = 0.00038009596755728126\n",
      "Epoch 135, Batch 14/32 : Loss = 0.0008167250198312104\n",
      "Epoch 135, Batch 15/32 : Loss = 0.0006213189335539937\n",
      "Epoch 135, Batch 16/32 : Loss = 0.0021279773209244013\n",
      "Epoch 135, Batch 17/32 : Loss = 0.0004507731646299362\n",
      "Epoch 135, Batch 18/32 : Loss = 0.0005316316382959485\n",
      "Epoch 135, Batch 19/32 : Loss = 0.002390965586528182\n",
      "Epoch 135, Batch 20/32 : Loss = 0.00046903101610951126\n",
      "Epoch 135, Batch 21/32 : Loss = 0.00042080433922819793\n",
      "Epoch 135, Batch 22/32 : Loss = 0.004315756261348724\n",
      "Epoch 135, Batch 23/32 : Loss = 0.001316010719165206\n",
      "Epoch 135, Batch 24/32 : Loss = 0.0006539509631693363\n",
      "Epoch 135, Batch 25/32 : Loss = 0.00028940170886926353\n",
      "Epoch 135, Batch 26/32 : Loss = 0.0003952452098019421\n",
      "Epoch 135, Batch 27/32 : Loss = 0.0003565815859474242\n",
      "Epoch 135, Batch 28/32 : Loss = 0.003119328524917364\n",
      "Epoch 135, Batch 29/32 : Loss = 0.000974698516074568\n",
      "Epoch 135, Batch 30/32 : Loss = 0.0015718701761215925\n",
      "Epoch 135, Batch 31/32 : Loss = 0.0004648050235118717\n",
      "Epoch 135 finished in 0.05232836008071899 minutes\n",
      "Epoch 135 training_loss = 0.0010359755251556635\n",
      "---B----..---Y----I---W--------6-----FF----hh-----X---'---Y-----22---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----++---:---z-----7---88----d------S-----v---5-----S----JJ----BB---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "---77---n-----D-----P----nn---tt--w-----d----\\\\----Q-----a----RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----CC----DD----E----g----mm-----\"---m------F---<<----Q-----8---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 135 val_loss = 0.7624812126159668, word_accuracy = 0.69\n",
      "Epoch 136, Batch 0/32 : Loss = 0.0006796701345592737\n",
      "Epoch 136, Batch 1/32 : Loss = 0.00031552533619105816\n",
      "Epoch 136, Batch 2/32 : Loss = 0.00040610702126286924\n",
      "Epoch 136, Batch 3/32 : Loss = 0.0005332477740012109\n",
      "Epoch 136, Batch 4/32 : Loss = 0.2592112123966217\n",
      "Epoch 136, Batch 5/32 : Loss = 0.0013414120767265558\n",
      "Epoch 136, Batch 6/32 : Loss = 0.019974973052740097\n",
      "Epoch 136, Batch 7/32 : Loss = 0.0004876615130342543\n",
      "Epoch 136, Batch 8/32 : Loss = 0.001055449596606195\n",
      "Epoch 136, Batch 9/32 : Loss = 0.0006988848326727748\n",
      "Epoch 136, Batch 10/32 : Loss = 0.000568095245398581\n",
      "Epoch 136, Batch 11/32 : Loss = 0.00043531067785806954\n",
      "Epoch 136, Batch 12/32 : Loss = 0.0010318774729967117\n",
      "Epoch 136, Batch 13/32 : Loss = 0.0010242173448204994\n",
      "Epoch 136, Batch 14/32 : Loss = 0.008138967677950859\n",
      "Epoch 136, Batch 15/32 : Loss = 0.0005725420778617263\n",
      "Epoch 136, Batch 16/32 : Loss = 0.0009338761446997523\n",
      "Epoch 136, Batch 17/32 : Loss = 0.000864441622979939\n",
      "Epoch 136, Batch 18/32 : Loss = 0.0004998963559046388\n",
      "Epoch 136, Batch 19/32 : Loss = 0.21054895222187042\n",
      "Epoch 136, Batch 20/32 : Loss = 0.0007919790223240852\n",
      "Epoch 136, Batch 21/32 : Loss = 0.0006614404264837503\n",
      "Epoch 136, Batch 22/32 : Loss = 0.0005807157140225172\n",
      "Epoch 136, Batch 23/32 : Loss = 0.0011814504396170378\n",
      "Epoch 136, Batch 24/32 : Loss = 0.004499351605772972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136, Batch 25/32 : Loss = 0.0007571919122710824\n",
      "Epoch 136, Batch 26/32 : Loss = 0.003349965438246727\n",
      "Epoch 136, Batch 27/32 : Loss = 0.0009923194302245975\n",
      "Epoch 136, Batch 28/32 : Loss = 0.0007608721498399973\n",
      "Epoch 136, Batch 29/32 : Loss = 0.0005870339227840304\n",
      "Epoch 136, Batch 30/32 : Loss = 0.0007712076767347753\n",
      "Epoch 136, Batch 31/32 : Loss = 0.0030148851219564676\n",
      "Epoch 136 finished in 0.0875869075457255 minutes\n",
      "Epoch 136 training_loss = 0.016855670139193535\n",
      "---\"---]---t--4-----e----^---WW-------Q-----44---->-----g---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "--.---c----O-----w-----u---`---u---.--R----{---3---\"---#----- => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----z----00---\"\"----G-----/----~-----$----cc----11---j--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "------z----0----\"----G-----//---~-----$-----c----11--jj--t--- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 136 val_loss = 0.7844437956809998, word_accuracy = 0.69\n",
      "Epoch 137, Batch 0/32 : Loss = 0.0005092447390779853\n",
      "Epoch 137, Batch 1/32 : Loss = 0.0014649054501205683\n",
      "Epoch 137, Batch 2/32 : Loss = 0.02946990728378296\n",
      "Epoch 137, Batch 3/32 : Loss = 0.000615741650108248\n",
      "Epoch 137, Batch 4/32 : Loss = 0.0003995173319708556\n",
      "Epoch 137, Batch 5/32 : Loss = 0.0024786870926618576\n",
      "Epoch 137, Batch 6/32 : Loss = 0.0005602575838565826\n",
      "Epoch 137, Batch 7/32 : Loss = 0.0026115148793905973\n",
      "Epoch 137, Batch 8/32 : Loss = 0.0006822526920586824\n",
      "Epoch 137, Batch 9/32 : Loss = 0.0006221659714356065\n",
      "Epoch 137, Batch 10/32 : Loss = 0.004313238896429539\n",
      "Epoch 137, Batch 11/32 : Loss = 0.000797887914814055\n",
      "Epoch 137, Batch 12/32 : Loss = 0.0007630038890056312\n",
      "Epoch 137, Batch 13/32 : Loss = 0.0005069207982160151\n",
      "Epoch 137, Batch 14/32 : Loss = 0.0008659532759338617\n",
      "Epoch 137, Batch 15/32 : Loss = 0.014381728135049343\n",
      "Epoch 137, Batch 16/32 : Loss = 0.0007640266558155417\n",
      "Epoch 137, Batch 17/32 : Loss = 0.0019109451677650213\n",
      "Epoch 137, Batch 18/32 : Loss = 0.0007056807517074049\n",
      "Epoch 137, Batch 19/32 : Loss = 0.005541910417377949\n",
      "Epoch 137, Batch 20/32 : Loss = 0.0005197901627980173\n",
      "Epoch 137, Batch 21/32 : Loss = 0.0010056497994810343\n",
      "Epoch 137, Batch 22/32 : Loss = 0.0022067916579544544\n",
      "Epoch 137, Batch 23/32 : Loss = 0.0007257696124725044\n",
      "Epoch 137, Batch 24/32 : Loss = 0.0013308827765285969\n",
      "Epoch 137, Batch 25/32 : Loss = 0.0008612890378572047\n",
      "Epoch 137, Batch 26/32 : Loss = 0.0013085321988910437\n",
      "Epoch 137, Batch 27/32 : Loss = 0.000517230830155313\n",
      "Epoch 137, Batch 28/32 : Loss = 0.0014204869512468576\n",
      "Epoch 137, Batch 29/32 : Loss = 0.0009393912041559815\n",
      "Epoch 137, Batch 30/32 : Loss = 0.0007875250885263085\n",
      "Epoch 137, Batch 31/32 : Loss = 0.0006446781917475164\n",
      "Epoch 137 finished in 0.05308069785435995 minutes\n",
      "Epoch 137 training_loss = 0.0026239173021167517\n",
      "----JJ--;;--q-----++---//---zz---y----UU--------%-----U-----1----x---__- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "----W------=-----2----+----E----11---n----TT---XX---r---C-----a---I----- => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "---77---n-----DD----PP----n----t---w-----d----\\\\----Q-----a----RR------- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----X----77---0---j---@------S----Z----L---4-----C----mm------MM-------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "Epoch 137 val_loss = 0.7924374341964722, word_accuracy = 0.66\n",
      "Epoch 138, Batch 0/32 : Loss = 0.0027884275186806917\n",
      "Epoch 138, Batch 1/32 : Loss = 0.000818725093267858\n",
      "Epoch 138, Batch 2/32 : Loss = 0.004854178987443447\n",
      "Epoch 138, Batch 3/32 : Loss = 0.002481382340192795\n",
      "Epoch 138, Batch 4/32 : Loss = 0.0014917738735675812\n",
      "Epoch 138, Batch 5/32 : Loss = 0.0006002355366945267\n",
      "Epoch 138, Batch 6/32 : Loss = 0.0006044775946065784\n",
      "Epoch 138, Batch 7/32 : Loss = 0.011012574657797813\n",
      "Epoch 138, Batch 8/32 : Loss = 0.0011375308968126774\n",
      "Epoch 138, Batch 9/32 : Loss = 0.000616019475273788\n",
      "Epoch 138, Batch 10/32 : Loss = 0.003818290075287223\n",
      "Epoch 138, Batch 11/32 : Loss = 0.0015350647736340761\n",
      "Epoch 138, Batch 12/32 : Loss = 0.0006554435822181404\n",
      "Epoch 138, Batch 13/32 : Loss = 0.0005427445285022259\n",
      "Epoch 138, Batch 14/32 : Loss = 0.0009598833275958896\n",
      "Epoch 138, Batch 15/32 : Loss = 0.0011918542440980673\n",
      "Epoch 138, Batch 16/32 : Loss = 0.001957270549610257\n",
      "Epoch 138, Batch 17/32 : Loss = 0.0008275022846646607\n",
      "Epoch 138, Batch 18/32 : Loss = 0.0014142765430733562\n",
      "Epoch 138, Batch 19/32 : Loss = 0.0012111489195376635\n",
      "Epoch 138, Batch 20/32 : Loss = 0.00041995770880021155\n",
      "Epoch 138, Batch 21/32 : Loss = 0.0011889414163306355\n",
      "Epoch 138, Batch 22/32 : Loss = 0.0006204185774549842\n",
      "Epoch 138, Batch 23/32 : Loss = 0.00123891094699502\n",
      "Epoch 138, Batch 24/32 : Loss = 0.00048224348574876785\n",
      "Epoch 138, Batch 25/32 : Loss = 0.0026092976331710815\n",
      "Epoch 138, Batch 26/32 : Loss = 0.003569738008081913\n",
      "Epoch 138, Batch 27/32 : Loss = 0.0017098002135753632\n",
      "Epoch 138, Batch 28/32 : Loss = 0.0018226229585707188\n",
      "Epoch 138, Batch 29/32 : Loss = 0.0004333022516220808\n",
      "Epoch 138, Batch 30/32 : Loss = 0.0070287506096065044\n",
      "Epoch 138, Batch 31/32 : Loss = 0.0005754195153713226\n",
      "Epoch 138 finished in 0.051624170939127606 minutes\n",
      "Epoch 138 training_loss = 0.0019828020595014095\n",
      "---#----GG----9---EE--I--=----h---55---#---22---J---)--k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----kk---OO-----/--,--y---c----*---11---P----}--##----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----##---GG----9---E---I-=---hh---5---#---22---J--))-kk----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "--{---B----Y----R----a----y--hh---#----2---->----E---4------ => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 138 val_loss = 0.7614649534225464, word_accuracy = 0.72\n",
      "Epoch 139, Batch 0/32 : Loss = 0.0005259960307739675\n",
      "Epoch 139, Batch 1/32 : Loss = 0.0018471016082912683\n",
      "Epoch 139, Batch 2/32 : Loss = 0.0005319947958923876\n",
      "Epoch 139, Batch 3/32 : Loss = 0.0008119017584249377\n",
      "Epoch 139, Batch 4/32 : Loss = 0.000648513319902122\n",
      "Epoch 139, Batch 5/32 : Loss = 0.010291071608662605\n",
      "Epoch 139, Batch 6/32 : Loss = 0.0011086834128946066\n",
      "Epoch 139, Batch 7/32 : Loss = 0.0008505367441102862\n",
      "Epoch 139, Batch 8/32 : Loss = 0.007861585356295109\n",
      "Epoch 139, Batch 9/32 : Loss = 0.0009478991851210594\n",
      "Epoch 139, Batch 10/32 : Loss = 0.0018797876546159387\n",
      "Epoch 139, Batch 11/32 : Loss = 0.0014036069624125957\n",
      "Epoch 139, Batch 12/32 : Loss = 0.0010419354075565934\n",
      "Epoch 139, Batch 13/32 : Loss = 0.005349612329155207\n",
      "Epoch 139, Batch 14/32 : Loss = 0.0013513886369764805\n",
      "Epoch 139, Batch 15/32 : Loss = 0.0017159796552732587\n",
      "Epoch 139, Batch 16/32 : Loss = 0.0013583437539637089\n",
      "Epoch 139, Batch 17/32 : Loss = 0.011125040240585804\n",
      "Epoch 139, Batch 18/32 : Loss = 0.0003981302725151181\n",
      "Epoch 139, Batch 19/32 : Loss = 0.0027112304233014584\n",
      "Epoch 139, Batch 20/32 : Loss = 0.0007054165471345186\n",
      "Epoch 139, Batch 21/32 : Loss = 0.00046526797814294696\n",
      "Epoch 139, Batch 22/32 : Loss = 0.0016895317239686847\n",
      "Epoch 139, Batch 23/32 : Loss = 0.0007644863799214363\n",
      "Epoch 139, Batch 24/32 : Loss = 0.0010438712779432535\n",
      "Epoch 139, Batch 25/32 : Loss = 0.0008345776004716754\n",
      "Epoch 139, Batch 26/32 : Loss = 0.0005771986907348037\n",
      "Epoch 139, Batch 27/32 : Loss = 0.0019371557282283902\n",
      "Epoch 139, Batch 28/32 : Loss = 0.0011718383757397532\n",
      "Epoch 139, Batch 29/32 : Loss = 0.012042533606290817\n",
      "Epoch 139, Batch 30/32 : Loss = 0.0005219732411205769\n",
      "Epoch 139, Batch 31/32 : Loss = 0.001460369094274938\n",
      "Epoch 139 finished in 0.05169019301732381 minutes\n",
      "Epoch 139 training_loss = 0.0024320234078913927\n",
      "--.---C----O-------w------u---`---uu---.--RR----{{---3----\"---#------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "----{---BB----YY----RR----a----yy---h---##----2----->----EE----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "-----dd----!---NN-----r----A----j--**----$$----3----hh----55----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "----0-----JJ----!---(--;---AA------3----,--'---)---r----r----7------- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 139 val_loss = 0.7855600714683533, word_accuracy = 0.69\n",
      "Epoch 140, Batch 0/32 : Loss = 0.004004872869700193\n",
      "Epoch 140, Batch 1/32 : Loss = 0.002861857181414962\n",
      "Epoch 140, Batch 2/32 : Loss = 0.03589631989598274\n",
      "Epoch 140, Batch 3/32 : Loss = 0.001082769362255931\n",
      "Epoch 140, Batch 4/32 : Loss = 0.0005243893247097731\n",
      "Epoch 140, Batch 5/32 : Loss = 0.0019914088770747185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140, Batch 6/32 : Loss = 0.000452573352959007\n",
      "Epoch 140, Batch 7/32 : Loss = 0.0006167692481540143\n",
      "Epoch 140, Batch 8/32 : Loss = 0.0035090858582407236\n",
      "Epoch 140, Batch 9/32 : Loss = 0.007234279066324234\n",
      "Epoch 140, Batch 10/32 : Loss = 0.0014120095875114202\n",
      "Epoch 140, Batch 11/32 : Loss = 0.00047356123104691505\n",
      "Epoch 140, Batch 12/32 : Loss = 0.000512927770614624\n",
      "Epoch 140, Batch 13/32 : Loss = 0.0006123210769146681\n",
      "Epoch 140, Batch 14/32 : Loss = 0.0011387496488168836\n",
      "Epoch 140, Batch 15/32 : Loss = 0.0010038118343800306\n",
      "Epoch 140, Batch 16/32 : Loss = 0.0005692917038686574\n",
      "Epoch 140, Batch 17/32 : Loss = 0.06653529405593872\n",
      "Epoch 140, Batch 18/32 : Loss = 0.00046361039858311415\n",
      "Epoch 140, Batch 19/32 : Loss = 0.0010771722299978137\n",
      "Epoch 140, Batch 20/32 : Loss = 0.0008508599712513387\n",
      "Epoch 140, Batch 21/32 : Loss = 0.0005938283866271377\n",
      "Epoch 140, Batch 22/32 : Loss = 0.0011811081785708666\n",
      "Epoch 140, Batch 23/32 : Loss = 0.0020706206560134888\n",
      "Epoch 140, Batch 24/32 : Loss = 0.020642763003706932\n",
      "Epoch 140, Batch 25/32 : Loss = 0.007150871213525534\n",
      "Epoch 140, Batch 26/32 : Loss = 0.007325452286750078\n",
      "Epoch 140, Batch 27/32 : Loss = 0.0034863855689764023\n",
      "Epoch 140, Batch 28/32 : Loss = 0.0014895328786224127\n",
      "Epoch 140, Batch 29/32 : Loss = 0.0038646331522613764\n",
      "Epoch 140, Batch 30/32 : Loss = 0.0024627852253615856\n",
      "Epoch 140, Batch 31/32 : Loss = 0.8980856537818909\n",
      "Epoch 140 finished in 0.05237546761830648 minutes\n",
      "Epoch 140 training_loss = 0.009489241056144238\n",
      "----0------C-----++----bb-----|--\"----bb-----66----..---Q----------- => 0C+b|\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "---0------c-----++-----bb----|--\"\"----bb-----6----..---Q------------ => 0c+b|\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----{---BB----Y-----R-----a----y---hh---#-----2---->-----E----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "--2----p---:--mm-----x---a----z---n----@------C----y------%-----%%-- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "Epoch 140 val_loss = 0.7515847086906433, word_accuracy = 0.73\n",
      "Epoch 141, Batch 0/32 : Loss = 0.011222138069570065\n",
      "Epoch 141, Batch 1/32 : Loss = 0.000749064376577735\n",
      "Epoch 141, Batch 2/32 : Loss = 0.0009510510135442019\n",
      "Epoch 141, Batch 3/32 : Loss = 0.002050934359431267\n",
      "Epoch 141, Batch 4/32 : Loss = 0.006976246368139982\n",
      "Epoch 141, Batch 5/32 : Loss = 0.0016243977006524801\n",
      "Epoch 141, Batch 6/32 : Loss = 0.0020107808522880077\n",
      "Epoch 141, Batch 7/32 : Loss = 0.0011158306151628494\n",
      "Epoch 141, Batch 8/32 : Loss = 0.02527185156941414\n",
      "Epoch 141, Batch 9/32 : Loss = 0.0026129353791475296\n",
      "Epoch 141, Batch 10/32 : Loss = 0.0038991079200059175\n",
      "Epoch 141, Batch 11/32 : Loss = 0.002032150747254491\n",
      "Epoch 141, Batch 12/32 : Loss = 0.008175745606422424\n",
      "Epoch 141, Batch 13/32 : Loss = 0.001507316716015339\n",
      "Epoch 141, Batch 14/32 : Loss = 0.0019295320380479097\n",
      "Epoch 141, Batch 15/32 : Loss = 0.003976459614932537\n",
      "Epoch 141, Batch 16/32 : Loss = 0.009437932632863522\n",
      "Epoch 141, Batch 17/32 : Loss = 0.0013675758382305503\n",
      "Epoch 141, Batch 18/32 : Loss = 0.0027154237031936646\n",
      "Epoch 141, Batch 19/32 : Loss = 0.004809567704796791\n",
      "Epoch 141, Batch 20/32 : Loss = 0.0011903190752491355\n",
      "Epoch 141, Batch 21/32 : Loss = 0.0017784582450985909\n",
      "Epoch 141, Batch 22/32 : Loss = 0.02294001914560795\n",
      "Epoch 141, Batch 23/32 : Loss = 0.001297981129027903\n",
      "Epoch 141, Batch 24/32 : Loss = 0.009308085776865482\n",
      "Epoch 141, Batch 25/32 : Loss = 0.0017167114419862628\n",
      "Epoch 141, Batch 26/32 : Loss = 0.010608038865029812\n",
      "Epoch 141, Batch 27/32 : Loss = 0.0012394711375236511\n",
      "Epoch 141, Batch 28/32 : Loss = 0.0020263914484530687\n",
      "Epoch 141, Batch 29/32 : Loss = 0.0008555621025152504\n",
      "Epoch 141, Batch 30/32 : Loss = 0.001234153169207275\n",
      "Epoch 141, Batch 31/32 : Loss = 0.0004068611597176641\n",
      "Epoch 141 finished in 0.052047324180603025 minutes\n",
      "Epoch 141 training_loss = 0.004776934627443552\n",
      "---{--BB----Y----R----a----y---h---#----2--->>----E----4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----##---GG----9---E---I-=---hh---5---#---22---J--))-kk----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---//---MM-----o----EE---^^---3---xx---/---&----66---XX----- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----d----!--NN-----r--AA---j--*----$---33---hh---55---n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 141 val_loss = 0.8002037405967712, word_accuracy = 0.69\n",
      "Epoch 142, Batch 0/32 : Loss = 0.0008108010515570641\n",
      "Epoch 142, Batch 1/32 : Loss = 0.0010077714687213302\n",
      "Epoch 142, Batch 2/32 : Loss = 0.0024465022142976522\n",
      "Epoch 142, Batch 3/32 : Loss = 0.0007486764807254076\n",
      "Epoch 142, Batch 4/32 : Loss = 0.001200254657305777\n",
      "Epoch 142, Batch 5/32 : Loss = 0.0013357060961425304\n",
      "Epoch 142, Batch 6/32 : Loss = 0.0007100341026671231\n",
      "Epoch 142, Batch 7/32 : Loss = 0.0012530680978670716\n",
      "Epoch 142, Batch 8/32 : Loss = 0.0005705776857212186\n",
      "Epoch 142, Batch 9/32 : Loss = 0.0005338521441444755\n",
      "Epoch 142, Batch 10/32 : Loss = 0.0010726673062890768\n",
      "Epoch 142, Batch 11/32 : Loss = 0.0006427478510886431\n",
      "Epoch 142, Batch 12/32 : Loss = 0.002324554603546858\n",
      "Epoch 142, Batch 13/32 : Loss = 0.0014486087020486593\n",
      "Epoch 142, Batch 14/32 : Loss = 0.15124507248401642\n",
      "Epoch 142, Batch 15/32 : Loss = 0.0007886886596679688\n",
      "Epoch 142, Batch 16/32 : Loss = 0.0009201620705425739\n",
      "Epoch 142, Batch 17/32 : Loss = 0.0012267343699932098\n",
      "Epoch 142, Batch 18/32 : Loss = 0.000980918761342764\n",
      "Epoch 142, Batch 19/32 : Loss = 0.00613256823271513\n",
      "Epoch 142, Batch 20/32 : Loss = 0.0011281543411314487\n",
      "Epoch 142, Batch 21/32 : Loss = 0.0013466064119711518\n",
      "Epoch 142, Batch 22/32 : Loss = 0.001110111246816814\n",
      "Epoch 142, Batch 23/32 : Loss = 0.001342706149443984\n",
      "Epoch 142, Batch 24/32 : Loss = 0.0007924408419057727\n",
      "Epoch 142, Batch 25/32 : Loss = 0.009811388328671455\n",
      "Epoch 142, Batch 26/32 : Loss = 0.0019423901103436947\n",
      "Epoch 142, Batch 27/32 : Loss = 0.04420268535614014\n",
      "Epoch 142, Batch 28/32 : Loss = 0.004739257041364908\n",
      "Epoch 142, Batch 29/32 : Loss = 0.0010098099010065198\n",
      "Epoch 142, Batch 30/32 : Loss = 0.010191126726567745\n",
      "Epoch 142, Batch 31/32 : Loss = 0.002774151973426342\n",
      "Epoch 142 finished in 0.051764297485351565 minutes\n",
      "Epoch 142 training_loss = 0.00820444617420435\n",
      "-----Y------W-------]---i--\\\\---|------M-------<-------M------88----88---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----WW------=-----2----+----E----11---n----TT---X----r---C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "----dd-----!---NN------rr---A-----j---*-----$-----3----hh------5----nn---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "------O------c------++-----bb-----I--\"-----bb-----66----..---Q------------ => Oc+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 142 val_loss = 0.7606735229492188, word_accuracy = 0.71\n",
      "Epoch 143, Batch 0/32 : Loss = 0.0011033258633688092\n",
      "Epoch 143, Batch 1/32 : Loss = 0.0007119195070117712\n",
      "Epoch 143, Batch 2/32 : Loss = 0.0008152903174050152\n",
      "Epoch 143, Batch 3/32 : Loss = 0.003220075275748968\n",
      "Epoch 143, Batch 4/32 : Loss = 0.0009406761964783072\n",
      "Epoch 143, Batch 5/32 : Loss = 0.0011265233624726534\n",
      "Epoch 143, Batch 6/32 : Loss = 0.004578540101647377\n",
      "Epoch 143, Batch 7/32 : Loss = 0.0007570358575321734\n",
      "Epoch 143, Batch 8/32 : Loss = 0.0007899731863290071\n",
      "Epoch 143, Batch 9/32 : Loss = 0.0012123718624934554\n",
      "Epoch 143, Batch 10/32 : Loss = 0.0013385877246037126\n",
      "Epoch 143, Batch 11/32 : Loss = 0.0005694743013009429\n",
      "Epoch 143, Batch 12/32 : Loss = 0.0005790347931906581\n",
      "Epoch 143, Batch 13/32 : Loss = 0.0005375311593525112\n",
      "Epoch 143, Batch 14/32 : Loss = 0.00108326633926481\n",
      "Epoch 143, Batch 15/32 : Loss = 0.00539759173989296\n",
      "Epoch 143, Batch 16/32 : Loss = 0.01367943361401558\n",
      "Epoch 143, Batch 17/32 : Loss = 0.0006057978607714176\n",
      "Epoch 143, Batch 18/32 : Loss = 0.0015628214459866285\n",
      "Epoch 143, Batch 19/32 : Loss = 0.00066806172253564\n",
      "Epoch 143, Batch 20/32 : Loss = 0.0007354080444201827\n",
      "Epoch 143, Batch 21/32 : Loss = 0.0029551053885370493\n",
      "Epoch 143, Batch 22/32 : Loss = 0.002403426915407181\n",
      "Epoch 143, Batch 23/32 : Loss = 0.000815401435829699\n",
      "Epoch 143, Batch 24/32 : Loss = 0.0044976635836064816\n",
      "Epoch 143, Batch 25/32 : Loss = 0.0011162599548697472\n",
      "Epoch 143, Batch 26/32 : Loss = 0.0007804327178746462\n",
      "Epoch 143, Batch 27/32 : Loss = 0.004405070561915636\n",
      "Epoch 143, Batch 28/32 : Loss = 0.0014916774816811085\n",
      "Epoch 143, Batch 29/32 : Loss = 0.0011841042432934046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143, Batch 30/32 : Loss = 0.0010464127408340573\n",
      "Epoch 143, Batch 31/32 : Loss = 0.009913170710206032\n",
      "Epoch 143 finished in 0.052318227291107175 minutes\n",
      "Epoch 143 training_loss = 0.002054536249488592\n",
      "-----d---::-XX---9----e---aa---F---,-8-------V----R---- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "---o---\"--}}-!!-BB---rr--&----9---;-``--O----w----}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-------33---\\----$----->-----S----\\\\---MM-----i---B---- => -3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "---\"\"--]--t--4----e---^----W------Q----4---->>---g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "Epoch 143 val_loss = 0.7617515325546265, word_accuracy = 0.67\n",
      "Epoch 144, Batch 0/32 : Loss = 0.0007213884964585304\n",
      "Epoch 144, Batch 1/32 : Loss = 0.0004092095769010484\n",
      "Epoch 144, Batch 2/32 : Loss = 0.0007991198217496276\n",
      "Epoch 144, Batch 3/32 : Loss = 0.00041732474346645176\n",
      "Epoch 144, Batch 4/32 : Loss = 0.001263274229131639\n",
      "Epoch 144, Batch 5/32 : Loss = 0.0006135860458016396\n",
      "Epoch 144, Batch 6/32 : Loss = 0.0004966320120729506\n",
      "Epoch 144, Batch 7/32 : Loss = 0.0006754153873771429\n",
      "Epoch 144, Batch 8/32 : Loss = 0.00455948943272233\n",
      "Epoch 144, Batch 9/32 : Loss = 0.0007972295861691236\n",
      "Epoch 144, Batch 10/32 : Loss = 0.003916952293366194\n",
      "Epoch 144, Batch 11/32 : Loss = 0.0006252992898225784\n",
      "Epoch 144, Batch 12/32 : Loss = 0.00081629678606987\n",
      "Epoch 144, Batch 13/32 : Loss = 0.0017197525594383478\n",
      "Epoch 144, Batch 14/32 : Loss = 0.0009900630684569478\n",
      "Epoch 144, Batch 15/32 : Loss = 0.0005129402852617204\n",
      "Epoch 144, Batch 16/32 : Loss = 0.002212375635281205\n",
      "Epoch 144, Batch 17/32 : Loss = 0.0009671255829744041\n",
      "Epoch 144, Batch 18/32 : Loss = 0.0007222042186185718\n",
      "Epoch 144, Batch 19/32 : Loss = 0.0007751464145258069\n",
      "Epoch 144, Batch 20/32 : Loss = 0.0012734089978039265\n",
      "Epoch 144, Batch 21/32 : Loss = 0.0009404354495927691\n",
      "Epoch 144, Batch 22/32 : Loss = 0.0071137575432658195\n",
      "Epoch 144, Batch 23/32 : Loss = 0.021814782172441483\n",
      "Epoch 144, Batch 24/32 : Loss = 0.0005042036063969135\n",
      "Epoch 144, Batch 25/32 : Loss = 0.0009409687481820583\n",
      "Epoch 144, Batch 26/32 : Loss = 0.0009936437709257007\n",
      "Epoch 144, Batch 27/32 : Loss = 0.0011724458308890462\n",
      "Epoch 144, Batch 28/32 : Loss = 0.0012534663546830416\n",
      "Epoch 144, Batch 29/32 : Loss = 0.0003819032572209835\n",
      "Epoch 144, Batch 30/32 : Loss = 0.0007756184786558151\n",
      "Epoch 144, Batch 31/32 : Loss = 0.002406127518042922\n",
      "Epoch 144 finished in 0.05188460350036621 minutes\n",
      "Epoch 144 training_loss = 0.0019751396030187607\n",
      "---//---MM-----o----E----^---3---x---/---&----6----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---<<-----v---O------T--``--4-----V---[---0-------Q------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "-----<-----v---O------T---`---4----V----[--0-------QQ----- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "---#----G----9----E---I-==---h---55---#---2----J---)--k--- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 144 val_loss = 0.8134797215461731, word_accuracy = 0.71\n",
      "Epoch 145, Batch 0/32 : Loss = 0.00042220158502459526\n",
      "Epoch 145, Batch 1/32 : Loss = 0.001347551355138421\n",
      "Epoch 145, Batch 2/32 : Loss = 0.001691550249233842\n",
      "Epoch 145, Batch 3/32 : Loss = 0.0004952087765559554\n",
      "Epoch 145, Batch 4/32 : Loss = 0.0032353028655052185\n",
      "Epoch 145, Batch 5/32 : Loss = 0.0023942377883940935\n",
      "Epoch 145, Batch 6/32 : Loss = 0.03135143592953682\n",
      "Epoch 145, Batch 7/32 : Loss = 0.0007890933193266392\n",
      "Epoch 145, Batch 8/32 : Loss = 0.0006151478737592697\n",
      "Epoch 145, Batch 9/32 : Loss = 0.0006434834795072675\n",
      "Epoch 145, Batch 10/32 : Loss = 0.00041257153498008847\n",
      "Epoch 145, Batch 11/32 : Loss = 0.0012527033686637878\n",
      "Epoch 145, Batch 12/32 : Loss = 0.0007177394581958652\n",
      "Epoch 145, Batch 13/32 : Loss = 0.0010238061659038067\n",
      "Epoch 145, Batch 14/32 : Loss = 0.0014514439972117543\n",
      "Epoch 145, Batch 15/32 : Loss = 0.001199445454403758\n",
      "Epoch 145, Batch 16/32 : Loss = 0.00742004020139575\n",
      "Epoch 145, Batch 17/32 : Loss = 0.005162751767784357\n",
      "Epoch 145, Batch 18/32 : Loss = 0.0005240889731794596\n",
      "Epoch 145, Batch 19/32 : Loss = 0.0005209986120462418\n",
      "Epoch 145, Batch 20/32 : Loss = 0.001690138946287334\n",
      "Epoch 145, Batch 21/32 : Loss = 0.0011014019837602973\n",
      "Epoch 145, Batch 22/32 : Loss = 0.0009617586620151997\n",
      "Epoch 145, Batch 23/32 : Loss = 0.0005528424517251551\n",
      "Epoch 145, Batch 24/32 : Loss = 0.0006776599329896271\n",
      "Epoch 145, Batch 25/32 : Loss = 0.0006763201672583818\n",
      "Epoch 145, Batch 26/32 : Loss = 0.0005725532537326217\n",
      "Epoch 145, Batch 27/32 : Loss = 0.0005824146792292595\n",
      "Epoch 145, Batch 28/32 : Loss = 0.0009324676357209682\n",
      "Epoch 145, Batch 29/32 : Loss = 0.0010225202422589064\n",
      "Epoch 145, Batch 30/32 : Loss = 0.0008722960483282804\n",
      "Epoch 145, Batch 31/32 : Loss = 0.0941026359796524\n",
      "Epoch 145 finished in 0.05181459585825602 minutes\n",
      "Epoch 145 training_loss = 0.0027012373320758343\n",
      "---o---\"\"--}--!--BB---rr--&-----9---;``--O-----w----}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "-----d----:--X----9---ee---aa---FF--,-88------VV---RR---- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "------Q-----3----g---II--z--##----Y---:--]--q----++---*-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----k---$---I--F----DD---ee---h----]--k---o---\\---XX----- => k$IFDeh]ko\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 145 val_loss = 0.8236261606216431, word_accuracy = 0.7\n",
      "Epoch 146, Batch 0/32 : Loss = 0.00044486107071861625\n",
      "Epoch 146, Batch 1/32 : Loss = 0.001156543381512165\n",
      "Epoch 146, Batch 2/32 : Loss = 0.0008338611223734915\n",
      "Epoch 146, Batch 3/32 : Loss = 0.0008144775056280196\n",
      "Epoch 146, Batch 4/32 : Loss = 0.000601236883085221\n",
      "Epoch 146, Batch 5/32 : Loss = 0.0004087352426722646\n",
      "Epoch 146, Batch 6/32 : Loss = 0.0009230158175341785\n",
      "Epoch 146, Batch 7/32 : Loss = 0.00086489028763026\n",
      "Epoch 146, Batch 8/32 : Loss = 0.000978192314505577\n",
      "Epoch 146, Batch 9/32 : Loss = 0.001534762792289257\n",
      "Epoch 146, Batch 10/32 : Loss = 0.0007114542531780899\n",
      "Epoch 146, Batch 11/32 : Loss = 0.0009198338957503438\n",
      "Epoch 146, Batch 12/32 : Loss = 0.0005351892905309796\n",
      "Epoch 146, Batch 13/32 : Loss = 0.0013689291663467884\n",
      "Epoch 146, Batch 14/32 : Loss = 0.0005101555143482983\n",
      "Epoch 146, Batch 15/32 : Loss = 0.0007265866734087467\n",
      "Epoch 146, Batch 16/32 : Loss = 0.0014780289493501186\n",
      "Epoch 146, Batch 17/32 : Loss = 0.0012782540870830417\n",
      "Epoch 146, Batch 18/32 : Loss = 0.0013801298337057233\n",
      "Epoch 146, Batch 19/32 : Loss = 0.0006275231717154384\n",
      "Epoch 146, Batch 20/32 : Loss = 0.00044662351137958467\n",
      "Epoch 146, Batch 21/32 : Loss = 0.001126479241065681\n",
      "Epoch 146, Batch 22/32 : Loss = 0.002294320845976472\n",
      "Epoch 146, Batch 23/32 : Loss = 0.0008262025658041239\n",
      "Epoch 146, Batch 24/32 : Loss = 0.0008337413892149925\n",
      "Epoch 146, Batch 25/32 : Loss = 0.0006944448687136173\n",
      "Epoch 146, Batch 26/32 : Loss = 0.0005649429513141513\n",
      "Epoch 146, Batch 27/32 : Loss = 0.011280752718448639\n",
      "Epoch 146, Batch 28/32 : Loss = 0.0004797440196853131\n",
      "Epoch 146, Batch 29/32 : Loss = 0.0007955702021718025\n",
      "Epoch 146, Batch 30/32 : Loss = 0.0005325642414391041\n",
      "Epoch 146, Batch 31/32 : Loss = 0.004698420409113169\n",
      "Epoch 146 finished in 0.053053792317708334 minutes\n",
      "Epoch 146 training_loss = 0.0012388544855639338\n",
      "--/----MM----o----E---^---3---xx--/---&---66---XX---- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----z---0---\"\"---G-----/---~----$$---c----1---j--t--- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "---d---:--X----9---e----a----F--,-88------VV---R----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "---o---\"--}--!--B----r--&----9---;-`--O-----w---}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 146 val_loss = 0.8325265049934387, word_accuracy = 0.72\n",
      "Epoch 147, Batch 0/32 : Loss = 0.0004007642564829439\n",
      "Epoch 147, Batch 1/32 : Loss = 0.0016778175486251712\n",
      "Epoch 147, Batch 2/32 : Loss = 0.0005916603840887547\n",
      "Epoch 147, Batch 3/32 : Loss = 0.0008708913810551167\n",
      "Epoch 147, Batch 4/32 : Loss = 0.0008248700178228319\n",
      "Epoch 147, Batch 5/32 : Loss = 0.0010431945556774735\n",
      "Epoch 147, Batch 6/32 : Loss = 0.0009603506769053638\n",
      "Epoch 147, Batch 7/32 : Loss = 0.0005038017989136279\n",
      "Epoch 147, Batch 8/32 : Loss = 0.00039783521788194776\n",
      "Epoch 147, Batch 9/32 : Loss = 0.0007581468671560287\n",
      "Epoch 147, Batch 10/32 : Loss = 0.0005263012717477977\n",
      "Epoch 147, Batch 11/32 : Loss = 0.000738670933060348\n",
      "Epoch 147, Batch 12/32 : Loss = 0.0005117733962833881\n",
      "Epoch 147, Batch 13/32 : Loss = 0.0031035887077450752\n",
      "Epoch 147, Batch 14/32 : Loss = 0.0006049098446965218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, Batch 15/32 : Loss = 0.004521287977695465\n",
      "Epoch 147, Batch 16/32 : Loss = 0.009664387442171574\n",
      "Epoch 147, Batch 17/32 : Loss = 0.003180672414600849\n",
      "Epoch 147, Batch 18/32 : Loss = 0.0007111334707587957\n",
      "Epoch 147, Batch 19/32 : Loss = 0.0007268126355484128\n",
      "Epoch 147, Batch 20/32 : Loss = 0.0012480099685490131\n",
      "Epoch 147, Batch 21/32 : Loss = 0.0004961257800459862\n",
      "Epoch 147, Batch 22/32 : Loss = 0.013914411887526512\n",
      "Epoch 147, Batch 23/32 : Loss = 0.0007517177145928144\n",
      "Epoch 147, Batch 24/32 : Loss = 0.0008331001154147089\n",
      "Epoch 147, Batch 25/32 : Loss = 0.0003679947112686932\n",
      "Epoch 147, Batch 26/32 : Loss = 0.00428465660661459\n",
      "Epoch 147, Batch 27/32 : Loss = 0.0013952581211924553\n",
      "Epoch 147, Batch 28/32 : Loss = 0.0008841590024530888\n",
      "Epoch 147, Batch 29/32 : Loss = 0.0009180711931549013\n",
      "Epoch 147, Batch 30/32 : Loss = 0.0004625055007636547\n",
      "Epoch 147, Batch 31/32 : Loss = 0.00044637301471084356\n",
      "Epoch 147 finished in 0.05140379269917806 minutes\n",
      "Epoch 147 training_loss = 0.0018612267449498177\n",
      "------Q-----3----g---I--z---#----Y---::-]--q----++---*--- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----7--nn---DD---P---nn--tt--w---dd---\\---Q---a----R----- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "----dd---!--NN----r---A---j--*---$$---3---hh---55---n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----<-----v---O-----TT--``--4----V---[[--0-------Q------ => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "Epoch 147 val_loss = 0.7937982678413391, word_accuracy = 0.73\n",
      "Epoch 148, Batch 0/32 : Loss = 0.0007951193256303668\n",
      "Epoch 148, Batch 1/32 : Loss = 0.00045526825124397874\n",
      "Epoch 148, Batch 2/32 : Loss = 0.003545930841937661\n",
      "Epoch 148, Batch 3/32 : Loss = 0.002299351617693901\n",
      "Epoch 148, Batch 4/32 : Loss = 0.000383934035198763\n",
      "Epoch 148, Batch 5/32 : Loss = 0.000453550077509135\n",
      "Epoch 148, Batch 6/32 : Loss = 0.002557974075898528\n",
      "Epoch 148, Batch 7/32 : Loss = 0.0037823994643986225\n",
      "Epoch 148, Batch 8/32 : Loss = 0.011955002322793007\n",
      "Epoch 148, Batch 9/32 : Loss = 0.00045787953422404826\n",
      "Epoch 148, Batch 10/32 : Loss = 0.0009287656866945326\n",
      "Epoch 148, Batch 11/32 : Loss = 0.0006624931120313704\n",
      "Epoch 148, Batch 12/32 : Loss = 0.009575748816132545\n",
      "Epoch 148, Batch 13/32 : Loss = 0.003077578265219927\n",
      "Epoch 148, Batch 14/32 : Loss = 0.003237646073102951\n",
      "Epoch 148, Batch 15/32 : Loss = 0.001432435936294496\n",
      "Epoch 148, Batch 16/32 : Loss = 0.0003943466581404209\n",
      "Epoch 148, Batch 17/32 : Loss = 0.0007609919412061572\n",
      "Epoch 148, Batch 18/32 : Loss = 0.0006658454658463597\n",
      "Epoch 148, Batch 19/32 : Loss = 0.000721098156645894\n",
      "Epoch 148, Batch 20/32 : Loss = 0.001318567432463169\n",
      "Epoch 148, Batch 21/32 : Loss = 0.0004704405437223613\n",
      "Epoch 148, Batch 22/32 : Loss = 0.0019895965233445168\n",
      "Epoch 148, Batch 23/32 : Loss = 0.009786592796444893\n",
      "Epoch 148, Batch 24/32 : Loss = 0.004393343813717365\n",
      "Epoch 148, Batch 25/32 : Loss = 0.005205038469284773\n",
      "Epoch 148, Batch 26/32 : Loss = 0.0008521182462573051\n",
      "Epoch 148, Batch 27/32 : Loss = 0.0005569648346863687\n",
      "Epoch 148, Batch 28/32 : Loss = 0.0004189598257653415\n",
      "Epoch 148, Batch 29/32 : Loss = 0.0003806350287050009\n",
      "Epoch 148, Batch 30/32 : Loss = 0.0008972971118055284\n",
      "Epoch 148, Batch 31/32 : Loss = 0.23244532942771912\n",
      "Epoch 148 finished in 0.05334414641062419 minutes\n",
      "Epoch 148 training_loss = 0.003324291668832302\n",
      "---J--;;-qq----+---//---z--yy---U-------%%---U-----1---x---_)-- => J;q+/zyU%U1x_), Ground Truth is J;q+/zyU%U1x_\n",
      "----YY----W------]--i--\\---/-----MM----<<-----MM-----8----8---- => YW]i\\/M<M88, Ground Truth is YW]i\\|M<M88\n",
      "----kk---$----|--'-,,-99---Y----NN----WW-----mm------T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      ":-------33----\\----$$---->>------S-----\\\\---MM------ii--BB----- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 148 val_loss = 0.8172937035560608, word_accuracy = 0.74\n",
      "Epoch 149, Batch 0/32 : Loss = 0.0010944590903818607\n",
      "Epoch 149, Batch 1/32 : Loss = 0.0010074169840663671\n",
      "Epoch 149, Batch 2/32 : Loss = 0.003050408326089382\n",
      "Epoch 149, Batch 3/32 : Loss = 0.001018024981021881\n",
      "Epoch 149, Batch 4/32 : Loss = 0.0016400977037847042\n",
      "Epoch 149, Batch 5/32 : Loss = 0.000623091240413487\n",
      "Epoch 149, Batch 6/32 : Loss = 0.022405991330742836\n",
      "Epoch 149, Batch 7/32 : Loss = 0.001229969086125493\n",
      "Epoch 149, Batch 8/32 : Loss = 0.0006957269506528974\n",
      "Epoch 149, Batch 9/32 : Loss = 0.0010571323800832033\n",
      "Epoch 149, Batch 10/32 : Loss = 0.001474488526582718\n",
      "Epoch 149, Batch 11/32 : Loss = 0.001350283157080412\n",
      "Epoch 149, Batch 12/32 : Loss = 0.0003806525201071054\n",
      "Epoch 149, Batch 13/32 : Loss = 0.0013648377498611808\n",
      "Epoch 149, Batch 14/32 : Loss = 0.0005060525145381689\n",
      "Epoch 149, Batch 15/32 : Loss = 0.0006055423291400075\n",
      "Epoch 149, Batch 16/32 : Loss = 0.016023939475417137\n",
      "Epoch 149, Batch 17/32 : Loss = 0.001303382683545351\n",
      "Epoch 149, Batch 18/32 : Loss = 0.00046407507034018636\n",
      "Epoch 149, Batch 19/32 : Loss = 0.0039946045726537704\n",
      "Epoch 149, Batch 20/32 : Loss = 0.0004507197008933872\n",
      "Epoch 149, Batch 21/32 : Loss = 0.0013528733979910612\n",
      "Epoch 149, Batch 22/32 : Loss = 0.001047856523655355\n",
      "Epoch 149, Batch 23/32 : Loss = 0.0015886742621660233\n",
      "Epoch 149, Batch 24/32 : Loss = 0.0006307675503194332\n",
      "Epoch 149, Batch 25/32 : Loss = 0.0006861112779006362\n",
      "Epoch 149, Batch 26/32 : Loss = 0.0007064074743539095\n",
      "Epoch 149, Batch 27/32 : Loss = 0.008951556868851185\n",
      "Epoch 149, Batch 28/32 : Loss = 0.000911580165848136\n",
      "Epoch 149, Batch 29/32 : Loss = 0.0009948697406798601\n",
      "Epoch 149, Batch 30/32 : Loss = 0.0014639860019087791\n",
      "Epoch 149, Batch 31/32 : Loss = 0.012242307886481285\n",
      "Epoch 149 finished in 0.05336958567301432 minutes\n",
      "Epoch 149 training_loss = 0.0026218751445412636\n",
      "-----Y-----W------]--i--\\---|----MM-----<-----MM-----8----8----- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "---55---->---:--*---YY----A---'--O-----D----*---#----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "-----#----G-----9---E---II-=----h---55---#---22---J---)--kk----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----0-----C-----++----bb----|--\"----bb-----6----..--QQ---------- => 0C+b|\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 149 val_loss = 0.7811291813850403, word_accuracy = 0.75\n",
      "Epoch 150, Batch 0/32 : Loss = 0.009472539648413658\n",
      "Epoch 150, Batch 1/32 : Loss = 0.0004827301891054958\n",
      "Epoch 150, Batch 2/32 : Loss = 0.0005807123961858451\n",
      "Epoch 150, Batch 3/32 : Loss = 0.0005464953137561679\n",
      "Epoch 150, Batch 4/32 : Loss = 0.00047808897215873003\n",
      "Epoch 150, Batch 5/32 : Loss = 0.0017583665903657675\n",
      "Epoch 150, Batch 6/32 : Loss = 0.00106882955878973\n",
      "Epoch 150, Batch 7/32 : Loss = 0.000582476903218776\n",
      "Epoch 150, Batch 8/32 : Loss = 0.0005244496278464794\n",
      "Epoch 150, Batch 9/32 : Loss = 0.0036239726468920708\n",
      "Epoch 150, Batch 10/32 : Loss = 0.0013176633510738611\n",
      "Epoch 150, Batch 11/32 : Loss = 0.0005497140809893608\n",
      "Epoch 150, Batch 12/32 : Loss = 0.035327546298503876\n",
      "Epoch 150, Batch 13/32 : Loss = 0.00032782735070213675\n",
      "Epoch 150, Batch 14/32 : Loss = 0.0006572531419806182\n",
      "Epoch 150, Batch 15/32 : Loss = 0.0022881433833390474\n",
      "Epoch 150, Batch 16/32 : Loss = 0.0004549319273792207\n",
      "Epoch 150, Batch 17/32 : Loss = 0.0004118570068385452\n",
      "Epoch 150, Batch 18/32 : Loss = 0.000778543297201395\n",
      "Epoch 150, Batch 19/32 : Loss = 0.0006387954927049577\n",
      "Epoch 150, Batch 20/32 : Loss = 0.000705193670000881\n",
      "Epoch 150, Batch 21/32 : Loss = 0.000838381820358336\n",
      "Epoch 150, Batch 22/32 : Loss = 0.000904106127563864\n",
      "Epoch 150, Batch 23/32 : Loss = 0.00043554441072046757\n",
      "Epoch 150, Batch 24/32 : Loss = 0.0032120102550834417\n",
      "Epoch 150, Batch 25/32 : Loss = 0.0006114428979344666\n",
      "Epoch 150, Batch 26/32 : Loss = 0.0004976383643224835\n",
      "Epoch 150, Batch 27/32 : Loss = 0.000667156302370131\n",
      "Epoch 150, Batch 28/32 : Loss = 0.0006117136217653751\n",
      "Epoch 150, Batch 29/32 : Loss = 0.0048179784789681435\n",
      "Epoch 150, Batch 30/32 : Loss = 0.00044117876677773893\n",
      "Epoch 150, Batch 31/32 : Loss = 0.0028874725103378296\n",
      "Epoch 150 finished in 0.05318624178568522 minutes\n",
      "Epoch 150 training_loss = 0.002440938726067543\n",
      "------0-----JJ----!---(---;---A------33---,,,-'--))---r----r----77---- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----00---QQ----6----<-----<---((---T----N----55--==----P---(--mm------ => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----X----77---0---j---@-----S----ZZ---L---4-----C----mm------MM------- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----kk----OO------/--,,--yy---c----**----11---PP----}----#-----B------ => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 150 val_loss = 0.7931306958198547, word_accuracy = 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, Batch 0/32 : Loss = 0.0006029278738424182\n",
      "Epoch 151, Batch 1/32 : Loss = 0.0005035435315221548\n",
      "Epoch 151, Batch 2/32 : Loss = 0.0006769997999072075\n",
      "Epoch 151, Batch 3/32 : Loss = 0.000486701144836843\n",
      "Epoch 151, Batch 4/32 : Loss = 0.0005975499516353011\n",
      "Epoch 151, Batch 5/32 : Loss = 0.0007889106636866927\n",
      "Epoch 151, Batch 6/32 : Loss = 0.00040643481770530343\n",
      "Epoch 151, Batch 7/32 : Loss = 0.002448180690407753\n",
      "Epoch 151, Batch 8/32 : Loss = 0.0016442451160401106\n",
      "Epoch 151, Batch 9/32 : Loss = 0.00039578144787810743\n",
      "Epoch 151, Batch 10/32 : Loss = 0.00036779011134058237\n",
      "Epoch 151, Batch 11/32 : Loss = 0.0011707129888236523\n",
      "Epoch 151, Batch 12/32 : Loss = 0.0004225607553962618\n",
      "Epoch 151, Batch 13/32 : Loss = 0.0004110466979909688\n",
      "Epoch 151, Batch 14/32 : Loss = 0.00035659753484651446\n",
      "Epoch 151, Batch 15/32 : Loss = 0.0003659139038063586\n",
      "Epoch 151, Batch 16/32 : Loss = 0.0006695430492982268\n",
      "Epoch 151, Batch 17/32 : Loss = 0.00047670278581790626\n",
      "Epoch 151, Batch 18/32 : Loss = 0.0010712589137256145\n",
      "Epoch 151, Batch 19/32 : Loss = 0.0006820852868258953\n",
      "Epoch 151, Batch 20/32 : Loss = 0.0003662934759631753\n",
      "Epoch 151, Batch 21/32 : Loss = 0.000566043658182025\n",
      "Epoch 151, Batch 22/32 : Loss = 0.0016418808372691274\n",
      "Epoch 151, Batch 23/32 : Loss = 0.0003886292106471956\n",
      "Epoch 151, Batch 24/32 : Loss = 0.00037059298483654857\n",
      "Epoch 151, Batch 25/32 : Loss = 0.00048569292994216084\n",
      "Epoch 151, Batch 26/32 : Loss = 0.0010710348142310977\n",
      "Epoch 151, Batch 27/32 : Loss = 0.0009151468984782696\n",
      "Epoch 151, Batch 28/32 : Loss = 0.000412575900554657\n",
      "Epoch 151, Batch 29/32 : Loss = 0.00034480495378375053\n",
      "Epoch 151, Batch 30/32 : Loss = 0.0002872921759262681\n",
      "Epoch 151, Batch 31/32 : Loss = 0.050819333642721176\n",
      "Epoch 151 finished in 0.05341279109319051 minutes\n",
      "Epoch 151 training_loss = 0.0008914986392483115\n",
      "-----d-----!---NN------rr---A-----j---*----$$-----3----hh-----55-----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----C-----DD----E-----g----mm-----\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----88-----KK-----ll--ZZ------5-----p------$$----aa-----}----w------,----- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "--..---c----OO------w-------u---``---uu---..--R-----{{---3----\"----#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 151 val_loss = 0.7989861965179443, word_accuracy = 0.74\n",
      "Epoch 152, Batch 0/32 : Loss = 0.0005576814291998744\n",
      "Epoch 152, Batch 1/32 : Loss = 0.000971083645708859\n",
      "Epoch 152, Batch 2/32 : Loss = 0.0006973140989430249\n",
      "Epoch 152, Batch 3/32 : Loss = 0.0005886625731363893\n",
      "Epoch 152, Batch 4/32 : Loss = 0.035895224660634995\n",
      "Epoch 152, Batch 5/32 : Loss = 0.0003909702936653048\n",
      "Epoch 152, Batch 6/32 : Loss = 0.00035161705454811454\n",
      "Epoch 152, Batch 7/32 : Loss = 0.00043476937571540475\n",
      "Epoch 152, Batch 8/32 : Loss = 0.00026505690766498446\n",
      "Epoch 152, Batch 9/32 : Loss = 0.0005164092290215194\n",
      "Epoch 152, Batch 10/32 : Loss = 0.0008275758009403944\n",
      "Epoch 152, Batch 11/32 : Loss = 0.0031550417188555002\n",
      "Epoch 152, Batch 12/32 : Loss = 0.0004617834056261927\n",
      "Epoch 152, Batch 13/32 : Loss = 0.0003019589348696172\n",
      "Epoch 152, Batch 14/32 : Loss = 0.0004400678735692054\n",
      "Epoch 152, Batch 15/32 : Loss = 0.0007708915509283543\n",
      "Epoch 152, Batch 16/32 : Loss = 0.0003779208054766059\n",
      "Epoch 152, Batch 17/32 : Loss = 0.0005960306152701378\n",
      "Epoch 152, Batch 18/32 : Loss = 0.0006651001749560237\n",
      "Epoch 152, Batch 19/32 : Loss = 0.003502033418044448\n",
      "Epoch 152, Batch 20/32 : Loss = 0.007643431890755892\n",
      "Epoch 152, Batch 21/32 : Loss = 0.0005894041387364268\n",
      "Epoch 152, Batch 22/32 : Loss = 0.0004572063044179231\n",
      "Epoch 152, Batch 23/32 : Loss = 0.001051571685820818\n",
      "Epoch 152, Batch 24/32 : Loss = 0.0006764177232980728\n",
      "Epoch 152, Batch 25/32 : Loss = 0.0005805785767734051\n",
      "Epoch 152, Batch 26/32 : Loss = 0.0003092468832619488\n",
      "Epoch 152, Batch 27/32 : Loss = 0.00365476799197495\n",
      "Epoch 152, Batch 28/32 : Loss = 0.0013260317500680685\n",
      "Epoch 152, Batch 29/32 : Loss = 0.016928642988204956\n",
      "Epoch 152, Batch 30/32 : Loss = 0.0003517563745845109\n",
      "Epoch 152, Batch 31/32 : Loss = 0.0007269526831805706\n",
      "Epoch 152 finished in 0.052497891585032146 minutes\n",
      "Epoch 152 training_loss = 0.0027446462772786617\n",
      "-----J---;;--q-----++---//----z---yy----U--------%%----U------1----x------ => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "-----k-----$----I---F-----D------e-----h-----]---k----0-----\\----X-------- => k$IFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----C-----DD----E-----g----mm-----\"\"---m------F----<<-----Q-----8---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----WW------=-----2---++----E----11---n----TT---XX---r---C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 152 val_loss = 0.776729941368103, word_accuracy = 0.68\n",
      "Epoch 153, Batch 0/32 : Loss = 0.00045687612146139145\n",
      "Epoch 153, Batch 1/32 : Loss = 0.0004896789905615151\n",
      "Epoch 153, Batch 2/32 : Loss = 0.0010441679041832685\n",
      "Epoch 153, Batch 3/32 : Loss = 0.0004459461197257042\n",
      "Epoch 153, Batch 4/32 : Loss = 0.0004040610510855913\n",
      "Epoch 153, Batch 5/32 : Loss = 0.00044899433851242065\n",
      "Epoch 153, Batch 6/32 : Loss = 0.0036426703445613384\n",
      "Epoch 153, Batch 7/32 : Loss = 0.0005662000039592385\n",
      "Epoch 153, Batch 8/32 : Loss = 0.0005321837961673737\n",
      "Epoch 153, Batch 9/32 : Loss = 0.0004814883868675679\n",
      "Epoch 153, Batch 10/32 : Loss = 0.0006400564452633262\n",
      "Epoch 153, Batch 11/32 : Loss = 0.012646862305700779\n",
      "Epoch 153, Batch 12/32 : Loss = 0.0007129889563657343\n",
      "Epoch 153, Batch 13/32 : Loss = 0.000497305765748024\n",
      "Epoch 153, Batch 14/32 : Loss = 0.0008461115648970008\n",
      "Epoch 153, Batch 15/32 : Loss = 0.0006345303263515234\n",
      "Epoch 153, Batch 16/32 : Loss = 0.0007917018956504762\n",
      "Epoch 153, Batch 17/32 : Loss = 0.00038365888758562505\n",
      "Epoch 153, Batch 18/32 : Loss = 0.0004564744886010885\n",
      "Epoch 153, Batch 19/32 : Loss = 0.00031245563877746463\n",
      "Epoch 153, Batch 20/32 : Loss = 0.0005295890150591731\n",
      "Epoch 153, Batch 21/32 : Loss = 0.00036430140607990324\n",
      "Epoch 153, Batch 22/32 : Loss = 0.002633580006659031\n",
      "Epoch 153, Batch 23/32 : Loss = 0.0007191518670879304\n",
      "Epoch 153, Batch 24/32 : Loss = 0.0007873166468925774\n",
      "Epoch 153, Batch 25/32 : Loss = 0.0005694418796338141\n",
      "Epoch 153, Batch 26/32 : Loss = 0.0006604020018130541\n",
      "Epoch 153, Batch 27/32 : Loss = 0.0010975089389830828\n",
      "Epoch 153, Batch 28/32 : Loss = 0.010310801677405834\n",
      "Epoch 153, Batch 29/32 : Loss = 0.00047508347779512405\n",
      "Epoch 153, Batch 30/32 : Loss = 0.0006687025306746364\n",
      "Epoch 153, Batch 31/32 : Loss = 0.0003043649485334754\n",
      "Epoch 153 finished in 0.0533128301302592 minutes\n",
      "Epoch 153 training_loss = 0.0014550468185916543\n",
      "----/----MM----oo---EE---^---33---x---/---&----66---XX------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "----5----9----g----mm------J----u---x---c----x--.--d----\\--- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "----##---GG----9---E---I-=---hh---5---#---22---J--))-kk----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----Y-----W-----jj-ii-\\---|----MM-----<-----MM----88---88--- => YWji\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 153 val_loss = 0.7728641629219055, word_accuracy = 0.7\n",
      "Epoch 154, Batch 0/32 : Loss = 0.0006697887438349426\n",
      "Epoch 154, Batch 1/32 : Loss = 0.0003362455463502556\n",
      "Epoch 154, Batch 2/32 : Loss = 0.007091961335390806\n",
      "Epoch 154, Batch 3/32 : Loss = 0.0003862729645334184\n",
      "Epoch 154, Batch 4/32 : Loss = 0.0003599250630941242\n",
      "Epoch 154, Batch 5/32 : Loss = 0.0007299830904230475\n",
      "Epoch 154, Batch 6/32 : Loss = 0.002156592207029462\n",
      "Epoch 154, Batch 7/32 : Loss = 0.00031509873224422336\n",
      "Epoch 154, Batch 8/32 : Loss = 0.0009113561827689409\n",
      "Epoch 154, Batch 9/32 : Loss = 0.004754943307489157\n",
      "Epoch 154, Batch 10/32 : Loss = 0.0006729359156452119\n",
      "Epoch 154, Batch 11/32 : Loss = 0.0006007293122820556\n",
      "Epoch 154, Batch 12/32 : Loss = 0.0005084932199679315\n",
      "Epoch 154, Batch 13/32 : Loss = 0.0008392903255298734\n",
      "Epoch 154, Batch 14/32 : Loss = 0.0005476593505591154\n",
      "Epoch 154, Batch 15/32 : Loss = 0.0013092660810798407\n",
      "Epoch 154, Batch 16/32 : Loss = 0.0009380282717756927\n",
      "Epoch 154, Batch 17/32 : Loss = 0.003730765078216791\n",
      "Epoch 154, Batch 18/32 : Loss = 0.014445413835346699\n",
      "Epoch 154, Batch 19/32 : Loss = 0.00041570450412109494\n",
      "Epoch 154, Batch 20/32 : Loss = 0.000383829785278067\n",
      "Epoch 154, Batch 21/32 : Loss = 0.0005958204274065793\n",
      "Epoch 154, Batch 22/32 : Loss = 0.0006194926099851727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154, Batch 23/32 : Loss = 0.00037488891393877566\n",
      "Epoch 154, Batch 24/32 : Loss = 0.00036528625059872866\n",
      "Epoch 154, Batch 25/32 : Loss = 0.0005933791981078684\n",
      "Epoch 154, Batch 26/32 : Loss = 0.0004803108749911189\n",
      "Epoch 154, Batch 27/32 : Loss = 0.0005203550681471825\n",
      "Epoch 154, Batch 28/32 : Loss = 0.0002938786637969315\n",
      "Epoch 154, Batch 29/32 : Loss = 0.005819388665258884\n",
      "Epoch 154, Batch 30/32 : Loss = 0.000997941242530942\n",
      "Epoch 154, Batch 31/32 : Loss = 0.00037619960494339466\n",
      "Epoch 154 finished in 0.05188175837198893 minutes\n",
      "Epoch 154 training_loss = 0.0016967725241556764\n",
      "----=-----0---[[--xx---)---R----88---ii--P-----w-----)-- => =0[x)R8iPw), Ground Truth is Err:509\n",
      "-----k---$$--l--F----DD---e---hh---]--k---0---\\---X----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "-----<-----v---O-----T---`---4----V---[--0--------Q----- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----o---\"---}--!--B----r---&----9---;-`---O----ww----}-- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 154 val_loss = 0.7963922023773193, word_accuracy = 0.69\n",
      "Epoch 155, Batch 0/32 : Loss = 0.0003738905943464488\n",
      "Epoch 155, Batch 1/32 : Loss = 0.0004460148047655821\n",
      "Epoch 155, Batch 2/32 : Loss = 0.0006592673598788679\n",
      "Epoch 155, Batch 3/32 : Loss = 0.00037706311559304595\n",
      "Epoch 155, Batch 4/32 : Loss = 0.0007350781233981252\n",
      "Epoch 155, Batch 5/32 : Loss = 0.0004650272603612393\n",
      "Epoch 155, Batch 6/32 : Loss = 0.0002699427423067391\n",
      "Epoch 155, Batch 7/32 : Loss = 0.0005479859537445009\n",
      "Epoch 155, Batch 8/32 : Loss = 0.000942522136028856\n",
      "Epoch 155, Batch 9/32 : Loss = 0.0012963396729901433\n",
      "Epoch 155, Batch 10/32 : Loss = 0.0003230218426324427\n",
      "Epoch 155, Batch 11/32 : Loss = 0.00035853718873113394\n",
      "Epoch 155, Batch 12/32 : Loss = 0.0004223948926664889\n",
      "Epoch 155, Batch 13/32 : Loss = 0.0003599451738409698\n",
      "Epoch 155, Batch 14/32 : Loss = 0.002585788955911994\n",
      "Epoch 155, Batch 15/32 : Loss = 0.0002634085831232369\n",
      "Epoch 155, Batch 16/32 : Loss = 0.0003066262579523027\n",
      "Epoch 155, Batch 17/32 : Loss = 0.0015997681766748428\n",
      "Epoch 155, Batch 18/32 : Loss = 0.0005068294703960419\n",
      "Epoch 155, Batch 19/32 : Loss = 0.00036591378739103675\n",
      "Epoch 155, Batch 20/32 : Loss = 0.0006747613078914583\n",
      "Epoch 155, Batch 21/32 : Loss = 0.0004328198847360909\n",
      "Epoch 155, Batch 22/32 : Loss = 0.0003015677211806178\n",
      "Epoch 155, Batch 23/32 : Loss = 0.0003257366188336164\n",
      "Epoch 155, Batch 24/32 : Loss = 0.00032041218946687877\n",
      "Epoch 155, Batch 25/32 : Loss = 0.0003269941662438214\n",
      "Epoch 155, Batch 26/32 : Loss = 0.0005916719092056155\n",
      "Epoch 155, Batch 27/32 : Loss = 0.0015317208599299192\n",
      "Epoch 155, Batch 28/32 : Loss = 0.004453758243471384\n",
      "Epoch 155, Batch 29/32 : Loss = 0.0005190827650949359\n",
      "Epoch 155, Batch 30/32 : Loss = 0.000562613713555038\n",
      "Epoch 155, Batch 31/32 : Loss = 0.009497764520347118\n",
      "Epoch 155 finished in 0.05406746069590251 minutes\n",
      "Epoch 155 training_loss = 0.0007850191905163229\n",
      "---XX---7---0---j--@-----S---Z---L---4----C---mm-----MM------ => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "--BB---..--Y----I--WW------66----F---hh----XX--'--Y-----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "------Q------3----g----I--z---#-----Y---::-]]--q----++---**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----kk---$---|---'-,--9---Y----NN----W------m------T---8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "Epoch 155 val_loss = 0.7694835662841797, word_accuracy = 0.67\n",
      "Epoch 156, Batch 0/32 : Loss = 0.0009829557966440916\n",
      "Epoch 156, Batch 1/32 : Loss = 0.00029775977600365877\n",
      "Epoch 156, Batch 2/32 : Loss = 0.0021253805607557297\n",
      "Epoch 156, Batch 3/32 : Loss = 0.0004346538335084915\n",
      "Epoch 156, Batch 4/32 : Loss = 0.00029358104802668095\n",
      "Epoch 156, Batch 5/32 : Loss = 0.003418762469664216\n",
      "Epoch 156, Batch 6/32 : Loss = 0.0002470017643645406\n",
      "Epoch 156, Batch 7/32 : Loss = 0.0004853683349210769\n",
      "Epoch 156, Batch 8/32 : Loss = 0.0004609436437021941\n",
      "Epoch 156, Batch 9/32 : Loss = 0.0010059420019388199\n",
      "Epoch 156, Batch 10/32 : Loss = 0.00033882015850394964\n",
      "Epoch 156, Batch 11/32 : Loss = 0.0005061581614427269\n",
      "Epoch 156, Batch 12/32 : Loss = 0.0006349883624352515\n",
      "Epoch 156, Batch 13/32 : Loss = 0.0003059088485315442\n",
      "Epoch 156, Batch 14/32 : Loss = 0.0006232969462871552\n",
      "Epoch 156, Batch 15/32 : Loss = 0.0009104625787585974\n",
      "Epoch 156, Batch 16/32 : Loss = 0.0005991195794194937\n",
      "Epoch 156, Batch 17/32 : Loss = 0.0012022838927805424\n",
      "Epoch 156, Batch 18/32 : Loss = 0.0007611548062413931\n",
      "Epoch 156, Batch 19/32 : Loss = 0.0009000045247375965\n",
      "Epoch 156, Batch 20/32 : Loss = 0.00038722134195268154\n",
      "Epoch 156, Batch 21/32 : Loss = 0.002157689770683646\n",
      "Epoch 156, Batch 22/32 : Loss = 0.0005883318372070789\n",
      "Epoch 156, Batch 23/32 : Loss = 0.0005867343861609697\n",
      "Epoch 156, Batch 24/32 : Loss = 0.000542126945219934\n",
      "Epoch 156, Batch 25/32 : Loss = 0.0006538352463394403\n",
      "Epoch 156, Batch 26/32 : Loss = 0.0003838519915007055\n",
      "Epoch 156, Batch 27/32 : Loss = 0.0006342253764159977\n",
      "Epoch 156, Batch 28/32 : Loss = 0.000934846349991858\n",
      "Epoch 156, Batch 29/32 : Loss = 0.00040541731868870556\n",
      "Epoch 156, Batch 30/32 : Loss = 0.00032355435541830957\n",
      "Epoch 156, Batch 31/32 : Loss = 0.0011672073742374778\n",
      "Epoch 156 finished in 0.05310855706532796 minutes\n",
      "Epoch 156 training_loss = 0.0007800251478329301\n",
      "----55---->----:--*----Y----AA---'--OO-----DD----*---#-----O-----g------ => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----0-----c------++-----bb-----I--\"-----b------6----...---Q------------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----\"----]---t---4-----e-----^----WW--------Q------4----->------g------- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "----W------=-----2----+----E----11---n----TT---XX---r---C-----a---I----- => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 156 val_loss = 0.7990737557411194, word_accuracy = 0.72\n",
      "Epoch 157, Batch 0/32 : Loss = 0.0004454381996765733\n",
      "Epoch 157, Batch 1/32 : Loss = 0.0016298264963552356\n",
      "Epoch 157, Batch 2/32 : Loss = 0.00034141322248615324\n",
      "Epoch 157, Batch 3/32 : Loss = 0.003484311979264021\n",
      "Epoch 157, Batch 4/32 : Loss = 0.00030455499654635787\n",
      "Epoch 157, Batch 5/32 : Loss = 0.0006188104161992669\n",
      "Epoch 157, Batch 6/32 : Loss = 0.000988163985311985\n",
      "Epoch 157, Batch 7/32 : Loss = 0.0012669911375269294\n",
      "Epoch 157, Batch 8/32 : Loss = 0.0005767556140199304\n",
      "Epoch 157, Batch 9/32 : Loss = 0.0022818332072347403\n",
      "Epoch 157, Batch 10/32 : Loss = 0.00035619846312329173\n",
      "Epoch 157, Batch 11/32 : Loss = 0.00029908795841038227\n",
      "Epoch 157, Batch 12/32 : Loss = 0.0004724453901872039\n",
      "Epoch 157, Batch 13/32 : Loss = 0.0004608985618688166\n",
      "Epoch 157, Batch 14/32 : Loss = 0.00035046390257775784\n",
      "Epoch 157, Batch 15/32 : Loss = 0.05390913411974907\n",
      "Epoch 157, Batch 16/32 : Loss = 0.0005266460939310491\n",
      "Epoch 157, Batch 17/32 : Loss = 0.026440199464559555\n",
      "Epoch 157, Batch 18/32 : Loss = 0.0004972298629581928\n",
      "Epoch 157, Batch 19/32 : Loss = 0.00034989116829819977\n",
      "Epoch 157, Batch 20/32 : Loss = 0.00018282729433849454\n",
      "Epoch 157, Batch 21/32 : Loss = 0.0002663849154487252\n",
      "Epoch 157, Batch 22/32 : Loss = 0.0003611086867749691\n",
      "Epoch 157, Batch 23/32 : Loss = 0.0015476396074518561\n",
      "Epoch 157, Batch 24/32 : Loss = 0.00044895795872434974\n",
      "Epoch 157, Batch 25/32 : Loss = 0.00043193745659664273\n",
      "Epoch 157, Batch 26/32 : Loss = 0.0006345120491459966\n",
      "Epoch 157, Batch 27/32 : Loss = 0.00097722711507231\n",
      "Epoch 157, Batch 28/32 : Loss = 0.00673078466206789\n",
      "Epoch 157, Batch 29/32 : Loss = 0.00033305250690318644\n",
      "Epoch 157, Batch 30/32 : Loss = 0.000683063524775207\n",
      "Epoch 157, Batch 31/32 : Loss = 0.09207643568515778\n",
      "Epoch 157 finished in 0.053221563498179116 minutes\n",
      "Epoch 157 training_loss = 0.0038460195064544678\n",
      "----2---p----:-mm-----XX--a----z---n----@-----C----yy-----%------%--- => 2p:mXazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "--.--CC----O-------w-----uu---`---uu---.--RR----{{---3----\"---#------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----QQ------3-----g-----I---z----#-----Y----::--]---q------++---***- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----BB----.---Y----|---W--------6-----F----hh----XX---\"---Y-----2---- => B.Y|W6FhX\"Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "Epoch 157 val_loss = 0.7800849676132202, word_accuracy = 0.7\n",
      "Epoch 158, Batch 0/32 : Loss = 0.00043016401468776166\n",
      "Epoch 158, Batch 1/32 : Loss = 0.005443607922643423\n",
      "Epoch 158, Batch 2/32 : Loss = 0.0007777782157063484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158, Batch 3/32 : Loss = 0.0004093291936442256\n",
      "Epoch 158, Batch 4/32 : Loss = 0.00048611414968036115\n",
      "Epoch 158, Batch 5/32 : Loss = 0.00033585046185180545\n",
      "Epoch 158, Batch 6/32 : Loss = 0.00048437059740535915\n",
      "Epoch 158, Batch 7/32 : Loss = 0.00026865131803788245\n",
      "Epoch 158, Batch 8/32 : Loss = 0.003968545235693455\n",
      "Epoch 158, Batch 9/32 : Loss = 0.007809109520167112\n",
      "Epoch 158, Batch 10/32 : Loss = 0.0005694706924259663\n",
      "Epoch 158, Batch 11/32 : Loss = 0.026554705575108528\n",
      "Epoch 158, Batch 12/32 : Loss = 0.0007335966802202165\n",
      "Epoch 158, Batch 13/32 : Loss = 0.0005786098772659898\n",
      "Epoch 158, Batch 14/32 : Loss = 0.000626520486548543\n",
      "Epoch 158, Batch 15/32 : Loss = 0.0004782593750860542\n",
      "Epoch 158, Batch 16/32 : Loss = 0.0008184959879145026\n",
      "Epoch 158, Batch 17/32 : Loss = 0.0008974276715889573\n",
      "Epoch 158, Batch 18/32 : Loss = 0.00047276762779802084\n",
      "Epoch 158, Batch 19/32 : Loss = 0.0007590919267386198\n",
      "Epoch 158, Batch 20/32 : Loss = 0.008328277617692947\n",
      "Epoch 158, Batch 21/32 : Loss = 0.001194630516692996\n",
      "Epoch 158, Batch 22/32 : Loss = 0.005844722036272287\n",
      "Epoch 158, Batch 23/32 : Loss = 0.0004084043321199715\n",
      "Epoch 158, Batch 24/32 : Loss = 0.011206249706447124\n",
      "Epoch 158, Batch 25/32 : Loss = 0.0009097075671888888\n",
      "Epoch 158, Batch 26/32 : Loss = 0.0007123291725292802\n",
      "Epoch 158, Batch 27/32 : Loss = 0.0005278181633912027\n",
      "Epoch 158, Batch 28/32 : Loss = 0.0013704383745789528\n",
      "Epoch 158, Batch 29/32 : Loss = 0.0035889982245862484\n",
      "Epoch 158, Batch 30/32 : Loss = 0.000642813858576119\n",
      "Epoch 158, Batch 31/32 : Loss = 0.0067664808593690395\n",
      "Epoch 158 finished in 0.05245551268259684 minutes\n",
      "Epoch 158 training_loss = 0.0028428160585463047\n",
      "----<-----v----O------TT---`---4-----v----[--00--------Q------- => <vOT`4v[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "----d----!!---NN----r---A----j--**---$----3----hh----5----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-----0-----c-----++----b----II-\"\"---bb-----6---...--Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "---J--;---q----++--//---z--yy---U-------%%---U-----1---x---_--- => J;q+/zyU%U1x_, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 158 val_loss = 0.8200210332870483, word_accuracy = 0.73\n",
      "Epoch 159, Batch 0/32 : Loss = 0.00043653699685819447\n",
      "Epoch 159, Batch 1/32 : Loss = 0.0013815530110150576\n",
      "Epoch 159, Batch 2/32 : Loss = 0.0012117051519453526\n",
      "Epoch 159, Batch 3/32 : Loss = 0.0006604385562241077\n",
      "Epoch 159, Batch 4/32 : Loss = 0.000661853002384305\n",
      "Epoch 159, Batch 5/32 : Loss = 0.0008772738510742784\n",
      "Epoch 159, Batch 6/32 : Loss = 0.0006896689301356673\n",
      "Epoch 159, Batch 7/32 : Loss = 0.0015422593569383025\n",
      "Epoch 159, Batch 8/32 : Loss = 0.0006472966633737087\n",
      "Epoch 159, Batch 9/32 : Loss = 0.0005031578475609422\n",
      "Epoch 159, Batch 10/32 : Loss = 0.0018687332049012184\n",
      "Epoch 159, Batch 11/32 : Loss = 0.0004682685830630362\n",
      "Epoch 159, Batch 12/32 : Loss = 0.0004393106501083821\n",
      "Epoch 159, Batch 13/32 : Loss = 0.0006876394618302584\n",
      "Epoch 159, Batch 14/32 : Loss = 0.0005069741746410728\n",
      "Epoch 159, Batch 15/32 : Loss = 0.0006684241816401482\n",
      "Epoch 159, Batch 16/32 : Loss = 0.008069420233368874\n",
      "Epoch 159, Batch 17/32 : Loss = 0.02025369182229042\n",
      "Epoch 159, Batch 18/32 : Loss = 0.0027744208928197622\n",
      "Epoch 159, Batch 19/32 : Loss = 0.0002954062365461141\n",
      "Epoch 159, Batch 20/32 : Loss = 0.0006187836988829076\n",
      "Epoch 159, Batch 21/32 : Loss = 0.0027763445395976305\n",
      "Epoch 159, Batch 22/32 : Loss = 0.0014455299824476242\n",
      "Epoch 159, Batch 23/32 : Loss = 0.0006782190757803619\n",
      "Epoch 159, Batch 24/32 : Loss = 0.00046407783520407975\n",
      "Epoch 159, Batch 25/32 : Loss = 0.0008548138430342078\n",
      "Epoch 159, Batch 26/32 : Loss = 0.0017788921250030398\n",
      "Epoch 159, Batch 27/32 : Loss = 0.0002774476888589561\n",
      "Epoch 159, Batch 28/32 : Loss = 0.0010831791441887617\n",
      "Epoch 159, Batch 29/32 : Loss = 0.0004138959338888526\n",
      "Epoch 159, Batch 30/32 : Loss = 0.001216441043652594\n",
      "Epoch 159, Batch 31/32 : Loss = 0.1306847482919693\n",
      "Epoch 159 finished in 0.05142480532328288 minutes\n",
      "Epoch 159 training_loss = 0.0023321204353123903\n",
      "--4----r--{------%---/--`-))--w-----&-----NN---++---P------ => 4r{%/`)w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---++---:--z---77---8----d----S-----v--5----S----J----B---- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "----k----$---l--F----D----e----h----]--k---0----\\---X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "----Y----WW-----]--i--\\--|-----MM----<-----MM----8----8---- => YW]i\\|M<M88, Ground Truth is YW]i\\|M<M88\n",
      "Epoch 159 val_loss = 0.7798103094100952, word_accuracy = 0.71\n",
      "Epoch 160, Batch 0/32 : Loss = 0.000848044641315937\n",
      "Epoch 160, Batch 1/32 : Loss = 0.000599790015257895\n",
      "Epoch 160, Batch 2/32 : Loss = 0.0009437052067369223\n",
      "Epoch 160, Batch 3/32 : Loss = 0.001388384262099862\n",
      "Epoch 160, Batch 4/32 : Loss = 0.004515314009040594\n",
      "Epoch 160, Batch 5/32 : Loss = 0.0010161965619772673\n",
      "Epoch 160, Batch 6/32 : Loss = 0.0007122044917196035\n",
      "Epoch 160, Batch 7/32 : Loss = 0.0004078451602254063\n",
      "Epoch 160, Batch 8/32 : Loss = 0.0006863790331408381\n",
      "Epoch 160, Batch 9/32 : Loss = 0.0020762195345014334\n",
      "Epoch 160, Batch 10/32 : Loss = 0.0004936227342113853\n",
      "Epoch 160, Batch 11/32 : Loss = 0.0015546143986284733\n",
      "Epoch 160, Batch 12/32 : Loss = 0.0018255625618621707\n",
      "Epoch 160, Batch 13/32 : Loss = 0.001536836614832282\n",
      "Epoch 160, Batch 14/32 : Loss = 0.0005300202174112201\n",
      "Epoch 160, Batch 15/32 : Loss = 0.005326789803802967\n",
      "Epoch 160, Batch 16/32 : Loss = 0.000447324535343796\n",
      "Epoch 160, Batch 17/32 : Loss = 0.0005539208650588989\n",
      "Epoch 160, Batch 18/32 : Loss = 0.0015336808282881975\n",
      "Epoch 160, Batch 19/32 : Loss = 0.0003031534142792225\n",
      "Epoch 160, Batch 20/32 : Loss = 0.0008976132376119494\n",
      "Epoch 160, Batch 21/32 : Loss = 0.0004041437350679189\n",
      "Epoch 160, Batch 22/32 : Loss = 0.00045163679169490933\n",
      "Epoch 160, Batch 23/32 : Loss = 0.0006861257134005427\n",
      "Epoch 160, Batch 24/32 : Loss = 0.0003114218998234719\n",
      "Epoch 160, Batch 25/32 : Loss = 0.0005199979059398174\n",
      "Epoch 160, Batch 26/32 : Loss = 0.021247096359729767\n",
      "Epoch 160, Batch 27/32 : Loss = 0.00038655108073726296\n",
      "Epoch 160, Batch 28/32 : Loss = 0.0008766188402660191\n",
      "Epoch 160, Batch 29/32 : Loss = 0.0006759482785128057\n",
      "Epoch 160, Batch 30/32 : Loss = 0.0008125286549329758\n",
      "Epoch 160, Batch 31/32 : Loss = 0.0028634921181946993\n",
      "Epoch 160 finished in 0.05217341184616089 minutes\n",
      "Epoch 160 training_loss = 0.0017647300846874714\n",
      "---0----c---++----b----I-\"---bb---66--..--QQ-------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "--8----KK---ll--Z----5----p----$$---a----}---w-----. => 8KlZ5p$a}w., Ground Truth is 8KIZ5p$a}w,\n",
      "----#----0---[[--x---)---R----8---ii-PP----w----))-. => #0[x)R8iPw)., Ground Truth is Err:509\n",
      "---o---\"--}-!!-BB---r--&&---9---;``--O----w----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 160 val_loss = 0.7977766394615173, word_accuracy = 0.69\n",
      "Epoch 161, Batch 0/32 : Loss = 0.0003866689221467823\n",
      "Epoch 161, Batch 1/32 : Loss = 0.0003874386311508715\n",
      "Epoch 161, Batch 2/32 : Loss = 0.002202917356044054\n",
      "Epoch 161, Batch 3/32 : Loss = 0.0008826553239487112\n",
      "Epoch 161, Batch 4/32 : Loss = 0.004226620774716139\n",
      "Epoch 161, Batch 5/32 : Loss = 0.0008284694631583989\n",
      "Epoch 161, Batch 6/32 : Loss = 0.000531764526385814\n",
      "Epoch 161, Batch 7/32 : Loss = 0.0005531786591745913\n",
      "Epoch 161, Batch 8/32 : Loss = 0.0006460372824221849\n",
      "Epoch 161, Batch 9/32 : Loss = 0.0007639974355697632\n",
      "Epoch 161, Batch 10/32 : Loss = 0.003118495922535658\n",
      "Epoch 161, Batch 11/32 : Loss = 0.0005564461462199688\n",
      "Epoch 161, Batch 12/32 : Loss = 0.0005139169516041875\n",
      "Epoch 161, Batch 13/32 : Loss = 0.0008034806814976037\n",
      "Epoch 161, Batch 14/32 : Loss = 0.015169027261435986\n",
      "Epoch 161, Batch 15/32 : Loss = 0.004614049568772316\n",
      "Epoch 161, Batch 16/32 : Loss = 0.0003994544385932386\n",
      "Epoch 161, Batch 17/32 : Loss = 0.0006965993670746684\n",
      "Epoch 161, Batch 18/32 : Loss = 0.0005783239030279219\n",
      "Epoch 161, Batch 19/32 : Loss = 0.0004075917531736195\n",
      "Epoch 161, Batch 20/32 : Loss = 0.00037530550616793334\n",
      "Epoch 161, Batch 21/32 : Loss = 0.0011096862144768238\n",
      "Epoch 161, Batch 22/32 : Loss = 0.0002913399366661906\n",
      "Epoch 161, Batch 23/32 : Loss = 0.006906062830239534\n",
      "Epoch 161, Batch 24/32 : Loss = 0.0005942920106463134\n",
      "Epoch 161, Batch 25/32 : Loss = 0.015462508425116539\n",
      "Epoch 161, Batch 26/32 : Loss = 0.0004568144213408232\n",
      "Epoch 161, Batch 27/32 : Loss = 0.00097783119417727\n",
      "Epoch 161, Batch 28/32 : Loss = 0.0009428504854440689\n",
      "Epoch 161, Batch 29/32 : Loss = 0.0011970270425081253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161, Batch 30/32 : Loss = 0.001006175298243761\n",
      "Epoch 161, Batch 31/32 : Loss = 0.15051378309726715\n",
      "Epoch 161 finished in 0.051929386456807454 minutes\n",
      "Epoch 161 training_loss = 0.002775943838059902\n",
      "--.---c----O-------w-----uu---``--uu---.--RR----{{---3----\"---#------ => .cOwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----#-------0-----[---x----))---R------8-----i---P------w------)---- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "----#-----G-----9----EE---I--=----hh----5----#----2-----J---)---k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----dd----!!---NN-----r---AA----j--**----$-----3----hh----55----nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 161 val_loss = 0.7717953324317932, word_accuracy = 0.73\n",
      "Epoch 162, Batch 0/32 : Loss = 0.0012595313601195812\n",
      "Epoch 162, Batch 1/32 : Loss = 0.001518119010142982\n",
      "Epoch 162, Batch 2/32 : Loss = 0.0032834457233548164\n",
      "Epoch 162, Batch 3/32 : Loss = 0.002087367232888937\n",
      "Epoch 162, Batch 4/32 : Loss = 0.02477899193763733\n",
      "Epoch 162, Batch 5/32 : Loss = 0.0005009592860005796\n",
      "Epoch 162, Batch 6/32 : Loss = 0.0008564873714931309\n",
      "Epoch 162, Batch 7/32 : Loss = 0.0008563619339838624\n",
      "Epoch 162, Batch 8/32 : Loss = 0.0004715191316790879\n",
      "Epoch 162, Batch 9/32 : Loss = 0.0012259908253327012\n",
      "Epoch 162, Batch 10/32 : Loss = 0.0015251459553837776\n",
      "Epoch 162, Batch 11/32 : Loss = 0.006447886116802692\n",
      "Epoch 162, Batch 12/32 : Loss = 0.006587238982319832\n",
      "Epoch 162, Batch 13/32 : Loss = 0.0004006470553576946\n",
      "Epoch 162, Batch 14/32 : Loss = 0.001348040415905416\n",
      "Epoch 162, Batch 15/32 : Loss = 0.001269343658350408\n",
      "Epoch 162, Batch 16/32 : Loss = 0.0007986656855791807\n",
      "Epoch 162, Batch 17/32 : Loss = 0.00047222879948094487\n",
      "Epoch 162, Batch 18/32 : Loss = 0.0009744130657054484\n",
      "Epoch 162, Batch 19/32 : Loss = 0.0004281096626073122\n",
      "Epoch 162, Batch 20/32 : Loss = 0.0004747349885292351\n",
      "Epoch 162, Batch 21/32 : Loss = 0.000899224542081356\n",
      "Epoch 162, Batch 22/32 : Loss = 0.0005386064876802266\n",
      "Epoch 162, Batch 23/32 : Loss = 0.0026751672849059105\n",
      "Epoch 162, Batch 24/32 : Loss = 0.00040838035056367517\n",
      "Epoch 162, Batch 25/32 : Loss = 0.00045552453957498074\n",
      "Epoch 162, Batch 26/32 : Loss = 0.0013557010097429156\n",
      "Epoch 162, Batch 27/32 : Loss = 0.0007798594888299704\n",
      "Epoch 162, Batch 28/32 : Loss = 0.0005639709997922182\n",
      "Epoch 162, Batch 29/32 : Loss = 0.0008816321496851742\n",
      "Epoch 162, Batch 30/32 : Loss = 0.0005772530566900969\n",
      "Epoch 162, Batch 31/32 : Loss = 0.0018400114495307207\n",
      "Epoch 162 finished in 0.05229535897572835 minutes\n",
      "Epoch 162 training_loss = 0.0021503788884729147\n",
      "----k----O-----/--,,-y---c---**---1---PP---}--##----B---- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "--22--p---:-mm----x--a---z--n----@----c---yy---%%----%%-- => 2p:mxazn@cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "-----Q-----3----gg---I--z---#----YY--::-]]--q----++---**- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "----5----9---gg----m------J---uu---x---c---x--.--d----\\-- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 162 val_loss = 0.850233793258667, word_accuracy = 0.69\n",
      "Epoch 163, Batch 0/32 : Loss = 0.0010017520980909467\n",
      "Epoch 163, Batch 1/32 : Loss = 0.0016298661939799786\n",
      "Epoch 163, Batch 2/32 : Loss = 0.0019805491901934147\n",
      "Epoch 163, Batch 3/32 : Loss = 0.010671840980648994\n",
      "Epoch 163, Batch 4/32 : Loss = 0.0002568752970546484\n",
      "Epoch 163, Batch 5/32 : Loss = 0.00037508399691432714\n",
      "Epoch 163, Batch 6/32 : Loss = 0.00031960723572410643\n",
      "Epoch 163, Batch 7/32 : Loss = 0.0005627394421026111\n",
      "Epoch 163, Batch 8/32 : Loss = 0.00043704299605451524\n",
      "Epoch 163, Batch 9/32 : Loss = 0.0038837732281535864\n",
      "Epoch 163, Batch 10/32 : Loss = 0.0008129252819344401\n",
      "Epoch 163, Batch 11/32 : Loss = 0.0011020178208127618\n",
      "Epoch 163, Batch 12/32 : Loss = 0.00047024470404721797\n",
      "Epoch 163, Batch 13/32 : Loss = 0.0004369557718746364\n",
      "Epoch 163, Batch 14/32 : Loss = 0.00048505509039387107\n",
      "Epoch 163, Batch 15/32 : Loss = 0.0007008474203757942\n",
      "Epoch 163, Batch 16/32 : Loss = 0.0012565194629132748\n",
      "Epoch 163, Batch 17/32 : Loss = 0.0007243469590321183\n",
      "Epoch 163, Batch 18/32 : Loss = 0.0003113693674094975\n",
      "Epoch 163, Batch 19/32 : Loss = 0.0003593942674342543\n",
      "Epoch 163, Batch 20/32 : Loss = 0.0009013032540678978\n",
      "Epoch 163, Batch 21/32 : Loss = 0.0005208319053053856\n",
      "Epoch 163, Batch 22/32 : Loss = 0.00041931914165616035\n",
      "Epoch 163, Batch 23/32 : Loss = 0.04438304901123047\n",
      "Epoch 163, Batch 24/32 : Loss = 0.0004340507148299366\n",
      "Epoch 163, Batch 25/32 : Loss = 0.0011827039998024702\n",
      "Epoch 163, Batch 26/32 : Loss = 0.0005711319390684366\n",
      "Epoch 163, Batch 27/32 : Loss = 0.0014579545240849257\n",
      "Epoch 163, Batch 28/32 : Loss = 0.0004581292159855366\n",
      "Epoch 163, Batch 29/32 : Loss = 0.0005576698458753526\n",
      "Epoch 163, Batch 30/32 : Loss = 0.000868728500790894\n",
      "Epoch 163, Batch 31/32 : Loss = 0.0004937721532769501\n",
      "Epoch 163 finished in 0.05311418374379476 minutes\n",
      "Epoch 163 training_loss = 0.0025572816375643015\n",
      "-----kk---$$---||---!,,--9----YY-----NN----W-------m-------TT---8----- => k$|!,9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----CC----DD----E----g----mm-----\"---m------F---<<----Q-----8---22---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----E-------0-----[---x-----)---RR------8----ii--PP------w------)---- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "----55---->----:--*---YY----A----'--O-----DD----*---#-----O-----g----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 163 val_loss = 0.7918751835823059, word_accuracy = 0.71\n",
      "Epoch 164, Batch 0/32 : Loss = 0.0005539729027077556\n",
      "Epoch 164, Batch 1/32 : Loss = 0.0004251589998602867\n",
      "Epoch 164, Batch 2/32 : Loss = 0.00033603693009354174\n",
      "Epoch 164, Batch 3/32 : Loss = 0.0010525170946493745\n",
      "Epoch 164, Batch 4/32 : Loss = 0.0004786192439496517\n",
      "Epoch 164, Batch 5/32 : Loss = 0.0006995875155553222\n",
      "Epoch 164, Batch 6/32 : Loss = 0.0005701287882402539\n",
      "Epoch 164, Batch 7/32 : Loss = 0.000561279768589884\n",
      "Epoch 164, Batch 8/32 : Loss = 0.0021852299105376005\n",
      "Epoch 164, Batch 9/32 : Loss = 0.0005712378770112991\n",
      "Epoch 164, Batch 10/32 : Loss = 0.0004630515759345144\n",
      "Epoch 164, Batch 11/32 : Loss = 0.0006388752372004092\n",
      "Epoch 164, Batch 12/32 : Loss = 0.007380077615380287\n",
      "Epoch 164, Batch 13/32 : Loss = 0.0006143874488770962\n",
      "Epoch 164, Batch 14/32 : Loss = 0.0003088753146585077\n",
      "Epoch 164, Batch 15/32 : Loss = 0.0009412227082066238\n",
      "Epoch 164, Batch 16/32 : Loss = 0.00043460339657031\n",
      "Epoch 164, Batch 17/32 : Loss = 0.00028103444492444396\n",
      "Epoch 164, Batch 18/32 : Loss = 0.00042125966865569353\n",
      "Epoch 164, Batch 19/32 : Loss = 0.0004400282050482929\n",
      "Epoch 164, Batch 20/32 : Loss = 0.00027747915009967983\n",
      "Epoch 164, Batch 21/32 : Loss = 0.0023462260141968727\n",
      "Epoch 164, Batch 22/32 : Loss = 0.0011145791504532099\n",
      "Epoch 164, Batch 23/32 : Loss = 0.0007292503723874688\n",
      "Epoch 164, Batch 24/32 : Loss = 0.0008186378399841487\n",
      "Epoch 164, Batch 25/32 : Loss = 0.0005318737821653485\n",
      "Epoch 164, Batch 26/32 : Loss = 0.0010072679724544287\n",
      "Epoch 164, Batch 27/32 : Loss = 0.0008272496634162962\n",
      "Epoch 164, Batch 28/32 : Loss = 0.0016117100603878498\n",
      "Epoch 164, Batch 29/32 : Loss = 0.00025866174837574363\n",
      "Epoch 164, Batch 30/32 : Loss = 0.0003222795785404742\n",
      "Epoch 164, Batch 31/32 : Loss = 0.008441738784313202\n",
      "Epoch 164 finished in 0.05270253419876099 minutes\n",
      "Epoch 164 training_loss = 0.0009721321985125542\n",
      "----{---BB----Y-----R----aa----y---hh---#----22---->-----E----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----o----\"---}---!--BB-----r---&------9---;--``--O------w-----}}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "--{---BB----YY----RR----a----yy---h----#----22---->-----E----4------ => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----<<------v---O-------TT---``--4------V---[[---0---------Q-------- => <vOT`4V[0-Q, Ground Truth is <vOT`4V[0-Q\n",
      "Epoch 164 val_loss = 0.754962682723999, word_accuracy = 0.73\n",
      "Epoch 165, Batch 0/32 : Loss = 0.0022566267289221287\n",
      "Epoch 165, Batch 1/32 : Loss = 0.0003099180175922811\n",
      "Epoch 165, Batch 2/32 : Loss = 0.00027436367236077785\n",
      "Epoch 165, Batch 3/32 : Loss = 0.0006411191425286233\n",
      "Epoch 165, Batch 4/32 : Loss = 0.0005338694900274277\n",
      "Epoch 165, Batch 5/32 : Loss = 0.0004662352439481765\n",
      "Epoch 165, Batch 6/32 : Loss = 0.00039206736255437136\n",
      "Epoch 165, Batch 7/32 : Loss = 0.00023031454475130886\n",
      "Epoch 165, Batch 8/32 : Loss = 0.00032557485974393785\n",
      "Epoch 165, Batch 9/32 : Loss = 0.0007952112937346101\n",
      "Epoch 165, Batch 10/32 : Loss = 0.00034204224357381463\n",
      "Epoch 165, Batch 11/32 : Loss = 0.014542707242071629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, Batch 12/32 : Loss = 0.0002958941040560603\n",
      "Epoch 165, Batch 13/32 : Loss = 0.0005160541040822864\n",
      "Epoch 165, Batch 14/32 : Loss = 0.0007330967346206307\n",
      "Epoch 165, Batch 15/32 : Loss = 0.0005197571590542793\n",
      "Epoch 165, Batch 16/32 : Loss = 0.0007569657173007727\n",
      "Epoch 165, Batch 17/32 : Loss = 0.0002706369268707931\n",
      "Epoch 165, Batch 18/32 : Loss = 0.0014227383071556687\n",
      "Epoch 165, Batch 19/32 : Loss = 0.0006498456350527704\n",
      "Epoch 165, Batch 20/32 : Loss = 0.00025185843696817756\n",
      "Epoch 165, Batch 21/32 : Loss = 0.00044560845708474517\n",
      "Epoch 165, Batch 22/32 : Loss = 0.0013796307612210512\n",
      "Epoch 165, Batch 23/32 : Loss = 0.0003071631654165685\n",
      "Epoch 165, Batch 24/32 : Loss = 0.0002795573673211038\n",
      "Epoch 165, Batch 25/32 : Loss = 0.0005798792699351907\n",
      "Epoch 165, Batch 26/32 : Loss = 0.0002825988922268152\n",
      "Epoch 165, Batch 27/32 : Loss = 0.00038279936416074634\n",
      "Epoch 165, Batch 28/32 : Loss = 0.0002801011432893574\n",
      "Epoch 165, Batch 29/32 : Loss = 0.0013226899318397045\n",
      "Epoch 165, Batch 30/32 : Loss = 0.00042375506018288434\n",
      "Epoch 165, Batch 31/32 : Loss = 0.0007054427987895906\n",
      "Epoch 165 finished in 0.05241729815800985 minutes\n",
      "Epoch 165 training_loss = 0.0010377143044024706\n",
      "----E-------0-----[[---x----))---RR-----88----ii--PP------w------))-- => E0[x)R8iPw), Ground Truth is Err:509\n",
      "----k----OO-----//--,,--yy---c----**----1----PP----}}---#-----BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "------d-----:---X----99----ee----aa----F---,--88--------VV----RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "--.---C----O-------w-----uu---``--uu---.--RR----{{---3----\"---#------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "Epoch 165 val_loss = 0.8488190174102783, word_accuracy = 0.7\n",
      "Epoch 166, Batch 0/32 : Loss = 0.0010560075752437115\n",
      "Epoch 166, Batch 1/32 : Loss = 0.0011026222491636872\n",
      "Epoch 166, Batch 2/32 : Loss = 0.0003854690003208816\n",
      "Epoch 166, Batch 3/32 : Loss = 0.0005844166735187173\n",
      "Epoch 166, Batch 4/32 : Loss = 0.002113545313477516\n",
      "Epoch 166, Batch 5/32 : Loss = 0.0004858636821154505\n",
      "Epoch 166, Batch 6/32 : Loss = 0.0003680989029817283\n",
      "Epoch 166, Batch 7/32 : Loss = 0.0002476689114701003\n",
      "Epoch 166, Batch 8/32 : Loss = 0.0008386113913729787\n",
      "Epoch 166, Batch 9/32 : Loss = 0.0007842384511604905\n",
      "Epoch 166, Batch 10/32 : Loss = 0.00042522954754531384\n",
      "Epoch 166, Batch 11/32 : Loss = 0.0029757320880889893\n",
      "Epoch 166, Batch 12/32 : Loss = 0.000335248711053282\n",
      "Epoch 166, Batch 13/32 : Loss = 0.0006227010162547231\n",
      "Epoch 166, Batch 14/32 : Loss = 0.0003621404175646603\n",
      "Epoch 166, Batch 15/32 : Loss = 0.0004921049112454057\n",
      "Epoch 166, Batch 16/32 : Loss = 0.00022392484243027866\n",
      "Epoch 166, Batch 17/32 : Loss = 0.00027902284637093544\n",
      "Epoch 166, Batch 18/32 : Loss = 0.0003122385824099183\n",
      "Epoch 166, Batch 19/32 : Loss = 0.004795815795660019\n",
      "Epoch 166, Batch 20/32 : Loss = 0.0005260125617496669\n",
      "Epoch 166, Batch 21/32 : Loss = 0.0002535036765038967\n",
      "Epoch 166, Batch 22/32 : Loss = 0.0002883669803850353\n",
      "Epoch 166, Batch 23/32 : Loss = 0.00048698578029870987\n",
      "Epoch 166, Batch 24/32 : Loss = 0.0008384224493056536\n",
      "Epoch 166, Batch 25/32 : Loss = 0.00031915365252643824\n",
      "Epoch 166, Batch 26/32 : Loss = 0.0002044314460363239\n",
      "Epoch 166, Batch 27/32 : Loss = 0.000538068707101047\n",
      "Epoch 166, Batch 28/32 : Loss = 0.0002712929272092879\n",
      "Epoch 166, Batch 29/32 : Loss = 0.01222149282693863\n",
      "Epoch 166, Batch 30/32 : Loss = 0.00029845302924513817\n",
      "Epoch 166, Batch 31/32 : Loss = 0.0023722045589238405\n",
      "Epoch 166 finished in 0.05215137799580892 minutes\n",
      "Epoch 166 training_loss = 0.0011352099245414138\n",
      "----c-----R-----;---9-----yy-----?----2-----d-----i---OO------{--!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----/-----MM------o-----E----^^---33----x----/----&-----6-----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "--.---C----O-------w-----uu---``--uu---.--RR----{{---3----\"---#------ => .COwu`u.R{3\"#, Ground Truth is .cOwu`u.R{3\"#\n",
      "-----dd----!----N-----r----A----j--**----$-----3----hh----55----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 166 val_loss = 0.7378251552581787, word_accuracy = 0.79\n",
      "Epoch 167, Batch 0/32 : Loss = 0.0017081685364246368\n",
      "Epoch 167, Batch 1/32 : Loss = 0.00017627401393838227\n",
      "Epoch 167, Batch 2/32 : Loss = 0.0003438691492192447\n",
      "Epoch 167, Batch 3/32 : Loss = 0.00019320749561302364\n",
      "Epoch 167, Batch 4/32 : Loss = 0.0005105215823277831\n",
      "Epoch 167, Batch 5/32 : Loss = 0.0002637003781273961\n",
      "Epoch 167, Batch 6/32 : Loss = 0.0003859818971250206\n",
      "Epoch 167, Batch 7/32 : Loss = 0.0008734269649721682\n",
      "Epoch 167, Batch 8/32 : Loss = 0.00040836003609001637\n",
      "Epoch 167, Batch 9/32 : Loss = 0.0002584221074357629\n",
      "Epoch 167, Batch 10/32 : Loss = 0.0004656419041566551\n",
      "Epoch 167, Batch 11/32 : Loss = 0.0006319946842268109\n",
      "Epoch 167, Batch 12/32 : Loss = 0.00027689291164278984\n",
      "Epoch 167, Batch 13/32 : Loss = 0.0006984583451412618\n",
      "Epoch 167, Batch 14/32 : Loss = 0.00034103519283235073\n",
      "Epoch 167, Batch 15/32 : Loss = 0.0002798193017952144\n",
      "Epoch 167, Batch 16/32 : Loss = 0.010442306287586689\n",
      "Epoch 167, Batch 17/32 : Loss = 0.0021022120490670204\n",
      "Epoch 167, Batch 18/32 : Loss = 0.0035941232927143574\n",
      "Epoch 167, Batch 19/32 : Loss = 0.00032604788430035114\n",
      "Epoch 167, Batch 20/32 : Loss = 0.0003095492138527334\n",
      "Epoch 167, Batch 21/32 : Loss = 0.00039676460437476635\n",
      "Epoch 167, Batch 22/32 : Loss = 0.0003621715004555881\n",
      "Epoch 167, Batch 23/32 : Loss = 0.00017095491057261825\n",
      "Epoch 167, Batch 24/32 : Loss = 0.0005782829830422997\n",
      "Epoch 167, Batch 25/32 : Loss = 0.010076232254505157\n",
      "Epoch 167, Batch 26/32 : Loss = 0.0006455816328525543\n",
      "Epoch 167, Batch 27/32 : Loss = 0.0006145319784991443\n",
      "Epoch 167, Batch 28/32 : Loss = 0.0005028777522966266\n",
      "Epoch 167, Batch 29/32 : Loss = 0.0009649284183979034\n",
      "Epoch 167, Batch 30/32 : Loss = 0.019300350919365883\n",
      "Epoch 167, Batch 31/32 : Loss = 0.04533576965332031\n",
      "Epoch 167 finished in 0.052299869060516355 minutes\n",
      "Epoch 167 training_loss = 0.0020520370453596115\n",
      "----BB---.---Y----I--WW------66----F---hh----XX---'--Y----22---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "------z----0----\"\"----G-----//---~~----$$----cc----11---j--t---- => z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "-----cc----R----;;--9-----y----?----2----d-----i--OO-----{--!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---C-----D----E----g----m-----\"--mm-----F---<<----Q----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "Epoch 167 val_loss = 0.7932917475700378, word_accuracy = 0.74\n",
      "Epoch 168, Batch 0/32 : Loss = 0.00015964257181622088\n",
      "Epoch 168, Batch 1/32 : Loss = 0.0004998797667212784\n",
      "Epoch 168, Batch 2/32 : Loss = 0.0009184633381664753\n",
      "Epoch 168, Batch 3/32 : Loss = 0.00030198917374946177\n",
      "Epoch 168, Batch 4/32 : Loss = 0.0003115289728157222\n",
      "Epoch 168, Batch 5/32 : Loss = 0.008641543798148632\n",
      "Epoch 168, Batch 6/32 : Loss = 0.0008494240464642644\n",
      "Epoch 168, Batch 7/32 : Loss = 0.0006321141263470054\n",
      "Epoch 168, Batch 8/32 : Loss = 0.00027239383780397475\n",
      "Epoch 168, Batch 9/32 : Loss = 0.0005955096567049623\n",
      "Epoch 168, Batch 10/32 : Loss = 0.0003942148759961128\n",
      "Epoch 168, Batch 11/32 : Loss = 0.0007386304205283523\n",
      "Epoch 168, Batch 12/32 : Loss = 0.001344707328826189\n",
      "Epoch 168, Batch 13/32 : Loss = 0.028523974120616913\n",
      "Epoch 168, Batch 14/32 : Loss = 0.00039211788680404425\n",
      "Epoch 168, Batch 15/32 : Loss = 0.004235290922224522\n",
      "Epoch 168, Batch 16/32 : Loss = 0.00034293567296117544\n",
      "Epoch 168, Batch 17/32 : Loss = 0.0002807913115248084\n",
      "Epoch 168, Batch 18/32 : Loss = 0.0003282721736468375\n",
      "Epoch 168, Batch 19/32 : Loss = 0.0003238189092371613\n",
      "Epoch 168, Batch 20/32 : Loss = 0.00044364194036461413\n",
      "Epoch 168, Batch 21/32 : Loss = 0.0003450645017437637\n",
      "Epoch 168, Batch 22/32 : Loss = 0.0004433346912264824\n",
      "Epoch 168, Batch 23/32 : Loss = 0.0003379995469003916\n",
      "Epoch 168, Batch 24/32 : Loss = 0.001268776599317789\n",
      "Epoch 168, Batch 25/32 : Loss = 0.0007808607770130038\n",
      "Epoch 168, Batch 26/32 : Loss = 0.0004113067116122693\n",
      "Epoch 168, Batch 27/32 : Loss = 0.005964283831417561\n",
      "Epoch 168, Batch 28/32 : Loss = 0.0011548798065632582\n",
      "Epoch 168, Batch 29/32 : Loss = 0.0016762242885306478\n",
      "Epoch 168, Batch 30/32 : Loss = 0.0004124197585042566\n",
      "Epoch 168, Batch 31/32 : Loss = 0.00025973652373068035\n",
      "Epoch 168 finished in 0.05254585345586141 minutes\n",
      "Epoch 168 training_loss = 0.002035614335909486\n",
      "----0----Q---66---<----<---(---T----N---5---=---PP--((-mm----- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----##----G----9----E---I-=----h---55--#----2----J---)--k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "---BB---..--Y----I--WW------6----FF---hh----X------Y----22---- => B.YIW6FhXY2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----c----RR---;---9----yy---?----2----d----i---O-----{{-!!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "Epoch 168 val_loss = 0.8302872180938721, word_accuracy = 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, Batch 0/32 : Loss = 0.0006978517048992217\n",
      "Epoch 169, Batch 1/32 : Loss = 0.0008411677554249763\n",
      "Epoch 169, Batch 2/32 : Loss = 0.0004125296254642308\n",
      "Epoch 169, Batch 3/32 : Loss = 0.0011002692626789212\n",
      "Epoch 169, Batch 4/32 : Loss = 0.0004530335427261889\n",
      "Epoch 169, Batch 5/32 : Loss = 0.0004796657303813845\n",
      "Epoch 169, Batch 6/32 : Loss = 0.0005970096681267023\n",
      "Epoch 169, Batch 7/32 : Loss = 0.00026262717437930405\n",
      "Epoch 169, Batch 8/32 : Loss = 0.0012535362038761377\n",
      "Epoch 169, Batch 9/32 : Loss = 0.001000422053039074\n",
      "Epoch 169, Batch 10/32 : Loss = 0.02727523259818554\n",
      "Epoch 169, Batch 11/32 : Loss = 0.0003717788204085082\n",
      "Epoch 169, Batch 12/32 : Loss = 0.0015516842249780893\n",
      "Epoch 169, Batch 13/32 : Loss = 0.0004475688620004803\n",
      "Epoch 169, Batch 14/32 : Loss = 0.00021448740153573453\n",
      "Epoch 169, Batch 15/32 : Loss = 0.00036759552313014865\n",
      "Epoch 169, Batch 16/32 : Loss = 0.0005604848847724497\n",
      "Epoch 169, Batch 17/32 : Loss = 0.0011219200678169727\n",
      "Epoch 169, Batch 18/32 : Loss = 0.000521689304150641\n",
      "Epoch 169, Batch 19/32 : Loss = 0.00040976976742967963\n",
      "Epoch 169, Batch 20/32 : Loss = 0.001701737754046917\n",
      "Epoch 169, Batch 21/32 : Loss = 0.001089887460693717\n",
      "Epoch 169, Batch 22/32 : Loss = 0.0006424537277780473\n",
      "Epoch 169, Batch 23/32 : Loss = 0.01994295045733452\n",
      "Epoch 169, Batch 24/32 : Loss = 0.000890045368578285\n",
      "Epoch 169, Batch 25/32 : Loss = 0.0003385817108210176\n",
      "Epoch 169, Batch 26/32 : Loss = 0.00033014133805409074\n",
      "Epoch 169, Batch 27/32 : Loss = 0.025045888498425484\n",
      "Epoch 169, Batch 28/32 : Loss = 0.0005932737840339541\n",
      "Epoch 169, Batch 29/32 : Loss = 0.000810752622783184\n",
      "Epoch 169, Batch 30/32 : Loss = 0.0005250825779512525\n",
      "Epoch 169, Batch 31/32 : Loss = 0.00020469006267376244\n",
      "Epoch 169 finished in 0.05276921987533569 minutes\n",
      "Epoch 169 training_loss = 0.0029518618248403072\n",
      "----Z----0----\"\"---GG-----/---~~----$$----c----1---jj--t---- => Z0\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "----##---GG----9---E---I-=---hh---5---#---22---J--))--k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "----kk--$$---|---!,--99---Y----NN----W-----mm------T---8---- => k$|!,9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----55---9----g----mm-----JJ---u---x---c----x--.--d---\\\\---- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "Epoch 169 val_loss = 0.8383614420890808, word_accuracy = 0.69\n",
      "Epoch 170, Batch 0/32 : Loss = 0.0005382697563618422\n",
      "Epoch 170, Batch 1/32 : Loss = 0.0019392616814002395\n",
      "Epoch 170, Batch 2/32 : Loss = 0.0003134080325253308\n",
      "Epoch 170, Batch 3/32 : Loss = 0.0013110761065036058\n",
      "Epoch 170, Batch 4/32 : Loss = 0.0005867988220416009\n",
      "Epoch 170, Batch 5/32 : Loss = 0.0005051073967479169\n",
      "Epoch 170, Batch 6/32 : Loss = 0.0009464340982958674\n",
      "Epoch 170, Batch 7/32 : Loss = 0.0005377769120968878\n",
      "Epoch 170, Batch 8/32 : Loss = 0.0003458767314441502\n",
      "Epoch 170, Batch 9/32 : Loss = 0.0016690047923475504\n",
      "Epoch 170, Batch 10/32 : Loss = 0.0006771591142751276\n",
      "Epoch 170, Batch 11/32 : Loss = 0.015376903116703033\n",
      "Epoch 170, Batch 12/32 : Loss = 0.000585346482694149\n",
      "Epoch 170, Batch 13/32 : Loss = 0.0004664194129873067\n",
      "Epoch 170, Batch 14/32 : Loss = 0.001370843849144876\n",
      "Epoch 170, Batch 15/32 : Loss = 0.0010138690704479814\n",
      "Epoch 170, Batch 16/32 : Loss = 0.0004226270248182118\n",
      "Epoch 170, Batch 17/32 : Loss = 0.04752371087670326\n",
      "Epoch 170, Batch 18/32 : Loss = 0.0007542177918367088\n",
      "Epoch 170, Batch 19/32 : Loss = 0.0009469464421272278\n",
      "Epoch 170, Batch 20/32 : Loss = 0.00045032595517113805\n",
      "Epoch 170, Batch 21/32 : Loss = 0.0009130340185947716\n",
      "Epoch 170, Batch 22/32 : Loss = 0.009820559993386269\n",
      "Epoch 170, Batch 23/32 : Loss = 0.0008595302351750433\n",
      "Epoch 170, Batch 24/32 : Loss = 0.0006349214818328619\n",
      "Epoch 170, Batch 25/32 : Loss = 0.0006460239528678358\n",
      "Epoch 170, Batch 26/32 : Loss = 0.32680919766426086\n",
      "Epoch 170, Batch 27/32 : Loss = 0.0003277042997069657\n",
      "Epoch 170, Batch 28/32 : Loss = 0.0005337194306775928\n",
      "Epoch 170, Batch 29/32 : Loss = 0.0008978672558441758\n",
      "Epoch 170, Batch 30/32 : Loss = 0.000749226426705718\n",
      "Epoch 170, Batch 31/32 : Loss = 0.0013159532099962234\n",
      "Epoch 170 finished in 0.14196697870890299 minutes\n",
      "Epoch 170 training_loss = 0.013514462858438492\n",
      "-----0------JJ----!!--(---;;---A-------3----,,--'--))----r---rr----77----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "-----WW------=-----2---++---EE----1----n----TT---XX---r---C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "-----o----\"\"---}---!!--BB-----rr---&------9----;--`----O------ww-----}---- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----//-----MM------o-----EE----^^---33----x----/----&-----66-----X-------- => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "Epoch 170 val_loss = 0.799905002117157, word_accuracy = 0.73\n",
      "Epoch 171, Batch 0/32 : Loss = 0.000617599580436945\n",
      "Epoch 171, Batch 1/32 : Loss = 0.0007793828845024109\n",
      "Epoch 171, Batch 2/32 : Loss = 0.0004574685590341687\n",
      "Epoch 171, Batch 3/32 : Loss = 0.0006584303919225931\n",
      "Epoch 171, Batch 4/32 : Loss = 0.09137634187936783\n",
      "Epoch 171, Batch 5/32 : Loss = 0.00044839963084086776\n",
      "Epoch 171, Batch 6/32 : Loss = 0.0005634733242914081\n",
      "Epoch 171, Batch 7/32 : Loss = 0.0005486732115969062\n",
      "Epoch 171, Batch 8/32 : Loss = 0.002052011899650097\n",
      "Epoch 171, Batch 9/32 : Loss = 0.002008567564189434\n",
      "Epoch 171, Batch 10/32 : Loss = 0.009964114055037498\n",
      "Epoch 171, Batch 11/32 : Loss = 0.0008618574356660247\n",
      "Epoch 171, Batch 12/32 : Loss = 0.0006623187800869346\n",
      "Epoch 171, Batch 13/32 : Loss = 0.008704107254743576\n",
      "Epoch 171, Batch 14/32 : Loss = 0.0008870572783052921\n",
      "Epoch 171, Batch 15/32 : Loss = 0.0007165708229877055\n",
      "Epoch 171, Batch 16/32 : Loss = 0.0021949433721601963\n",
      "Epoch 171, Batch 17/32 : Loss = 0.001867969403974712\n",
      "Epoch 171, Batch 18/32 : Loss = 0.0008392054587602615\n",
      "Epoch 171, Batch 19/32 : Loss = 0.0007875714218243957\n",
      "Epoch 171, Batch 20/32 : Loss = 0.0056839874014258385\n",
      "Epoch 171, Batch 21/32 : Loss = 0.008162540383636951\n",
      "Epoch 171, Batch 22/32 : Loss = 0.0007980114314705133\n",
      "Epoch 171, Batch 23/32 : Loss = 0.02025056630373001\n",
      "Epoch 171, Batch 24/32 : Loss = 0.0018574476707726717\n",
      "Epoch 171, Batch 25/32 : Loss = 0.0010013404535129666\n",
      "Epoch 171, Batch 26/32 : Loss = 0.000545204384252429\n",
      "Epoch 171, Batch 27/32 : Loss = 0.021385129541158676\n",
      "Epoch 171, Batch 28/32 : Loss = 0.0007445518858730793\n",
      "Epoch 171, Batch 29/32 : Loss = 0.0004658870166167617\n",
      "Epoch 171, Batch 30/32 : Loss = 0.0028989440761506557\n",
      "Epoch 171, Batch 31/32 : Loss = 0.0009914515540003777\n",
      "Epoch 171 finished in 0.05242828528086344 minutes\n",
      "Epoch 171 training_loss = 0.006133770104497671\n",
      "-----k----$----|---'-,--9----Y------N----WW------mm------T----8----- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---kk---$$---|---'-,---9----Y-----NN----WW------mm------T----8------ => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "-----55----9----gg----mm-------J----u----x---C----xx--.--d-----\\---- => 59gmJuxCx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "----{----B----Y-----RR----a----y---hh---#-----2---->-----E----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "Epoch 171 val_loss = 0.7404919862747192, word_accuracy = 0.74\n",
      "Epoch 172, Batch 0/32 : Loss = 0.0007882156642153859\n",
      "Epoch 172, Batch 1/32 : Loss = 0.004460575524717569\n",
      "Epoch 172, Batch 2/32 : Loss = 0.0005804439424537122\n",
      "Epoch 172, Batch 3/32 : Loss = 0.0005377193447202444\n",
      "Epoch 172, Batch 4/32 : Loss = 0.0008362496737390757\n",
      "Epoch 172, Batch 5/32 : Loss = 0.0003060904855374247\n",
      "Epoch 172, Batch 6/32 : Loss = 0.0007335153059102595\n",
      "Epoch 172, Batch 7/32 : Loss = 0.0008257867302745581\n",
      "Epoch 172, Batch 8/32 : Loss = 0.0010177840013056993\n",
      "Epoch 172, Batch 9/32 : Loss = 0.002897456055507064\n",
      "Epoch 172, Batch 10/32 : Loss = 0.0006942019099369645\n",
      "Epoch 172, Batch 11/32 : Loss = 0.0008377900812774897\n",
      "Epoch 172, Batch 12/32 : Loss = 0.00030325312400236726\n",
      "Epoch 172, Batch 13/32 : Loss = 0.003078630194067955\n",
      "Epoch 172, Batch 14/32 : Loss = 0.0015593146672472358\n",
      "Epoch 172, Batch 15/32 : Loss = 0.0008728820248506963\n",
      "Epoch 172, Batch 16/32 : Loss = 0.010346341878175735\n",
      "Epoch 172, Batch 17/32 : Loss = 0.0244993194937706\n",
      "Epoch 172, Batch 18/32 : Loss = 0.0004185236757621169\n",
      "Epoch 172, Batch 19/32 : Loss = 0.0010383476037532091\n",
      "Epoch 172, Batch 20/32 : Loss = 0.0005111981881782413\n",
      "Epoch 172, Batch 21/32 : Loss = 0.0004450055421330035\n",
      "Epoch 172, Batch 22/32 : Loss = 0.0006995308212935925\n",
      "Epoch 172, Batch 23/32 : Loss = 0.0010446897940710187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172, Batch 24/32 : Loss = 0.014686680398881435\n",
      "Epoch 172, Batch 25/32 : Loss = 0.0038242207374423742\n",
      "Epoch 172, Batch 26/32 : Loss = 0.0006900635198689997\n",
      "Epoch 172, Batch 27/32 : Loss = 0.0005377395427785814\n",
      "Epoch 172, Batch 28/32 : Loss = 0.014653840102255344\n",
      "Epoch 172, Batch 29/32 : Loss = 0.0004763442848343402\n",
      "Epoch 172, Batch 30/32 : Loss = 0.0002727123210206628\n",
      "Epoch 172, Batch 31/32 : Loss = 0.000492782040964812\n",
      "Epoch 172 finished in 0.053646894296010335 minutes\n",
      "Epoch 172 training_loss = 0.003037303453311324\n",
      "-----WW------=-----2----+----E----1----n----TT---XX---r--CC----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "---rr---GG------II---T-----##------3-----PP------Q--------w------'-ii----- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "---{{---BB-----Y-----RR----a-----yy---h----#------2---->------EE----4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----z----OO-----\"\"----G--------//---~~------$------c------11---jj---t----- => zO\"G/~$c1jt, Ground Truth is z0\"G/~$c1jt\n",
      "Epoch 172 val_loss = 0.7234750986099243, word_accuracy = 0.76\n",
      "Epoch 173, Batch 0/32 : Loss = 0.0008203625329770148\n",
      "Epoch 173, Batch 1/32 : Loss = 0.008354747667908669\n",
      "Epoch 173, Batch 2/32 : Loss = 0.00045514130033552647\n",
      "Epoch 173, Batch 3/32 : Loss = 0.00088074104860425\n",
      "Epoch 173, Batch 4/32 : Loss = 0.0061869234777987\n",
      "Epoch 173, Batch 5/32 : Loss = 0.0004755968984682113\n",
      "Epoch 173, Batch 6/32 : Loss = 0.0004923056112602353\n",
      "Epoch 173, Batch 7/32 : Loss = 0.0007230389164760709\n",
      "Epoch 173, Batch 8/32 : Loss = 0.0017651774687692523\n",
      "Epoch 173, Batch 9/32 : Loss = 0.0006966927903704345\n",
      "Epoch 173, Batch 10/32 : Loss = 0.0008176480769179761\n",
      "Epoch 173, Batch 11/32 : Loss = 0.003839905373752117\n",
      "Epoch 173, Batch 12/32 : Loss = 0.02815059944987297\n",
      "Epoch 173, Batch 13/32 : Loss = 0.043981682509183884\n",
      "Epoch 173, Batch 14/32 : Loss = 0.0005052234046161175\n",
      "Epoch 173, Batch 15/32 : Loss = 0.0011295113945379853\n",
      "Epoch 173, Batch 16/32 : Loss = 0.041227322071790695\n",
      "Epoch 173, Batch 17/32 : Loss = 0.0005369012942537665\n",
      "Epoch 173, Batch 18/32 : Loss = 0.0005521591519936919\n",
      "Epoch 173, Batch 19/32 : Loss = 0.0005612937966361642\n",
      "Epoch 173, Batch 20/32 : Loss = 0.00040337088285014033\n",
      "Epoch 173, Batch 21/32 : Loss = 0.0004765364865306765\n",
      "Epoch 173, Batch 22/32 : Loss = 0.002519697416573763\n",
      "Epoch 173, Batch 23/32 : Loss = 0.0005457872757688165\n",
      "Epoch 173, Batch 24/32 : Loss = 0.001256472896784544\n",
      "Epoch 173, Batch 25/32 : Loss = 0.0005481333937495947\n",
      "Epoch 173, Batch 26/32 : Loss = 0.00043790123891085386\n",
      "Epoch 173, Batch 27/32 : Loss = 0.0007976146880537271\n",
      "Epoch 173, Batch 28/32 : Loss = 0.01123073324561119\n",
      "Epoch 173, Batch 29/32 : Loss = 0.0008846014970913529\n",
      "Epoch 173, Batch 30/32 : Loss = 0.00039048478356562555\n",
      "Epoch 173, Batch 31/32 : Loss = 0.000351349706761539\n",
      "Epoch 173 finished in 0.05285458167394002 minutes\n",
      "Epoch 173 training_loss = 0.0051948027685284615\n",
      "---J--;---q----+---/----z--yy---U-------%----U-----1---x---_)-- => J;q+/zyU%U1x_), Ground Truth is J;q+/zyU%U1x_\n",
      "----o----\"---}--!!--BB----r---&----99---;--`--O------w-----}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "--{---BB---YY----R----a----y---h---##----2--->>----EE---4------ => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----4----r---{-----%%---/--'--)---w-----&-----NN----+---PP----- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "Epoch 173 val_loss = 0.7674946188926697, word_accuracy = 0.72\n",
      "Epoch 174, Batch 0/32 : Loss = 0.0006818043766543269\n",
      "Epoch 174, Batch 1/32 : Loss = 0.00028872195980511606\n",
      "Epoch 174, Batch 2/32 : Loss = 0.000683116028085351\n",
      "Epoch 174, Batch 3/32 : Loss = 0.002150225918740034\n",
      "Epoch 174, Batch 4/32 : Loss = 0.0003075163112953305\n",
      "Epoch 174, Batch 5/32 : Loss = 0.00041468729614280164\n",
      "Epoch 174, Batch 6/32 : Loss = 0.0007595793576911092\n",
      "Epoch 174, Batch 7/32 : Loss = 0.0006297495565377176\n",
      "Epoch 174, Batch 8/32 : Loss = 0.0008751389104872942\n",
      "Epoch 174, Batch 9/32 : Loss = 0.001096636289730668\n",
      "Epoch 174, Batch 10/32 : Loss = 0.0005766678950749338\n",
      "Epoch 174, Batch 11/32 : Loss = 0.006043253932148218\n",
      "Epoch 174, Batch 12/32 : Loss = 0.000388996209949255\n",
      "Epoch 174, Batch 13/32 : Loss = 0.0010984553955495358\n",
      "Epoch 174, Batch 14/32 : Loss = 0.0005520631093531847\n",
      "Epoch 174, Batch 15/32 : Loss = 0.0016960334032773972\n",
      "Epoch 174, Batch 16/32 : Loss = 0.0004247198812663555\n",
      "Epoch 174, Batch 17/32 : Loss = 0.0010115935001522303\n",
      "Epoch 174, Batch 18/32 : Loss = 0.0011174901155754924\n",
      "Epoch 174, Batch 19/32 : Loss = 0.010377917438745499\n",
      "Epoch 174, Batch 20/32 : Loss = 0.0006459238356910646\n",
      "Epoch 174, Batch 21/32 : Loss = 0.000521139067132026\n",
      "Epoch 174, Batch 22/32 : Loss = 0.0005361192743293941\n",
      "Epoch 174, Batch 23/32 : Loss = 0.0019479554612189531\n",
      "Epoch 174, Batch 24/32 : Loss = 0.0011052046902477741\n",
      "Epoch 174, Batch 25/32 : Loss = 0.0014335834421217442\n",
      "Epoch 174, Batch 26/32 : Loss = 0.00036698312032967806\n",
      "Epoch 174, Batch 27/32 : Loss = 0.0003488575457595289\n",
      "Epoch 174, Batch 28/32 : Loss = 0.000647819135338068\n",
      "Epoch 174, Batch 29/32 : Loss = 0.007436751388013363\n",
      "Epoch 174, Batch 30/32 : Loss = 0.001698802807368338\n",
      "Epoch 174, Batch 31/32 : Loss = 0.0006433741073124111\n",
      "Epoch 174 finished in 0.05264123280843099 minutes\n",
      "Epoch 174 training_loss = 0.0015403671422973275\n",
      "---r---GG-----I---T----##-----3-----P-----Q-------w----''-i----- => rGIT#3PQw'i, Ground Truth is rGIT#3PQw'i\n",
      "---kk--$$---||--'-,--9----Y-----NN----W------m------T----8------ => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "---7---n----DD----P----n---tt--w----dd---\\\\--Q-----a----RR------ => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "-----c-----R----;--9-----y----?----2----d----i---O-----{{--!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "Epoch 174 val_loss = 0.7633792757987976, word_accuracy = 0.73\n",
      "Epoch 175, Batch 0/32 : Loss = 0.0008801856311038136\n",
      "Epoch 175, Batch 1/32 : Loss = 0.0004346867208369076\n",
      "Epoch 175, Batch 2/32 : Loss = 0.00048375024925917387\n",
      "Epoch 175, Batch 3/32 : Loss = 0.0013285784516483545\n",
      "Epoch 175, Batch 4/32 : Loss = 0.0005993482191115618\n",
      "Epoch 175, Batch 5/32 : Loss = 0.0009327006991952658\n",
      "Epoch 175, Batch 6/32 : Loss = 0.0004288686905056238\n",
      "Epoch 175, Batch 7/32 : Loss = 0.0022105940151959658\n",
      "Epoch 175, Batch 8/32 : Loss = 0.0007749231299385428\n",
      "Epoch 175, Batch 9/32 : Loss = 0.0015705988043919206\n",
      "Epoch 175, Batch 10/32 : Loss = 0.0007406399818137288\n",
      "Epoch 175, Batch 11/32 : Loss = 0.0010214776266366243\n",
      "Epoch 175, Batch 12/32 : Loss = 0.0004653724026866257\n",
      "Epoch 175, Batch 13/32 : Loss = 0.0019987246487289667\n",
      "Epoch 175, Batch 14/32 : Loss = 0.0004160291573498398\n",
      "Epoch 175, Batch 15/32 : Loss = 0.0008020069217309356\n",
      "Epoch 175, Batch 16/32 : Loss = 0.0004526283300947398\n",
      "Epoch 175, Batch 17/32 : Loss = 0.0005574168171733618\n",
      "Epoch 175, Batch 18/32 : Loss = 0.003079055342823267\n",
      "Epoch 175, Batch 19/32 : Loss = 0.0005278947064653039\n",
      "Epoch 175, Batch 20/32 : Loss = 0.0015882088337093592\n",
      "Epoch 175, Batch 21/32 : Loss = 0.0010169979650527239\n",
      "Epoch 175, Batch 22/32 : Loss = 0.0006374961230903864\n",
      "Epoch 175, Batch 23/32 : Loss = 0.0004541599191725254\n",
      "Epoch 175, Batch 24/32 : Loss = 0.0004491236468311399\n",
      "Epoch 175, Batch 25/32 : Loss = 0.006999110337346792\n",
      "Epoch 175, Batch 26/32 : Loss = 0.0004391095426399261\n",
      "Epoch 175, Batch 27/32 : Loss = 0.0003933351254090667\n",
      "Epoch 175, Batch 28/32 : Loss = 0.0007128164870664477\n",
      "Epoch 175, Batch 29/32 : Loss = 0.00027453515212982893\n",
      "Epoch 175, Batch 30/32 : Loss = 0.0007007458480075002\n",
      "Epoch 175, Batch 31/32 : Loss = 0.0006394540541805327\n",
      "Epoch 175 finished in 0.053831291198730466 minutes\n",
      "Epoch 175 training_loss = 0.0010747325140982866\n",
      "----k---$$----|---'-,,--9----YY-----NN----WW------mm-------T----8------- => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "----55---->----:--*----Y----AA---'--O-------D----*---#----OO-----g------ => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----W------==----2----+----E----11---n----TT---XX---r---C-----a---I----- => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "----00---Q-----66---<<----<<---(----T----NN---55---=----P---(--mm------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 175 val_loss = 0.7601022720336914, word_accuracy = 0.72\n",
      "Epoch 176, Batch 0/32 : Loss = 0.0005046970909461379\n",
      "Epoch 176, Batch 1/32 : Loss = 0.008967364206910133\n",
      "Epoch 176, Batch 2/32 : Loss = 0.0007827548542991281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176, Batch 3/32 : Loss = 0.0017156843096017838\n",
      "Epoch 176, Batch 4/32 : Loss = 0.0016055575106292963\n",
      "Epoch 176, Batch 5/32 : Loss = 0.0005990487989038229\n",
      "Epoch 176, Batch 6/32 : Loss = 0.0005147986812517047\n",
      "Epoch 176, Batch 7/32 : Loss = 0.00045620324090123177\n",
      "Epoch 176, Batch 8/32 : Loss = 0.0004027794348075986\n",
      "Epoch 176, Batch 9/32 : Loss = 0.0011961553245782852\n",
      "Epoch 176, Batch 10/32 : Loss = 0.00041896331822499633\n",
      "Epoch 176, Batch 11/32 : Loss = 0.0006813654908910394\n",
      "Epoch 176, Batch 12/32 : Loss = 0.000536861945874989\n",
      "Epoch 176, Batch 13/32 : Loss = 0.000310168310534209\n",
      "Epoch 176, Batch 14/32 : Loss = 0.0004880601482000202\n",
      "Epoch 176, Batch 15/32 : Loss = 0.0005523730651475489\n",
      "Epoch 176, Batch 16/32 : Loss = 0.0004371462855488062\n",
      "Epoch 176, Batch 17/32 : Loss = 0.000986992265097797\n",
      "Epoch 176, Batch 18/32 : Loss = 0.000454756838735193\n",
      "Epoch 176, Batch 19/32 : Loss = 0.00043496277066878974\n",
      "Epoch 176, Batch 20/32 : Loss = 0.0008007980650290847\n",
      "Epoch 176, Batch 21/32 : Loss = 0.0006338409148156643\n",
      "Epoch 176, Batch 22/32 : Loss = 0.0009176371386274695\n",
      "Epoch 176, Batch 23/32 : Loss = 0.0002897288359235972\n",
      "Epoch 176, Batch 24/32 : Loss = 0.0005793484160676599\n",
      "Epoch 176, Batch 25/32 : Loss = 0.0020066015422344208\n",
      "Epoch 176, Batch 26/32 : Loss = 0.00037644177791662514\n",
      "Epoch 176, Batch 27/32 : Loss = 0.00043916411232203245\n",
      "Epoch 176, Batch 28/32 : Loss = 0.0005363679374568164\n",
      "Epoch 176, Batch 29/32 : Loss = 0.00043725641444325447\n",
      "Epoch 176, Batch 30/32 : Loss = 0.0005228258669376373\n",
      "Epoch 176, Batch 31/32 : Loss = 0.00019191562023479491\n",
      "Epoch 176 finished in 0.052767964204152425 minutes\n",
      "Epoch 176 training_loss = 0.0009513474069535732\n",
      "----J--;;--q----++--/----z---y---U-------%----U-----1---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "----0-----c----++----bb----I--\"----bb----6----..--Q----------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "-----d----!---NN----r---A---jj-**---$$---33---hh----5----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "---0-----J----!--(--;;--AA-----3---,,-''--)---r---r---77------ => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 176 val_loss = 0.7769926190376282, word_accuracy = 0.7\n",
      "Epoch 177, Batch 0/32 : Loss = 0.01115383394062519\n",
      "Epoch 177, Batch 1/32 : Loss = 0.0007233080104924738\n",
      "Epoch 177, Batch 2/32 : Loss = 0.00028356999973766506\n",
      "Epoch 177, Batch 3/32 : Loss = 0.0003707525902427733\n",
      "Epoch 177, Batch 4/32 : Loss = 0.0006894987891428173\n",
      "Epoch 177, Batch 5/32 : Loss = 0.0004708458436653018\n",
      "Epoch 177, Batch 6/32 : Loss = 0.0004661782004404813\n",
      "Epoch 177, Batch 7/32 : Loss = 0.0007318183779716492\n",
      "Epoch 177, Batch 8/32 : Loss = 0.00045738599146716297\n",
      "Epoch 177, Batch 9/32 : Loss = 0.00042514654342085123\n",
      "Epoch 177, Batch 10/32 : Loss = 0.00026534151402302086\n",
      "Epoch 177, Batch 11/32 : Loss = 0.0003681192174553871\n",
      "Epoch 177, Batch 12/32 : Loss = 0.00047510687727481127\n",
      "Epoch 177, Batch 13/32 : Loss = 0.00022005828213877976\n",
      "Epoch 177, Batch 14/32 : Loss = 0.00029800369520671666\n",
      "Epoch 177, Batch 15/32 : Loss = 0.0001974178885575384\n",
      "Epoch 177, Batch 16/32 : Loss = 0.00045223854249343276\n",
      "Epoch 177, Batch 17/32 : Loss = 0.0001980375382117927\n",
      "Epoch 177, Batch 18/32 : Loss = 0.000768638274166733\n",
      "Epoch 177, Batch 19/32 : Loss = 0.0004754713736474514\n",
      "Epoch 177, Batch 20/32 : Loss = 0.00043256377102807164\n",
      "Epoch 177, Batch 21/32 : Loss = 0.00023643876193091273\n",
      "Epoch 177, Batch 22/32 : Loss = 0.00024247568217106164\n",
      "Epoch 177, Batch 23/32 : Loss = 0.0003358125686645508\n",
      "Epoch 177, Batch 24/32 : Loss = 0.00046282814582809806\n",
      "Epoch 177, Batch 25/32 : Loss = 0.0008243375923484564\n",
      "Epoch 177, Batch 26/32 : Loss = 0.000591609044931829\n",
      "Epoch 177, Batch 27/32 : Loss = 0.0026118101086467505\n",
      "Epoch 177, Batch 28/32 : Loss = 0.0003637933114077896\n",
      "Epoch 177, Batch 29/32 : Loss = 0.0008847775752656162\n",
      "Epoch 177, Batch 30/32 : Loss = 0.00028487120289355516\n",
      "Epoch 177, Batch 31/32 : Loss = 0.00038339250022545457\n",
      "Epoch 177 finished in 0.0538178284962972 minutes\n",
      "Epoch 177 training_loss = 0.0008613659883849323\n",
      "-----#------0----[[--xx---))---RR----88---ii--PP-----w-----))--- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "----5---->---:--**--YY----A---'--O-----DD---*---#----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "-----cc----RR---;;--9-----yy----?----2----d----i---O------{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----/-----MM----oo----EE---^----3----x---/---&&----6-----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "Epoch 177 val_loss = 0.8088663220405579, word_accuracy = 0.73\n",
      "Epoch 178, Batch 0/32 : Loss = 0.00030059978598728776\n",
      "Epoch 178, Batch 1/32 : Loss = 0.0028006918728351593\n",
      "Epoch 178, Batch 2/32 : Loss = 0.0011812528828158975\n",
      "Epoch 178, Batch 3/32 : Loss = 0.00029563644784502685\n",
      "Epoch 178, Batch 4/32 : Loss = 0.0007196769583970308\n",
      "Epoch 178, Batch 5/32 : Loss = 0.00029918659129180014\n",
      "Epoch 178, Batch 6/32 : Loss = 0.00023461597447749227\n",
      "Epoch 178, Batch 7/32 : Loss = 0.00028708172612823546\n",
      "Epoch 178, Batch 8/32 : Loss = 0.00026046897983178496\n",
      "Epoch 178, Batch 9/32 : Loss = 0.0011539331171661615\n",
      "Epoch 178, Batch 10/32 : Loss = 0.00029543048003688455\n",
      "Epoch 178, Batch 11/32 : Loss = 0.00027758575743064284\n",
      "Epoch 178, Batch 12/32 : Loss = 0.0006367297610267997\n",
      "Epoch 178, Batch 13/32 : Loss = 0.0002916571102105081\n",
      "Epoch 178, Batch 14/32 : Loss = 0.014600827358663082\n",
      "Epoch 178, Batch 15/32 : Loss = 0.015920810401439667\n",
      "Epoch 178, Batch 16/32 : Loss = 0.00026500283274799585\n",
      "Epoch 178, Batch 17/32 : Loss = 0.0003976435400545597\n",
      "Epoch 178, Batch 18/32 : Loss = 0.00026281349710188806\n",
      "Epoch 178, Batch 19/32 : Loss = 0.0002855744678527117\n",
      "Epoch 178, Batch 20/32 : Loss = 0.00026877515483647585\n",
      "Epoch 178, Batch 21/32 : Loss = 0.00019490299746394157\n",
      "Epoch 178, Batch 22/32 : Loss = 0.00024551557726226747\n",
      "Epoch 178, Batch 23/32 : Loss = 0.00048414626508019865\n",
      "Epoch 178, Batch 24/32 : Loss = 0.0004130201996304095\n",
      "Epoch 178, Batch 25/32 : Loss = 0.002552720485255122\n",
      "Epoch 178, Batch 26/32 : Loss = 0.0005803372478112578\n",
      "Epoch 178, Batch 27/32 : Loss = 0.0034379891585558653\n",
      "Epoch 178, Batch 28/32 : Loss = 0.0003616753965616226\n",
      "Epoch 178, Batch 29/32 : Loss = 0.0006703521939925849\n",
      "Epoch 178, Batch 30/32 : Loss = 0.0005846722051501274\n",
      "Epoch 178, Batch 31/32 : Loss = 0.0008179711294360459\n",
      "Epoch 178 finished in 0.054223489761352536 minutes\n",
      "Epoch 178 training_loss = 0.0016277451068162918\n",
      "----5---->---:--**--YY----A---'--O-----DD---*---#----O-----g---- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "----55----9----g-----m------JJ---uu---x---c----x--.--d-----\\---- => 59gmJuxcx.d\\, Ground Truth is 59gmJuxcx.d\\\n",
      "---\"---]]--tt--4-----e----^----WW-------Q------4---->>-----g---- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "----kk---$$---l--FF----DD----e----h----]---k---0----\\----X------ => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "Epoch 178 val_loss = 0.7988573908805847, word_accuracy = 0.74\n",
      "Epoch 179, Batch 0/32 : Loss = 0.0004977867938578129\n",
      "Epoch 179, Batch 1/32 : Loss = 0.000291402917355299\n",
      "Epoch 179, Batch 2/32 : Loss = 0.0004090262809768319\n",
      "Epoch 179, Batch 3/32 : Loss = 0.0002243068302050233\n",
      "Epoch 179, Batch 4/32 : Loss = 0.0020061905961483717\n",
      "Epoch 179, Batch 5/32 : Loss = 0.00023679094738326967\n",
      "Epoch 179, Batch 6/32 : Loss = 0.00047800588072277606\n",
      "Epoch 179, Batch 7/32 : Loss = 0.0006219560746103525\n",
      "Epoch 179, Batch 8/32 : Loss = 0.000504662748426199\n",
      "Epoch 179, Batch 9/32 : Loss = 0.0005004707491025329\n",
      "Epoch 179, Batch 10/32 : Loss = 0.0004905996611341834\n",
      "Epoch 179, Batch 11/32 : Loss = 0.00021498676505871117\n",
      "Epoch 179, Batch 12/32 : Loss = 0.00043677492067217827\n",
      "Epoch 179, Batch 13/32 : Loss = 0.00022811765666119754\n",
      "Epoch 179, Batch 14/32 : Loss = 0.00018610406550578773\n",
      "Epoch 179, Batch 15/32 : Loss = 0.00025949598057195544\n",
      "Epoch 179, Batch 16/32 : Loss = 0.00046996460878290236\n",
      "Epoch 179, Batch 17/32 : Loss = 0.000436842703493312\n",
      "Epoch 179, Batch 18/32 : Loss = 0.0006077844300307333\n",
      "Epoch 179, Batch 19/32 : Loss = 0.0014706586953252554\n",
      "Epoch 179, Batch 20/32 : Loss = 0.000746147008612752\n",
      "Epoch 179, Batch 21/32 : Loss = 0.0004010883276350796\n",
      "Epoch 179, Batch 22/32 : Loss = 0.0019720676355063915\n",
      "Epoch 179, Batch 23/32 : Loss = 0.00034884497290477157\n",
      "Epoch 179, Batch 24/32 : Loss = 0.00026755817816592753\n",
      "Epoch 179, Batch 25/32 : Loss = 0.00019930752750951797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Batch 26/32 : Loss = 0.0005110831116326153\n",
      "Epoch 179, Batch 27/32 : Loss = 0.0005319805932231247\n",
      "Epoch 179, Batch 28/32 : Loss = 0.00022716941020917147\n",
      "Epoch 179, Batch 29/32 : Loss = 0.00033388694282621145\n",
      "Epoch 179, Batch 30/32 : Loss = 0.0005616834387183189\n",
      "Epoch 179, Batch 31/32 : Loss = 0.0006340756663121283\n",
      "Epoch 179 finished in 0.053375311692555744 minutes\n",
      "Epoch 179 training_loss = 0.0005382170202210546\n",
      "----kk---OO------/--,,--y---cc---**----1----P----}---##-----B----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----/----MMM-----o-----E----^----3----x----/---&&----66----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "-----0---Q----66---<----<<---(---T----N---5---==---P---(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "-----d----!---NN-----r---A----j--*----$$----3----hh----5----n----- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 179 val_loss = 0.8058335781097412, word_accuracy = 0.75\n",
      "Epoch 180, Batch 0/32 : Loss = 0.0008193484973162413\n",
      "Epoch 180, Batch 1/32 : Loss = 0.0003456980630289763\n",
      "Epoch 180, Batch 2/32 : Loss = 0.0008286692900583148\n",
      "Epoch 180, Batch 3/32 : Loss = 0.0002991436922457069\n",
      "Epoch 180, Batch 4/32 : Loss = 0.0002525341114960611\n",
      "Epoch 180, Batch 5/32 : Loss = 0.0005665146745741367\n",
      "Epoch 180, Batch 6/32 : Loss = 0.00022430269746109843\n",
      "Epoch 180, Batch 7/32 : Loss = 0.0003498222795315087\n",
      "Epoch 180, Batch 8/32 : Loss = 0.00022296281531453133\n",
      "Epoch 180, Batch 9/32 : Loss = 0.00042673072312027216\n",
      "Epoch 180, Batch 10/32 : Loss = 0.0003426362236496061\n",
      "Epoch 180, Batch 11/32 : Loss = 0.011842699721455574\n",
      "Epoch 180, Batch 12/32 : Loss = 0.00022050359984859824\n",
      "Epoch 180, Batch 13/32 : Loss = 0.000916146906092763\n",
      "Epoch 180, Batch 14/32 : Loss = 0.0006408793851733208\n",
      "Epoch 180, Batch 15/32 : Loss = 0.0008139199344441295\n",
      "Epoch 180, Batch 16/32 : Loss = 0.0003954925632569939\n",
      "Epoch 180, Batch 17/32 : Loss = 0.002349601127207279\n",
      "Epoch 180, Batch 18/32 : Loss = 0.0009160995832644403\n",
      "Epoch 180, Batch 19/32 : Loss = 0.0015443842858076096\n",
      "Epoch 180, Batch 20/32 : Loss = 0.0006150160334073007\n",
      "Epoch 180, Batch 21/32 : Loss = 0.00026815716410055757\n",
      "Epoch 180, Batch 22/32 : Loss = 0.00026747098308987916\n",
      "Epoch 180, Batch 23/32 : Loss = 0.001182385254651308\n",
      "Epoch 180, Batch 24/32 : Loss = 0.0014756127493456006\n",
      "Epoch 180, Batch 25/32 : Loss = 0.0008795566391199827\n",
      "Epoch 180, Batch 26/32 : Loss = 0.006608444731682539\n",
      "Epoch 180, Batch 27/32 : Loss = 0.0012619945919141173\n",
      "Epoch 180, Batch 28/32 : Loss = 0.030681250616908073\n",
      "Epoch 180, Batch 29/32 : Loss = 0.005286194384098053\n",
      "Epoch 180, Batch 30/32 : Loss = 0.00033289578277617693\n",
      "Epoch 180, Batch 31/32 : Loss = 0.00021392441703937948\n",
      "Epoch 180 finished in 0.05223534107208252 minutes\n",
      "Epoch 180 training_loss = 0.002351929433643818\n",
      "----44----r---{------%%---/---'-))---w------&------N-----+----P------ => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----2---p----:-mm-----x---a----z--nn----@-----C----y------%-----%---- => 2p:mxazn@Cy%%, Ground Truth is 2p:mxazn@Cy%%\n",
      "-----0---Q-----6----<----<----(---T----NN---5---=----P---(--m-------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "-----k-----O------/--..--yy---c----**----1----PP---}}---#-----BB----- => kO/.yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "Epoch 180 val_loss = 0.8093655705451965, word_accuracy = 0.7\n",
      "Epoch 181, Batch 0/32 : Loss = 0.0003068455553147942\n",
      "Epoch 181, Batch 1/32 : Loss = 0.000995453679934144\n",
      "Epoch 181, Batch 2/32 : Loss = 0.00031416563433595\n",
      "Epoch 181, Batch 3/32 : Loss = 0.0003207065165042877\n",
      "Epoch 181, Batch 4/32 : Loss = 0.00029088740120641887\n",
      "Epoch 181, Batch 5/32 : Loss = 0.005595049355179071\n",
      "Epoch 181, Batch 6/32 : Loss = 0.0020212014205753803\n",
      "Epoch 181, Batch 7/32 : Loss = 0.00048564482131041586\n",
      "Epoch 181, Batch 8/32 : Loss = 0.00038330949610099196\n",
      "Epoch 181, Batch 9/32 : Loss = 0.0004334325494710356\n",
      "Epoch 181, Batch 10/32 : Loss = 0.0008537592366337776\n",
      "Epoch 181, Batch 11/32 : Loss = 0.0005076074157841504\n",
      "Epoch 181, Batch 12/32 : Loss = 0.00026407174300402403\n",
      "Epoch 181, Batch 13/32 : Loss = 0.003185170702636242\n",
      "Epoch 181, Batch 14/32 : Loss = 0.0002951092610601336\n",
      "Epoch 181, Batch 15/32 : Loss = 0.00034463597694411874\n",
      "Epoch 181, Batch 16/32 : Loss = 0.0025138065684586763\n",
      "Epoch 181, Batch 17/32 : Loss = 0.00023451060405932367\n",
      "Epoch 181, Batch 18/32 : Loss = 0.0004009258991573006\n",
      "Epoch 181, Batch 19/32 : Loss = 0.00031841802410781384\n",
      "Epoch 181, Batch 20/32 : Loss = 0.0003047098871320486\n",
      "Epoch 181, Batch 21/32 : Loss = 0.0006424561142921448\n",
      "Epoch 181, Batch 22/32 : Loss = 0.0002400278754066676\n",
      "Epoch 181, Batch 23/32 : Loss = 0.0003163969377055764\n",
      "Epoch 181, Batch 24/32 : Loss = 0.0006040700245648623\n",
      "Epoch 181, Batch 25/32 : Loss = 0.00022454331337939948\n",
      "Epoch 181, Batch 26/32 : Loss = 0.0013078968040645123\n",
      "Epoch 181, Batch 27/32 : Loss = 0.0004008508985862136\n",
      "Epoch 181, Batch 28/32 : Loss = 0.00034557419712655246\n",
      "Epoch 181, Batch 29/32 : Loss = 0.000330050679622218\n",
      "Epoch 181, Batch 30/32 : Loss = 0.0004262711445335299\n",
      "Epoch 181, Batch 31/32 : Loss = 0.000539262080565095\n",
      "Epoch 181 finished in 0.05212576389312744 minutes\n",
      "Epoch 181 training_loss = 0.0008120471029542387\n",
      "------#------0------[----x----))---RR------8-----i---P------ww------))---- => #0[x)R8iPw), Ground Truth is Err:509\n",
      "------0------c------++-----bb-----I--\"\"----bb-----66----..---Q------------ => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "-----C------D-----E----g----mm-----\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "-----55--->>----:--**---Y-----A----'--O------DD----*---##----O-----gg----- => 5>:*YA'OD*#Og, Ground Truth is 5>:*YA'OD*#Og\n",
      "Epoch 181 val_loss = 0.7897336483001709, word_accuracy = 0.69\n",
      "Epoch 182, Batch 0/32 : Loss = 0.00047356265713460743\n",
      "Epoch 182, Batch 1/32 : Loss = 0.0002536414540372789\n",
      "Epoch 182, Batch 2/32 : Loss = 0.00020261245663277805\n",
      "Epoch 182, Batch 3/32 : Loss = 0.0006252543535083532\n",
      "Epoch 182, Batch 4/32 : Loss = 0.00032833885052241385\n",
      "Epoch 182, Batch 5/32 : Loss = 0.00019915874872822315\n",
      "Epoch 182, Batch 6/32 : Loss = 0.00031558057526126504\n",
      "Epoch 182, Batch 7/32 : Loss = 0.00035976601066067815\n",
      "Epoch 182, Batch 8/32 : Loss = 0.0013388798106461763\n",
      "Epoch 182, Batch 9/32 : Loss = 0.00025968803674913943\n",
      "Epoch 182, Batch 10/32 : Loss = 0.0007305010803975165\n",
      "Epoch 182, Batch 11/32 : Loss = 0.0009563378989696503\n",
      "Epoch 182, Batch 12/32 : Loss = 0.0003821591380983591\n",
      "Epoch 182, Batch 13/32 : Loss = 0.0009187903488054872\n",
      "Epoch 182, Batch 14/32 : Loss = 0.0004646332818083465\n",
      "Epoch 182, Batch 15/32 : Loss = 0.0002607816713862121\n",
      "Epoch 182, Batch 16/32 : Loss = 0.0004981033271178603\n",
      "Epoch 182, Batch 17/32 : Loss = 0.0019210651516914368\n",
      "Epoch 182, Batch 18/32 : Loss = 0.0007111305603757501\n",
      "Epoch 182, Batch 19/32 : Loss = 0.0005848017171956599\n",
      "Epoch 182, Batch 20/32 : Loss = 0.00022895268921274692\n",
      "Epoch 182, Batch 21/32 : Loss = 0.00039302295772358775\n",
      "Epoch 182, Batch 22/32 : Loss = 0.0002731801650952548\n",
      "Epoch 182, Batch 23/32 : Loss = 0.0003661017108242959\n",
      "Epoch 182, Batch 24/32 : Loss = 0.00031475184368900955\n",
      "Epoch 182, Batch 25/32 : Loss = 0.0002574764075689018\n",
      "Epoch 182, Batch 26/32 : Loss = 0.00026511598844081163\n",
      "Epoch 182, Batch 27/32 : Loss = 0.00035328854573890567\n",
      "Epoch 182, Batch 28/32 : Loss = 0.00019855241407640278\n",
      "Epoch 182, Batch 29/32 : Loss = 0.00021825151634402573\n",
      "Epoch 182, Batch 30/32 : Loss = 0.0011524561559781432\n",
      "Epoch 182, Batch 31/32 : Loss = 0.00651897769421339\n",
      "Epoch 182 finished in 0.1839812159538269 minutes\n",
      "Epoch 182 training_loss = 0.0005340018542483449\n",
      "----0----c----+----bb---I-\"\"--bb----6---..-Q-------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----0--Q---66--<---<<--(--T---N---5--=---P--(-m----- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "---k---O-----/--,-yy--c---*---11---P---}--#----BB--- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "---c----R---;--9----y----?--22---d---ii--O----{--!-- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "Epoch 182 val_loss = 0.8095723390579224, word_accuracy = 0.74\n",
      "Epoch 183, Batch 0/32 : Loss = 0.0004822940391022712\n",
      "Epoch 183, Batch 1/32 : Loss = 0.0001616722729522735\n",
      "Epoch 183, Batch 2/32 : Loss = 0.00028087885584682226\n",
      "Epoch 183, Batch 3/32 : Loss = 0.0005819655489176512\n",
      "Epoch 183, Batch 4/32 : Loss = 0.0003907628997694701\n",
      "Epoch 183, Batch 5/32 : Loss = 0.0003330539620947093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, Batch 6/32 : Loss = 0.00021070113871246576\n",
      "Epoch 183, Batch 7/32 : Loss = 0.0005928511382080615\n",
      "Epoch 183, Batch 8/32 : Loss = 0.00022346689365804195\n",
      "Epoch 183, Batch 9/32 : Loss = 0.00016888888785615563\n",
      "Epoch 183, Batch 10/32 : Loss = 0.00029671992524527013\n",
      "Epoch 183, Batch 11/32 : Loss = 0.0002234206476714462\n",
      "Epoch 183, Batch 12/32 : Loss = 0.0004078174533788115\n",
      "Epoch 183, Batch 13/32 : Loss = 0.0003533453564159572\n",
      "Epoch 183, Batch 14/32 : Loss = 0.00038459745701402426\n",
      "Epoch 183, Batch 15/32 : Loss = 0.00036257179453969\n",
      "Epoch 183, Batch 16/32 : Loss = 0.0001893438893603161\n",
      "Epoch 183, Batch 17/32 : Loss = 0.00025383499450981617\n",
      "Epoch 183, Batch 18/32 : Loss = 0.00015636382158845663\n",
      "Epoch 183, Batch 19/32 : Loss = 0.0001728383358567953\n",
      "Epoch 183, Batch 20/32 : Loss = 0.03645177558064461\n",
      "Epoch 183, Batch 21/32 : Loss = 0.0007539151702076197\n",
      "Epoch 183, Batch 22/32 : Loss = 0.0002381213998887688\n",
      "Epoch 183, Batch 23/32 : Loss = 0.00021102032042108476\n",
      "Epoch 183, Batch 24/32 : Loss = 0.00021018277038820088\n",
      "Epoch 183, Batch 25/32 : Loss = 0.011591088026762009\n",
      "Epoch 183, Batch 26/32 : Loss = 0.0003151212877128273\n",
      "Epoch 183, Batch 27/32 : Loss = 0.0004016142338514328\n",
      "Epoch 183, Batch 28/32 : Loss = 0.0010453083086758852\n",
      "Epoch 183, Batch 29/32 : Loss = 0.00026209017960354686\n",
      "Epoch 183, Batch 30/32 : Loss = 0.0002751943829935044\n",
      "Epoch 183, Batch 31/32 : Loss = 0.0042925490997731686\n",
      "Epoch 183 finished in 0.0525574525197347 minutes\n",
      "Epoch 183 training_loss = 0.0018801410915330052\n",
      "--88-----K----ll--ZZ----5----p------$----a----}}---w-----. => 8KlZ5p$a}w., Ground Truth is 8KIZ5p$a}w,\n",
      "----4---rr--{-----%---//-'-))--w-----&-----NN---++---P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "---k---$$---l--F-----D----e---hh---]---k---0---\\\\---X----- => k$lFDeh]k0\\X, Ground Truth is k$IFDeh]k0\\X\n",
      "---#----G----9----E---I--=---h---55---#---22---J---)--k--- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 183 val_loss = 0.8386974334716797, word_accuracy = 0.72\n",
      "Epoch 184, Batch 0/32 : Loss = 0.0005106752505525947\n",
      "Epoch 184, Batch 1/32 : Loss = 0.00038074649637565017\n",
      "Epoch 184, Batch 2/32 : Loss = 0.0026909722946584225\n",
      "Epoch 184, Batch 3/32 : Loss = 0.00037661578971892595\n",
      "Epoch 184, Batch 4/32 : Loss = 0.02201833203434944\n",
      "Epoch 184, Batch 5/32 : Loss = 0.0003197711776010692\n",
      "Epoch 184, Batch 6/32 : Loss = 0.0006009256467223167\n",
      "Epoch 184, Batch 7/32 : Loss = 0.27617347240448\n",
      "Epoch 184, Batch 8/32 : Loss = 0.0006408918416127563\n",
      "Epoch 184, Batch 9/32 : Loss = 0.0005404605763033032\n",
      "Epoch 184, Batch 10/32 : Loss = 0.0004904799279756844\n",
      "Epoch 184, Batch 11/32 : Loss = 0.0005046903388574719\n",
      "Epoch 184, Batch 12/32 : Loss = 0.0018473193049430847\n",
      "Epoch 184, Batch 13/32 : Loss = 0.0002768339472822845\n",
      "Epoch 184, Batch 14/32 : Loss = 0.00040155224269255996\n",
      "Epoch 184, Batch 15/32 : Loss = 0.00037703622365370393\n",
      "Epoch 184, Batch 16/32 : Loss = 0.008011332713067532\n",
      "Epoch 184, Batch 17/32 : Loss = 0.0004236128879711032\n",
      "Epoch 184, Batch 18/32 : Loss = 0.0007850376423448324\n",
      "Epoch 184, Batch 19/32 : Loss = 0.0008556465618312359\n",
      "Epoch 184, Batch 20/32 : Loss = 0.00533885695040226\n",
      "Epoch 184, Batch 21/32 : Loss = 0.008339636027812958\n",
      "Epoch 184, Batch 22/32 : Loss = 0.0012248959392309189\n",
      "Epoch 184, Batch 23/32 : Loss = 0.015373442322015762\n",
      "Epoch 184, Batch 24/32 : Loss = 0.003219000529497862\n",
      "Epoch 184, Batch 25/32 : Loss = 0.000596000230871141\n",
      "Epoch 184, Batch 26/32 : Loss = 0.0002208280493505299\n",
      "Epoch 184, Batch 27/32 : Loss = 0.0009829229675233364\n",
      "Epoch 184, Batch 28/32 : Loss = 0.012274009175598621\n",
      "Epoch 184, Batch 29/32 : Loss = 0.000544448965229094\n",
      "Epoch 184, Batch 30/32 : Loss = 0.0006189164123497903\n",
      "Epoch 184, Batch 31/32 : Loss = 0.25646042823791504\n",
      "Epoch 184 finished in 0.051602669556935626 minutes\n",
      "Epoch 184 training_loss = 0.012819820083677769\n",
      "--BB---..-Y----I--W------6----FF---h----XX--'--Y----2---- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----QQ-----3---gg---I---z--#----YY---:-]---q----+---**-- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "---{--BB---YY---RR---a---yy--h---#----2--->>----E---4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----d----!---N----r---A---j--**--$$---3---hh----5---nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "Epoch 184 val_loss = 0.759181022644043, word_accuracy = 0.78\n",
      "Epoch 185, Batch 0/32 : Loss = 0.00023485877318307757\n",
      "Epoch 185, Batch 1/32 : Loss = 0.0006842258153483272\n",
      "Epoch 185, Batch 2/32 : Loss = 0.000790824240539223\n",
      "Epoch 185, Batch 3/32 : Loss = 0.0004805645439773798\n",
      "Epoch 185, Batch 4/32 : Loss = 0.0002585643669590354\n",
      "Epoch 185, Batch 5/32 : Loss = 0.00025636606733314693\n",
      "Epoch 185, Batch 6/32 : Loss = 0.0022417993750423193\n",
      "Epoch 185, Batch 7/32 : Loss = 0.0007188322488218546\n",
      "Epoch 185, Batch 8/32 : Loss = 0.0049612936563789845\n",
      "Epoch 185, Batch 9/32 : Loss = 0.00045748206321150064\n",
      "Epoch 185, Batch 10/32 : Loss = 0.0004105999250896275\n",
      "Epoch 185, Batch 11/32 : Loss = 0.0008223357726819813\n",
      "Epoch 185, Batch 12/32 : Loss = 0.0011309339897707105\n",
      "Epoch 185, Batch 13/32 : Loss = 0.00039225065847858787\n",
      "Epoch 185, Batch 14/32 : Loss = 0.004141548182815313\n",
      "Epoch 185, Batch 15/32 : Loss = 0.01982792653143406\n",
      "Epoch 185, Batch 16/32 : Loss = 0.0004882706853095442\n",
      "Epoch 185, Batch 17/32 : Loss = 0.00036085653118789196\n",
      "Epoch 185, Batch 18/32 : Loss = 0.018727483227849007\n",
      "Epoch 185, Batch 19/32 : Loss = 0.00032763549825176597\n",
      "Epoch 185, Batch 20/32 : Loss = 0.0010234711226075888\n",
      "Epoch 185, Batch 21/32 : Loss = 0.0005619332077912986\n",
      "Epoch 185, Batch 22/32 : Loss = 0.003476343583315611\n",
      "Epoch 185, Batch 23/32 : Loss = 0.0107781533151865\n",
      "Epoch 185, Batch 24/32 : Loss = 0.022709710523486137\n",
      "Epoch 185, Batch 25/32 : Loss = 0.0027840121183544397\n",
      "Epoch 185, Batch 26/32 : Loss = 0.00021150740212760866\n",
      "Epoch 185, Batch 27/32 : Loss = 0.0005035654176026583\n",
      "Epoch 185, Batch 28/32 : Loss = 0.0005697515443898737\n",
      "Epoch 185, Batch 29/32 : Loss = 0.004132556729018688\n",
      "Epoch 185, Batch 30/32 : Loss = 0.002402631565928459\n",
      "Epoch 185, Batch 31/32 : Loss = 0.003139893990010023\n",
      "Epoch 185 finished in 0.051698795954386395 minutes\n",
      "Epoch 185 training_loss = 0.003446129383519292\n",
      "----4----r--{------%---//-'--)--ww-----&-----N----++---P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----0---Q----6---<<---<---(---T---NN---5--==---P--(-m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "----X---77--0---j--@-----S---ZZ---L--4----C----mm------M---- => X70j@SZL4CmM, Ground Truth is X70j@SZL4CmM\n",
      "----##---G----99---E---I-=---hh---5---#---22---J--))--k----- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "Epoch 185 val_loss = 0.7529048919677734, word_accuracy = 0.76\n",
      "Epoch 186, Batch 0/32 : Loss = 0.00021316511265467852\n",
      "Epoch 186, Batch 1/32 : Loss = 0.08972819149494171\n",
      "Epoch 186, Batch 2/32 : Loss = 0.0004413441347423941\n",
      "Epoch 186, Batch 3/32 : Loss = 0.001187145127914846\n",
      "Epoch 186, Batch 4/32 : Loss = 0.013441670686006546\n",
      "Epoch 186, Batch 5/32 : Loss = 0.0003333711647428572\n",
      "Epoch 186, Batch 6/32 : Loss = 0.0003813974326476455\n",
      "Epoch 186, Batch 7/32 : Loss = 0.010335341095924377\n",
      "Epoch 186, Batch 8/32 : Loss = 0.0008950387709774077\n",
      "Epoch 186, Batch 9/32 : Loss = 0.0007235837401822209\n",
      "Epoch 186, Batch 10/32 : Loss = 0.00033596198773011565\n",
      "Epoch 186, Batch 11/32 : Loss = 0.00033375038765370846\n",
      "Epoch 186, Batch 12/32 : Loss = 0.0009889311622828245\n",
      "Epoch 186, Batch 13/32 : Loss = 0.0001800160389393568\n",
      "Epoch 186, Batch 14/32 : Loss = 0.008646465837955475\n",
      "Epoch 186, Batch 15/32 : Loss = 0.01145226415246725\n",
      "Epoch 186, Batch 16/32 : Loss = 0.0005435037892311811\n",
      "Epoch 186, Batch 17/32 : Loss = 0.0003373430809006095\n",
      "Epoch 186, Batch 18/32 : Loss = 0.00039484837907366455\n",
      "Epoch 186, Batch 19/32 : Loss = 0.00024754772312007844\n",
      "Epoch 186, Batch 20/32 : Loss = 0.0005305478116497397\n",
      "Epoch 186, Batch 21/32 : Loss = 0.00047196168452501297\n",
      "Epoch 186, Batch 22/32 : Loss = 0.00042175600538030267\n",
      "Epoch 186, Batch 23/32 : Loss = 0.0034898617304861546\n",
      "Epoch 186, Batch 24/32 : Loss = 0.0003284262493252754\n",
      "Epoch 186, Batch 25/32 : Loss = 0.0013136642519384623\n",
      "Epoch 186, Batch 26/32 : Loss = 0.0002810153237078339\n",
      "Epoch 186, Batch 27/32 : Loss = 0.009556299075484276\n",
      "Epoch 186, Batch 28/32 : Loss = 0.0017968686297535896\n",
      "Epoch 186, Batch 29/32 : Loss = 0.003117974614724517\n",
      "Epoch 186, Batch 30/32 : Loss = 0.06569407880306244\n",
      "Epoch 186, Batch 31/32 : Loss = 0.0034420285373926163\n",
      "Epoch 186 finished in 0.05506144762039185 minutes\n",
      "Epoch 186 training_loss = 0.007343729957938194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----E-----00----[---x----)---RR----8----i---P-----w------)-- => E-0[x)R8iPw), Ground Truth is Err:509\n",
      "---JJ--;--q----++---/---z---y---U-------%----U-----1---x---- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "---0-----J----!--(--;---A-----33--,,--'--)---r---r---7------ => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---7---n----DD----P----n---t--w-----d----\\--QQ----a----RR--- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "Epoch 186 val_loss = 0.7759115695953369, word_accuracy = 0.67\n",
      "Epoch 187, Batch 0/32 : Loss = 0.016662804409861565\n",
      "Epoch 187, Batch 1/32 : Loss = 0.0015322251711040735\n",
      "Epoch 187, Batch 2/32 : Loss = 0.012344330549240112\n",
      "Epoch 187, Batch 3/32 : Loss = 0.0025554527528584003\n",
      "Epoch 187, Batch 4/32 : Loss = 0.00032707356149330735\n",
      "Epoch 187, Batch 5/32 : Loss = 0.0045097218826413155\n",
      "Epoch 187, Batch 6/32 : Loss = 0.0003692147438414395\n",
      "Epoch 187, Batch 7/32 : Loss = 0.0007314280373975635\n",
      "Epoch 187, Batch 8/32 : Loss = 0.0006781365373171866\n",
      "Epoch 187, Batch 9/32 : Loss = 0.0002750004059635103\n",
      "Epoch 187, Batch 10/32 : Loss = 0.00192617392167449\n",
      "Epoch 187, Batch 11/32 : Loss = 0.0008356809848919511\n",
      "Epoch 187, Batch 12/32 : Loss = 0.0004710022476501763\n",
      "Epoch 187, Batch 13/32 : Loss = 0.0012176113668829203\n",
      "Epoch 187, Batch 14/32 : Loss = 0.0012547944206744432\n",
      "Epoch 187, Batch 15/32 : Loss = 0.0005182939348742366\n",
      "Epoch 187, Batch 16/32 : Loss = 0.0007203667773865163\n",
      "Epoch 187, Batch 17/32 : Loss = 0.000619619560893625\n",
      "Epoch 187, Batch 18/32 : Loss = 0.01020266767591238\n",
      "Epoch 187, Batch 19/32 : Loss = 0.0003332827764097601\n",
      "Epoch 187, Batch 20/32 : Loss = 0.0004606521688401699\n",
      "Epoch 187, Batch 21/32 : Loss = 0.000456964218756184\n",
      "Epoch 187, Batch 22/32 : Loss = 0.0026080210227519274\n",
      "Epoch 187, Batch 23/32 : Loss = 0.00043003231985494494\n",
      "Epoch 187, Batch 24/32 : Loss = 0.002894833218306303\n",
      "Epoch 187, Batch 25/32 : Loss = 0.0008284940849989653\n",
      "Epoch 187, Batch 26/32 : Loss = 0.00034973843139596283\n",
      "Epoch 187, Batch 27/32 : Loss = 0.0003902085591107607\n",
      "Epoch 187, Batch 28/32 : Loss = 0.0004413209389895201\n",
      "Epoch 187, Batch 29/32 : Loss = 0.00043874839320778847\n",
      "Epoch 187, Batch 30/32 : Loss = 0.004229272250086069\n",
      "Epoch 187, Batch 31/32 : Loss = 0.001222404302097857\n",
      "Epoch 187 finished in 0.05127476056416829 minutes\n",
      "Epoch 187 training_loss = 0.0023057335056364536\n",
      "-----+--::--z----7---8----dd----S-----v--55----S----JJ---BB----- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "--{---BB---YY----RR----a----y---h---#-----2---->----EE---4------ => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "----d----!!---NN----rr--AA----j--**---$----33---hh----55---nn--- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "-:-------3-----\\----$$---->>-----SS-----\\-----MM-----i---BB----- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 187 val_loss = 0.8238165974617004, word_accuracy = 0.69\n",
      "Epoch 188, Batch 0/32 : Loss = 0.0006682794773951173\n",
      "Epoch 188, Batch 1/32 : Loss = 0.00034383454476483166\n",
      "Epoch 188, Batch 2/32 : Loss = 0.0004425908555276692\n",
      "Epoch 188, Batch 3/32 : Loss = 0.000320873805321753\n",
      "Epoch 188, Batch 4/32 : Loss = 0.0007927837432362139\n",
      "Epoch 188, Batch 5/32 : Loss = 0.03965817391872406\n",
      "Epoch 188, Batch 6/32 : Loss = 0.001244580838829279\n",
      "Epoch 188, Batch 7/32 : Loss = 0.0002972194051835686\n",
      "Epoch 188, Batch 8/32 : Loss = 0.0004097796918358654\n",
      "Epoch 188, Batch 9/32 : Loss = 0.0006467109778895974\n",
      "Epoch 188, Batch 10/32 : Loss = 0.0016209724126383662\n",
      "Epoch 188, Batch 11/32 : Loss = 0.0003413850790821016\n",
      "Epoch 188, Batch 12/32 : Loss = 0.0011301429476588964\n",
      "Epoch 188, Batch 13/32 : Loss = 0.0010143909603357315\n",
      "Epoch 188, Batch 14/32 : Loss = 0.00039300762000493705\n",
      "Epoch 188, Batch 15/32 : Loss = 0.00034594425233080983\n",
      "Epoch 188, Batch 16/32 : Loss = 0.0003080272290389985\n",
      "Epoch 188, Batch 17/32 : Loss = 0.0008591696387156844\n",
      "Epoch 188, Batch 18/32 : Loss = 0.00047617254313081503\n",
      "Epoch 188, Batch 19/32 : Loss = 0.001326374476775527\n",
      "Epoch 188, Batch 20/32 : Loss = 0.013831553980708122\n",
      "Epoch 188, Batch 21/32 : Loss = 0.0003413106605876237\n",
      "Epoch 188, Batch 22/32 : Loss = 0.000660390593111515\n",
      "Epoch 188, Batch 23/32 : Loss = 0.0033295336179435253\n",
      "Epoch 188, Batch 24/32 : Loss = 0.0025953655131161213\n",
      "Epoch 188, Batch 25/32 : Loss = 0.001079879468306899\n",
      "Epoch 188, Batch 26/32 : Loss = 0.015060348436236382\n",
      "Epoch 188, Batch 27/32 : Loss = 0.00035493168979883194\n",
      "Epoch 188, Batch 28/32 : Loss = 0.00043207412818446755\n",
      "Epoch 188, Batch 29/32 : Loss = 0.0004110467853024602\n",
      "Epoch 188, Batch 30/32 : Loss = 0.0004295514081604779\n",
      "Epoch 188, Batch 31/32 : Loss = 0.007664219010621309\n",
      "Epoch 188 finished in 0.05160079002380371 minutes\n",
      "Epoch 188 training_loss = 0.0029598206747323275\n",
      "-----WW------=-----2----+----E----1----n----TT---XX---r---C----a----n----- => W=2+E1nTXrCan, Ground Truth is W=2+E1nTXrCan\n",
      "------0------JJ----!---(---;----A-------3----,---'--)----r----r-----7----- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "----o----\"\"---}---!---BB-----r----&-----9----;;-``---O-------w-----}------ => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----WW------=-----2---++----E----1----n-----T----X---r---C-----a---I------ => W=2+E1nTXrCaI, Ground Truth is W=2+E1nTXrCan\n",
      "Epoch 188 val_loss = 0.792395293712616, word_accuracy = 0.67\n",
      "Epoch 189, Batch 0/32 : Loss = 0.004232944920659065\n",
      "Epoch 189, Batch 1/32 : Loss = 0.0009631958673708141\n",
      "Epoch 189, Batch 2/32 : Loss = 0.0016755745746195316\n",
      "Epoch 189, Batch 3/32 : Loss = 0.0004025408998131752\n",
      "Epoch 189, Batch 4/32 : Loss = 0.00025488343089818954\n",
      "Epoch 189, Batch 5/32 : Loss = 0.00038033328019082546\n",
      "Epoch 189, Batch 6/32 : Loss = 0.0007290536304935813\n",
      "Epoch 189, Batch 7/32 : Loss = 0.0003584919613786042\n",
      "Epoch 189, Batch 8/32 : Loss = 0.00035612768260762095\n",
      "Epoch 189, Batch 9/32 : Loss = 0.00042771821608766913\n",
      "Epoch 189, Batch 10/32 : Loss = 0.0003524991334415972\n",
      "Epoch 189, Batch 11/32 : Loss = 0.0004285998293198645\n",
      "Epoch 189, Batch 12/32 : Loss = 0.0011149249039590359\n",
      "Epoch 189, Batch 13/32 : Loss = 0.0003322909469716251\n",
      "Epoch 189, Batch 14/32 : Loss = 0.0003816349199041724\n",
      "Epoch 189, Batch 15/32 : Loss = 0.0003109712270088494\n",
      "Epoch 189, Batch 16/32 : Loss = 0.00025473313871771097\n",
      "Epoch 189, Batch 17/32 : Loss = 0.013455808162689209\n",
      "Epoch 189, Batch 18/32 : Loss = 0.0003445563488639891\n",
      "Epoch 189, Batch 19/32 : Loss = 0.0004861016641370952\n",
      "Epoch 189, Batch 20/32 : Loss = 0.0022458562161773443\n",
      "Epoch 189, Batch 21/32 : Loss = 0.00023808577680028975\n",
      "Epoch 189, Batch 22/32 : Loss = 0.00040450849337503314\n",
      "Epoch 189, Batch 23/32 : Loss = 0.0003520235768519342\n",
      "Epoch 189, Batch 24/32 : Loss = 0.0002725171798374504\n",
      "Epoch 189, Batch 25/32 : Loss = 0.0007152450270950794\n",
      "Epoch 189, Batch 26/32 : Loss = 0.017528118565678596\n",
      "Epoch 189, Batch 27/32 : Loss = 0.00039529294008389115\n",
      "Epoch 189, Batch 28/32 : Loss = 0.0007578785298392177\n",
      "Epoch 189, Batch 29/32 : Loss = 0.0031096991151571274\n",
      "Epoch 189, Batch 30/32 : Loss = 0.0004727599734906107\n",
      "Epoch 189, Batch 31/32 : Loss = 0.0004193202476017177\n",
      "Epoch 189 finished in 0.05079406499862671 minutes\n",
      "Epoch 189 training_loss = 0.0017281087348237634\n",
      "----dd---::--XX---9-----e----a-----F--,--8--------V----RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----BB---.---Y---II-WW------66----F----h----XX---'--Y----22--- => B.YIW6FhX'Y2, Ground Truth is B.YIW6FhX'Y2\n",
      "-----c----RR---;;--9----yy---??---2----d----i---O-----{{-!!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---:------33----\\---$$---->>-----S----\\\\----MM----ii---BB----- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 189 val_loss = 0.830133855342865, word_accuracy = 0.68\n",
      "Epoch 190, Batch 0/32 : Loss = 0.0008966156747192144\n",
      "Epoch 190, Batch 1/32 : Loss = 0.0005327988765202463\n",
      "Epoch 190, Batch 2/32 : Loss = 0.0006315280916169286\n",
      "Epoch 190, Batch 3/32 : Loss = 0.003511983435600996\n",
      "Epoch 190, Batch 4/32 : Loss = 0.006419087294489145\n",
      "Epoch 190, Batch 5/32 : Loss = 0.0003183755907230079\n",
      "Epoch 190, Batch 6/32 : Loss = 0.0007385528297163546\n",
      "Epoch 190, Batch 7/32 : Loss = 0.0005162135348655283\n",
      "Epoch 190, Batch 8/32 : Loss = 0.0008097899844869971\n",
      "Epoch 190, Batch 9/32 : Loss = 0.0012371657649055123\n",
      "Epoch 190, Batch 10/32 : Loss = 0.00029594884836114943\n",
      "Epoch 190, Batch 11/32 : Loss = 0.0003482031752355397\n",
      "Epoch 190, Batch 12/32 : Loss = 0.001075995503924787\n",
      "Epoch 190, Batch 13/32 : Loss = 0.0007834761054255068\n",
      "Epoch 190, Batch 14/32 : Loss = 0.002148718573153019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Batch 15/32 : Loss = 0.004518902860581875\n",
      "Epoch 190, Batch 16/32 : Loss = 0.0004195378569420427\n",
      "Epoch 190, Batch 17/32 : Loss = 0.00017721098265610635\n",
      "Epoch 190, Batch 18/32 : Loss = 0.0006555357249453664\n",
      "Epoch 190, Batch 19/32 : Loss = 0.0003509953967295587\n",
      "Epoch 190, Batch 20/32 : Loss = 0.00017467368161305785\n",
      "Epoch 190, Batch 21/32 : Loss = 0.00019261080888099968\n",
      "Epoch 190, Batch 22/32 : Loss = 0.030117014423012733\n",
      "Epoch 190, Batch 23/32 : Loss = 0.0006931365933269262\n",
      "Epoch 190, Batch 24/32 : Loss = 0.0002352692245040089\n",
      "Epoch 190, Batch 25/32 : Loss = 0.00026854508905671537\n",
      "Epoch 190, Batch 26/32 : Loss = 0.001300775445997715\n",
      "Epoch 190, Batch 27/32 : Loss = 0.004301915410906076\n",
      "Epoch 190, Batch 28/32 : Loss = 0.0003382473369129002\n",
      "Epoch 190, Batch 29/32 : Loss = 0.00017427149577997625\n",
      "Epoch 190, Batch 30/32 : Loss = 0.0006648949347436428\n",
      "Epoch 190, Batch 31/32 : Loss = 0.25436651706695557\n",
      "Epoch 190 finished in 0.0508025328318278 minutes\n",
      "Epoch 190 training_loss = 0.003105021780356765\n",
      "---d---::--X----9----e----aa----F--,--8-------VV---RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----4---rr--{-----%---//--'))--w-----&-----NN---++---P---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "-----Y----W-----]]-i--\\--/----MM----<<----MM----8---88---- => YW]i\\/M<M88, Ground Truth is YW]i\\|M<M88\n",
      "-----0-----J---!--(--;---A-----33--,,-'--)---r---r---77--- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 190 val_loss = 0.8144667148590088, word_accuracy = 0.7\n",
      "Epoch 191, Batch 0/32 : Loss = 0.0002844827831722796\n",
      "Epoch 191, Batch 1/32 : Loss = 0.013764547184109688\n",
      "Epoch 191, Batch 2/32 : Loss = 0.0009951451793313026\n",
      "Epoch 191, Batch 3/32 : Loss = 0.0007062535732984543\n",
      "Epoch 191, Batch 4/32 : Loss = 0.00026066877762787044\n",
      "Epoch 191, Batch 5/32 : Loss = 0.0002452707849442959\n",
      "Epoch 191, Batch 6/32 : Loss = 0.000223255978198722\n",
      "Epoch 191, Batch 7/32 : Loss = 0.0006387153989635408\n",
      "Epoch 191, Batch 8/32 : Loss = 0.0006565502844750881\n",
      "Epoch 191, Batch 9/32 : Loss = 0.0007674855878576636\n",
      "Epoch 191, Batch 10/32 : Loss = 0.00027096987469121814\n",
      "Epoch 191, Batch 11/32 : Loss = 0.011675414629280567\n",
      "Epoch 191, Batch 12/32 : Loss = 0.00032898245262913406\n",
      "Epoch 191, Batch 13/32 : Loss = 0.0009637945331633091\n",
      "Epoch 191, Batch 14/32 : Loss = 0.00038817175664007664\n",
      "Epoch 191, Batch 15/32 : Loss = 0.020191190764307976\n",
      "Epoch 191, Batch 16/32 : Loss = 0.000371592934243381\n",
      "Epoch 191, Batch 17/32 : Loss = 0.0007652725325897336\n",
      "Epoch 191, Batch 18/32 : Loss = 0.0006825883174315095\n",
      "Epoch 191, Batch 19/32 : Loss = 0.0010182459373027086\n",
      "Epoch 191, Batch 20/32 : Loss = 0.0011034684721380472\n",
      "Epoch 191, Batch 21/32 : Loss = 0.0002428841544315219\n",
      "Epoch 191, Batch 22/32 : Loss = 0.0008370878058485687\n",
      "Epoch 191, Batch 23/32 : Loss = 0.0005182282766327262\n",
      "Epoch 191, Batch 24/32 : Loss = 0.0006351962802000344\n",
      "Epoch 191, Batch 25/32 : Loss = 0.0006691583548672497\n",
      "Epoch 191, Batch 26/32 : Loss = 0.00029245304176583886\n",
      "Epoch 191, Batch 27/32 : Loss = 0.00046671583550050855\n",
      "Epoch 191, Batch 28/32 : Loss = 0.0005016878712922335\n",
      "Epoch 191, Batch 29/32 : Loss = 0.000598287268076092\n",
      "Epoch 191, Batch 30/32 : Loss = 0.0006015895633026958\n",
      "Epoch 191, Batch 31/32 : Loss = 0.0072022113017737865\n",
      "Epoch 191 finished in 0.05541245142618815 minutes\n",
      "Epoch 191 training_loss = 0.002010141033679247\n",
      "-_--5---->---:--*---Y----A---'--O-----D----*--##---O0----g----- => _5>:*YA'OD*#O0g, Ground Truth is 5>:*YA'OD*#Og\n",
      "---\"---]---t---4----ee---^^---WW-------Q------4---->>----g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "---kk---OO-----/--,,--y---cc---**---11---PP----}--##----BB----- => kO/,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----J--;;--q----++---/---zz--y---UU-------%----U-----1---x----- => J;q+/zyU%U1x, Ground Truth is J;q+/zyU%U1x_\n",
      "Epoch 191 val_loss = 0.8546584844589233, word_accuracy = 0.68\n",
      "Epoch 192, Batch 0/32 : Loss = 0.0018144809873774648\n",
      "Epoch 192, Batch 1/32 : Loss = 0.000554392347112298\n",
      "Epoch 192, Batch 2/32 : Loss = 0.0005045246798545122\n",
      "Epoch 192, Batch 3/32 : Loss = 0.001105213537812233\n",
      "Epoch 192, Batch 4/32 : Loss = 0.0003179843770340085\n",
      "Epoch 192, Batch 5/32 : Loss = 0.0023205899633467197\n",
      "Epoch 192, Batch 6/32 : Loss = 0.0003106617368757725\n",
      "Epoch 192, Batch 7/32 : Loss = 0.00031509646214544773\n",
      "Epoch 192, Batch 8/32 : Loss = 0.0004207453166600317\n",
      "Epoch 192, Batch 9/32 : Loss = 0.00022376504784915596\n",
      "Epoch 192, Batch 10/32 : Loss = 0.0012964251218363643\n",
      "Epoch 192, Batch 11/32 : Loss = 0.0063743251375854015\n",
      "Epoch 192, Batch 12/32 : Loss = 0.0012804775033146143\n",
      "Epoch 192, Batch 13/32 : Loss = 0.00020971754565835\n",
      "Epoch 192, Batch 14/32 : Loss = 0.0024326108396053314\n",
      "Epoch 192, Batch 15/32 : Loss = 0.0008192081586457789\n",
      "Epoch 192, Batch 16/32 : Loss = 0.0012571001425385475\n",
      "Epoch 192, Batch 17/32 : Loss = 0.00021989729430060834\n",
      "Epoch 192, Batch 18/32 : Loss = 0.006567493546754122\n",
      "Epoch 192, Batch 19/32 : Loss = 0.0007868914981372654\n",
      "Epoch 192, Batch 20/32 : Loss = 0.0009672668529674411\n",
      "Epoch 192, Batch 21/32 : Loss = 0.0014143483713269234\n",
      "Epoch 192, Batch 22/32 : Loss = 0.0009923644829541445\n",
      "Epoch 192, Batch 23/32 : Loss = 0.0008991360664367676\n",
      "Epoch 192, Batch 24/32 : Loss = 0.0003347204183228314\n",
      "Epoch 192, Batch 25/32 : Loss = 0.0004003573558293283\n",
      "Epoch 192, Batch 26/32 : Loss = 0.008203542791306973\n",
      "Epoch 192, Batch 27/32 : Loss = 0.001160747604444623\n",
      "Epoch 192, Batch 28/32 : Loss = 0.000687453371938318\n",
      "Epoch 192, Batch 29/32 : Loss = 0.002750113606452942\n",
      "Epoch 192, Batch 30/32 : Loss = 0.004710968118160963\n",
      "Epoch 192, Batch 31/32 : Loss = 0.0007916660979390144\n",
      "Epoch 192 finished in 0.05284435749053955 minutes\n",
      "Epoch 192 training_loss = 0.0016627012519165874\n",
      "----0----c----++----bb---II-\"----b-----6---..--Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "----d----!---N----r---A---j--**--$$---33---h----5----n---- => d!NrAj*$3h5n, Ground Truth is d!NrAj*$3h5n\n",
      "----4---rr--{-----%---/--'-))--w-----&-----N----+---PP---- => 4r{%/')w&N+P, Ground Truth is 4r{%/')w&N+P\n",
      "----QQ-----3----g----I--z---#-----Y---:--]---q----++---**- => Q3gIz#Y:]q+*, Ground Truth is Q3glz#Y:]q+*\n",
      "Epoch 192 val_loss = 0.8342018723487854, word_accuracy = 0.72\n",
      "Epoch 193, Batch 0/32 : Loss = 0.0003102526825387031\n",
      "Epoch 193, Batch 1/32 : Loss = 0.0003277189680375159\n",
      "Epoch 193, Batch 2/32 : Loss = 0.00019835273269563913\n",
      "Epoch 193, Batch 3/32 : Loss = 0.0009302878170274198\n",
      "Epoch 193, Batch 4/32 : Loss = 0.0002574450627434999\n",
      "Epoch 193, Batch 5/32 : Loss = 0.0007644294528290629\n",
      "Epoch 193, Batch 6/32 : Loss = 0.0003032347885891795\n",
      "Epoch 193, Batch 7/32 : Loss = 0.0002663361665327102\n",
      "Epoch 193, Batch 8/32 : Loss = 0.0004333803371991962\n",
      "Epoch 193, Batch 9/32 : Loss = 0.00032602768624201417\n",
      "Epoch 193, Batch 10/32 : Loss = 0.0009255236363969743\n",
      "Epoch 193, Batch 11/32 : Loss = 0.00022066158999223262\n",
      "Epoch 193, Batch 12/32 : Loss = 0.0003376115928404033\n",
      "Epoch 193, Batch 13/32 : Loss = 0.0019558919593691826\n",
      "Epoch 193, Batch 14/32 : Loss = 0.006669680122286081\n",
      "Epoch 193, Batch 15/32 : Loss = 0.0005708562675863504\n",
      "Epoch 193, Batch 16/32 : Loss = 0.0005730619304813445\n",
      "Epoch 193, Batch 17/32 : Loss = 0.0002287674869876355\n",
      "Epoch 193, Batch 18/32 : Loss = 0.0008899886743165553\n",
      "Epoch 193, Batch 19/32 : Loss = 0.0007432204438373446\n",
      "Epoch 193, Batch 20/32 : Loss = 0.011983809061348438\n",
      "Epoch 193, Batch 21/32 : Loss = 0.003401170950382948\n",
      "Epoch 193, Batch 22/32 : Loss = 0.0003446358605287969\n",
      "Epoch 193, Batch 23/32 : Loss = 0.0002913770731538534\n",
      "Epoch 193, Batch 24/32 : Loss = 0.0006815899978391826\n",
      "Epoch 193, Batch 25/32 : Loss = 0.00037004181649535894\n",
      "Epoch 193, Batch 26/32 : Loss = 0.0007588745793327689\n",
      "Epoch 193, Batch 27/32 : Loss = 0.0006181703647598624\n",
      "Epoch 193, Batch 28/32 : Loss = 0.0007718538399785757\n",
      "Epoch 193, Batch 29/32 : Loss = 0.0002814176259562373\n",
      "Epoch 193, Batch 30/32 : Loss = 0.005187170580029488\n",
      "Epoch 193, Batch 31/32 : Loss = 0.0009749636519700289\n",
      "Epoch 193 finished in 0.053599341710408525 minutes\n",
      "Epoch 193 training_loss = 0.0013508339179679751\n",
      "---7---nn----DD----PP----n----t--w-----d-----\\---Q-----a-----R---- => 7nDPntwd\\QaR, Ground Truth is 7nDPntwd\\QaR\n",
      "---kk---$----|---'-,--9----Y-----NN----W------mm-------T---8------ => k$|',9YNWmT8, Ground Truth is k$|',9YNWmT8\n",
      "-----0-----cc-----++----bb----I--\"----bb-----66---..---Q---------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "-----0---Q----66---<----<<---(---T---NN---5---==---P---(--m------- => 0Q6<<(TN5=P(m, Ground Truth is 0Q6<<(TN5=P(m\n",
      "Epoch 193 val_loss = 0.8189225792884827, word_accuracy = 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, Batch 0/32 : Loss = 0.0007660623523406684\n",
      "Epoch 194, Batch 1/32 : Loss = 0.00036902871215716004\n",
      "Epoch 194, Batch 2/32 : Loss = 0.00040545265073888004\n",
      "Epoch 194, Batch 3/32 : Loss = 0.000284641602775082\n",
      "Epoch 194, Batch 4/32 : Loss = 0.0036837151274085045\n",
      "Epoch 194, Batch 5/32 : Loss = 0.0009622670477256179\n",
      "Epoch 194, Batch 6/32 : Loss = 0.00019137037452310324\n",
      "Epoch 194, Batch 7/32 : Loss = 0.0001694269012659788\n",
      "Epoch 194, Batch 8/32 : Loss = 0.0011640394804999232\n",
      "Epoch 194, Batch 9/32 : Loss = 0.0006228981073945761\n",
      "Epoch 194, Batch 10/32 : Loss = 0.0002715873997658491\n",
      "Epoch 194, Batch 11/32 : Loss = 0.00042630391544662416\n",
      "Epoch 194, Batch 12/32 : Loss = 0.00022116115724202245\n",
      "Epoch 194, Batch 13/32 : Loss = 0.0002954297815449536\n",
      "Epoch 194, Batch 14/32 : Loss = 0.00036073673982173204\n",
      "Epoch 194, Batch 15/32 : Loss = 0.0003707194118760526\n",
      "Epoch 194, Batch 16/32 : Loss = 0.00024115503765642643\n",
      "Epoch 194, Batch 17/32 : Loss = 0.00034064322244375944\n",
      "Epoch 194, Batch 18/32 : Loss = 0.0002913427888415754\n",
      "Epoch 194, Batch 19/32 : Loss = 0.00024180079344660044\n",
      "Epoch 194, Batch 20/32 : Loss = 0.0008972359355539083\n",
      "Epoch 194, Batch 21/32 : Loss = 0.0004988432046957314\n",
      "Epoch 194, Batch 22/32 : Loss = 0.0003497882280498743\n",
      "Epoch 194, Batch 23/32 : Loss = 0.0004137512296438217\n",
      "Epoch 194, Batch 24/32 : Loss = 0.000886825262568891\n",
      "Epoch 194, Batch 25/32 : Loss = 0.0016130786389112473\n",
      "Epoch 194, Batch 26/32 : Loss = 0.00028024151106365025\n",
      "Epoch 194, Batch 27/32 : Loss = 0.018008172512054443\n",
      "Epoch 194, Batch 28/32 : Loss = 0.0009604606893844903\n",
      "Epoch 194, Batch 29/32 : Loss = 0.0004254523664712906\n",
      "Epoch 194, Batch 30/32 : Loss = 0.00039253273280337453\n",
      "Epoch 194, Batch 31/32 : Loss = 0.00017019908409565687\n",
      "Epoch 194 finished in 0.05278064807256063 minutes\n",
      "Epoch 194 training_loss = 0.0011703595519065857\n",
      "----kk----O-----Y---,--y---c----**---1----P----}--##----BB---- => kOY,yc*1P}#B, Ground Truth is kO/,yc*1P}#B\n",
      "----dd---::--X----9----ee----a-----F--,--8--------V----RR----- => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "-----8-----KK----l--ZZ-----5----p-----$----aa----}---w-----,-- => 8KlZ5p$a}w,, Ground Truth is 8KIZ5p$a}w,\n",
      "---0-----c-----++----bb----I--\"---bb-----6----.---Q----------- => 0c+bI\"b6.Q-, Ground Truth is 0c+bl\"b6.Q-\n",
      "Epoch 194 val_loss = 0.8240057229995728, word_accuracy = 0.7\n",
      "Epoch 195, Batch 0/32 : Loss = 0.00023436237825080752\n",
      "Epoch 195, Batch 1/32 : Loss = 0.00044896765029989183\n",
      "Epoch 195, Batch 2/32 : Loss = 0.00017387481057085097\n",
      "Epoch 195, Batch 3/32 : Loss = 0.0006059079896658659\n",
      "Epoch 195, Batch 4/32 : Loss = 0.000425737292971462\n",
      "Epoch 195, Batch 5/32 : Loss = 0.002033595461398363\n",
      "Epoch 195, Batch 6/32 : Loss = 0.00047591462498530746\n",
      "Epoch 195, Batch 7/32 : Loss = 0.00019077342585660517\n",
      "Epoch 195, Batch 8/32 : Loss = 0.0017770956037566066\n",
      "Epoch 195, Batch 9/32 : Loss = 0.000505004427395761\n",
      "Epoch 195, Batch 10/32 : Loss = 0.00037671407335437834\n",
      "Epoch 195, Batch 11/32 : Loss = 0.0009523972403258085\n",
      "Epoch 195, Batch 12/32 : Loss = 0.0006413134979084134\n",
      "Epoch 195, Batch 13/32 : Loss = 0.0005327710532583296\n",
      "Epoch 195, Batch 14/32 : Loss = 0.0007390214595943689\n",
      "Epoch 195, Batch 15/32 : Loss = 0.00026146718300879\n",
      "Epoch 195, Batch 16/32 : Loss = 0.00029717653524130583\n",
      "Epoch 195, Batch 17/32 : Loss = 0.00034732496715150774\n",
      "Epoch 195, Batch 18/32 : Loss = 0.0009580048499628901\n",
      "Epoch 195, Batch 19/32 : Loss = 0.00030078805866651237\n",
      "Epoch 195, Batch 20/32 : Loss = 0.00041613378562033176\n",
      "Epoch 195, Batch 21/32 : Loss = 0.0004641120904125273\n",
      "Epoch 195, Batch 22/32 : Loss = 0.0001805631909519434\n",
      "Epoch 195, Batch 23/32 : Loss = 0.00021941078011877835\n",
      "Epoch 195, Batch 24/32 : Loss = 0.00079130451194942\n",
      "Epoch 195, Batch 25/32 : Loss = 0.0005009606247767806\n",
      "Epoch 195, Batch 26/32 : Loss = 0.0004080411745235324\n",
      "Epoch 195, Batch 27/32 : Loss = 0.00023276098363567144\n",
      "Epoch 195, Batch 28/32 : Loss = 0.00047974317567422986\n",
      "Epoch 195, Batch 29/32 : Loss = 0.0005304181249812245\n",
      "Epoch 195, Batch 30/32 : Loss = 0.00047476624604314566\n",
      "Epoch 195, Batch 31/32 : Loss = 0.0004901211941614747\n",
      "Epoch 195 finished in 0.053459258874257405 minutes\n",
      "Epoch 195 training_loss = 0.0005473956698551774\n",
      "-----c-----R----;--9-----y----?----2----d---ii---O-----{{--!---- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "---{---BB---YY----RR---aa----y---h---#-----2--->>----EE---4----- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "---C-----D----E----g----m-----\"--mm-----F---<<----Q----8---2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "---o----\"---}--!!--BB----r---&-----9---;--`--OO-----w-----}----- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 195 val_loss = 0.8559066653251648, word_accuracy = 0.74\n",
      "Epoch 196, Batch 0/32 : Loss = 0.0003324851568322629\n",
      "Epoch 196, Batch 1/32 : Loss = 0.00042808603029698133\n",
      "Epoch 196, Batch 2/32 : Loss = 0.00040165099198929965\n",
      "Epoch 196, Batch 3/32 : Loss = 0.0006304119015112519\n",
      "Epoch 196, Batch 4/32 : Loss = 0.00018559632007963955\n",
      "Epoch 196, Batch 5/32 : Loss = 0.0033391942270100117\n",
      "Epoch 196, Batch 6/32 : Loss = 0.00039460876723751426\n",
      "Epoch 196, Batch 7/32 : Loss = 0.0003831179637927562\n",
      "Epoch 196, Batch 8/32 : Loss = 0.0004002053174190223\n",
      "Epoch 196, Batch 9/32 : Loss = 0.0006312348414212465\n",
      "Epoch 196, Batch 10/32 : Loss = 0.00033323955722153187\n",
      "Epoch 196, Batch 11/32 : Loss = 0.0006595332524739206\n",
      "Epoch 196, Batch 12/32 : Loss = 0.00046361034037545323\n",
      "Epoch 196, Batch 13/32 : Loss = 0.00015458837151527405\n",
      "Epoch 196, Batch 14/32 : Loss = 0.00016566576960030943\n",
      "Epoch 196, Batch 15/32 : Loss = 0.0005116803222335875\n",
      "Epoch 196, Batch 16/32 : Loss = 0.00018772974726743996\n",
      "Epoch 196, Batch 17/32 : Loss = 0.00023203366436064243\n",
      "Epoch 196, Batch 18/32 : Loss = 0.0002916835364885628\n",
      "Epoch 196, Batch 19/32 : Loss = 0.00022610524320043623\n",
      "Epoch 196, Batch 20/32 : Loss = 0.00012394340592436492\n",
      "Epoch 196, Batch 21/32 : Loss = 0.0008264753269031644\n",
      "Epoch 196, Batch 22/32 : Loss = 0.0007451052661053836\n",
      "Epoch 196, Batch 23/32 : Loss = 0.00017873177421279252\n",
      "Epoch 196, Batch 24/32 : Loss = 0.00045089624472893775\n",
      "Epoch 196, Batch 25/32 : Loss = 0.008362452499568462\n",
      "Epoch 196, Batch 26/32 : Loss = 0.0004570716409943998\n",
      "Epoch 196, Batch 27/32 : Loss = 0.0003114039427600801\n",
      "Epoch 196, Batch 28/32 : Loss = 0.001131981029175222\n",
      "Epoch 196, Batch 29/32 : Loss = 0.00023818263434804976\n",
      "Epoch 196, Batch 30/32 : Loss = 0.00016225682338699698\n",
      "Epoch 196, Batch 31/32 : Loss = 0.015086736530065536\n",
      "Epoch 196 finished in 0.05173289775848389 minutes\n",
      "Epoch 196 training_loss = 0.0008104996522888541\n",
      "-----C-----DD----EE----g----mm-----\"---mm------F----<-----Q-----8----2---- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----C------D-----E----g----mm-----\"\"--mm------F----<-----Q-----8----2----- => CDEgm\"mF<Q82, Ground Truth is CDEgm\"mF<Q82\n",
      "----\"-----]---t---4-----ee----^^----WW--------Q-------4------>-----gg----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "-----++---:---z-----7---88----dd-----S-----vv---5-----S-----J-----B------- => +:z78dSv5SJB, Ground Truth is +:z78dSv5SJB\n",
      "Epoch 196 val_loss = 0.8758079409599304, word_accuracy = 0.71\n",
      "Epoch 197, Batch 0/32 : Loss = 0.0002472494961693883\n",
      "Epoch 197, Batch 1/32 : Loss = 0.00038270215736702085\n",
      "Epoch 197, Batch 2/32 : Loss = 0.00030188800883479416\n",
      "Epoch 197, Batch 3/32 : Loss = 0.0002166546182706952\n",
      "Epoch 197, Batch 4/32 : Loss = 0.0003831247449852526\n",
      "Epoch 197, Batch 5/32 : Loss = 0.002829544711858034\n",
      "Epoch 197, Batch 6/32 : Loss = 0.00021811906481161714\n",
      "Epoch 197, Batch 7/32 : Loss = 0.0002048407623078674\n",
      "Epoch 197, Batch 8/32 : Loss = 0.0001493408635724336\n",
      "Epoch 197, Batch 9/32 : Loss = 0.0004007092793472111\n",
      "Epoch 197, Batch 10/32 : Loss = 0.0001604606513865292\n",
      "Epoch 197, Batch 11/32 : Loss = 0.0005835627671331167\n",
      "Epoch 197, Batch 12/32 : Loss = 0.00021681505313608795\n",
      "Epoch 197, Batch 13/32 : Loss = 0.0001629209436941892\n",
      "Epoch 197, Batch 14/32 : Loss = 0.0003712884499691427\n",
      "Epoch 197, Batch 15/32 : Loss = 0.00020644289907068014\n",
      "Epoch 197, Batch 16/32 : Loss = 0.00046781188575550914\n",
      "Epoch 197, Batch 17/32 : Loss = 0.00014479670790024102\n",
      "Epoch 197, Batch 18/32 : Loss = 0.0007571065798401833\n",
      "Epoch 197, Batch 19/32 : Loss = 0.00031005722121335566\n",
      "Epoch 197, Batch 20/32 : Loss = 0.0009331551846116781\n",
      "Epoch 197, Batch 21/32 : Loss = 0.00015796149091329426\n",
      "Epoch 197, Batch 22/32 : Loss = 0.000322872307151556\n",
      "Epoch 197, Batch 23/32 : Loss = 0.0003665161784738302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, Batch 24/32 : Loss = 0.00014534805086441338\n",
      "Epoch 197, Batch 25/32 : Loss = 0.0007714082021266222\n",
      "Epoch 197, Batch 26/32 : Loss = 0.0007900800555944443\n",
      "Epoch 197, Batch 27/32 : Loss = 0.0001483802916482091\n",
      "Epoch 197, Batch 28/32 : Loss = 0.0006772043998353183\n",
      "Epoch 197, Batch 29/32 : Loss = 0.00029165527666918933\n",
      "Epoch 197, Batch 30/32 : Loss = 0.0005848376313224435\n",
      "Epoch 197, Batch 31/32 : Loss = 0.00023684208281338215\n",
      "Epoch 197 finished in 0.05203169584274292 minutes\n",
      "Epoch 197 training_loss = 0.00044769348460249603\n",
      "---d----:---X----9----e-----a----F--,,-88-------VV----R------ => d:X9eaF,8-VR, Ground Truth is d:X9eaF,8-VR\n",
      "----/----MM-----o----EE---^---3----x---/---&----66----X------ => /MoE^3x/&6X, Ground Truth is /MoE^3x/&6X\n",
      "---#-----G----9----E---I--=---hh---5---#----2----J---)--k---- => #G9EI=h5#2J)k, Ground Truth is #G9EI=h5#2J)k\n",
      "-:------33----\\\\---$$---->------S-----\\-----MM----ii---B----- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "Epoch 197 val_loss = 0.8271462917327881, word_accuracy = 0.73\n",
      "Epoch 198, Batch 0/32 : Loss = 0.00026815966702997684\n",
      "Epoch 198, Batch 1/32 : Loss = 0.005632778163999319\n",
      "Epoch 198, Batch 2/32 : Loss = 0.0001610612089280039\n",
      "Epoch 198, Batch 3/32 : Loss = 0.00010650729382177815\n",
      "Epoch 198, Batch 4/32 : Loss = 0.0003856639959849417\n",
      "Epoch 198, Batch 5/32 : Loss = 0.0001615391520317644\n",
      "Epoch 198, Batch 6/32 : Loss = 0.000515418709255755\n",
      "Epoch 198, Batch 7/32 : Loss = 0.00022012143745087087\n",
      "Epoch 198, Batch 8/32 : Loss = 0.004654416814446449\n",
      "Epoch 198, Batch 9/32 : Loss = 0.0002470285980962217\n",
      "Epoch 198, Batch 10/32 : Loss = 0.0002281652414239943\n",
      "Epoch 198, Batch 11/32 : Loss = 0.0005433931364677846\n",
      "Epoch 198, Batch 12/32 : Loss = 0.00040269334567710757\n",
      "Epoch 198, Batch 13/32 : Loss = 0.00020264938939362764\n",
      "Epoch 198, Batch 14/32 : Loss = 0.0014982488937675953\n",
      "Epoch 198, Batch 15/32 : Loss = 0.0004098617937415838\n",
      "Epoch 198, Batch 16/32 : Loss = 0.00035280935117043555\n",
      "Epoch 198, Batch 17/32 : Loss = 0.00022822999744676054\n",
      "Epoch 198, Batch 18/32 : Loss = 0.00032377231400460005\n",
      "Epoch 198, Batch 19/32 : Loss = 0.0009831165662035346\n",
      "Epoch 198, Batch 20/32 : Loss = 0.00020436976046767086\n",
      "Epoch 198, Batch 21/32 : Loss = 0.00030980532756075263\n",
      "Epoch 198, Batch 22/32 : Loss = 0.00043287963490001857\n",
      "Epoch 198, Batch 23/32 : Loss = 0.0001333971304120496\n",
      "Epoch 198, Batch 24/32 : Loss = 0.00020379552734084427\n",
      "Epoch 198, Batch 25/32 : Loss = 0.0002021732070716098\n",
      "Epoch 198, Batch 26/32 : Loss = 0.00043878023279830813\n",
      "Epoch 198, Batch 27/32 : Loss = 0.00021258174092508852\n",
      "Epoch 198, Batch 28/32 : Loss = 0.000278747669653967\n",
      "Epoch 198, Batch 29/32 : Loss = 0.00014101772103458643\n",
      "Epoch 198, Batch 30/32 : Loss = 0.00014926213771104813\n",
      "Epoch 198, Batch 31/32 : Loss = 0.0002446791622787714\n",
      "Epoch 198 finished in 0.05255589485168457 minutes\n",
      "Epoch 198 training_loss = 0.0006510209641419351\n",
      "----0----JJ---!-((--;--AA----3---,--'-))--r---r---77--- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "---o---\"--}}-!!-BB---rr--&----9---;--`--O----w----}}--- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "----\"--]--t--4----e---^---WW------Q----4---->>---g----- => \"]t4e^WQ4>g, Ground Truth is \"]t4e^WQ4>g\n",
      "-----0----J---!--(--;---A-----3--,,-'--)---r--r----7--- => 0J!(;A3,')rr7, Ground Truth is 0J!(;A3,')rr7\n",
      "Epoch 198 val_loss = 0.8428234457969666, word_accuracy = 0.73\n",
      "Epoch 199, Batch 0/32 : Loss = 0.0003231760347262025\n",
      "Epoch 199, Batch 1/32 : Loss = 0.00012168109242338687\n",
      "Epoch 199, Batch 2/32 : Loss = 0.00047080367221497\n",
      "Epoch 199, Batch 3/32 : Loss = 0.00025185843696817756\n",
      "Epoch 199, Batch 4/32 : Loss = 0.0007126241689547896\n",
      "Epoch 199, Batch 5/32 : Loss = 0.00026529619935899973\n",
      "Epoch 199, Batch 6/32 : Loss = 0.00017887949070427567\n",
      "Epoch 199, Batch 7/32 : Loss = 0.0005477259401232004\n",
      "Epoch 199, Batch 8/32 : Loss = 0.00020086733275093138\n",
      "Epoch 199, Batch 9/32 : Loss = 0.0001766808272805065\n",
      "Epoch 199, Batch 10/32 : Loss = 0.00023274023260455579\n",
      "Epoch 199, Batch 11/32 : Loss = 0.00012725421402137727\n",
      "Epoch 199, Batch 12/32 : Loss = 0.00014392068260349333\n",
      "Epoch 199, Batch 13/32 : Loss = 0.0017094018403440714\n",
      "Epoch 199, Batch 14/32 : Loss = 0.00046679977094754577\n",
      "Epoch 199, Batch 15/32 : Loss = 0.00017486882279627025\n",
      "Epoch 199, Batch 16/32 : Loss = 0.0003253476752433926\n",
      "Epoch 199, Batch 17/32 : Loss = 0.00042396076605655253\n",
      "Epoch 199, Batch 18/32 : Loss = 0.00031357118859887123\n",
      "Epoch 199, Batch 19/32 : Loss = 0.000258184561971575\n",
      "Epoch 199, Batch 20/32 : Loss = 0.0001267516490770504\n",
      "Epoch 199, Batch 21/32 : Loss = 0.0034685852006077766\n",
      "Epoch 199, Batch 22/32 : Loss = 0.00012110464740544558\n",
      "Epoch 199, Batch 23/32 : Loss = 0.0002098398981615901\n",
      "Epoch 199, Batch 24/32 : Loss = 0.0006320282118394971\n",
      "Epoch 199, Batch 25/32 : Loss = 0.000262196350377053\n",
      "Epoch 199, Batch 26/32 : Loss = 0.0003009875654242933\n",
      "Epoch 199, Batch 27/32 : Loss = 0.0006222836091183126\n",
      "Epoch 199, Batch 28/32 : Loss = 0.00016534666065126657\n",
      "Epoch 199, Batch 29/32 : Loss = 0.0002918079844675958\n",
      "Epoch 199, Batch 30/32 : Loss = 0.0001938685600180179\n",
      "Epoch 199, Batch 31/32 : Loss = 0.12795530259609222\n",
      "Epoch 199 finished in 0.05305513143539429 minutes\n",
      "Epoch 199 training_loss = 0.0009579070610925555\n",
      "---{--BB---Y----RR---a---yy--h--##----2--->----E---4---- => {BYRayh#2>E4, Ground Truth is {BYRayh#2>E4\n",
      "-:------3----\\---$$---->-----SS----\\\\---MM----ii--BB---- => :-3\\$>S\\MiB, Ground Truth is :-3\\$>S\\MiB\n",
      "----cc---RR---;--9----y----?---2---d---ii--O-----{--!--- => cR;9y?2diO{!, Ground Truth is cR;9y?2diO{!\n",
      "----o---\"---}--!--B----r---&----9---;-`--O-----ww----}-- => o\"}!Br&9;`Ow}, Ground Truth is o\"}!Br&9;`Ow}\n",
      "Epoch 199 val_loss = 0.83514004945755, word_accuracy = 0.72\n",
      "the best accurcay is 0.82\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(epochs_num):\n",
    "    \n",
    "    tick = time.time()\n",
    "    train_loss = train(crnn, criterion, optimizer, logger, \n",
    "                       train_dataloder, batch_size, epoch)\n",
    "    tock = time.time()\n",
    "    \n",
    "    print(f'Epoch {epoch} finished in {(tock - tick) / 60} minutes')\n",
    "    print(f'Epoch {epoch} training_loss = {train_loss}')\n",
    "    \n",
    "    if epoch % val_each == 0:\n",
    "        val_loss, val_accurcy = val(crnn, criterion, logger, val_dataloder,\n",
    "                                    epoch, batch_size)\n",
    "        print(f'Epoch {epoch} val_loss = {val_loss}, word_accuracy = {val_accurcy}')\n",
    "            \n",
    "        # save best checkpoint\n",
    "        if best_acc <= val_accurcy:\n",
    "            best_acc = val_accurcy\n",
    "            checkpoint = {'input_hight':32,\n",
    "                          'output_size':len(alphapet)+1,\n",
    "                          'alphapet':alphapet,\n",
    "                          'train_transforms':train_transforms,\n",
    "                          'optim_dic':optimizer.state_dict(),\n",
    "                          'state_dic':crnn.state_dict(),\n",
    "                          'epoch':epoch\n",
    "                         }\n",
    "            torch.save(checkpoint,'checkpoints/best_checkpoint.pth')\n",
    "    \n",
    "    # save last epoch\n",
    "    checkpoint = {'input_hight':32,\n",
    "                  'output_size':len(alphapet)+1,\n",
    "                  'alphapet':alphapet,\n",
    "                  'train_transforms':train_transforms,\n",
    "                  'optim_dic':optimizer.state_dict(),\n",
    "                  'state_dic':crnn.state_dict(),\n",
    "                  'epoch':epoch\n",
    "                 }\n",
    "    torch.save(checkpoint,'checkpoints/last_checkpoint.pth')\n",
    "    \n",
    "    lr_sheduler.step()\n",
    "    \n",
    "print(f'the best accurcay is {best_acc}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "def F1_score_comp(model, criterion, val_dataloder, epoch_num, batch_size=16):\n",
    "    model.eval()\n",
    "    \n",
    "    nCorrect_words = 0\n",
    "    #nCorrect_chars = 0\n",
    "    val_loss = 0\n",
    "    samples_num = 0\n",
    "    y_pred = []\n",
    "    y_targets = []\n",
    "    worng_samples = []\n",
    "    worng_preds = []\n",
    "    \n",
    "    for batch_i, (img_path, imgs, targets) in enumerate(val_dataloder):\n",
    "        samples_num += imgs.shape[0]\n",
    "        \n",
    "        # move to device and create variables\n",
    "        imgs = Variable(imgs.to(device), requires_grad=False)\n",
    "        targets_encoded, lenghts = converter.encode(targets)\n",
    "        targets_encoded = Variable(targets_encoded.to(device), requires_grad=False)\n",
    "        t_lens = Variable(lenghts, requires_grad=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # pass to the network\n",
    "            preds = model(imgs)\n",
    "            preds_size = Variable(torch.IntTensor([preds.shape[0]] * imgs.shape[0]))\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(preds, targets_encoded.cpu(), preds_size, t_lens)\n",
    "            val_loss += loss * imgs.shape[0]\n",
    "            \n",
    "            # get the nework prediction\n",
    "            _, preds = preds.max(2)\n",
    "            preds = preds.transpose(1,0).contiguous().view(-1)\n",
    "            words_preds, lables_preds = converter.decode(preds.cpu(), preds_size)\n",
    "            \n",
    "            for i, (word_pred, target) in enumerate(zip(words_preds, targets)):\n",
    "                if word_pred == target:\n",
    "                    nCorrect_words += 1\n",
    "                else:\n",
    "                    worng_samples.append(img_path[i])\n",
    "                    worng_preds.append(word_pred)\n",
    "                    \n",
    "                # char level evalutaion\n",
    "                target_lable = [converter.dict[c] for c in target]\n",
    "                pred_lable = [converter.dict[c] for c in word_pred]\n",
    "                if len(target_lable) > len(pred_lable):\n",
    "                    pred_lable.extend([len(alphapet)]*abs(len(target_lable) - len(pred_lable)))\n",
    "                elif len(target_lable) < len(pred_lable):\n",
    "                    target_lable.extend([len(alphapet)]*abs(len(target_lable) - len(pred_lable)))\n",
    "                assert len(pred_lable) == len(target_lable), f'not matched{len(target_lable)}, {len(pred_lable)}'\n",
    "                y_pred.extend(pred_lable)\n",
    "                y_targets.extend(target_lable)\n",
    "                for c_p, c_t in zip(word_pred, target):\n",
    "                    if(c_t == ''and c_p=='m'):\n",
    "                        print(img_path[i])\n",
    "                \n",
    "    \n",
    "    # display some of the network prediction\n",
    "    row_preds, _ = converter.decode(preds, preds_size, raw=True)[:test_display]\n",
    "\n",
    "    for row_pred, word_pred, gt in zip(row_preds, words_preds, targets):\n",
    "        print(f'{row_pred} => {word_pred}, Ground Truth is {gt}')\n",
    "    \n",
    "    #compute loss and accurcy\n",
    "    word_accurcy = nCorrect_words / samples_num\n",
    "    val_loss /= samples_num\n",
    "    \n",
    "    char_acc = 0\n",
    "    for c_p, c_t in zip(y_pred, y_targets):\n",
    "        if(c_p == c_t): char_acc += 1\n",
    "    char_acc /= len(y_pred)\n",
    "    \n",
    "    prec = metrics.precision_score(y_targets, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_targets, y_pred, average='macro')\n",
    "    f1_score = metrics.f1_score(y_targets, y_pred, average='macro')\n",
    "    \n",
    "    return val_loss, word_accurcy, char_acc, prec, recall, f1_score, y_pred, y_targets, worng_samples, worng_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, word_accurcy, char_acc, prec, recall, f1_score, y_pred, y_targets, wrong_smaples, wrong_pred = F1_score_comp(crnn, criterion, val_dataloder,\n",
    "                                    epoch, batch_size)\n",
    "\n",
    "print(f'loss: {val_loss}, word_acc: {word_accurcy}, char_acc: {char_acc}, \\\n",
    "prec: {prec}, recall: {recall}, f1_score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot confusion matrix\n",
    "confusion_matrix = np.zeros((len(alphapet)+1, len(alphapet)+1))\n",
    "for c_p, c_t in zip(y_pred, y_targets):\n",
    "    confusion_matrix[c_t, c_p] += 1\n",
    "    \n",
    "df = pd.DataFrame(confusion_matrix, index=[c for c in (alphapet+'-')], columns=[c for c in (alphapet+'-')])\n",
    "plt.figure(figsize=(30,30))\n",
    "sn.set(font_scale=1)\n",
    "sn.heatmap(df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display wrong samples\\\n",
    "for path, pred in zip(wrong_smaples, wrong_pred):\n",
    "    print(pred)\n",
    "    plt.imshow(np.array(Image.open(path).convert('L')), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix[converter.dict[''], converter.dict['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
